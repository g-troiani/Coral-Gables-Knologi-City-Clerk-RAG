# Concatenated Project Code - Part 3 of 3
# Generated: 2025-06-12 12:11:13
# Root Directory: /Users/gianmariatroiani/Documents/knologi/graph_database
================================================================================

# File Index - Which Files Are in Which Parts
################################################################################

## Part 1 (10 files):
  - scripts/microsoft_framework/query_engine.py
  - scripts/graph_stages/agenda_ontology_extractor.py
  - graphrag_query_ui.py
  - scripts/microsoft_framework/query_router.py
  - scripts/graph_stages/document_linker.py
  - scripts/microsoft_framework/README.md
  - scripts/microsoft_framework/cosmos_synchronizer.py
  - extract_documents_for_graphrag.py
  - scripts/microsoft_framework/city_clerk_settings_template.yaml
  - config.py

## Part 2 (11 files):
  - scripts/graph_stages/agenda_graph_builder.py
  - scripts/graph_stages/agenda_pdf_extractor.py
  - scripts/graph_stages/enhanced_document_linker.py
  - scripts/microsoft_framework/run_graphrag_pipeline.py
  - scripts/graph_stages/cosmos_db_client.py
  - scripts/microsoft_framework/graphrag_output_processor.py
  - scripts/microsoft_framework/graphrag_initializer.py
  - settings.yaml
  - scripts/microsoft_framework/graphrag_pipeline.py
  - scripts/microsoft_framework/__init__.py
  - scripts/graph_stages/__init__.py

## Part 3 (10 files):
  - scripts/microsoft_framework/enhanced_entity_deduplicator.py
  - scripts/graph_stages/ontology_extractor.py
  - scripts/microsoft_framework/document_adapter.py
  - scripts/graph_stages/verbatim_transcript_linker.py
  - scripts/extract_all_to_markdown.py
  - scripts/microsoft_framework/prompt_tuner.py
  - scripts/graph_stages/pdf_extractor.py
  - scripts/microsoft_framework/source_tracker.py
  - scripts/microsoft_framework/incremental_processor.py
  - requirements.txt


================================================================================


################################################################################
# File: scripts/microsoft_framework/enhanced_entity_deduplicator.py
################################################################################

# File: scripts/microsoft_framework/enhanced_entity_deduplicator.py

#!/usr/bin/env python3
"""
Enhanced Entity Deduplication for GraphRAG output.

This module provides sophisticated entity deduplication capabilities including:
- Partial name matching ("Vince Lago" matches "Lago")
- Token-based overlap
- Semantic similarity using TF-IDF
- Graph structure analysis
- Abbreviation matching ("V. Lago" matches "Vince Lago")
- Role-based matching ("Mayor" matches "Mayor Lago")
- Multiple scoring strategies with configurable weights
"""

import pandas as pd
import networkx as nx
from pathlib import Path
import difflib
from typing import Dict, List, Tuple, Set, Any, Optional, Union
import logging
import json
import re
from datetime import datetime
from collections import defaultdict
import math
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
import multiprocessing as mp

# Optional dependencies with fallbacks
try:
    import Levenshtein
    HAS_LEVENSHTEIN = True
except ImportError:
    HAS_LEVENSHTEIN = False
    
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    import numpy as np
    HAS_SKLEARN = True
except ImportError:
    HAS_SKLEARN = False

logger = logging.getLogger(__name__)

def _compare_entity_pairs_worker(args):
    """Worker function for parallel entity pair comparison."""
    entity_pairs, config, weights = args
    
    candidates = []
    
    for entity1, entity2 in entity_pairs:
        # Calculate similarity scores
        scores = _calculate_similarity_scores_standalone(entity1, entity2, config)
        
        # Calculate combined score
        combined_score = _calculate_combined_score_standalone(scores, weights)
        
        # Check threshold
        if combined_score >= config.get('min_combined_score', 0.7):
            # Validate candidate
            scores['combined_score'] = combined_score
            if _validate_merge_candidate_standalone(entity1, entity2, scores):
                # Determine merge reason
                merge_reason = _determine_merge_reason_standalone(scores)
                
                candidates.append({
                    'entity1_title': entity1['title'],
                    'entity2_title': entity2['title'],
                    'entity1_id': entity1.get('id', entity1.get('human_readable_id', '')),
                    'entity2_id': entity2.get('id', entity2.get('human_readable_id', '')),
                    'combined_score': combined_score,
                    'individual_scores': scores,
                    'merge_reason': merge_reason,
                    'primary_entity': _determine_primary_entity_standalone(entity1, entity2)
                })
    
    return candidates

def _calculate_similarity_scores_standalone(entity1: Dict, entity2: Dict, config: Dict) -> Dict[str, float]:
    """Standalone version of similarity calculation for parallel processing."""
    scores = {}
    
    title1 = entity1['title'].lower().strip()
    title2 = entity2['title'].lower().strip()
    
    # String similarity
    scores['string_similarity'] = _string_similarity_standalone(title1, title2)
    
    # Token overlap
    if config.get('enable_token_matching', True):
        scores['token_overlap'] = _token_overlap_similarity_standalone(title1, title2)
    else:
        scores['token_overlap'] = 0.0
    
    # Partial name matching
    if config.get('enable_partial_name_matching', True):
        scores['partial_name_match'] = _partial_name_similarity_standalone(title1, title2)
    else:
        scores['partial_name_match'] = 0.0
    
    # Abbreviation matching
    if config.get('enable_abbreviation_matching', True):
        scores['abbreviation_match'] = _abbreviation_similarity_standalone(title1, title2)
    else:
        scores['abbreviation_match'] = 0.0
    
    # Role-based matching
    if config.get('enable_role_based_matching', True):
        scores['role_match'] = _role_based_similarity_standalone(title1, title2)
    else:
        scores['role_match'] = 0.0
    
    # Graph structure similarity
    if config.get('enable_graph_structure_matching', True):
        scores['graph_structure'] = _graph_structure_similarity_standalone(entity1, entity2, config)
    else:
        scores['graph_structure'] = 0.0
    
    # Semantic similarity (simplified for parallel processing)
    scores['semantic_similarity'] = 0.0  # Skip for parallel version to avoid pickling issues
    
    return scores

def _string_similarity_standalone(str1: str, str2: str) -> float:
    """Standalone string similarity calculation."""
    if HAS_LEVENSHTEIN:
        lev_sim = 1 - (Levenshtein.distance(str1, str2) / max(len(str1), len(str2), 1))
    else:
        lev_sim = 0.0
    seq_sim = difflib.SequenceMatcher(None, str1, str2).ratio()
    return max(lev_sim, seq_sim)

def _token_overlap_similarity_standalone(str1: str, str2: str) -> float:
    """Standalone token overlap similarity calculation."""
    tokens1 = set(re.findall(r'\b\w+\b', str1.lower()))
    tokens2 = set(re.findall(r'\b\w+\b', str2.lower()))
    
    # Remove stop words
    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
    tokens1 = tokens1 - stop_words
    tokens2 = tokens2 - stop_words
    
    if not tokens1 or not tokens2:
        return 0.0
    
    intersection = tokens1.intersection(tokens2)
    union = tokens1.union(tokens2)
    
    if not union:
        return 0.0
    
    jaccard = len(intersection) / len(union)
    
    # Boost for subset matches
    if tokens1.issubset(tokens2) or tokens2.issubset(tokens1):
        min_tokens = min(len(tokens1), len(tokens2))
        if min_tokens >= 2 or (min_tokens == 1 and jaccard >= 0.5):
            return min(1.0, jaccard + 0.2)
    
    return jaccard

def _partial_name_similarity_standalone(str1: str, str2: str) -> float:
    """Standalone partial name similarity calculation."""
    tokens1 = re.findall(r'\b\w+\b', str1.lower())
    tokens2 = re.findall(r'\b\w+\b', str2.lower())
    
    if len(tokens1) <= len(tokens2):
        shorter, longer = tokens1, tokens2
    else:
        shorter, longer = tokens2, tokens1
    
    if not shorter:
        return 0.0
    
    matches = sum(1 for token in shorter if token in longer)
    return matches / len(shorter)

def _abbreviation_similarity_standalone(str1: str, str2: str) -> float:
    """Standalone abbreviation similarity calculation."""
    def get_initials(text):
        words = re.findall(r'\b\w+\b', text)
        return ''.join(word[0].lower() for word in words if word)
    
    def has_abbreviation_pattern(short, long):
        short_clean = re.sub(r'[^\w\s]', '', short).strip()
        long_clean = re.sub(r'[^\w\s]', '', long).strip()
        
        initials = get_initials(long_clean)
        if short_clean.replace('.', '').replace(' ', '').lower() == initials:
            return 1.0
        
        short_parts = short_clean.split()
        long_parts = long_clean.split()
        
        if len(short_parts) == len(long_parts):
            matches = 0
            for s_part, l_part in zip(short_parts, long_parts):
                s_clean = s_part.replace('.', '').lower()
                l_clean = l_part.lower()
                
                if s_clean == l_clean or (len(s_clean) == 1 and l_clean.startswith(s_clean)):
                    matches += 1
            
            return matches / len(short_parts)
        
        return 0.0
    
    score1 = has_abbreviation_pattern(str1, str2)
    score2 = has_abbreviation_pattern(str2, str1)
    return max(score1, score2)

def _role_based_similarity_standalone(str1: str, str2: str) -> float:
    """Standalone role-based similarity calculation."""
    roles = ['mayor', 'commissioner', 'councilman', 'councilwoman', 'director', 'manager', 'chief']
    
    def extract_role_and_name(text):
        text_lower = text.lower()
        for role in roles:
            if role in text_lower:
                name_part = text_lower.replace(role, '').strip()
                return role, name_part
        return None, text_lower
    
    role1, name1 = extract_role_and_name(str1)
    role2, name2 = extract_role_and_name(str2)
    
    if role1 and not role2:
        return _string_similarity_standalone(name1, str2)
    elif role2 and not role1:
        return _string_similarity_standalone(name2, str1)
    elif role1 and role2:
        if role1 == role2:
            return _string_similarity_standalone(name1, name2)
        else:
            return 0.0
    
    return 0.0

def _graph_structure_similarity_standalone(entity1: Dict, entity2: Dict, config: Dict) -> float:
    """Standalone graph structure similarity calculation."""
    neighbors1 = entity1.get('neighbors', set())
    neighbors2 = entity2.get('neighbors', set())
    
    if not neighbors1 and not neighbors2:
        return 1.0
    elif not neighbors1 or not neighbors2:
        return 0.0
    
    intersection = len(neighbors1.intersection(neighbors2))
    union = len(neighbors1.union(neighbors2))
    
    jaccard_sim = intersection / union if union > 0 else 0.0
    
    coeff1 = entity1.get('clustering_coeff', 0.0)
    coeff2 = entity2.get('clustering_coeff', 0.0)
    coeff_diff = abs(coeff1 - coeff2)
    coeff_sim = 1.0 - min(coeff_diff / config.get('clustering_tolerance', 0.15), 1.0)
    
    return (jaccard_sim + coeff_sim) / 2.0

def _calculate_combined_score_standalone(scores: Dict[str, float], weights: Dict[str, float]) -> float:
    """Standalone combined score calculation."""
    if scores.get('token_overlap', 0) > 0.7 and scores.get('string_similarity', 0) < 0.8:
        if scores.get('graph_structure', 0) < 0.6:
            return 0.0
    
    combined = 0.0
    total_weight = 0.0
    
    for score_type, weight in weights.items():
        if score_type in scores:
            combined += scores[score_type] * weight
            total_weight += weight
    
    bonus_scores = ['partial_name_match', 'abbreviation_match', 'role_match']
    max_bonus = 0.0
    for bonus_type in bonus_scores:
        if bonus_type in scores:
            max_bonus = max(max_bonus, scores[bonus_type])
    
    bonus_factor = min(max_bonus * 0.2, 0.2)
    
    if total_weight > 0:
        final_score = (combined / total_weight) + bonus_factor
        return min(final_score, 1.0)
    
    return 0.0

def _validate_merge_candidate_standalone(entity1: Dict, entity2: Dict, scores: Dict[str, float]) -> bool:
    """Standalone validation for merge candidates."""
    if scores.get('token_overlap', 0) > scores.get('string_similarity', 0):
        has_strong_evidence = (
            scores.get('graph_structure', 0) > 0.7 or
            scores.get('semantic_similarity', 0) > 0.8 or
            scores.get('abbreviation_match', 0) > 0.8 or
            scores.get('role_match', 0) > 0.7
        )
        
        if not has_strong_evidence:
            return False
    
    if entity1.get('description') and entity2.get('description'):
        desc1_len = len(str(entity1['description']))
        desc2_len = len(str(entity2['description']))
        
        if max(desc1_len, desc2_len) > 3 * min(desc1_len, desc2_len):
            return scores.get('combined_score', 0) > 0.9
    
    return True

def _determine_merge_reason_standalone(scores: Dict[str, float]) -> str:
    """Standalone merge reason determination."""
    main_scores = {
        'string_similarity': 'High string similarity',
        'token_overlap': 'Token overlap',
        'graph_structure': 'Similar graph structure',
        'semantic_similarity': 'Semantic similarity'
    }
    
    special_scores = {
        'partial_name_match': 'Partial name match',
        'abbreviation_match': 'Abbreviation pattern',
        'role_match': 'Role-based match'
    }
    
    for score_type, reason in special_scores.items():
        if scores.get(score_type, 0.0) > 0.7:
            return reason
    
    max_score = 0.0
    max_reason = "Combined similarity"
    
    for score_type, reason in main_scores.items():
        if scores.get(score_type, 0.0) > max_score:
            max_score = scores[score_type]
            max_reason = reason
    
    return max_reason

def _determine_primary_entity_standalone(entity1: Dict, entity2: Dict) -> str:
    """Standalone primary entity determination."""
    degree1 = entity1.get('degree_centrality', 0)
    degree2 = entity2.get('degree_centrality', 0)
    
    if degree1 > degree2:
        return entity1['title']
    elif degree2 > degree1:
        return entity2['title']
    
    if len(entity1['title']) > len(entity2['title']):
        return entity1['title']
    else:
        return entity2['title']

# Configuration presets
DEDUP_CONFIGS = {
    'aggressive': {
        'min_combined_score': 0.75,  # INCREASED from 0.65
        'enable_partial_name_matching': True,
        'enable_token_matching': True,
        'enable_semantic_matching': True,
        'enable_graph_structure_matching': True,
        'enable_abbreviation_matching': True,
        'enable_role_based_matching': True,
        'exact_match_threshold': 0.85,
        'high_similarity_threshold': 0.8,
        'partial_match_threshold': 0.6,
        'clustering_tolerance': 0.2,
        'weights': {
            'string_similarity': 0.2,
            'token_overlap': 0.2,
            'graph_structure': 0.4,  # Emphasize network evidence
            'semantic_similarity': 0.2
        }
    },
    'conservative': {
        'min_combined_score': 0.85,  # Already strict
        'enable_partial_name_matching': True,
        'enable_token_matching': True,
        'enable_semantic_matching': True,
        'enable_graph_structure_matching': True,
        'enable_abbreviation_matching': True,
        'enable_role_based_matching': False,
        'exact_match_threshold': 0.95,
        'high_similarity_threshold': 0.9,
        'partial_match_threshold': 0.8,
        'clustering_tolerance': 0.1,
        'weights': {
            'string_similarity': 0.4,
            'token_overlap': 0.1,    # Reduced to avoid false positives
            'graph_structure': 0.4,
            'semantic_similarity': 0.1
        }
    },
    'name_focused': {
        'min_combined_score': 0.8,   # INCREASED from 0.7
        'enable_partial_name_matching': True,
        'enable_token_matching': True,
        'enable_semantic_matching': True,
        'enable_graph_structure_matching': True,
        'enable_abbreviation_matching': True,
        'enable_role_based_matching': True,
        'exact_match_threshold': 0.95,
        'high_similarity_threshold': 0.85,
        'partial_match_threshold': 0.7,
        'clustering_tolerance': 0.15,
        'weights': {
            'string_similarity': 0.2,
            'token_overlap': 0.3,     # REDUCED from 0.4
            'graph_structure': 0.3,   # INCREASED from 0.2
            'semantic_similarity': 0.2
        }
    }
}


class EnhancedEntityDeduplicator:
    """
    Enhanced entity deduplication using multiple similarity metrics and advanced matching strategies.
    """
    
    def __init__(self, output_dir: Path, config: Dict[str, Any] = None):
        """
        Initialize the enhanced entity deduplicator.
        
        Args:
            output_dir: Path to GraphRAG output directory
            config: Configuration dictionary with matching strategies and thresholds
        """
        self.output_dir = Path(output_dir)
        self.config = config or DEDUP_CONFIGS['name_focused']
        
        # Initialize data containers
        self.entities_df = None
        self.relationships_df = None
        self.graph = None
        self.tfidf_vectorizer = None
        self.tfidf_matrix = None
        
        # Merge tracking
        self.merge_report = {
            'timestamp': datetime.now().isoformat(),
            'config': self.config.copy(),
            'merges': [],
            'statistics': {}
        }
        
        logger.info(f"Initialized enhanced deduplicator for {output_dir}")
        logger.info(f"Configuration: {self.config.get('min_combined_score', 0.7)} min score")
    
    def deduplicate_entities(self) -> Dict[str, Any]:
        """
        Main enhanced deduplication process.
        
        Returns:
            Dictionary with deduplication statistics
        """
        print("ðŸš€ Starting Enhanced Entity Deduplication Process")
        print("=" * 60)
        logger.info("Starting enhanced entity deduplication process")
        
        # Load data
        print("\nðŸ“‚ Step 1/6: Loading data...")
        self._load_data()
        original_entity_count = len(self.entities_df)
        print(f"   âœ“ Loaded {original_entity_count:,} entities and {len(self.relationships_df):,} relationships")
        
        # Build graph and calculate features
        print("\nðŸ—ï¸ Step 2/6: Building graph structure...")
        self.graph = self._build_graph(self.entities_df, self.relationships_df)
        
        print("\nðŸ“Š Step 3/6: Analyzing graph features...")
        self.entities_df = self._calculate_graph_features(self.entities_df)
        
        # Initialize semantic similarity if enabled
        if self.config.get('enable_semantic_matching', True) and HAS_SKLEARN:
            print("\nðŸ§  Step 4/6: Setting up semantic analysis...")
            self._initialize_semantic_similarity()
        else:
            print("\nâ­ï¸ Step 4/6: Skipping semantic analysis (disabled or scikit-learn unavailable)")
        
        # Find merge candidates using enhanced strategies
        print("\nðŸ” Step 5/6: Finding merge candidates...")
        merge_candidates = self._find_merge_candidates(self.entities_df, self.relationships_df)
        
        if merge_candidates:
            # Execute merges
            print("\nðŸ”§ Step 6/6: Executing merges...")
            merged_entities_df = self._execute_merges(self.entities_df, merge_candidates)
        else:
            print("\nâœ¨ Step 6/6: No merges needed - all entities are sufficiently distinct")
            merged_entities_df = self.entities_df
        
        # Save results
        print("\nðŸ’¾ Saving results...")
        self._save_deduplicated_data(merged_entities_df)
        self._save_enhanced_report(merge_candidates)
        
        # Calculate statistics
        final_entity_count = len(merged_entities_df)
        merged_count = original_entity_count - final_entity_count
        
        stats = {
            'original_entities': original_entity_count,
            'merged_entities': final_entity_count,
            'merged_count': merged_count,
            'merge_candidates': len(merge_candidates),
            'output_dir': str(self.output_dir / "deduplicated"),
            'config_used': self.config
        }
        
        print("\nâœ… Enhanced Deduplication Complete!")
        print("=" * 60)
        print(f"ðŸ“Š Final Results:")
        print(f"   â€¢ Original entities: {original_entity_count:,}")
        print(f"   â€¢ Final entities: {final_entity_count:,}")
        print(f"   â€¢ Entities merged: {merged_count:,}")
        print(f"   â€¢ Reduction: {(merged_count/original_entity_count)*100:.1f}%")
        print(f"   â€¢ Output saved to: output/deduplicated/")
        
        logger.info(f"Enhanced deduplication complete: {original_entity_count} -> {final_entity_count} entities")
        return stats
    
    def _load_data(self):
        """Load GraphRAG entities and relationships data."""
        entities_path = self.output_dir / "entities.parquet"
        relationships_path = self.output_dir / "relationships.parquet"
        
        if not entities_path.exists():
            raise FileNotFoundError(f"Entities file not found: {entities_path}")
        if not relationships_path.exists():
            raise FileNotFoundError(f"Relationships file not found: {relationships_path}")
        
        self.entities_df = pd.read_parquet(entities_path)
        self.relationships_df = pd.read_parquet(relationships_path)
        
        logger.info(f"Loaded {len(self.entities_df)} entities and {len(self.relationships_df)} relationships")
    
    def _build_graph(self, entities_df: pd.DataFrame, relationships_df: pd.DataFrame) -> nx.Graph:
        """Build NetworkX graph from relationships for analysis."""
        graph = nx.Graph()
        
        # Add entities as nodes
        for _, entity in entities_df.iterrows():
            graph.add_node(entity['title'], **entity.to_dict())
        
        # Add relationships as edges
        for _, rel in relationships_df.iterrows():
            if 'source' in rel and 'target' in rel:
                graph.add_edge(rel['source'], rel['target'], **rel.to_dict())
        
        logger.info(f"Built graph with {graph.number_of_nodes()} nodes and {graph.number_of_edges()} edges")
        return graph
    
    def _calculate_graph_features(self, entities_df: pd.DataFrame) -> pd.DataFrame:
        """Calculate graph-based features for entities."""
        df = entities_df.copy()
        
        print("ðŸ” Calculating graph features...")
        
        # Calculate clustering coefficients and degree centrality
        clustering_coeffs = {}
        degree_centrality = {}
        
        entity_names = df['title'].tolist()
        for entity_name in tqdm(entity_names, desc="Computing clustering & centrality", unit="entities"):
            if entity_name in self.graph:
                clustering_coeffs[entity_name] = nx.clustering(self.graph, entity_name)
                degree_centrality[entity_name] = self.graph.degree(entity_name)
            else:
                clustering_coeffs[entity_name] = 0.0
                degree_centrality[entity_name] = 0
        
        df['clustering_coeff'] = df['title'].map(clustering_coeffs)
        df['degree_centrality'] = df['title'].map(degree_centrality)
        
        # Calculate neighbor sets for structure comparison
        neighbor_sets = {}
        for entity_name in tqdm(entity_names, desc="Building neighbor sets", unit="entities"):
            if entity_name in self.graph:
                neighbor_sets[entity_name] = set(self.graph.neighbors(entity_name))
            else:
                neighbor_sets[entity_name] = set()
        
        df['neighbors'] = df['title'].map(neighbor_sets)
        
        logger.info("Calculated graph features for all entities")
        return df
    
    def _initialize_semantic_similarity(self):
        """Initialize TF-IDF vectorizer for semantic similarity."""
        if not HAS_SKLEARN:
            logger.warning("scikit-learn not available, skipping semantic similarity")
            return
        
        print("ðŸ§  Initializing semantic similarity analysis...")
        
        # Prepare text corpus from entity descriptions
        descriptions = []
        for _, entity in tqdm(self.entities_df.iterrows(), desc="Preparing text data", 
                             total=len(self.entities_df), unit="entities"):
            desc = entity.get('description', '') or ''
            title = entity.get('title', '') or ''
            combined_text = f"{title} {desc}".strip()
            descriptions.append(combined_text if combined_text else title)
        
        # Initialize TF-IDF vectorizer
        print("ðŸ“Š Fitting TF-IDF vectorizer...")
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=1000,
            stop_words='english',
            lowercase=True,
            ngram_range=(1, 2)
        )
        
        try:
            self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(descriptions)
            logger.info(f"Initialized semantic similarity with {self.tfidf_matrix.shape[1]} features")
        except Exception as e:
            logger.warning(f"Failed to initialize semantic similarity: {e}")
            self.tfidf_matrix = None
    
    def _find_merge_candidates(self, entities_df: pd.DataFrame, relationships_df: pd.DataFrame) -> List[Dict]:
        """Find entity merge candidates using enhanced matching strategies with optimized processing."""
        entity_list = entities_df.to_dict('records')
        total_comparisons = len(entity_list) * (len(entity_list) - 1) // 2
        
        print(f"\nðŸ” Analyzing {total_comparisons:,} entity pairs for potential merges...")
        
        # Use optimized sequential processing with better chunking
        return self._find_merge_candidates_optimized(entity_list, total_comparisons)
    
    def _process_entity_chunk(self, entity_pairs: List[Tuple[Dict, Dict]]) -> List[Dict]:
        """Process a chunk of entity pairs using instance methods."""
        candidates = []
        
        for entity1, entity2 in entity_pairs:
            # Calculate similarity scores
            scores = self._calculate_similarity_scores(entity1, entity2)
            
            # Calculate combined score
            combined_score = self._calculate_combined_score(scores)
            
            # Check threshold
            if combined_score >= self.config.get('min_combined_score', 0.7):
                # Validate candidate
                scores['combined_score'] = combined_score
                if self._validate_merge_candidate(entity1, entity2, scores):
                    # Determine merge reason
                    merge_reason = self._determine_merge_reason(scores)
                    
                    candidates.append({
                        'entity1_title': entity1['title'],
                        'entity2_title': entity2['title'],
                        'entity1_id': entity1.get('id', entity1.get('human_readable_id', '')),
                        'entity2_id': entity2.get('id', entity2.get('human_readable_id', '')),
                        'combined_score': combined_score,
                        'individual_scores': scores,
                        'merge_reason': merge_reason,
                        'primary_entity': self._determine_primary_entity(entity1, entity2)
                    })
        
        return candidates
    
    def _find_merge_candidates_optimized(self, entity_list: List[Dict], total_comparisons: int) -> List[Dict]:
        """Optimized version with smart filtering and progress tracking."""
        candidates = []
        
        print("   Using optimized processing with smart filtering")
        
        # First pass: Quick filtering based on simple criteria
        print("   ðŸ“‹ Pre-filtering entities for faster processing...")
        
        # Group entities by first letter and length for quick filtering
        entity_groups = {}
        for i, entity in enumerate(entity_list):
            title = entity['title'].lower().strip()
            # Group by first letter and rough length category
            key = (title[0] if title else '', len(title) // 5)  # Length buckets of 5
            if key not in entity_groups:
                entity_groups[key] = []
            entity_groups[key].append((i, entity))
        
        # Only compare entities within similar groups or with high connectivity
        comparison_pairs = []
        for group_entities in entity_groups.values():
            # Within-group comparisons (more likely to match)
            for i, (idx1, entity1) in enumerate(group_entities):
                for j, (idx2, entity2) in enumerate(group_entities[i+1:], i+1):
                    comparison_pairs.append((entity1, entity2))
        
        # Add cross-group comparisons for highly connected entities
        high_connectivity_threshold = 5  # entities with 5+ connections
        high_connectivity_entities = [
            entity for entity in entity_list 
            if entity.get('degree_centrality', 0) >= high_connectivity_threshold
        ]
        
        # Cross-group comparisons for high-connectivity entities
        for i, entity1 in enumerate(high_connectivity_entities):
            for entity2 in high_connectivity_entities[i+1:]:
                # Avoid duplicates
                pair_exists = any(
                    (p[0]['title'] == entity1['title'] and p[1]['title'] == entity2['title']) or
                    (p[0]['title'] == entity2['title'] and p[1]['title'] == entity1['title'])
                    for p in comparison_pairs
                )
                if not pair_exists:
                    comparison_pairs.append((entity1, entity2))
        
        actual_comparisons = len(comparison_pairs)
        reduction_percent = (1 - actual_comparisons/total_comparisons) * 100
        
        print(f"   âœ‚ï¸ Reduced from {total_comparisons:,} to {actual_comparisons:,} comparisons ({reduction_percent:.1f}% reduction)")
        
        # Process the filtered comparisons with progress
        processed = 0
        update_frequency = max(1, actual_comparisons // 100)  # Update every 1%
        
        with tqdm(total=actual_comparisons, desc="Comparing entity pairs", unit="pairs") as pbar:
            for entity1, entity2 in comparison_pairs:
                # Calculate similarity scores
                scores = self._calculate_similarity_scores(entity1, entity2)
                
                # Calculate combined score
                combined_score = self._calculate_combined_score(scores)
                
                # Check if it meets the threshold
                if combined_score >= self.config.get('min_combined_score', 0.7):
                    # Additional validation
                    scores['combined_score'] = combined_score
                    if self._validate_merge_candidate(entity1, entity2, scores):
                        # Determine merge reason
                        merge_reason = self._determine_merge_reason(scores)
                        
                        candidates.append({
                            'entity1_title': entity1['title'],
                            'entity2_title': entity2['title'],
                            'entity1_id': entity1.get('id', entity1.get('human_readable_id', '')),
                            'entity2_id': entity2.get('id', entity2.get('human_readable_id', '')),
                            'combined_score': combined_score,
                            'individual_scores': scores,
                            'merge_reason': merge_reason,
                            'primary_entity': self._determine_primary_entity(entity1, entity2)
                        })
                
                processed += 1
                if processed % update_frequency == 0:
                    pbar.update(update_frequency)
                    pbar.set_description(f"Comparing pairs (found {len(candidates)} candidates)")
            
            # Update any remaining
            pbar.update(actual_comparisons - pbar.n)
        
        # Sort by combined score (highest first)
        candidates.sort(key=lambda x: x['combined_score'], reverse=True)
        
        logger.info(f"Found {len(candidates)} merge candidates using optimized processing")
        return candidates
    
    def _find_merge_candidates_sequential(self, entity_list: List[Dict]) -> List[Dict]:
        """Sequential version for smaller datasets or fallback."""
        candidates = []
        processed_pairs = set()
        total_comparisons = len(entity_list) * (len(entity_list) - 1) // 2
        
        comparisons_made = 0
        with tqdm(total=total_comparisons, desc="Comparing entity pairs", unit="pairs") as pbar:
            for i, entity1 in enumerate(entity_list):
                for j, entity2 in enumerate(entity_list[i+1:], i+1):
                    pair_key = tuple(sorted([entity1['title'], entity2['title']]))
                    if pair_key in processed_pairs:
                        continue
                    processed_pairs.add(pair_key)
                    
                    # Calculate multiple similarity scores
                    scores = self._calculate_similarity_scores(entity1, entity2)
                    
                    # Combine scores using weighted average
                    combined_score = self._calculate_combined_score(scores)
                    
                    # Check if it meets the threshold
                    if combined_score >= self.config.get('min_combined_score', 0.7):
                        # Additional validation
                        scores['combined_score'] = combined_score
                        if self._validate_merge_candidate(entity1, entity2, scores):
                            # Determine merge reason
                            merge_reason = self._determine_merge_reason(scores)
                            
                            candidates.append({
                                'entity1_title': entity1['title'],
                                'entity2_title': entity2['title'],
                                'entity1_id': entity1.get('id', i),
                                'entity2_id': entity2.get('id', j),
                                'combined_score': combined_score,
                                'individual_scores': scores,
                                'merge_reason': merge_reason,
                                'primary_entity': self._determine_primary_entity(entity1, entity2)
                            })
                    
                    comparisons_made += 1
                    pbar.update(1)
                    
                    # Update description with current progress stats
                    if comparisons_made % 10000 == 0:
                        pbar.set_description(f"Comparing pairs (found {len(candidates)} candidates)")
        
        # Sort by combined score (highest first)
        candidates.sort(key=lambda x: x['combined_score'], reverse=True)
        
        logger.info(f"Found {len(candidates)} merge candidates using sequential processing")
        return candidates
    
    def _calculate_similarity_scores(self, entity1: Dict, entity2: Dict) -> Dict[str, float]:
        """Calculate multiple similarity scores between two entities."""
        scores = {}
        
        title1 = entity1['title'].lower().strip()
        title2 = entity2['title'].lower().strip()
        
        # 1. String similarity
        scores['string_similarity'] = self._string_similarity(title1, title2)
        
        # 2. Token overlap
        if self.config.get('enable_token_matching', True):
            scores['token_overlap'] = self._token_overlap_similarity(title1, title2)
        else:
            scores['token_overlap'] = 0.0
        
        # 3. Partial name matching
        if self.config.get('enable_partial_name_matching', True):
            scores['partial_name_match'] = self._partial_name_similarity(title1, title2)
        else:
            scores['partial_name_match'] = 0.0
        
        # 4. Abbreviation matching
        if self.config.get('enable_abbreviation_matching', True):
            scores['abbreviation_match'] = self._abbreviation_similarity(title1, title2)
        else:
            scores['abbreviation_match'] = 0.0
        
        # 5. Role-based matching
        if self.config.get('enable_role_based_matching', True):
            scores['role_match'] = self._role_based_similarity(title1, title2)
        else:
            scores['role_match'] = 0.0
        
        # 6. Graph structure similarity
        if self.config.get('enable_graph_structure_matching', True):
            scores['graph_structure'] = self._graph_structure_similarity(entity1, entity2)
        else:
            scores['graph_structure'] = 0.0
        
        # 7. Semantic similarity
        if self.config.get('enable_semantic_matching', True) and self.tfidf_matrix is not None:
            scores['semantic_similarity'] = self._semantic_similarity(entity1, entity2)
        else:
            scores['semantic_similarity'] = 0.0
        
        return scores
    
    def _string_similarity(self, str1: str, str2: str) -> float:
        """Calculate string similarity using multiple methods."""
        if HAS_LEVENSHTEIN:
            # Use Levenshtein distance if available
            lev_sim = 1 - (Levenshtein.distance(str1, str2) / max(len(str1), len(str2), 1))
        else:
            lev_sim = 0.0
        
        # SequenceMatcher similarity
        seq_sim = difflib.SequenceMatcher(None, str1, str2).ratio()
        
        # Return the maximum of available similarities
        return max(lev_sim, seq_sim)
    
    def _token_overlap_similarity(self, str1: str, str2: str) -> float:
        """Calculate token overlap score with stricter criteria."""
        tokens1 = set(self._tokenize_and_clean(str1))
        tokens2 = set(self._tokenize_and_clean(str2))
        
        if not tokens1 or not tokens2:
            return 0.0
        
        intersection = tokens1.intersection(tokens2)
        union = tokens1.union(tokens2)
        
        if not union:
            return 0.0
        
        jaccard = len(intersection) / len(union)
        
        # STRICTER: Only boost if one is complete subset AND it's a meaningful match
        if tokens1.issubset(tokens2) or tokens2.issubset(tokens1):
            # Require at least 2 tokens in the subset for meaningful match
            min_tokens = min(len(tokens1), len(tokens2))
            if min_tokens >= 2 or (min_tokens == 1 and jaccard >= 0.5):
                return min(1.0, jaccard + 0.2)  # Reduced boost from 0.3
        
        return jaccard
    
    def _tokenize_and_clean(self, text: str) -> set:
        """Helper method to tokenize and clean text."""
        # Tokenize and clean
        tokens = set(re.findall(r'\b\w+\b', text.lower()))
        
        # Remove common stop words
        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
        return tokens - stop_words
    
    def _partial_name_similarity(self, str1: str, str2: str) -> float:
        """Check if one name is a partial match of another."""
        tokens1 = re.findall(r'\b\w+\b', str1.lower())
        tokens2 = re.findall(r'\b\w+\b', str2.lower())
        
        # Check if all tokens of shorter name are in longer name
        if len(tokens1) <= len(tokens2):
            shorter, longer = tokens1, tokens2
        else:
            shorter, longer = tokens2, tokens1
        
        if not shorter:
            return 0.0
        
        matches = sum(1 for token in shorter if token in longer)
        return matches / len(shorter)
    
    def _abbreviation_similarity(self, str1: str, str2: str) -> float:
        """Check for abbreviation patterns."""
        # Extract initials and check against full names
        def get_initials(text):
            words = re.findall(r'\b\w+\b', text)
            return ''.join(word[0].lower() for word in words if word)
        
        def has_abbreviation_pattern(short, long):
            # Check if short form could be abbreviation of long form
            short_clean = re.sub(r'[^\w\s]', '', short).strip()
            long_clean = re.sub(r'[^\w\s]', '', long).strip()
            
            # Check initials
            initials = get_initials(long_clean)
            if short_clean.replace('.', '').replace(' ', '').lower() == initials:
                return 1.0
            
            # Check partial abbreviations (e.g., "V. Lago" vs "Vince Lago")
            short_parts = short_clean.split()
            long_parts = long_clean.split()
            
            if len(short_parts) == len(long_parts):
                matches = 0
                for s_part, l_part in zip(short_parts, long_parts):
                    s_clean = s_part.replace('.', '').lower()
                    l_clean = l_part.lower()
                    
                    if s_clean == l_clean or (len(s_clean) == 1 and l_clean.startswith(s_clean)):
                        matches += 1
                
                return matches / len(short_parts)
            
            return 0.0
        
        # Check both directions
        score1 = has_abbreviation_pattern(str1, str2)
        score2 = has_abbreviation_pattern(str2, str1)
        
        return max(score1, score2)
    
    def _role_based_similarity(self, str1: str, str2: str) -> float:
        """Check for role-based matches."""
        roles = ['mayor', 'commissioner', 'councilman', 'councilwoman', 'director', 'manager', 'chief']
        
        def extract_role_and_name(text):
            text_lower = text.lower()
            for role in roles:
                if role in text_lower:
                    # Extract the name part
                    name_part = text_lower.replace(role, '').strip()
                    return role, name_part
            return None, text_lower
        
        role1, name1 = extract_role_and_name(str1)
        role2, name2 = extract_role_and_name(str2)
        
        # If one has a role and the other doesn't, check if names match
        if role1 and not role2:
            return self._string_similarity(name1, str2)
        elif role2 and not role1:
            return self._string_similarity(name2, str1)
        elif role1 and role2:
            # Both have roles, check if they match and names are similar
            if role1 == role2:
                return self._string_similarity(name1, name2)
            else:
                return 0.0
        
        return 0.0
    
    def _graph_structure_similarity(self, entity1: Dict, entity2: Dict) -> float:
        """Calculate graph structure similarity."""
        # Get neighbors from pre-calculated neighbor sets
        neighbors1 = entity1.get('neighbors', set())
        neighbors2 = entity2.get('neighbors', set())
        
        if not neighbors1 and not neighbors2:
            return 1.0  # Both isolated
        elif not neighbors1 or not neighbors2:
            return 0.0  # One isolated, one connected
        
        # Jaccard similarity of neighbor sets
        intersection = len(neighbors1.intersection(neighbors2))
        union = len(neighbors1.union(neighbors2))
        
        jaccard_sim = intersection / union if union > 0 else 0.0
        
        # Also compare clustering coefficients
        coeff1 = entity1.get('clustering_coeff', 0.0)
        coeff2 = entity2.get('clustering_coeff', 0.0)
        coeff_diff = abs(coeff1 - coeff2)
        coeff_sim = 1.0 - min(coeff_diff / self.config.get('clustering_tolerance', 0.15), 1.0)
        
        return (jaccard_sim + coeff_sim) / 2.0
    
    def _semantic_similarity(self, entity1: Dict, entity2: Dict) -> float:
        """Calculate semantic similarity using TF-IDF."""
        if self.tfidf_matrix is None:
            return 0.0
        
        try:
            # Get entity indices (this is a simplified approach)
            entity1_idx = None
            entity2_idx = None
            
            for idx, row in self.entities_df.iterrows():
                if row['title'] == entity1['title']:
                    entity1_idx = idx
                elif row['title'] == entity2['title']:
                    entity2_idx = idx
                
                if entity1_idx is not None and entity2_idx is not None:
                    break
            
            if entity1_idx is None or entity2_idx is None:
                return 0.0
            
            # Calculate cosine similarity
            vec1 = self.tfidf_matrix[entity1_idx:entity1_idx+1]
            vec2 = self.tfidf_matrix[entity2_idx:entity2_idx+1]
            
            similarity = cosine_similarity(vec1, vec2)[0, 0]
            return similarity
        except Exception as e:
            logger.debug(f"Error calculating semantic similarity: {e}")
            return 0.0
    
    def _calculate_combined_score(self, scores: Dict[str, float]) -> float:
        """Combine scores with strict requirements for partial matches."""
        # For partial name matches, require strong corroborating evidence
        if scores.get('token_overlap', 0) > 0.7 and scores.get('string_similarity', 0) < 0.8:
            # This is likely a partial name match (e.g., "Lago" vs "Vince Lago")
            # Require high neighbor overlap as confirmation
            if scores.get('graph_structure', 0) < 0.6:
                return 0.0  # Reject without strong network evidence
        
        # Original weighted combination
        weights = self.config.get('weights', {
            'string_similarity': 0.2,
            'token_overlap': 0.4,
            'graph_structure': 0.2,
            'semantic_similarity': 0.2
        })
        
        # Combine main scores
        combined = 0.0
        total_weight = 0.0
        
        for score_type, weight in weights.items():
            if score_type in scores:
                combined += scores[score_type] * weight
                total_weight += weight
        
        # Add bonus scores for special matches
        bonus_scores = ['partial_name_match', 'abbreviation_match', 'role_match']
        max_bonus = 0.0
        for bonus_type in bonus_scores:
            if bonus_type in scores:
                max_bonus = max(max_bonus, scores[bonus_type])
        
        # Apply bonus (up to 20% boost)
        bonus_factor = min(max_bonus * 0.2, 0.2)
        
        if total_weight > 0:
            final_score = (combined / total_weight) + bonus_factor
            return min(final_score, 1.0)
        
        return 0.0
    
    def _determine_merge_reason(self, scores: Dict[str, float]) -> str:
        """Determine the primary reason for merging."""
        # Find the highest contributing factor
        main_scores = {
            'string_similarity': 'High string similarity',
            'token_overlap': 'Token overlap',
            'graph_structure': 'Similar graph structure',
            'semantic_similarity': 'Semantic similarity'
        }
        
        special_scores = {
            'partial_name_match': 'Partial name match',
            'abbreviation_match': 'Abbreviation pattern',
            'role_match': 'Role-based match'
        }
        
        # Check special patterns first
        for score_type, reason in special_scores.items():
            if scores.get(score_type, 0.0) > 0.7:
                return reason
        
        # Find highest main score
        max_score = 0.0
        max_reason = "Combined similarity"
        
        for score_type, reason in main_scores.items():
            if scores.get(score_type, 0.0) > max_score:
                max_score = scores[score_type]
                max_reason = reason
        
        return max_reason
    
    def _determine_primary_entity(self, entity1: Dict, entity2: Dict) -> str:
        """Determine which entity should be kept as primary."""
        # Prefer entity with higher degree centrality
        degree1 = entity1.get('degree_centrality', 0)
        degree2 = entity2.get('degree_centrality', 0)
        
        if degree1 > degree2:
            return entity1['title']
        elif degree2 > degree1:
            return entity2['title']
        
        # If same degree, prefer longer/more descriptive name
        if len(entity1['title']) > len(entity2['title']):
            return entity1['title']
        else:
            return entity2['title']
    
    def _validate_merge_candidate(self, entity1: Dict, entity2: Dict, 
                                scores: Dict[str, float]) -> bool:
        """Additional validation to prevent false positives."""
        # Special validation for partial name matches
        if scores.get('token_overlap', 0) > scores.get('string_similarity', 0):
            # This suggests a partial match like "Lago" vs "Vince Lago"
            
            # Require either:
            # 1. Very high neighbor overlap (>70%)
            # 2. High semantic similarity (>80%) 
            # 3. Abbreviation or role match
            
            has_strong_evidence = (
                scores.get('graph_structure', 0) > 0.7 or
                scores.get('semantic_similarity', 0) > 0.8 or
                scores.get('abbreviation_match', 0) > 0.8 or
                scores.get('role_match', 0) > 0.7
            )
            
            if not has_strong_evidence:
                return False
        
        # Prevent merging entities with very different descriptions
        if entity1.get('description') and entity2.get('description'):
            desc1_len = len(str(entity1['description']))
            desc2_len = len(str(entity2['description']))
            
            # If descriptions differ significantly in length, be more cautious
            if max(desc1_len, desc2_len) > 3 * min(desc1_len, desc2_len):
                # Require higher combined score for very different descriptions
                return scores.get('combined_score', 0) > 0.9
        
        return True
    
    def _execute_merges(self, entities_df: pd.DataFrame, merge_candidates: List[Dict]) -> pd.DataFrame:
        """Execute the entity merges."""
        merged_df = entities_df.copy()
        entities_to_remove = set()
        merge_map = {}  # Maps old entity -> new entity
        
        # Add aliases column if it doesn't exist
        if 'aliases' not in merged_df.columns:
            merged_df['aliases'] = ''
        
        print(f"\nðŸ”§ Executing {len(merge_candidates)} entity merges...")
        
        for candidate in tqdm(merge_candidates, desc="Executing merges", unit="merges"):
            entity1_title = candidate['entity1_title']
            entity2_title = candidate['entity2_title']
            primary_entity = candidate['primary_entity']
            
            # Skip if either entity was already merged
            if entity1_title in entities_to_remove or entity2_title in entities_to_remove:
                continue
            
            # Determine which entity to remove
            if primary_entity == entity1_title:
                keep_entity = entity1_title
                remove_entity = entity2_title
            else:
                keep_entity = entity2_title
                remove_entity = entity1_title
            
            # Update the primary entity
            primary_mask = merged_df['title'] == keep_entity
            remove_mask = merged_df['title'] == remove_entity
            
            if primary_mask.any() and remove_mask.any():
                primary_row = merged_df[primary_mask].iloc[0]
                remove_row = merged_df[remove_mask].iloc[0]
                
                # Merge descriptions
                primary_desc = primary_row.get('description', '') or ''
                remove_desc = remove_row.get('description', '') or ''
                
                if remove_desc and remove_desc not in primary_desc:
                    merged_desc = f"{primary_desc}\n[Also known as: {remove_entity}] {remove_desc}".strip()
                else:
                    merged_desc = f"{primary_desc}\n[Also known as: {remove_entity}]".strip()
                
                # Update aliases
                existing_aliases = primary_row.get('aliases', '') or ''
                if existing_aliases:
                    new_aliases = f"{existing_aliases}|{remove_entity}"
                else:
                    new_aliases = remove_entity
                
                # Apply updates
                merged_df.loc[primary_mask, 'description'] = merged_desc
                merged_df.loc[primary_mask, 'aliases'] = new_aliases
                
                # Mark for removal
                entities_to_remove.add(remove_entity)
                merge_map[remove_entity] = keep_entity
                
                # Record merge
                self.merge_report['merges'].append({
                    'primary_entity': keep_entity,
                    'merged_entity': remove_entity,
                    'combined_score': candidate['combined_score'],
                    'merge_reason': candidate['merge_reason'],
                    'individual_scores': candidate['individual_scores']
                })
        
        # Remove merged entities
        merged_df = merged_df[~merged_df['title'].isin(entities_to_remove)]
        
        logger.info(f"Executed {len(entities_to_remove)} merges")
        return merged_df
    
    def _save_deduplicated_data(self, merged_entities_df: pd.DataFrame):
        """Save deduplicated data to output directory."""
        output_subdir = self.output_dir / "deduplicated"
        output_subdir.mkdir(exist_ok=True)
        
        # Save merged entities
        entities_output = output_subdir / "entities.parquet"
        merged_entities_df.to_parquet(entities_output, index=False)
        
        # Copy other files unchanged
        other_files = [
            "relationships.parquet",
            "communities.parquet", 
            "community_reports.parquet"
        ]
        
        for filename in other_files:
            source_path = self.output_dir / filename
            target_path = output_subdir / filename
            
            if source_path.exists():
                import shutil
                shutil.copy2(source_path, target_path)
        
        logger.info(f"Saved deduplicated data to {output_subdir}")
    
    def _save_enhanced_report(self, merge_candidates: List[Dict]):
        """Save detailed enhanced deduplication report."""
        # Save JSON report
        report_path = self.output_dir / "enhanced_deduplication_report.json"
        self.merge_report['statistics'] = {
            'total_candidates': len(merge_candidates),
            'executed_merges': len(self.merge_report['merges']),
            'config_used': self.config
        }
        
        with open(report_path, 'w') as f:
            json.dump(self.merge_report, f, indent=2)
        
        # Save human-readable text report
        text_report_path = self.output_dir / "enhanced_deduplication_report.txt"
        with open(text_report_path, 'w') as f:
            f.write("Enhanced Entity Deduplication Report\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"Timestamp: {self.merge_report['timestamp']}\n")
            f.write(f"Configuration: {self.config.get('min_combined_score', 0.7)} min score\n")
            f.write(f"Total candidates found: {len(merge_candidates)}\n")
            f.write(f"Merges executed: {len(self.merge_report['merges'])}\n\n")
            
            f.write("Executed Merges:\n")
            f.write("-" * 20 + "\n")
            
            for merge in self.merge_report['merges']:
                f.write(f"\n'{merge['primary_entity']}' â† '{merge['merged_entity']}'\n")
                f.write(f"  Score: {merge['combined_score']:.3f}\n")
                f.write(f"  Reason: {merge['merge_reason']}\n")
                f.write(f"  Details: {merge['individual_scores']}\n")
            
            if merge_candidates:
                f.write(f"\n\nTop candidates (not merged):\n")
                f.write("-" * 30 + "\n")
                
                # Show top 10 candidates that weren't merged
                unmerged = [c for c in merge_candidates 
                           if not any(m['merged_entity'] in [c['entity1_title'], c['entity2_title']] 
                                     for m in self.merge_report['merges'])]
                
                for candidate in unmerged[:10]:
                    f.write(f"\n'{candidate['entity1_title']}' â†” '{candidate['entity2_title']}'\n")
                    f.write(f"  Score: {candidate['combined_score']:.3f}\n")
                    f.write(f"  Reason: {candidate['merge_reason']}\n")
        
        logger.info(f"Saved enhanced deduplication report to {text_report_path}")


# Main execution for standalone testing
if __name__ == "__main__":
    import argparse
    import sys
    from pathlib import Path
    
    # Add project root to path
    project_root = Path(__file__).parent.parent.parent
    sys.path.append(str(project_root))
    
    # Setup logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    parser = argparse.ArgumentParser(description='Run enhanced entity deduplication')
    parser.add_argument('--output-dir', '-o', type=str, default='graphrag_data/output',
                       help='Path to GraphRAG output directory')
    parser.add_argument('--config', '-c', choices=['aggressive', 'conservative', 'name_focused'],
                       default='name_focused', help='Deduplication configuration preset')
    parser.add_argument('--min-score', type=float, help='Override minimum combined score')
    
    args = parser.parse_args()
    
    output_dir = project_root / args.output_dir
    
    if not output_dir.exists():
        print(f"âŒ Output directory not found: {output_dir}")
        sys.exit(1)
    
    # Get configuration
    config = DEDUP_CONFIGS.get(args.config, DEDUP_CONFIGS['name_focused']).copy()
    if args.min_score:
        config['min_combined_score'] = args.min_score
    
    print(f"ðŸ” Running enhanced entity deduplication on {output_dir}")
    print(f"   Configuration: {args.config}")
    print(f"   Min combined score: {config['min_combined_score']}")
    
    try:
        deduplicator = EnhancedEntityDeduplicator(output_dir, config)
        stats = deduplicator.deduplicate_entities()
        
        print(f"\nâœ… Enhanced deduplication complete!")
        print(f"   Original entities: {stats['original_entities']}")
        print(f"   After deduplication: {stats['merged_entities']}")
        print(f"   Entities merged: {stats['merged_count']}")
        print(f"   Merge candidates: {stats['merge_candidates']}")
        print(f"   Output saved to: {stats['output_dir']}")
        
    except Exception as e:
        print(f"âŒ Enhanced deduplication failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


================================================================================


################################################################################
# File: scripts/graph_stages/ontology_extractor.py
################################################################################

# File: scripts/graph_stages/ontology_extractor.py

from pathlib import Path
from typing import Dict, List, Optional, Tuple
import json
import logging
import re
from datetime import datetime
from groq import Groq
import os

log = logging.getLogger(__name__)

class OntologyExtractor:
    """Extract rich ontology from agenda data using LLM."""
    
    def __init__(self, output_dir: Optional[Path] = None):
        """Initialize the ontology extractor."""
        self.output_dir = output_dir or Path("city_clerk_documents/graph_json")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Debug directory for LLM responses
        self.debug_dir = Path("debug")
        self.debug_dir.mkdir(exist_ok=True)
        
        # Initialize OpenAI client
        self.client = Groq()
        # Use gpt-4.1-mini-2025-04-14
        self.model = "gpt-4.1-mini-2025-04-14"
    
    def extract_ontology(self, agenda_file: Path) -> Dict[str, any]:
        """Extract rich ontology from agenda data."""
        log.info(f"ðŸ§  Extracting ontology from {agenda_file.name}")
        
        # Load extracted data (from docling)
        extracted_file = self.output_dir / f"{agenda_file.stem}_extracted.json"
        if not extracted_file.exists():
            log.error(f"âŒ Extracted file not found: {extracted_file}")
            return self._create_empty_ontology(agenda_file.name)
        
        with open(extracted_file, 'r', encoding='utf-8') as f:
            agenda_data = json.load(f)
        
        full_text = agenda_data.get('full_text', '')
        
        # Get the pre-extracted agenda items
        extracted_items = agenda_data.get('agenda_items', [])
        log.info(f"ðŸ“Š Found {len(extracted_items)} pre-extracted agenda items")
        
        # Save debug info
        with open(self.debug_dir / "extracted_items.json", 'w') as f:
            json.dump(extracted_items, f, indent=2)
        
        # Extract meeting date
        meeting_date = self._extract_meeting_date(agenda_file.name)
        log.info(f"ðŸ“… Extracted meeting date: {meeting_date}")
        
        # Extract meeting info
        meeting_info = self._extract_meeting_info(full_text, meeting_date)
        
        # Extract sections and their items
        sections = self._extract_sections_with_items(full_text, extracted_items)
        
        # Extract entities from the entire document
        entities = self._extract_entities(full_text)
        
        # Extract relationships between entities
        relationships = self._extract_relationships(entities, sections)
        
        # Extract URLs using regex
        urls = self._extract_urls_regex(full_text)
        
        # Enhance agenda items with URLs (only if they don't already have URLs from PyMuPDF)
        for section in sections:
            for item in section.get('items', []):
                # Only extract URLs if the item doesn't already have them from PyMuPDF
                if not item.get('urls'):
                    item['urls'] = self._find_urls_for_item(item, urls, full_text)
                # If item already has URLs from PyMuPDF, keep them as they are more accurate
        
        # Build ontology
        ontology = {
            'source_file': agenda_file.name,
            'meeting_date': meeting_date,
            'meeting_info': meeting_info,
            'sections': sections,
            'entities': entities,
            'relationships': relationships,
            'metadata': {
                'extraction_method': 'llm+regex',
                'num_sections': len(sections),
                'num_items': sum(len(s.get('items', [])) for s in sections),
                'num_entities': len(entities),
                'num_relationships': len(relationships),
                'num_urls': len(urls)
            }
        }
        
        # Save ontology
        output_file = self.output_dir / f"{agenda_file.stem}_ontology.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(ontology, f, indent=2, ensure_ascii=False)
        
        log.info(f"âœ… Ontology extraction complete: {len(sections)} sections, {sum(len(s.get('items', [])) for s in sections)} items")
        
        return ontology
    
    def _extract_meeting_date(self, filename: str) -> str:
        """Extract meeting date from filename."""
        # Pattern: Agenda MM.DD.YYYY.pdf or Agenda MM.D.YYYY.pdf
        date_pattern = r'Agenda\s+(\d{1,2})\.(\d{1,2})\.(\d{4})'
        match = re.search(date_pattern, filename)
        if match:
            month = match.group(1).zfill(2)
            day = match.group(2).zfill(2)
            year = match.group(3)
            return f"{month}.{day}.{year}"
        return "01.01.2024"  # Default fallback
    
    def _extract_meeting_info(self, text: str, meeting_date: str) -> Dict[str, any]:
        """Extract detailed meeting information using LLM."""
        prompt = f"""Extract meeting information from this city commission agenda. Find:

1. Meeting type (Regular, Special, Workshop)
2. Meeting time
3. Meeting location/venue
4. Commission members present (if listed)
5. City officials (Mayor, City Manager, City Attorney, City Clerk)

Return ONLY the JSON object below, no other text:
{{
  "type": "Regular Meeting",
  "time": "5:30 PM",
  "location": "City Commission Chambers",
  "commissioners": ["Name1", "Name2"],
  "officials": {{
    "mayor": "Mayor Name",
    "city_manager": "Manager Name",
    "city_attorney": "Attorney Name",
    "city_clerk": "Clerk Name"
  }}
}}

Text (first 3000 chars):
{text[:3000]}"""

        try:
            response = self.client.chat.completions.create(
                model="meta-llama/llama-4-maverick-17b-128e-instruct",
                messages=[
                    {"role": "system", "content": "You are a JSON extraction assistant. Return ONLY valid JSON, no markdown formatting or code blocks."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_completion_tokens=8192,
                top_p=1,
                stream=False,
                stop=None
            )
            
            response_text = response.choices[0].message.content.strip()
            
            # Debug: save raw response
            debug_file = self.debug_dir / "meeting_info_response.txt"
            with open(debug_file, 'w') as f:
                f.write(response_text)
            
            # Clean the response
            response_text = self._clean_json_response(response_text)
            
            # Parse JSON
            result = json.loads(response_text)
            
            # Validate it's a dict
            if not isinstance(result, dict):
                log.error(f"Meeting info is not a dict: {type(result)}")
                return self._default_meeting_info()
                
            return result
            
        except Exception as e:
            log.error(f"Failed to extract meeting info: {e}")
            return self._default_meeting_info()

    def _default_meeting_info(self) -> Dict[str, any]:
        """Return default meeting info structure."""
        return {
            "type": "Regular Meeting",
            "time": "5:30 PM",
            "location": "City Commission Chambers",
            "commissioners": [],
            "officials": {}
        }
    
    def _extract_section_headers_from_text(self, text: str) -> List[Dict[str, any]]:
        """Extract section headers from the agenda text."""
        sections = []
        
        # Define all possible section headers
        section_headers = [
            'PRESENTATIONS AND PROTOCOL DOCUMENTS',
            'APPROVAL OF MINUTES',
            'PUBLIC COMMENTS',
            'CONSENT AGENDA',
            'ORDINANCES ON SECOND READING',
            'ORDINANCES ON FIRST READING',
            'PUBLIC HEARINGS',  # Sometimes used instead of ORDINANCES
            'RESOLUTIONS',
            'CITY COMMISSION ITEMS',
            'BOARDS/COMMITTEES ITEMS',
            'BOARDS AND COMMITTEES ITEMS',  # Alternative spelling
            'CITY MANAGER ITEMS',
            'CITY ATTORNEY ITEMS',
            'CITY CLERK ITEMS',
            'DISCUSSION ITEMS',
            'ADJOURNMENT'
        ]
        
        lines = text.split('\n')
        found_sections = []
        
        for i, line in enumerate(lines[:500]):  # Check first 500 lines for headers
            line_stripped = line.strip()
            if not line_stripped:
                continue
            
            # Check for letter-prefixed section (e.g., "A. PRESENTATIONS AND PROTOCOL DOCUMENTS")
            letter_match = re.match(r'^([A-Z])\.\s+(.+)$', line_stripped)
            if letter_match:
                letter = letter_match.group(1)
                section_name = letter_match.group(2).strip()
                
                # Check if this matches one of our known headers
                section_type = None
                for header in section_headers:
                    if header in section_name.upper():
                        section_type = header.replace(' ', '_').replace('/', '_')
                        break
                
                if not section_type:
                    section_type = f'SECTION_{letter}'
                
                found_sections.append({
                    'section_letter': letter,
                    'section_name': section_name,
                    'section_type': section_type,
                    'line_number': i,
                    'items': []
                })
                log.info(f"Found section: {letter}. {section_name}")
                continue
            
            # Check for non-letter section headers
            line_upper = line_stripped.upper()
            for header in section_headers:
                # Check for exact match or if the header is contained in the line
                if header == line_upper or header in line_upper:
                    # Try to find if there's a letter before this section
                    section_letter = None
                    if i > 0:
                        # Check previous lines for a letter
                        for j in range(max(0, i-3), i):
                            prev_line = lines[j].strip()
                            single_letter = re.match(r'^([A-Z])\.?$', prev_line)
                            if single_letter:
                                section_letter = single_letter.group(1)
                                break
                    
                    # If no letter found, assign based on order
                    if not section_letter and found_sections:
                        last_letter = found_sections[-1].get('section_letter', '@')
                        section_letter = chr(ord(last_letter) + 1)
                    elif not section_letter:
                        section_letter = 'A'
                    
                    found_sections.append({
                        'section_letter': section_letter,
                        'section_name': line_stripped,
                        'section_type': header.replace(' ', '_').replace('/', '_'),
                        'line_number': i,
                        'items': []
                    })
                    log.info(f"Found section: {section_letter}. {line_stripped}")
                    break
        
        # Sort sections by line number to maintain order
        found_sections.sort(key=lambda x: x['line_number'])
        
        # Add order field
        for i, section in enumerate(found_sections):
            section['order'] = i + 1
            del section['line_number']  # Remove line number as it's not needed anymore
        
        return found_sections
    
    def _assign_items_to_sections(self, sections: List[Dict], items: List[Dict]) -> List[Dict]:
        """Assign items to their appropriate sections based on item codes."""
        # Create a map of letter to section
        letter_to_section = {}
        for section in sections:
            if 'section_letter' in section:
                letter_to_section[section['section_letter']] = section
        
        # Assign items to sections based on their letter prefix
        for item in items:
            item_code = item.get('item_code', '')
            if not item_code:
                continue
            
            # Extract letter prefix (A.-1. -> A, D.-2. -> D, 1.-1. -> 1)
            letter_match = re.match(r'^([A-Z0-9])', item_code)
            if letter_match:
                letter = letter_match.group(1)
                
                if letter in letter_to_section:
                    section = letter_to_section[letter]
                    
                    # Add enhanced fields to item
                    enhanced_item = {
                        **item,
                        'section': section['section_name'],
                        'description': item.get('title', '')[:200],
                        'sponsors': [],
                        'departments': [],
                        'actions': [],
                        'stakeholders': [],
                        'urls': item.get('urls', [])  # Preserve existing URLs instead of overwriting
                    }
                    
                    section['items'].append(enhanced_item)
                    log.debug(f"Assigned item {item_code} to section {section['section_name']}")
        
        # Remove empty sections and reorder
        non_empty_sections = []
        for i, section in enumerate(sections):
            if section.get('items'):
                section['order'] = i + 1
                non_empty_sections.append(section)
                log.info(f"Section {section['section_name']} has {len(section['items'])} items")
        
        return non_empty_sections

    def _group_items_by_prefix(self, items: List[Dict]) -> List[Dict[str, any]]:
        """Group items by their letter prefix without assuming section types."""
        sections_map = {}
        
        for item in items:
            item_code = item.get('item_code', '')
            if not item_code:
                continue
            
            # Extract letter/number prefix (A.-1. -> A, D.-2. -> D, 1.-1. -> 1)
            prefix_match = re.match(r'^([A-Z0-9])', item_code)
            if prefix_match:
                prefix = prefix_match.group(1)
                
                if prefix not in sections_map:
                    # Create generic section name
                    if prefix.isalpha():
                        section_name = f'Section {prefix}'
                    else:
                        section_name = f'Numbered Items - Section {prefix}'
                    
                    sections_map[prefix] = {
                        'section_letter': prefix,
                        'section_name': section_name,
                        'section_type': f'SECTION_{prefix}',
                        'items': []
                    }
                
                # Add enhanced fields to item
                enhanced_item = {
                    **item,
                    'section': sections_map[prefix]['section_name'],
                    'description': item.get('title', '')[:200],
                    'sponsors': [],
                    'departments': [],
                    'actions': [],
                    'stakeholders': [],
                    'urls': item.get('urls', [])  # Preserve existing URLs instead of overwriting
                }
                
                sections_map[prefix]['items'].append(enhanced_item)
        
        # Convert to list and sort
        sections = []
        
        # Sort alphabetically first (A, B, C...), then numerically (1, 2, 3...)
        sorted_keys = sorted(sections_map.keys(), key=lambda x: (x.isdigit(), x))
        
        for i, key in enumerate(sorted_keys):
            section = sections_map[key]
            section['order'] = i + 1
            sections.append(section)
            log.info(f"Created section '{section['section_name']}' with {len(section['items'])} items")
        
        return sections

    def _determine_item_type(self, title: str, section_type: str) -> str:
        """Determine item type from title and section."""
        title_lower = title.lower()
        
        # Check title first for explicit type indicators
        if 'an ordinance' in title_lower:
            return 'Ordinance'
        elif 'a resolution' in title_lower:
            return 'Resolution'
        elif 'proclamation' in title_lower:
            return 'Proclamation'
        elif 'recognition' in title_lower:
            return 'Recognition'
        elif 'congratulations' in title_lower:
            return 'Recognition'
        elif 'presentation' in title_lower:
            return 'Presentation'
        elif 'appointment' in title_lower or 'appointing' in title_lower:
            return 'Appointment'
        elif 'minutes' in title_lower and 'approval' in title_lower:
            return 'Minutes Approval'
        
        # Use section type as hint if no explicit type in title
        if section_type:
            if 'ORDINANCE' in section_type:
                return 'Ordinance'
            elif 'RESOLUTION' in section_type:
                return 'Resolution'
            elif 'PRESENTATION' in section_type:
                return 'Presentation'
            elif 'MINUTES' in section_type:
                return 'Minutes Approval'
            elif 'CONSENT' in section_type:
                # Consent items could be various types
                return 'Consent Item'
        
        # Generic fallback
        return 'Agenda Item'
    
    def _extract_sections_with_items(self, text: str, extracted_items: List[Dict]) -> List[Dict[str, any]]:
        """Extract sections and organize items within them using LLM."""
        
        # If we have extracted items from the PDF extractor, use them
        if extracted_items:
            log.info(f"Using {len(extracted_items)} pre-extracted agenda items")
            
            # First, try to extract section headers from the text
            sections = self._extract_section_headers_from_text(text)
            
            if sections:
                log.info(f"Found {len(sections)} sections in agenda text")
                # Assign items to the extracted sections
                sections = self._assign_items_to_sections(sections, extracted_items)
            else:
                log.warning("No sections found in text, grouping items by prefix")
                # If no sections found, group items by their letter prefix
                sections = self._group_items_by_prefix(extracted_items)
            
            # Add any items that weren't assigned to a section
            unassigned_items = []
            assigned_codes = set()
            
            for section in sections:
                for item in section.get('items', []):
                    assigned_codes.add(item.get('item_code'))
            
            for item in extracted_items:
                if item.get('item_code') not in assigned_codes:
                    unassigned_items.append(item)
            
            if unassigned_items:
                log.warning(f"Found {len(unassigned_items)} unassigned items")
                # Create a miscellaneous section for unassigned items
                misc_section = {
                    'section_letter': 'MISC',
                    'section_name': 'Other Items',
                    'section_type': 'OTHER',
                    'order': len(sections) + 1,
                    'items': []
                }
                
                for item in unassigned_items:
                    enhanced_item = {
                        **item,
                        'section': 'Other Items',
                        'description': item.get('title', '')[:200],
                        'sponsors': [],
                        'departments': [],
                        'actions': [],
                        'stakeholders': [],
                        'urls': item.get('urls', [])  # Preserve existing URLs instead of overwriting
                    }
                    misc_section['items'].append(enhanced_item)
                
                sections.append(misc_section)
            
            log.info(f"Created {len(sections)} sections with {sum(len(s.get('items', [])) for s in sections)} total items")
            return sections
        
        # If no extracted items, fall back to LLM extraction
        log.warning("No pre-extracted items found, using LLM extraction")
        # First, get section structure from LLM
        prompt = f"""Identify all major sections in this city commission agenda. Common sections include:
- PRESENTATIONS AND PROCLAMATIONS
- CONSENT AGENDA  
- ORDINANCES ON FIRST READING
- ORDINANCES ON SECOND READING
- RESOLUTIONS
- CITY MANAGER REPORTS
- CITY ATTORNEY REPORTS
- GENERAL DISCUSSION

For each section found, provide:
1. section_name: The exact name as it appears
2. section_type: One of [presentations, consent, ordinances_first, ordinances_second, resolutions, reports, discussion, other]
3. description: Brief description of what this section contains

Return as JSON array:
[
  {{
    "section_name": "CONSENT AGENDA",
    "section_type": "consent",
    "description": "Items for routine approval"
  }}
]

Text (first 5000 chars):
{text[:5000]}"""

        sections = []
        
        try:
            response = self.client.chat.completions.create(
                model="meta-llama/llama-4-maverick-17b-128e-instruct",
                messages=[
                    {"role": "system", "content": "Extract agenda sections. Return only valid JSON array."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_completion_tokens=8192,
                top_p=1,
                stream=False,
                stop=None
            )
            
            response_text = response.choices[0].message.content.strip()
            response_text = self._clean_json_response(response_text)
            
            section_list = json.loads(response_text)
            
            # Now assign items to sections based on their location in text
            for section in section_list:
                section['items'] = []
                
                # Find items that belong to this section
                section_name = section['section_name']
                for item in extracted_items:
                    # Enhanced item with more details
                    enhanced_item = {
                        **item,
                        'section': section_name,
                        'description': '',
                        'sponsors': [],
                        'departments': [],
                        'actions': [],
                        'urls': item.get('urls', [])  # Preserve existing URLs
                    }
                    
                    # Try to find item in text and extract context
                    item_code = item.get('item_code', '')
                    if item_code:
                        # Find the item in the text and extract surrounding context
                        pattern = rf'{re.escape(item_code)}.*?(?=(?:[A-Z]\.-\d+\.|$))'
                        match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
                        if match:
                            context = match.group(0)[:1000]  # Get context around item
                            
                            # Extract additional details using LLM
                            details = self._extract_item_details(item_code, context)
                            enhanced_item.update(details)
                    
                    # Assign to appropriate section based on item type or position
                    if item.get('item_type') == 'Ordinance':
                        if section['section_type'] in ['ordinances_first', 'ordinances_second']:
                            section['items'].append(enhanced_item)
                    elif item.get('item_type') == 'Resolution' and section['section_type'] == 'resolutions':
                        section['items'].append(enhanced_item)
                    elif section['section_type'] == 'consent' and item_code.startswith('D'):
                        section['items'].append(enhanced_item)
            
            sections = section_list
            
        except Exception as e:
            log.error(f"Failed to extract sections: {e}")
            # Fallback: create basic sections
            sections = self._create_basic_sections(extracted_items)
        
        return sections
    
    def _extract_item_details(self, item_code: str, context: str) -> Dict[str, any]:
        """Extract detailed information about a specific agenda item."""
        prompt = f"""Extract details for agenda item {item_code} from this context:

Find:
1. Full description/summary
2. Sponsoring commissioners or departments
3. Departments involved
4. Recommended actions (approve, deny, discuss, defer, etc.)
5. Key stakeholders mentioned

Return as JSON:
{{
  "description": "Brief description",
  "sponsors": ["Commissioner Name"],
  "departments": ["Planning", "Finance"],
  "actions": ["approve", "authorize"],
  "stakeholders": ["Organization name"]
}}

Context:
{context}"""

        try:
            response = self.client.chat.completions.create(
                model="meta-llama/llama-4-maverick-17b-128e-instruct",
                messages=[
                    {"role": "system", "content": "You are a JSON extraction assistant. Return ONLY valid JSON with no additional text."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_completion_tokens=8192,
                top_p=1,
                stream=False,
                stop=None
            )
            
            response_text = response.choices[0].message.content.strip()
            response_text = self._clean_json_response(response_text)
            
            return json.loads(response_text)
            
        except Exception as e:
            log.error(f"Failed to extract item details for {item_code}: {e}")
            return {
                "description": "",
                "sponsors": [],
                "departments": [],
                "actions": [],
                "stakeholders": []
            }
    
    def _extract_entities(self, text: str) -> List[Dict[str, any]]:
        """Extract all entities (people, organizations, departments) from the document."""
        # Process in chunks
        max_chars = 10000
        chunks = [text[i:i+max_chars] for i in range(0, len(text), max_chars)]
        
        all_entities = []
        seen_entities = set()
        
        for i, chunk in enumerate(chunks[:5]):  # Process first 5 chunks
            prompt = f"""Extract all named entities from this government document:

Find:
1. People (commissioners, officials, citizens)
2. Organizations (companies, non-profits, agencies)
3. Departments (city departments, divisions)
4. Locations (addresses, buildings, areas)

For each entity, determine:
- name: Full name
- type: person, organization, department, location
- role: Their role if mentioned (e.g., "Commissioner", "Director", "Applicant")
- context: Brief context where they appear

Return as JSON array:
[
  {{
    "name": "John Smith",
    "type": "person",
    "role": "Commissioner",
    "context": "Sponsoring ordinance E-1"
  }}
]

Text chunk {i+1}:
{chunk}"""

            try:
                response = self.client.chat.completions.create(
                    model="meta-llama/llama-4-maverick-17b-128e-instruct",
                    messages=[
                        {"role": "system", "content": "You are a JSON extraction assistant. You must return ONLY a valid JSON array with no additional text, explanations, or markdown formatting."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0,
                    max_completion_tokens=8192,
                    top_p=1,
                    stream=False,
                    stop=None
                )
                
                response_text = response.choices[0].message.content.strip()
                
                # Debug: save raw response
                debug_file = self.debug_dir / f"entities_response_chunk_{i}.txt"
                with open(debug_file, 'w') as f:
                    f.write(response_text)
                
                response_text = self._clean_json_response(response_text)
                
                entities = json.loads(response_text)
                if not isinstance(entities, list):
                    log.error(f"Expected list but got {type(entities)} for chunk {i+1}")
                    entities = []
                
                # Deduplicate
                for entity in entities:
                    entity_key = f"{entity.get('type', '')}:{entity.get('name', '').lower()}"
                    if entity_key not in seen_entities:
                        seen_entities.add(entity_key)
                        all_entities.append(entity)
                
            except Exception as e:
                log.error(f"Failed to extract entities from chunk {i+1}: {e}")
                # Try basic regex extraction as fallback
                chunk_entities = self._basic_entity_extraction(chunk)
                all_entities.extend(chunk_entities)
        
        return all_entities
    
    def _basic_entity_extraction(self, text: str) -> List[Dict[str, any]]:
        """Basic entity extraction using patterns as fallback."""
        entities = []
        
        # Extract commissioners/council members
        commissioner_pattern = r'Commissioner\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)'
        for match in re.finditer(commissioner_pattern, text):
            entities.append({
                "name": match.group(1),
                "type": "person",
                "role": "Commissioner",
                "context": "City Commissioner"
            })
        
        # Extract Mayor
        mayor_pattern = r'Mayor\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)'
        for match in re.finditer(mayor_pattern, text):
            entities.append({
                "name": match.group(1),
                "type": "person",
                "role": "Mayor",
                "context": "City Mayor"
            })
        
        # Extract departments
        dept_pattern = r'(?:Department of|Dept\. of)\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)'
        for match in re.finditer(dept_pattern, text):
            entities.append({
                "name": f"Department of {match.group(1)}",
                "type": "department",
                "role": "",
                "context": "City Department"
            })
        
        # Extract City Manager, City Attorney, City Clerk
        for role in ["City Manager", "City Attorney", "City Clerk"]:
            pattern = rf'{role}\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)'
            for match in re.finditer(pattern, text):
                entities.append({
                    "name": match.group(1),
                    "type": "person",
                    "role": role,
                    "context": f"{role} of the City"
                })
        
        return entities
    
    def _extract_relationships(self, entities: List[Dict], sections: List[Dict]) -> List[Dict[str, any]]:
        """Extract relationships between entities and agenda items."""
        relationships = []
        
        # Extract relationships from agenda items
        for section in sections:
            for item in section.get('items', []):
                item_code = item.get('item_code', '')
                
                # Sponsors relationship
                for sponsor in item.get('sponsors', []):
                    relationships.append({
                        'source': sponsor,
                        'source_type': 'person',
                        'relationship': 'sponsors',
                        'target': item_code,
                        'target_type': 'agenda_item'
                    })
                
                # Department relationships
                for dept in item.get('departments', []):
                    relationships.append({
                        'source': dept,
                        'source_type': 'department',
                        'relationship': 'responsible_for',
                        'target': item_code,
                        'target_type': 'agenda_item'
                    })
                
                # Stakeholder relationships
                for stakeholder in item.get('stakeholders', []):
                    relationships.append({
                        'source': stakeholder,
                        'source_type': 'organization',
                        'relationship': 'involved_in',
                        'target': item_code,
                        'target_type': 'agenda_item'
                    })
        
        return relationships
    
    def _extract_urls_regex(self, text: str) -> List[Dict[str, str]]:
        """Extract all URLs from the text using regex."""
        urls = []
        
        # Comprehensive URL pattern
        url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+(?:[/?#][^\s<>"{}|\\^`\[\]]*)?'
        
        for match in re.finditer(url_pattern, text):
            url = match.group(0).rstrip('.,;:)')  # Clean trailing punctuation
            
            # Find surrounding context
            start = max(0, match.start() - 100)
            end = min(len(text), match.end() + 100)
            context = text[start:end]
            
            # Try to find associated agenda item
            item_pattern = r'([A-Z])[.-](\d+)'
            item_matches = list(re.finditer(item_pattern, context))
            
            associated_item = None
            if item_matches:
                # Find closest item reference
                closest_match = min(item_matches, 
                                  key=lambda m: abs(m.start() - (match.start() - start)))
                associated_item = f"{closest_match.group(1)}-{closest_match.group(2)}"
            
            urls.append({
                'url': url,
                'context': context.replace('\n', ' ').strip(),
                'associated_item': associated_item
            })
        
        return urls
    
    def _find_urls_for_item(self, item: Dict, all_urls: List[Dict], full_text: str) -> List[str]:
        """Find URLs associated with a specific agenda item."""
        item_code = item.get('item_code', '')
        doc_ref = item.get('document_reference', '')
        
        if not item_code:
            return []
        
        # Find the item's position in text
        item_pattern = rf'{re.escape(item_code)}.*?{re.escape(doc_ref)}' if doc_ref else rf'{re.escape(item_code)}'
        item_match = re.search(item_pattern, full_text, re.DOTALL)
        
        if item_match:
            item_start = item_match.start()
            # Look for next item to bound our search
            next_item_pattern = r'[A-Z]\.-\d+\.?\s+\d{2}-\d{4,5}'
            next_match = re.search(next_item_pattern, full_text[item_start + len(item_match.group(0)):])
            item_end = item_start + len(item_match.group(0)) + (next_match.start() if next_match else 1000)
            
            # Find URLs in this item's text range
            item_urls = []
            for url_info in all_urls:
                url_pos = full_text.find(url_info['url'])
                if item_start <= url_pos <= item_end:
                    item_urls.append(url_info['url'])
            
            return item_urls
        
        # Fallback to original method if no match found
        item_urls = []
        
        # Find URLs that mention this item code
        for url_info in all_urls:
            if url_info.get('associated_item') == item_code:
                item_urls.append(url_info['url'])
            elif item_code in url_info.get('context', ''):
                item_urls.append(url_info['url'])
        
        # Also search near the item in the text
        item_pattern = rf'{re.escape(item_code)}[^A-Z]*'
        match = re.search(item_pattern, full_text, re.IGNORECASE)
        if match:
            # Look for URLs within 500 chars of the item
            start = match.start()
            end = min(len(full_text), start + 1000)
            item_context = full_text[start:end]
            
            url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+(?:[/?#][^\s<>"{}|\\^`\[\]]*)?'
            for url_match in re.finditer(url_pattern, item_context):
                url = url_match.group(0).rstrip('.,;:)')
                if url not in item_urls:
                    item_urls.append(url)
        
        return item_urls
    
    def _create_basic_sections(self, items: List[Dict]) -> List[Dict[str, any]]:
        """Create basic sections as fallback."""
        sections = []
        
        # Group by item type
        ordinances = [item for item in items if item.get('item_type') == 'Ordinance']
        resolutions = [item for item in items if item.get('item_type') == 'Resolution']
        others = [item for item in items if item.get('item_type') not in ['Ordinance', 'Resolution']]
        
        if ordinances:
            sections.append({
                'section_name': 'ORDINANCES',
                'section_type': 'ordinances_first',
                'description': 'Ordinances for consideration',
                'items': ordinances
            })
        
        if resolutions:
            sections.append({
                'section_name': 'RESOLUTIONS',
                'section_type': 'resolutions',
                'description': 'Resolutions for consideration',
                'items': resolutions
            })
        
        if others:
            sections.append({
                'section_name': 'OTHER ITEMS',
                'section_type': 'other',
                'description': 'Other agenda items',
                'items': others
            })
        
        return sections
    
    def _clean_json_response(self, response: str) -> str:
        """Clean LLM response to extract valid JSON."""
        # Remove thinking tags if present
        if '<think>' in response:
            # Extract content after </think>
            parts = response.split('</think>')
            if len(parts) > 1:
                response = parts[1].strip()
        
        # Remove markdown code blocks
        if '```json' in response:
            response = response.split('```json')[1].split('```')[0]
        elif '```' in response:
            response = response.split('```')[1].split('```')[0]
        
        # Remove any non-JSON content before/after
        response = response.strip()
        
        # Find JSON array or object
        if '[' in response:
            # Find the first [ and matching ]
            start_idx = response.find('[')
            if start_idx != -1:
                bracket_count = 0
                for i in range(start_idx, len(response)):
                    if response[i] == '[':
                        bracket_count += 1
                    elif response[i] == ']':
                        bracket_count -= 1
                        if bracket_count == 0:
                            return response[start_idx:i+1]
        
        if '{' in response:
            # Find the first { and matching }
            start_idx = response.find('{')
            if start_idx != -1:
                brace_count = 0
                in_string = False
                escape_next = False
                
                for i in range(start_idx, len(response)):
                    char = response[i]
                    
                    if escape_next:
                        escape_next = False
                        continue
                        
                    if char == '\\':
                        escape_next = True
                        continue
                        
                    if char == '"' and not escape_next:
                        in_string = not in_string
                        
                    if not in_string:
                        if char == '{':
                            brace_count += 1
                        elif char == '}':
                            brace_count -= 1
                            if brace_count == 0:
                                return response[start_idx:i+1]
        
        return response
    
    def _create_empty_ontology(self, filename: str) -> Dict[str, any]:
        """Create empty ontology structure."""
        return {
            'source_file': filename,
            'meeting_date': self._extract_meeting_date(filename),
            'meeting_info': {
                'type': 'Regular Meeting',
                'time': '5:30 PM',
                'location': 'City Commission Chambers',
                'commissioners': [],
                'officials': {}
            },
            'sections': [],
            'entities': [],
            'relationships': [],
            'metadata': {
                'extraction_method': 'empty',
                'num_sections': 0,
                'num_items': 0,
                'num_entities': 0,
                'num_relationships': 0
            }
        }

    def _extract_and_associate_urls(self, text: str, agenda_items: List[Dict]) -> Dict[str, List[str]]:
        """Extract URLs and associate them with nearby agenda items."""
        url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+(?:[/?#][^\s<>"{}|\\^`\[\]]*)?'
        
        # Find all URLs with their positions
        url_matches = []
        for match in re.finditer(url_pattern, text):
            url = match.group(0).rstrip('.,;:)')
            position = match.start()
            
            # Find the nearest agenda item reference before this URL
            nearest_item = self._find_nearest_agenda_item(text, position, agenda_items)
            
            url_matches.append({
                'url': url,
                'position': position,
                'associated_item': nearest_item
            })
        
        return url_matches

    def _find_nearest_agenda_item(self, text: str, url_position: int, agenda_items: List[Dict]) -> Optional[str]:
        """Find the nearest agenda item reference before a URL position."""
        best_item = None
        best_distance = float('inf')
        
        for item in agenda_items:
            item_code = item.get('item_code', '')
            if not item_code:
                continue
                
            # Find all occurrences of this item code before the URL
            item_pattern = rf'{re.escape(item_code)}'
            for match in re.finditer(item_pattern, text[:url_position]):
                distance = url_position - match.end()
                if distance < best_distance:
                    best_distance = distance
                    best_item = item_code
        
        return best_item


================================================================================


################################################################################
# File: scripts/microsoft_framework/document_adapter.py
################################################################################

# File: scripts/microsoft_framework/document_adapter.py

from pathlib import Path
import json
import pandas as pd
import csv
import re
from typing import List, Dict, Any
from datetime import datetime
import yaml
from concurrent.futures import ThreadPoolExecutor, as_completed
import multiprocessing

class CityClerkDocumentAdapter:
    """Adapt city clerk documents for GraphRAG processing."""
    
    def __init__(self, extracted_text_dir: Path):
        self.extracted_text_dir = Path(extracted_text_dir)
        
    def prepare_documents_for_graphrag(self, output_dir: Path) -> pd.DataFrame:
        """Prepare documents with enhanced source tracking."""
        
        all_documents = []
        
        for json_file in self.extracted_text_dir.glob("*_extracted.json"):
            with open(json_file, 'r') as f:
                doc_data = json.load(f)
            
            # Extract metadata
            doc_type = doc_data.get('document_type', 'unknown')
            meeting_date = doc_data.get('meeting_date', '')
            
            # For agenda items, create separate documents
            if 'items' in doc_data:
                for item in doc_data['items']:
                    doc_dict = {
                        'text': self._format_agenda_item(item),
                        'title': f"Agenda Item {item['item_code']} - {meeting_date}",
                        'document_type': 'agenda_item',
                        'meeting_date': meeting_date,
                        'item_code': item['item_code'],
                        'source_file': json_file.name,
                        'source_id': f"{json_file.stem}_{item['item_code']}",  # Unique source ID
                        'metadata': json.dumps({
                            'original_file': json_file.name,
                            'extraction_method': doc_data.get('metadata', {}).get('extraction_method', 'unknown'),
                            'item_type': item.get('type', 'unknown')
                        })
                    }
                    all_documents.append(doc_dict)
            
            # For other documents
            else:
                doc_dict = {
                    'text': doc_data.get('full_text', ''),
                    'title': self._generate_title(doc_data),
                    'document_type': doc_type,
                    'meeting_date': meeting_date,
                    'item_code': doc_data.get('item_code', ''),
                    'source_file': json_file.name,
                    'source_id': json_file.stem,  # Unique source ID
                    'metadata': json.dumps({
                        'original_file': json_file.name,
                        'document_number': doc_data.get('document_number', ''),
                        'extraction_method': doc_data.get('metadata', {}).get('extraction_method', 'unknown')
                    })
                }
                all_documents.append(doc_dict)
        
        # Create DataFrame with source tracking columns
        df = pd.DataFrame(all_documents)
        
        # Ensure required columns exist
        required_columns = ['text', 'title', 'source_id', 'source_file', 'metadata']
        for col in required_columns:
            if col not in df.columns:
                df[col] = ''
        
        # Save with source tracking preserved
        csv_path = output_dir / "city_clerk_documents.csv"
        df.to_csv(csv_path, index=False, encoding='utf-8')
        
        return df
    
    def _format_agenda_item(self, item: Dict) -> str:
        """Format agenda item text."""
        parts = []
        
        if item.get('item_code'):
            parts.append(f"Item Code: {item['item_code']}")
        
        if item.get('title'):
            parts.append(f"Title: {item['title']}")
        
        if item.get('description'):
            parts.append(f"Description: {item['description']}")
        
        if item.get('sponsors'):
            sponsors = ', '.join(item['sponsors'])
            parts.append(f"Sponsors: {sponsors}")
        
        return "\n\n".join(parts)
    
    def _generate_title(self, doc_data: Dict) -> str:
        """Generate title for document."""
        doc_type = doc_data.get('document_type', 'Document')
        doc_number = doc_data.get('document_number', '')
        
        if doc_number:
            return f"{doc_type.title()} {doc_number}"
        else:
            return doc_type.title()
    
    def _process_json_file(self, json_file: Path) -> List[Dict]:
        """Process a single JSON file and return its documents."""
        documents = []
        
        with open(json_file, 'r') as f:
            data = json.load(f)
        
        doc_type = data.get('document_type', self._determine_document_type(json_file.name))
        meeting_date = data.get('meeting_date', self._extract_meeting_date(json_file.name))
        
        # CRITICAL: Process agenda items as completely separate documents
        if doc_type == 'agenda' and 'agenda_items' in data:
            # Each agenda item becomes its own document with NO references to other items
            for item in data['agenda_items']:
                # Create a unique, isolated document for this specific item
                doc_record = {
                    'id': f"agenda_item_{meeting_date}_{item['item_code']}",
                    'title': f"Agenda Item {item['item_code']} ONLY",
                    'text': self._prepare_isolated_item_text(item, meeting_date),
                    'document_type': 'agenda_item',
                    'meeting_date': meeting_date,
                    'item_code': item['item_code'],
                    'source_file': json_file.name,
                    'isolation_flag': True,  # Flag to indicate this must be treated in isolation
                    'urls': json.dumps(item.get('urls', []))
                }
                documents.append(doc_record)
        
        elif doc_type in ['ordinance', 'resolution']:
            # Process ordinances and resolutions with explicit item code isolation
            doc_record = {
                'id': f"{doc_type}_{data.get('document_number', json_file.stem)}",
                'title': f"{doc_type.title()} {data.get('document_number', '')}",
                'text': self._prepare_isolated_document_text(data),
                'document_type': doc_type,
                'meeting_date': meeting_date,
                'item_code': data.get('item_code', ''),
                'document_number': data.get('document_number', ''),
                'source_file': json_file.name,
                'isolation_flag': True,
                'urls': json.dumps([])
            }
            documents.append(doc_record)
        
        elif doc_type == 'verbatim_transcript':
            # Process transcripts with item code isolation
            item_codes = data.get('item_codes', [])
            # Create separate document for each item code mentioned
            if item_codes:
                for item_code in item_codes:
                    doc_record = {
                        'id': f"transcript_{json_file.stem}_{item_code}",
                        'title': f"Transcript for Item {item_code} ONLY",
                        'text': self._extract_item_specific_transcript(data, item_code),
                        'document_type': 'verbatim_transcript',
                        'meeting_date': meeting_date,
                        'item_code': item_code,
                        'transcript_type': data.get('transcript_type', ''),
                        'source_file': json_file.name,
                        'isolation_flag': True,
                        'urls': json.dumps([])
                    }
                    documents.append(doc_record)
            else:
                # General transcript without specific items
                doc_record = {
                    'id': f"transcript_{json_file.stem}",
                    'title': f"Transcript: {data.get('transcript_type', 'Meeting')}",
                    'text': data.get('full_text', ''),
                    'document_type': 'verbatim_transcript',
                    'meeting_date': meeting_date,
                    'item_code': '',
                    'transcript_type': data.get('transcript_type', ''),
                    'source_file': json_file.name,
                    'urls': json.dumps([])
                }
                documents.append(doc_record)
        
        return documents
    
    def _prepare_isolated_item_text(self, item: Dict, meeting_date: str) -> str:
        """Prepare agenda item text with STRICT isolation - no references to other items."""
        parts = []
        
        # Add isolation header
        parts.append(f"=== ISOLATED ENTITY: AGENDA ITEM {item['item_code']} ===")
        parts.append(f"THIS DOCUMENT CONTAINS INFORMATION ABOUT {item['item_code']} ONLY.")
        parts.append(f"DO NOT CONFUSE WITH OTHER AGENDA ITEMS.\n")
        
        # Add item-specific information
        parts.append(f"Agenda Item Code: {item['item_code']}")
        parts.append(f"Meeting Date: {meeting_date}")
        
        # Add title
        if item.get('title'):
            parts.append(f"Title: {item['title']}")
        
        # Add sponsors
        if item.get('sponsors'):
            sponsors = ', '.join(item['sponsors'])
            parts.append(f"Sponsors: {sponsors}")
        
        # Add description
        if item.get('description'):
            parts.append(f"Description: {item['description']}")
        
        # Add URLs as context
        if item.get('urls'):
            parts.append("\nReferenced Documents:")
            for url in item['urls']:
                parts.append(f"- {url.get('text', 'Link')}: {url.get('url', '')}")
        
        # Add isolation footer
        parts.append(f"\n=== END OF ISOLATED ENTITY {item['item_code']} ===")
        
        return "\n\n".join(parts)
    
    def _prepare_isolated_document_text(self, data: Dict) -> str:
        """Prepare document text with isolation markers."""
        parts = []
        
        doc_number = data.get('document_number', '')
        item_code = data.get('item_code', '')
        
        # Add isolation header
        if item_code:
            parts.append(f"=== ISOLATED ENTITY: {data.get('document_type', '').upper()} FOR ITEM {item_code} ===")
            parts.append(f"THIS DOCUMENT IS SPECIFICALLY ABOUT AGENDA ITEM {item_code}.")
        else:
            parts.append(f"=== ISOLATED ENTITY: {data.get('document_type', '').upper()} {doc_number} ===")
        
        # Add the full text
        parts.append(data.get('full_text', ''))
        
        # Add isolation footer
        parts.append(f"\n=== END OF ISOLATED ENTITY ===")
        
        return "\n\n".join(parts)
    
    def _extract_item_specific_transcript(self, data: Dict, item_code: str) -> str:
        """Extract only the transcript portions relevant to a specific item code."""
        full_text = data.get('full_text', '')
        
        # Try to extract sections mentioning this specific item
        lines = full_text.split('\n')
        relevant_sections = []
        in_relevant_section = False
        context_buffer = []
        
        for i, line in enumerate(lines):
            # Check if this line mentions the specific item code
            if item_code in line:
                in_relevant_section = True
                # Add some context before
                start_idx = max(0, i - 5)
                context_buffer = lines[start_idx:i]
                relevant_sections.extend(context_buffer)
                relevant_sections.append(line)
            elif in_relevant_section:
                # Continue adding lines until we hit another item code or section break
                if re.search(r'[A-Z]-\d+', line) and item_code not in line:
                    # Hit another item code, stop
                    in_relevant_section = False
                else:
                    relevant_sections.append(line)
        
        if relevant_sections:
            isolated_text = f"=== TRANSCRIPT EXCERPT FOR ITEM {item_code} ONLY ===\n\n"
            isolated_text += "\n".join(relevant_sections)
            isolated_text += f"\n\n=== END OF ITEM {item_code} TRANSCRIPT ==="
            return isolated_text
        else:
            return f"No specific transcript content found for item {item_code}"

    def _determine_document_type(self, filename: str) -> str:
        """Determine document type from filename."""
        filename_lower = filename.lower()
        if 'agenda' in filename_lower:
            return 'agenda'
        elif 'ordinance' in filename_lower:
            return 'ordinance'
        elif 'resolution' in filename_lower:
            return 'resolution'
        elif 'verbatim' in filename_lower:
            return 'verbatim_transcript'
        elif 'minutes' in filename_lower:
            return 'minutes'
        else:
            return 'document'
    
    def _extract_meeting_date(self, filename: str) -> str:
        """Extract meeting date from filename."""
        import re
        # Try different date patterns
        patterns = [
            r'(\d{4}-\d{2}-\d{2})',  # YYYY-MM-DD
            r'(\d{2}_\d{2}_\d{4})',  # MM_DD_YYYY
            r'(\d{2}\.\d{2}\.\d{4})'  # MM.DD.YYYY
        ]
        
        for pattern in patterns:
            match = re.search(pattern, filename)
            if match:
                date_str = match.group(1)
                # Normalize to MM.DD.YYYY format
                if '-' in date_str:
                    parts = date_str.split('-')
                    return f"{parts[1]}.{parts[2]}.{parts[0]}"
                elif '_' in date_str:
                    parts = date_str.split('_')
                    return f"{parts[0]}.{parts[1]}.{parts[2]}"
                else:
                    return date_str
        
        return ""

    def prepare_documents_from_markdown(self, output_dir: Path) -> pd.DataFrame:
        """Convert markdown files to GraphRAG CSV format with enhanced metadata."""
        
        markdown_dir = Path("city_clerk_documents/extracted_markdown")
        
        if not markdown_dir.exists():
            raise ValueError(f"Markdown directory not found: {markdown_dir}")
        
        documents = []
        
        # Process all markdown files
        for md_file in markdown_dir.glob("*.md"):
            try:
                with open(md_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # Extract metadata from the header
                metadata = self._extract_enhanced_metadata(content)
                
                # Build enriched text that includes all identifiers
                enriched_text = self._build_enriched_text(content, metadata)
                
                # Clean content for CSV
                enriched_text = self._clean_text_for_graphrag(enriched_text)
                
                # Ensure content is not empty
                if not enriched_text.strip():
                    print(f"âš ï¸  Skipping empty file: {md_file.name}")
                    continue
                
                # Parse document type from filename
                filename = md_file.stem
                if filename.startswith('agenda_'):
                    doc_type = 'agenda'
                elif filename.startswith('ordinance_'):
                    doc_type = 'ordinance'
                elif filename.startswith('resolution_'):
                    doc_type = 'resolution'
                elif filename.startswith('verbatim_'):
                    doc_type = 'verbatim_transcript'
                else:
                    doc_type = 'document'
                
                # Extract meeting date and item code from filename or content
                meeting_date = self._extract_meeting_date_from_markdown(filename, content)
                item_code = self._extract_item_code_from_markdown(filename, content)
                
                doc_record = {
                    'id': filename,
                    'title': self._build_comprehensive_title(metadata),
                    'text': enriched_text,
                    'document_type': doc_type,
                    'meeting_date': meeting_date,
                    'item_code': metadata.get('item_code', item_code),
                    'document_number': metadata.get('document_number', ''),
                    'related_items': json.dumps(metadata.get('related_items', [])),
                    'source_file': md_file.name
                }
                documents.append(doc_record)
                
            except Exception as e:
                print(f"âŒ Error processing {md_file.name}: {e}")
                continue
        
        if not documents:
            raise ValueError("No documents were successfully processed!")
        
        # Convert to DataFrame
        df = pd.DataFrame(documents)
        
        # Ensure no null values in required columns
        df['text'] = df['text'].fillna('')
        df['title'] = df['title'].fillna('Untitled')
        df['meeting_date'] = df['meeting_date'].fillna('')
        df['item_code'] = df['item_code'].fillna('')
        df['document_number'] = df['document_number'].fillna('')
        df['related_items'] = df['related_items'].fillna('[]')
        
        # Ensure text column has content
        empty_texts = df[df['text'].str.strip() == '']
        if len(empty_texts) > 0:
            print(f"âš ï¸  Warning: {len(empty_texts)} documents have empty text")
            df = df[df['text'].str.strip() != '']
        
        # Log summary
        print(f"ðŸ“Š Prepared {len(df)} documents from markdown:")
        for doc_type in df['document_type'].unique():
            count = len(df[df['document_type'] == doc_type])
            print(f"   - {doc_type}: {count}")
        
        # Save as CSV with proper escaping
        output_file = output_dir / "city_clerk_documents.csv"
        
        # Use pandas to_csv with specific parameters to handle special characters
        df.to_csv(
            output_file, 
            index=False,
            encoding='utf-8',
            escapechar='\\',
            doublequote=True,
            quoting=csv.QUOTE_MINIMAL
        )
        
        print(f"ðŸ’¾ Saved CSV to: {output_file}")
        
        # Verify the CSV can be read back
        try:
            test_df = pd.read_csv(output_file)
            print(f"âœ… CSV verification: {len(test_df)} rows can be read back")
        except Exception as e:
            print(f"âŒ CSV verification failed: {e}")
        
        return df

    def _clean_text_for_graphrag(self, text: str) -> str:
        """Clean markdown text for GraphRAG processing."""
        # Remove metadata header if present
        if text.startswith('---'):
            parts = text.split('---', 2)
            if len(parts) >= 3:
                # Keep only the main content after metadata
                text = parts[2].strip()
                
                # Also remove the "ORIGINAL DOCUMENT CONTENT" marker if present
                if "ORIGINAL DOCUMENT CONTENT" in text:
                    text = text.split("ORIGINAL DOCUMENT CONTENT", 1)[1].strip()
        
        # Remove excessive markdown formatting
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)  # Remove headers
        text = re.sub(r'\*\*([^*]+)\*\*', r'\1', text)  # Remove bold
        text = re.sub(r'\n{3,}', '\n\n', text)  # Reduce multiple newlines
        
        # DO NOT TRUNCATE - GraphRAG will handle chunking itself
        # Just ensure the text is clean and complete
        return text.strip()

    def _clean_text_for_csv(self, text: str) -> str:
        """Clean text to be CSV-safe."""
        # Remove markdown metadata header if present
        if text.startswith('---'):
            parts = text.split('---', 2)
            if len(parts) >= 3:
                # Keep only the main content
                text = parts[2]
        
        # Remove problematic characters
        text = text.replace('\x00', '')  # Null bytes
        text = text.replace('\r\n', '\n')  # Windows line endings
        
        # Replace multiple newlines with double newline
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        # Remove or escape quotes that might break CSV
        text = text.replace('"', '""')  # Escape quotes for CSV
        
        # Ensure text is not too long (GraphRAG might have limits)
        max_length = 50000  # Adjust as needed
        if len(text) > max_length:
            text = text[:max_length] + "... [truncated]"
        
        return text.strip()

    def _extract_meeting_date_from_markdown(self, filename: str, content: str) -> str:
        """Extract meeting date from filename or markdown content."""
        # First try from filename
        date_from_filename = self._extract_meeting_date(filename)
        if date_from_filename:
            return date_from_filename
        
        # Try from content metadata
        meeting_date_match = re.search(r'- Meeting Date: (.+)', content)
        if meeting_date_match:
            return meeting_date_match.group(1).strip()
        
        return ""

    def _extract_item_code_from_markdown(self, filename: str, content: str) -> str:
        """Extract item code from filename or markdown content."""
        # Try from filename
        item_match = re.search(r'([A-Z]-\d+)', filename)
        if item_match:
            return item_match.group(1)
        
        # Try from content
        item_match = re.search(r'- Agenda Items Discussed: (.+)', content)
        if item_match:
            return item_match.group(1).strip()
        
        return ""

    def _extract_title_from_markdown(self, content: str, filename: str) -> str:
        """Extract title from markdown content."""
        # Look for title in metadata section
        import re
        title_match = re.search(r'\*\*PARSED INFORMATION:\*\*.*?- Title: (.+)', content, re.DOTALL)
        if title_match:
            return title_match.group(1).strip()
        
        # Fallback to filename
        return filename.replace('_', ' ').title()

    def _parse_custom_metadata(self, metadata_text: str) -> Dict:
        """Parse our custom metadata format from the enhanced PDF extractor."""
        frontmatter = {}
        
        # Extract document type from the metadata
        import re
        
        # Look for document type
        doc_type_match = re.search(r'Document Type:\s*(.+)', metadata_text)
        if doc_type_match:
            frontmatter['document_type'] = doc_type_match.group(1).strip()
        
        # Look for filename
        filename_match = re.search(r'Filename:\s*(.+)', metadata_text)
        if filename_match:
            frontmatter['filename'] = filename_match.group(1).strip()
        
        # Look for document number
        doc_num_match = re.search(r'Document Number:\s*(.+)', metadata_text)
        if doc_num_match and doc_num_match.group(1).strip() != 'N/A':
            frontmatter['document_number'] = doc_num_match.group(1).strip()
        
        # Look for meeting date
        date_match = re.search(r'Meeting Date:\s*(.+)', metadata_text)
        if date_match and date_match.group(1).strip() != 'N/A':
            frontmatter['meeting_date'] = date_match.group(1).strip()
        
        # Look for agenda items
        items_match = re.search(r'Related Agenda Items:\s*(.+)', metadata_text)
        if items_match and items_match.group(1).strip() != 'N/A':
            frontmatter['agenda_items'] = items_match.group(1).strip()
        
        return frontmatter 

    def _extract_enhanced_metadata(self, content: str) -> Dict:
        """Extract all metadata including cross-references."""
        metadata = {}
        
        # Extract all document numbers
        doc_nums = re.findall(r'(?:Ordinance|Resolution)\s*(?:No\.\s*)?(\d{4}-\d+|\d+)', content)
        metadata['all_document_numbers'] = list(set(doc_nums))
        
        # Extract all agenda items
        agenda_items = re.findall(r'(?:Item|Agenda Item)\s*:?\s*([A-Z]-?\d+)', content)
        metadata['all_agenda_items'] = list(set(agenda_items))
        
        # Extract relationships
        metadata['related_items'] = self._extract_relationships(content)
        
        return metadata

    def _build_enriched_text(self, content: str, metadata: Dict) -> str:
        """Build text that prominently features all identifiers."""
        # Add a summary header with all identifiers
        identifier_summary = []
        
        if metadata.get('all_document_numbers'):
            identifier_summary.append(f"Document Numbers: {', '.join(metadata['all_document_numbers'])}")
        
        if metadata.get('all_agenda_items'):
            identifier_summary.append(f"Agenda Items: {', '.join(metadata['all_agenda_items'])}")
        
        if identifier_summary:
            enriched_header = "DOCUMENT IDENTIFIERS:\n" + '\n'.join(identifier_summary) + "\n\n"
            return enriched_header + content
        
        return content

    def _build_comprehensive_title(self, metadata: Dict) -> str:
        """Build a comprehensive title from metadata."""
        title_parts = []
        
        if metadata.get('all_agenda_items'):
            title_parts.append(f"Items: {', '.join(metadata['all_agenda_items'])}")
        
        if metadata.get('all_document_numbers'):
            title_parts.append(f"Docs: {', '.join(metadata['all_document_numbers'])}")
        
        if title_parts:
            return ' | '.join(title_parts)
        
        return "City Document"

    def _extract_relationships(self, content: str) -> List[Dict]:
        """Extract document relationships from content."""
        relationships = []
        
        # Look for patterns that indicate relationships
        relationship_patterns = [
            r'(?:amending|modifying|updating)\s+(?:Ordinance|Resolution)\s*(?:No\.\s*)?(\d+)',
            r'(?:relating to|concerning|regarding)\s+(?:agenda item|item)\s*([A-Z]-?\d+)',
            r'(?:pursuant to|under)\s+(?:agenda item|item)\s*([A-Z]-?\d+)'
        ]
        
        for pattern in relationship_patterns:
            matches = re.finditer(pattern, content, re.IGNORECASE)
            for match in matches:
                relationships.append({
                    'type': 'reference',
                    'target': match.group(1),
                    'context': match.group(0)
                })
        
        return relationships


================================================================================


################################################################################
# File: scripts/graph_stages/verbatim_transcript_linker.py
################################################################################

# File: scripts/graph_stages/verbatim_transcript_linker.py

"""
Verbatim Transcript Linker
Links verbatim transcript documents to their corresponding agenda items.
Now with full OCR support for all pages.
"""

import logging
import re
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Set
import json
from datetime import datetime
import PyPDF2
import os
import asyncio
import multiprocessing

# Import the PDF extractor for OCR support
from .pdf_extractor import PDFExtractor

log = logging.getLogger('verbatim_transcript_linker')


class VerbatimTranscriptLinker:
    """Links verbatim transcript documents to agenda items in the graph."""
    
    def __init__(self):
        """Initialize the verbatim transcript linker."""
        # Pattern to extract date and item info from filename
        self.filename_pattern = re.compile(
            r'(\d{2})_(\d{2})_(\d{4})\s*-\s*Verbatim Transcripts\s*-\s*(.+)\.pdf',
            re.IGNORECASE
        )
        
        # Debug directory for logging - ensure parent exists
        self.debug_dir = Path("city_clerk_documents/graph_json/debug/verbatim")
        self.debug_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize PDF extractor for OCR
        self.pdf_extractor = PDFExtractor(
            pdf_dir=Path("."),  # We'll use it file by file
            output_dir=Path("city_clerk_documents/extracted_text")
        )
    
    async def link_transcripts_for_meeting(self, 
                                         meeting_date: str,
                                         verbatim_dir: Path) -> Dict[str, List[Dict]]:
        """Find and link all verbatim transcripts for a specific meeting date."""
        log.info(f"ðŸŽ¤ Linking verbatim transcripts for meeting date: {meeting_date}")
        log.info(f"ðŸ“ Verbatim directory: {verbatim_dir}")
        
        # Debug logging for troubleshooting
        log.info(f"ðŸ” Looking for verbatim transcripts in: {verbatim_dir}")
        log.info(f"ðŸ” Directory exists: {verbatim_dir.exists()}")
        if verbatim_dir.exists():
            all_files = list(verbatim_dir.glob("*.pdf"))
            log.info(f"ðŸ” Total PDF files in directory: {len(all_files)}")
            if all_files:
                log.info(f"ðŸ” Sample files: {[f.name for f in all_files[:3]]}")
        
        # Convert meeting date format: "01.09.2024" -> "01_09_2024"
        date_underscore = meeting_date.replace(".", "_")
        
        # Initialize results
        linked_transcripts = {
            "item_transcripts": [],      # Transcripts for specific agenda items
            "public_comments": [],       # Public comment transcripts
            "section_transcripts": []    # Transcripts for entire sections
        }
        
        if not verbatim_dir.exists():
            log.warning(f"âš ï¸  Verbatim directory not found: {verbatim_dir}")
            return linked_transcripts
        
        # Find all transcript files for this date
        # Try multiple patterns to ensure we catch all files
        patterns = [
            f"{date_underscore}*Verbatim*.pdf",
            f"{date_underscore} - Verbatim*.pdf",
            f"*{date_underscore}*Verbatim*.pdf"
        ]

        transcript_files = []
        for pattern in patterns:
            files = list(verbatim_dir.glob(pattern))
            log.info(f"ðŸ” Pattern '{pattern}' found {len(files)} files")
            transcript_files.extend(files)

        # Remove duplicates
        transcript_files = list(set(transcript_files))
        
        log.info(f"ðŸ“„ Found {len(transcript_files)} transcript files")
        
        if transcript_files:
            # Process transcripts in parallel
            max_concurrent = min(multiprocessing.cpu_count(), 8)
            semaphore = asyncio.Semaphore(max_concurrent)
            
            async def process_with_semaphore(transcript_path):
                async with semaphore:
                    return await self._process_transcript(transcript_path, meeting_date)
            
            # Process all transcripts concurrently
            results = await asyncio.gather(
                *[process_with_semaphore(path) for path in transcript_files],
                return_exceptions=True
            )
            
            # Categorize results
            for transcript_info, path in zip(results, transcript_files):
                if isinstance(transcript_info, Exception):
                    log.error(f"Error processing transcript {path.name}: {transcript_info}")
                    continue
                if transcript_info:
                    # Categorize based on transcript type
                    if transcript_info['transcript_type'] == 'public_comment':
                        linked_transcripts['public_comments'].append(transcript_info)
                    elif transcript_info['transcript_type'] == 'section':
                        linked_transcripts['section_transcripts'].append(transcript_info)
                    else:
                        linked_transcripts['item_transcripts'].append(transcript_info)
                    
                    # Save extracted text (can also be done in parallel)
                    self._save_extracted_text(path, transcript_info)
        
        # Save linked transcripts info for debugging
        self._save_linking_report(meeting_date, linked_transcripts)
        
        # Log summary
        total_linked = (len(linked_transcripts['item_transcripts']) + 
                       len(linked_transcripts['public_comments']) + 
                       len(linked_transcripts['section_transcripts']))
        
        log.info(f"âœ… Verbatim transcript linking complete:")
        log.info(f"   ðŸŽ¤ Item transcripts: {len(linked_transcripts['item_transcripts'])}")
        log.info(f"   ðŸŽ¤ Public comments: {len(linked_transcripts['public_comments'])}")
        log.info(f"   ðŸŽ¤ Section transcripts: {len(linked_transcripts['section_transcripts'])}")
        log.info(f"   ðŸ“„ Total linked: {total_linked}")
        
        return linked_transcripts
    
    async def _process_transcript(self, transcript_path: Path, meeting_date: str) -> Optional[Dict[str, Any]]:
        """Process a single transcript file with full OCR."""
        try:
            # Parse filename
            match = self.filename_pattern.match(transcript_path.name)
            if not match:
                log.warning(f"Could not parse transcript filename: {transcript_path.name}")
                return None
            
            month, day, year = match.groups()[:3]
            item_info = match.group(4).strip()
            
            # Parse item codes from the item info
            parsed_items = self._parse_item_codes(item_info)
            
            # Extract text from ALL pages using Docling OCR
            log.info(f"ðŸ” Running OCR on full transcript: {transcript_path.name}...")
            full_text, pages = self.pdf_extractor.extract_text_from_pdf(transcript_path)
            
            if not full_text:
                log.warning(f"No text extracted from {transcript_path.name}")
                return None
            
            log.info(f"âœ… OCR extracted {len(full_text)} characters from {len(pages)} pages")
            
            # Determine transcript type and normalize item codes
            transcript_type = self._determine_transcript_type(item_info, parsed_items)
            
            transcript_info = {
                "path": str(transcript_path),
                "filename": transcript_path.name,
                "meeting_date": meeting_date,
                "item_info_raw": item_info,
                "item_codes": parsed_items['item_codes'],
                "section_codes": parsed_items['section_codes'],
                "transcript_type": transcript_type,
                "page_count": len(pages),
                "full_text": full_text,  # Store complete transcript text
                "pages": pages,          # Store page-level data
                "extraction_method": "docling_ocr"
            }
            
            log.info(f"ðŸ“„ Processed transcript: {transcript_path.name}")
            log.info(f"   Items: {parsed_items['item_codes']}")
            log.info(f"   Type: {transcript_type}")
            
            return transcript_info
            
        except Exception as e:
            log.error(f"Error processing transcript {transcript_path.name}: {e}")
            return None
    
    def _parse_item_codes(self, item_info: str) -> Dict[str, List[str]]:
        """Parse item codes from the filename item info section."""
        result = {
            'item_codes': [],
            'section_codes': []
        }
        
        # Check for public comment first
        if re.search(r'public\s+comment', item_info, re.IGNORECASE):
            result['section_codes'].append('PUBLIC_COMMENT')
            return result
        
        # Special case: Meeting Minutes or other general labels
        if re.search(r'meeting\s+minutes', item_info, re.IGNORECASE):
            result['item_codes'].append('MEETING_MINUTES')
            return result
        
        # Special case: Full meeting transcript
        if re.search(r'public|full\s+meeting', item_info, re.IGNORECASE) and not re.search(r'comment', item_info, re.IGNORECASE):
            result['item_codes'].append('FULL_MEETING')
            return result
        
        # Special case: Discussion Items (K section)
        if re.match(r'^K\s*$', item_info.strip()):
            result['section_codes'].append('K')
            return result
        
        # Clean the item info
        item_info = item_info.strip()
        
        # Handle multiple items with "and" or "AND"
        # Examples: "F-7 and F-10", "2-1 AND 2-2"
        if ' and ' in item_info.lower():
            parts = re.split(r'\s+and\s+', item_info, flags=re.IGNORECASE)
            for part in parts:
                codes = self._extract_single_item_codes(part.strip())
                result['item_codes'].extend(codes)
        
        # Handle space-separated items
        # Examples: "E-5 E-6 E-7 E-8 E-9 E-10"
        elif re.match(r'^([A-Z]-?\d+\s*)+$', item_info):
            # Split by spaces and extract each item
            items = item_info.split()
            for item in items:
                if re.match(r'^[A-Z]-?\d+$', item):
                    normalized = self._normalize_item_code(item)
                    if normalized:
                        result['item_codes'].append(normalized)
        
        # Handle comma-separated items
        elif ',' in item_info:
            parts = item_info.split(',')
            for part in parts:
                codes = self._extract_single_item_codes(part.strip())
                result['item_codes'].extend(codes)
        
        # Single item or other format
        else:
            codes = self._extract_single_item_codes(item_info)
            result['item_codes'].extend(codes)
        
        # Remove duplicates while preserving order
        result['item_codes'] = list(dict.fromkeys(result['item_codes']))
        result['section_codes'] = list(dict.fromkeys(result['section_codes']))
        
        return result
    
    def _extract_single_item_codes(self, text: str) -> List[str]:
        """Extract item codes from a single text segment."""
        codes = []
        
        # Pattern for item codes: letter-number, letter.number, or just number-number
        # Handles: E-1, E1, E.-1., E.1, 2-1, etc.
        patterns = [
            r'([A-Z])\.?\-?(\d+)\.?',  # Letter-based items
            r'(\d+)\-(\d+)'             # Number-only items like 2-1
        ]
        
        for pattern in patterns:
            for match in re.finditer(pattern, text):
                if pattern.startswith('(\\d'):  # Number-only pattern
                    # For number-only, just use as is
                    codes.append(f"{match.group(1)}-{match.group(2)}")
                else:
                    # For letter-number format
                    letter = match.group(1)
                    number = match.group(2)
                    codes.append(f"{letter}-{number}")
        
        return codes
    
    def _normalize_item_code(self, code: str) -> str:
        """Normalize item code to consistent format (e.g., E-1, 2-1)."""
        # Remove dots and ensure dash format
        code = code.strip('. ')
        
        # Pattern: letter followed by optional punctuation and number
        letter_match = re.match(r'^([A-Z])\.?\-?(\d+)\.?$', code)
        if letter_match:
            letter = letter_match.group(1)
            number = letter_match.group(2)
            return f"{letter}-{number}"
        
        # Pattern: number-number format
        number_match = re.match(r'^(\d+)\-(\d+)$', code)
        if number_match:
            return code  # Already in correct format
        
        return code
    
    def _determine_transcript_type(self, item_info: str, parsed_items: Dict) -> str:
        """Determine the type of transcript based on parsed information."""
        if 'PUBLIC_COMMENT' in parsed_items['item_codes']:
            return 'public_comment'
        elif parsed_items['section_codes']:
            return 'section'
        elif len(parsed_items['item_codes']) > 3:
            return 'multi_item'
        elif len(parsed_items['item_codes']) == 1:
            return 'single_item'
        else:
            return 'item_group'
    
    def _save_linking_report(self, meeting_date: str, linked_transcripts: Dict):
        """Save detailed report of transcript linking."""
        report = {
            "meeting_date": meeting_date,
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total_transcripts": sum(len(v) for v in linked_transcripts.values()),
                "item_transcripts": len(linked_transcripts["item_transcripts"]),
                "public_comments": len(linked_transcripts["public_comments"]),
                "section_transcripts": len(linked_transcripts["section_transcripts"])
            },
            "transcripts": linked_transcripts
        }
        
        report_filename = f"verbatim_linking_report_{meeting_date.replace('.', '_')}.json"
        report_path = self.debug_dir / report_filename
        
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        log.info(f"ðŸ“Š Verbatim linking report saved to: {report_path}")
    
    def _validate_meeting_date(self, meeting_date: str) -> bool:
        """Validate meeting date format MM.DD.YYYY"""
        return bool(re.match(r'^\d{2}\.\d{2}\.\d{4}$', meeting_date))

    def _save_extracted_text(self, transcript_path: Path, transcript_info: Dict[str, Any]):
        """Save extracted transcript text to JSON for GraphRAG processing."""
        output_dir = Path("city_clerk_documents/extracted_text")
        output_dir.mkdir(exist_ok=True)
        
        # Create filename based on transcript info
        meeting_date = transcript_info['meeting_date'].replace('.', '_')
        item_info_clean = re.sub(r'[^a-zA-Z0-9-]', '_', transcript_info['item_info_raw'])
        output_filename = f"verbatim_{meeting_date}_{item_info_clean}_extracted.json"
        output_path = output_dir / output_filename
        
        # Prepare data for saving
        save_data = {
            "document_type": "verbatim_transcript",
            "meeting_date": transcript_info['meeting_date'],
            "item_codes": transcript_info['item_codes'],
            "section_codes": transcript_info['section_codes'],
            "transcript_type": transcript_info['transcript_type'],
            "full_text": transcript_info['full_text'],
            "pages": transcript_info['pages'],
            "metadata": {
                "filename": transcript_info['filename'],
                "item_info_raw": transcript_info['item_info_raw'],
                "page_count": transcript_info['page_count'],
                "extraction_method": "docling_ocr",
                "extracted_at": datetime.now().isoformat()
            }
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, indent=2, ensure_ascii=False)
        
        log.info(f"ðŸ’¾ Saved transcript text to: {output_path}")
        
        # NEW: Also save as markdown
        self._save_transcript_as_markdown(transcript_path, transcript_info, output_dir)

    def _save_transcript_as_markdown(self, transcript_path: Path, transcript_info: Dict[str, Any], output_dir: Path):
        """Save transcript as markdown with metadata header."""
        markdown_dir = output_dir.parent / "extracted_markdown"
        markdown_dir.mkdir(exist_ok=True)
        
        # Build header
        items_str = ', '.join(transcript_info['item_codes']) if transcript_info['item_codes'] else 'N/A'
        
        header = f"""---
DOCUMENT METADATA AND CONTEXT
=============================

**DOCUMENT IDENTIFICATION:**
- Full Path: Verbatim Items/{transcript_info['meeting_date'].split('.')[2]}/{transcript_path.name}
- Document Type: VERBATIM_TRANSCRIPT
- Filename: {transcript_path.name}

**PARSED INFORMATION:**
- Meeting Date: {transcript_info['meeting_date']}
- Agenda Items Discussed: {items_str}
- Transcript Type: {transcript_info['transcript_type']}
- Page Count: {transcript_info['page_count']}

**SEARCHABLE IDENTIFIERS:**
- MEETING_DATE: {transcript_info['meeting_date']}
- DOCUMENT_TYPE: VERBATIM_TRANSCRIPT
{self._format_item_identifiers(transcript_info['item_codes'])}

**NATURAL LANGUAGE DESCRIPTION:**
This is the verbatim transcript from the {transcript_info['meeting_date']} City Commission meeting covering the discussion of {self._describe_items(transcript_info)}.

**QUERY HELPERS:**
{self._build_transcript_query_helpers(transcript_info)}

---

{self._build_item_questions(transcript_info['item_codes'])}

# VERBATIM TRANSCRIPT CONTENT
"""
        
        # Combine with text
        full_content = header + "\n\n" + transcript_info.get('full_text', '')
        
        # Save file
        meeting_date = transcript_info['meeting_date'].replace('.', '_')
        item_info_clean = re.sub(r'[^a-zA-Z0-9-]', '_', transcript_info['item_info_raw'])
        md_filename = f"verbatim_{meeting_date}_{item_info_clean}.md"
        md_path = markdown_dir / md_filename
        
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(full_content)
        
        log.info(f"ðŸ“ Saved transcript markdown to: {md_path}")

    def _format_item_identifiers(self, item_codes: List[str]) -> str:
        """Format agenda items as searchable identifiers."""
        lines = []
        for item in item_codes:
            lines.append(f"- AGENDA_ITEM: {item}")
        return '\n'.join(lines)

    def _describe_items(self, transcript_info: Dict) -> str:
        """Create natural language description of items."""
        if transcript_info['item_codes']:
            if len(transcript_info['item_codes']) == 1:
                return f"agenda item {transcript_info['item_codes'][0]}"
            else:
                return f"agenda items {', '.join(transcript_info['item_codes'])}"
        elif 'PUBLIC_COMMENT' in transcript_info.get('section_codes', []):
            return "public comments section"
        else:
            return "the meeting proceedings"

    def _build_transcript_query_helpers(self, transcript_info: Dict) -> str:
        """Build query helpers for transcripts."""
        helpers = []
        for item in transcript_info.get('item_codes', []):
            helpers.append(f"- To find discussion about {item}, search for 'Item {item}' or '{item} discussion'")
        helpers.append(f"- To find all discussions from this meeting, search for '{transcript_info['meeting_date']}'")
        helpers.append("- This transcript contains the exact words spoken during the meeting")
        return '\n'.join(helpers)

    def _build_item_questions(self, item_codes: List[str]) -> str:
        """Build Q&A style entries for items."""
        questions = []
        for item in item_codes:
            questions.append(f"## What was discussed about Item {item}?")
            questions.append(f"The discussion of Item {item} is transcribed in this document.\n")
        return '\n'.join(questions)


================================================================================


################################################################################
# File: scripts/extract_all_to_markdown.py
################################################################################

# File: scripts/extract_all_to_markdown.py

#!/usr/bin/env python3
"""
Extract all PDFs to markdown format for GraphRAG processing.
"""

import asyncio
from pathlib import Path
import logging
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import os

from graph_stages.pdf_extractor import PDFExtractor
from graph_stages.agenda_pdf_extractor import AgendaPDFExtractor
from graph_stages.enhanced_document_linker import EnhancedDocumentLinker
from graph_stages.verbatim_transcript_linker import VerbatimTranscriptLinker

logging.basicConfig(level=logging.INFO)
log = logging.getLogger(__name__)

async def extract_all_documents():
    """Extract all city clerk documents with parallel processing."""
    
    base_dir = Path("city_clerk_documents/global/City Comissions 2024")
    markdown_dir = Path("city_clerk_documents/extracted_markdown")
    markdown_dir.mkdir(exist_ok=True)
    
    if not base_dir.exists():
        log.error(f"âŒ Base directory not found: {base_dir}")
        log.error(f"   Current working directory: {Path.cwd()}")
        return
    
    log.info(f"ðŸ“ Base directory found: {base_dir}")
    
    # Increase max workers based on system capabilities
    max_workers = min(os.cpu_count() * 2, 16)  # Increased from min(cpu_count, 8)
    
    stats = {
        'agendas': 0,
        'ordinances': 0,
        'resolutions': 0,
        'transcripts': 0,
        'errors': 0
    }
    
    # Process Agendas in parallel
    log.info(f"ðŸ“‹ Extracting Agendas with {max_workers} workers...")
    agenda_dir = base_dir / "Agendas"
    if agenda_dir.exists():
        log.info(f"   Found agenda directory: {agenda_dir}")
        extractor = AgendaPDFExtractor()
        agenda_pdfs = list(agenda_dir.glob("*.pdf"))
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_pdf = {
                executor.submit(process_agenda_pdf, extractor, pdf): pdf 
                for pdf in agenda_pdfs
            }
            
            for future in as_completed(future_to_pdf):
                pdf = future_to_pdf[future]
                try:
                    success = future.result()
                    if success:
                        stats['agendas'] += 1
                except Exception as e:
                    log.error(f"Failed to extract {pdf}: {e}")
                    stats['errors'] += 1
    else:
        log.warning(f"âš ï¸  Agenda directory not found: {agenda_dir}")
    
    # Process Ordinances and Resolutions in parallel
    log.info("ðŸ“œ Extracting Ordinances and Resolutions in parallel...")
    ord_dir = base_dir / "Ordinances"
    res_dir = base_dir / "Resolutions"
    
    if ord_dir.exists() and res_dir.exists():
        linker = EnhancedDocumentLinker()
        
        # Combine ordinances and resolutions for parallel processing
        all_docs = []
        for pdf in ord_dir.rglob("*.pdf"):
            all_docs.append(('ordinance', pdf))
        for pdf in res_dir.rglob("*.pdf"):
            all_docs.append(('resolution', pdf))
        
        # Process in parallel with asyncio
        async def process_documents_batch(docs_batch):
            tasks = []
            for doc_type, pdf_path in docs_batch:
                meeting_date = extract_meeting_date_from_filename(pdf_path.name)
                if meeting_date:
                    task = process_document_async(linker, pdf_path, meeting_date, doc_type)
                    tasks.append(task)
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            return results
        
        # Process in batches to avoid overwhelming the system
        batch_size = max_workers * 2
        for i in range(0, len(all_docs), batch_size):
            batch = all_docs[i:i + batch_size]
            results = await process_documents_batch(batch)
            
            for result, (doc_type, _) in zip(results, batch):
                if isinstance(result, Exception):
                    stats['errors'] += 1
                    log.error(f"Error: {result}")
                elif result:
                    stats['ordinances' if doc_type == 'ordinance' else 'resolutions'] += 1
    else:
        log.warning(f"âš ï¸  Ordinances or Resolutions directory not found: {ord_dir}, {res_dir}")
    
    log.info("ðŸŽ¤ Extracting Verbatim Transcripts...")
    verbatim_dirs = [
        base_dir / "Verbatim Items",
        base_dir / "Verbating Items"
    ]
    
    verbatim_dir = None
    for vdir in verbatim_dirs:
        if vdir.exists():
            verbatim_dir = vdir
            break
    
    if verbatim_dir:
        log.info(f"   Found verbatim directory: {verbatim_dir}")
        transcript_linker = VerbatimTranscriptLinker()
        
        all_verb_pdfs = list(verbatim_dir.rglob("*.pdf"))
        log.info(f"   Found {len(all_verb_pdfs)} verbatim PDFs")
        
        # Process verbatim transcripts in parallel batches
        batch_size = max_workers
        for i in range(0, len(all_verb_pdfs), batch_size):
            batch = all_verb_pdfs[i:i + batch_size]
            
            tasks = []
            for pdf_path in batch:
                meeting_date = extract_meeting_date_from_verbatim(pdf_path.name)
                if meeting_date:
                    task = process_verbatim_async(transcript_linker, pdf_path, meeting_date)
                    tasks.append(task)
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in results:
                if isinstance(result, Exception):
                    stats['errors'] += 1
                    log.error(f"Error: {result}")
                elif result:
                    stats['transcripts'] += 1
    else:
        log.warning(f"âš ï¸  Verbatim directory not found. Tried: {verbatim_dirs}")
    
    log.info("\nðŸ“Š Extraction Summary:")
    log.info(f"   Agendas: {stats['agendas']}")
    log.info(f"   Ordinances: {stats['ordinances']}")
    log.info(f"   Resolutions: {stats['resolutions']}")
    log.info(f"   Transcripts: {stats['transcripts']}")
    log.info(f"   Errors: {stats['errors']}")
    log.info(f"   Total: {sum(stats.values()) - stats['errors']}")
    
    log.info(f"\nâœ… All documents extracted to:")
    log.info(f"   JSON: city_clerk_documents/extracted_text/")
    log.info(f"   Markdown: {markdown_dir}")

def process_agenda_pdf(extractor, pdf):
    """Process single agenda PDF (for thread pool)."""
    try:
        log.info(f"   Processing: {pdf.name}")
        agenda_data = extractor.extract_agenda(pdf)
        output_path = Path("city_clerk_documents/extracted_text") / f"{pdf.stem}_extracted.json"
        extractor.save_extracted_agenda(agenda_data, output_path)
        return True
    except Exception as e:
        log.error(f"Failed to extract {pdf}: {e}")
        return False

async def process_document_async(linker, pdf_path, meeting_date, doc_type):
    """Process document asynchronously."""
    try:
        doc_info = await linker._process_document(pdf_path, meeting_date, doc_type)
        if doc_info:
            linker._save_extracted_text(pdf_path, doc_info, doc_type)
            return True
        return False
    except Exception as e:
        raise e

# Add async helper
async def process_verbatim_async(transcript_linker, pdf_path, meeting_date):
    """Process verbatim transcript asynchronously."""
    try:
        transcript_info = await transcript_linker._process_transcript(pdf_path, meeting_date)
        if transcript_info:
            transcript_linker._save_extracted_text(pdf_path, transcript_info)
            return True
        return False
    except Exception as e:
        raise e

def extract_meeting_date_from_filename(filename: str) -> str:
    """Extract meeting date from ordinance/resolution filename."""
    import re
    
    match = re.search(r'(\d{2})_(\d{2})_(\d{4})', filename)
    if match:
        month, day, year = match.groups()
        return f"{month}.{day}.{year}"
    
    return None

def extract_meeting_date_from_verbatim(filename: str) -> str:
    """Extract meeting date from verbatim transcript filename."""
    import re
    
    match = re.match(r'(\d{2})_(\d{2})_(\d{4})', filename)
    if match:
        month, day, year = match.groups()
        return f"{month}.{day}.{year}"
    
    return None

if __name__ == "__main__":
    asyncio.run(extract_all_documents())


================================================================================


################################################################################
# File: scripts/microsoft_framework/prompt_tuner.py
################################################################################

# File: scripts/microsoft_framework/prompt_tuner.py

import subprocess
import sys
import os
from pathlib import Path
import shutil

class CityClerkPromptTuner:
    """Auto-tune GraphRAG prompts for city clerk documents."""
    
    def __init__(self, graphrag_root: Path):
        self.graphrag_root = Path(graphrag_root)
        self.prompts_dir = self.graphrag_root / "prompts"
        
    def run_auto_tuning(self):
        """Run GraphRAG auto-tuning for city clerk domain."""
        
        # First, ensure prompts directory is clean
        if self.prompts_dir.exists():
            import shutil
            shutil.rmtree(self.prompts_dir)
        self.prompts_dir.mkdir(parents=True, exist_ok=True)
        
        # Create domain-specific examples file
        examples_file = self.graphrag_root / "domain_examples.txt"
        with open(examples_file, 'w') as f:
            f.write("""
Examples of entities in city clerk documents:

AGENDA_ITEM: E-1, F-10, H-3 (format: letter-number identifying agenda items)
ORDINANCE: 2024-01, 2024-15 (format: year-number for city ordinances)  
RESOLUTION: 2024-123, 2024-45 (format: year-number for city resolutions)
MEETING: January 9, 2024 City Commission Meeting
PERSON: Commissioner Smith, Mayor Johnson
ORGANIZATION: City of Coral Gables, Parks Department
MONEY: $1.5 million, $250,000
PROJECT: Waterfront Development, Parks Renovation

Example text:
"Agenda Item E-1 relates to Ordinance 2024-01 regarding the Cocoplum security district."
"Commissioner Smith moved to approve Resolution 2024-15 for $1.5 million funding."
""")
        
        # Get the correct Python executable
        python_exe = self.get_venv_python()
        print(f"ðŸ Using Python: {python_exe}")
        
        # Run tuning with correct arguments
        cmd = [
            python_exe,
            "-m", "graphrag", "prompt-tune",
            "--root", str(self.graphrag_root),
            "--config", str(self.graphrag_root / "settings.yaml"),
            "--domain", "city government meetings, ordinances, resolutions, agenda items like E-1 and F-10",
            "--selection-method", "random",
            "--limit", "50",
            "--language", "English",
            "--max-tokens", "2000",
            "--chunk-size", "1200",
            "--output", str(self.prompts_dir)
        ]
        
        # Note: --examples flag might not exist in this version
        # Remove it if it causes issues
        
        subprocess.run(cmd, check=True)
        
    def get_venv_python(self):
        """Get the correct Python executable."""
        # Check if we're in a venv
        if sys.prefix != sys.base_prefix:
            return sys.executable
        
        # Try common venv locations
        venv_paths = [
            'venv/bin/python3',
            'venv/bin/python',
            '.venv/bin/python3',
            '.venv/bin/python',
            'city_clerk_rag/bin/python3',
            'city_clerk_rag/bin/python'
        ]
        
        for venv_path in venv_paths:
            full_path = os.path.join(os.getcwd(), venv_path)
            if os.path.exists(full_path):
                return full_path
        
        # Fallback
        return sys.executable
        
    def create_manual_prompts(self):
        """Create prompts manually without auto-tuning."""
        self.prompts_dir.mkdir(parents=True, exist_ok=True)
        
        # Entity extraction prompt - GraphRAG expects specific format
        entity_prompt = """
-Goal-
Given a text document from the City of Coral Gables, identify all entities and their relationships with high precision, following strict context rules.

**CRITICAL RULES FOR CONTEXT AND IDENTITY:**
1.  **Strict Association**: When you extract an entity, its description and relationships MUST come from the immediate surrounding text ONLY.
2.  **Detect Aliases and Identity**: If the text states or strongly implies that two different identifiers (e.g., "Agenda Item E-1" and "Ordinance 2024-01") refer to the SAME underlying legislative action, you MUST create a relationship between them.
    *   **Action**: Create both entities (e.g., `AGENDA_ITEM:E-1` and `ORDINANCE:2024-01`).
    *   **Relationship**: Link them with a relationship like `("relationship"<|>E-1<|>2024-01<|>is the same legislative action<|>10)`.
    *   **Description**: The descriptions for both entities should be consistent and reflect their shared identity.

-Steps-
1. Identify all entities. For each identified entity, extract the following information:
- title: Name of the entity, capitalized
- type: One of the following types: [{entity_types}]
- description: Comprehensive description of the entity's attributes and activities, **based ONLY on the immediate context and aliasing rules**.
Format each entity as ("entity"<|><title><|><type><|><description>)

2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly and directly related* to each other in the immediate text.
For each pair of related entities, extract the following information:
- source: name of the source entity, as identified in step 1
- target: name of the target entity, as identified in step 1
- description: explanation as to why you think the source entity and the target entity are related to each other, **citing direct evidence from the text**.
- weight: a numeric score indicating strength of the relationship between the source entity and target entity
Format each relationship as ("relationship"<|><source><|><target><|><description><|><weight>)

3. Return output in English as a single list of all the entities and relationships identified in steps 1 and 2. Use **{record_delimiter}** as the list delimiter.

4. When finished, output {completion_delimiter}

-Examples-
Entity types: AGENDA_ITEM, ORDINANCE, PERSON, ORGANIZATION
Text: "Agenda Item E-1, an ordinance to amend zoning, was passed. This is Ordinance 2024-01."
Output:
("entity"<|>E-1<|>AGENDA_ITEM<|>Agenda item E-1, which is the same as Ordinance 2024-01 and amends zoning.)
{record_delimiter}
("entity"<|>2024-01<|>ORDINANCE<|>Ordinance 2024-01, also known as agenda item E-1, which amends zoning.)
{record_delimiter}
("relationship"<|>E-1<|>2024-01<|>is the same as<|>10)
{completion_delimiter}

-Real Data-
Entity types: {entity_types}
Text: {input_text}
Output:
"""
        
        with open(self.prompts_dir / "entity_extraction.txt", 'w') as f:
            f.write(entity_prompt)
        
        # Community report prompt
        community_prompt = """
You are analyzing a community of related entities from city government documents.
Provide a comprehensive summary of the community, focusing on:
1. Key entities and their roles
2. Main relationships and interactions
3. Important decisions or actions
4. Overall significance to city governance

Community data:
{input_text}

Summary:
"""
        
        with open(self.prompts_dir / "community_report.txt", 'w') as f:
            f.write(community_prompt)
        
        print("âœ… Created manual prompts with GraphRAG format")
        
    def customize_prompts(self):
        """Further customize prompts for city clerk specifics."""
        # Load and modify entity extraction prompt
        entity_prompt_path = self.prompts_dir / "entity_extraction.txt"
        
        if entity_prompt_path.exists():
            with open(entity_prompt_path, 'r') as f:
                prompt = f.read()
            
            # Add city clerk specific examples
            custom_additions = """
### City Clerk Specific Instructions:
- Pay special attention to agenda item codes (e.g., E-1, F-10, H-3)
- Extract voting records (who voted yes/no on what)
- Identify ordinance and resolution numbers (e.g., 2024-01, Resolution 2024-123)
- Extract budget amounts and financial figures
- Identify project names and development proposals
- Note public comment speakers and their concerns
"""
            
            # Insert custom additions
            prompt = prompt.replace("-Real Data-", custom_additions + "\n-Real Data-")
            
            with open(entity_prompt_path, 'w') as f:
                f.write(prompt)


================================================================================


################################################################################
# File: scripts/graph_stages/pdf_extractor.py
################################################################################

# scripts/graph_stages/pdf_extractor.py

import os
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import logging
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.datamodel.base_models import InputFormat, DocumentStream
from docling.datamodel.pipeline_options import PdfPipelineOptions
import json
from io import BytesIO

# Set up logging
logging.basicConfig(level=logging.INFO)
log = logging.getLogger(__name__)

class PDFExtractor:
    """Extract text from PDFs using Docling for accurate OCR and text extraction."""
    
    def __init__(self, pdf_dir: Path, output_dir: Optional[Path] = None):
        """Initialize the PDF extractor with Docling."""
        self.pdf_dir = Path(pdf_dir)
        self.output_dir = output_dir or Path("city_clerk_documents/extracted_text")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Create debug directory
        self.debug_dir = self.output_dir / "debug"
        self.debug_dir.mkdir(exist_ok=True)
        
        # Initialize Docling converter with OCR enabled
        # Configure for better OCR and structure detection
        pipeline_options = PdfPipelineOptions()
        pipeline_options.do_ocr = True  # Enable OCR for better text extraction
        pipeline_options.do_table_structure = True  # Better table extraction
        
        self.converter = DocumentConverter(
            format_options={
                InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
            }
        )
    
    def extract_text_from_pdf(self, pdf_path: Path) -> Tuple[str, List[Dict[str, str]]]:
        """
        Extract text from PDF using Docling with OCR support.
        
        Returns:
            Tuple of (full_text, pages) where pages is a list of dicts with 'text' and 'page_num'
        """
        log.info(f"ðŸ“„ Extracting text from: {pdf_path.name}")
        
        # Convert PDF with Docling - just pass the path directly
        result = self.converter.convert(str(pdf_path))
        
        # Get the document
        doc = result.document
        
        # Get the markdown representation which preserves structure better
        full_markdown = doc.export_to_markdown()
        
        # Extract pages if available
        pages = []
        
        # Try to extract page-level content from the document structure
        if hasattr(doc, 'pages') and doc.pages:
            for page_num, page in enumerate(doc.pages, 1):
                # Get page text
                page_text = ""
                if hasattr(page, 'text'):
                    page_text = page.text
                elif hasattr(page, 'get_text'):
                    page_text = page.get_text()
                else:
                    # Try to extract from elements
                    page_elements = []
                    if hasattr(page, 'elements'):
                        for element in page.elements:
                            if hasattr(element, 'text'):
                                page_elements.append(element.text)
                    page_text = "\n".join(page_elements)
                
                if page_text:
                    pages.append({
                        'text': page_text,
                        'page_num': page_num
                    })
        
        # If we couldn't extract pages, create one page with all content
        if not pages and full_markdown:
            pages = [{
                'text': full_markdown,
                'page_num': 1
            }]
        
        # Use markdown as the full text (better structure preservation)
        full_text = full_markdown if full_markdown else ""
        
        # Save debug information
        debug_info = {
            'file': pdf_path.name,
            'total_pages': len(pages),
            'total_characters': len(full_text),
            'extraction_method': 'docling',
            'has_markdown': bool(full_markdown),
            'doc_attributes': list(dir(doc)) if doc else []
        }
        
        debug_file = self.debug_dir / f"{pdf_path.stem}_extraction_debug.json"
        with open(debug_file, 'w', encoding='utf-8') as f:
            json.dump(debug_info, f, indent=2)
        
        # Save the full extracted text for inspection
        text_file = self.debug_dir / f"{pdf_path.stem}_full_text.txt"
        with open(text_file, 'w', encoding='utf-8') as f:
            f.write(full_text)
        
        # Save markdown version separately for debugging
        if full_markdown:
            markdown_file = self.debug_dir / f"{pdf_path.stem}_markdown.md"
            with open(markdown_file, 'w', encoding='utf-8') as f:
                f.write(full_markdown)
        
        log.info(f"âœ… Extracted {len(pages)} pages, {len(full_text)} characters")
        
        return full_text, pages
    
    def extract_all_pdfs(self) -> Dict[str, Dict[str, any]]:
        """Extract text from all PDFs in the directory."""
        pdf_files = list(self.pdf_dir.glob("*.pdf"))
        log.info(f"ðŸ“š Found {len(pdf_files)} PDF files to process")
        
        extracted_data = {}
        
        for pdf_path in pdf_files:
            try:
                full_text, pages = self.extract_text_from_pdf(pdf_path)
                
                extracted_data[pdf_path.stem] = {
                    'full_text': full_text,
                    'pages': pages,
                    'metadata': {
                        'filename': pdf_path.name,
                        'num_pages': len(pages),
                        'total_chars': len(full_text),
                        'extraction_method': 'docling'
                    }
                }
                
                # Save extracted text
                output_file = self.output_dir / f"{pdf_path.stem}_extracted.json"
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(extracted_data[pdf_path.stem], f, indent=2, ensure_ascii=False)
                
                log.info(f"âœ… Saved extracted text to: {output_file}")
                
            except Exception as e:
                log.error(f"âŒ Failed to process {pdf_path.name}: {e}")
                import traceback
                traceback.print_exc()
                # No fallback - if Docling fails, we fail
                raise
        
        return extracted_data

# Add this to requirements.txt:
# unstructured[pdf]==0.10.30
# pytesseract>=0.3.10
# pdf2image>=1.16.3
# poppler-utils (system dependency - install with: brew install poppler)


================================================================================


################################################################################
# File: scripts/microsoft_framework/source_tracker.py
################################################################################

# File: scripts/microsoft_framework/source_tracker.py

"""Source tracking component for GraphRAG queries."""

from typing import Dict, List, Any, Set, Tuple
import logging

logger = logging.getLogger(__name__)

class SourceTracker:
    """Track sources used during GraphRAG queries."""
    
    def __init__(self):
        self.reset()
    
    def reset(self):
        """Reset tracking state."""
        self.entities_used: Dict[int, Dict[str, Any]] = {}
        self.relationships_used: Dict[int, Dict[str, Any]] = {}
        self.sources_used: Dict[int, Dict[str, Any]] = {}
        self.text_units_used: Dict[int, Dict[str, Any]] = {}
        self.communities_used: Dict[int, Dict[str, Any]] = {}
    
    def track_entity(self, entity_id: int, entity_data: Dict[str, Any]):
        """Track an entity being used."""
        self.entities_used[entity_id] = {
            'id': entity_id,
            'title': entity_data.get('title', 'Unknown'),
            'type': entity_data.get('type', 'Unknown'),
            'description': entity_data.get('description', '')[:200],
            'source_id': entity_data.get('source_id', '')
        }
    
    def track_relationship(self, rel_id: int, rel_data: Dict[str, Any]):
        """Track a relationship being used."""
        self.relationships_used[rel_id] = {
            'id': rel_id,
            'source': rel_data.get('source', ''),
            'target': rel_data.get('target', ''),
            'description': rel_data.get('description', '')[:200],
            'weight': rel_data.get('weight', 0)
        }
    
    def track_source(self, source_id: int, source_data: Dict[str, Any]):
        """Track a source document being used."""
        self.sources_used[source_id] = {
            'id': source_id,
            'title': source_data.get('title', 'Unknown'),
            'type': source_data.get('document_type', 'document'),
            'file': source_data.get('source_file', '')
        }
    
    def get_summary(self) -> Dict[str, Any]:
        """Get summary of all tracked sources."""
        return {
            'entities': list(self.entities_used.values()),
            'relationships': list(self.relationships_used.values()),
            'sources': list(self.sources_used.values()),
            'text_units': list(self.text_units_used.values()),
            'communities': list(self.communities_used.values())
        }
    
    def get_citation_map(self) -> Dict[str, List[int]]:
        """Get a map of content to source IDs for citations."""
        return {
            'entity_ids': list(self.entities_used.keys()),
            'relationship_ids': list(self.relationships_used.keys()),
            'source_ids': list(self.sources_used.keys())
        }


================================================================================


################################################################################
# File: scripts/microsoft_framework/incremental_processor.py
################################################################################

# File: scripts/microsoft_framework/incremental_processor.py

from pathlib import Path
import json
from typing import List, Set
import asyncio

class IncrementalGraphRAGProcessor:
    """Handle incremental updates to GraphRAG index."""
    
    def __init__(self, graphrag_root: Path):
        self.graphrag_root = Path(graphrag_root)
        self.processed_files = self._load_processed_files()
        
    async def process_new_documents(self, new_docs_dir: Path):
        """Process only new documents added since last run."""
        
        # Find new documents
        new_files = []
        for doc_path in new_docs_dir.glob("*.pdf"):
            if doc_path.name not in self.processed_files:
                new_files.append(doc_path)
        
        if not new_files:
            print("âœ… No new documents to process")
            return
        
        print(f"ðŸ“„ Processing {len(new_files)} new documents...")
        
        # Extract text using existing Docling pipeline
        from scripts.graph_stages.pdf_extractor import PDFExtractor
        extractor = PDFExtractor(new_docs_dir)
        
        # Process and add to GraphRAG
        # ... implementation details ...
        
        # Update processed files list
        self._update_processed_files(new_files)
    
    def _load_processed_files(self) -> Set[str]:
        """Load list of previously processed files."""
        processed_files_path = self.graphrag_root / "processed_files.json"
        
        if processed_files_path.exists():
            with open(processed_files_path, 'r') as f:
                return set(json.load(f))
        
        return set()
    
    def _update_processed_files(self, new_files: List[Path]):
        """Update the list of processed files."""
        for file_path in new_files:
            self.processed_files.add(file_path.name)
        
        processed_files_path = self.graphrag_root / "processed_files.json"
        with open(processed_files_path, 'w') as f:
            json.dump(list(self.processed_files), f)


================================================================================


################################################################################
# File: requirements.txt
################################################################################

################################################################################
################################################################################
# Python dependencies for Misophonia Research RAG System

# Core dependencies
openai>=1.82.0
groq>=0.4.1
gremlinpython>=3.7.3
aiofiles>=24.1.0
python-dotenv>=1.0.0

# Graph Visualization dependencies
dash>=2.14.0
plotly>=5.17.0
networkx>=3.2.0
dash-table>=5.0.0
dash-cytoscape>=0.3.0
dash-bootstrap-components>=1.0.0

# Database and API dependencies
supabase>=2.0.0

# GraphRAG dependencies
graphrag==2.3.0
pyyaml>=6.0.0
pyarrow>=14.0.0
scipy>=1.11.0

# Optional for async progress bars
tqdm

# Data processing
pandas>=2.0.0
numpy

# PDF processing
PyPDF2>=3.0.0
unstructured[pdf]==0.10.30
pytesseract>=0.3.10
pdf2image>=1.16.3
docling
pdfminer.six>=20221105

# PDF processing with hyperlink support
PyMuPDF>=1.23.0

# Utilities
colorama>=0.4.6

# For concurrent processing
concurrent-log-handler>=0.9.20

# Async dependencies for optimized pipeline
aiohttp>=3.8.0

# Token counting for OpenAI rate limiting
tiktoken>=0.5.0

# Enhanced deduplication dependencies
python-Levenshtein>=0.20.0
scikit-learn>=1.3.0


================================================================================

