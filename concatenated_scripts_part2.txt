# Concatenated Project Code - Part 2 of 3
# Generated: 2025-06-03 11:11:21
# Root Directory: /Users/gianmariatroiani/Documents/knologi̊/graph_database
================================================================================

# File Index - Which Files Are in Which Parts
################################################################################

## Part 1 (12 files):
  - scripts/graph_stages/agenda_graph_builder.py
  - scripts/stages/chunk_text.py
  - scripts/graph_stages/enhanced_document_linker.py
  - scripts/graph_stages/agenda_pdf_extractor.py
  - scripts/stages/db_upsert.py
  - city_clerk_documents/graph_json/debug/resolution_debug_report.json
  - scripts/find_duplicates.py
  - city_clerk_documents/graph_json/debug/enhanced_linked_documents.json
  - scripts/check_pipeline_setup.py
  - config.py
  - scripts/debug_pipeline_output.py
  - requirements.txt

## Part 2 (14 files):
  - scripts/stages/embed_vectors.py
  - scripts/stages/extract_clean.py
  - scripts/graph_pipeline.py
  - scripts/debug_missing_resolutions.py
  - scripts/pipeline_modular_optimized.py
  - scripts/graph_stages/pdf_extractor.py
  - scripts/stages/llm_enrich.py
  - scripts/topic_filter_and_title.py
  - scripts/debug_ordinance_2024_02.py
  - scripts/debug_agenda_items.py
  - scripts/test_graph_pipeline.py
  - scripts/clear_database.py
  - graph_clear_database.py
  - scripts/graph_stages/__init__.py

## Part 3 (14 files):
  - scripts/graph_stages/ontology_extractor.py
  - scripts/graph_stages/agenda_ontology_extractor.py
  - scripts/rag_local_web_app.py
  - scripts/graph_stages/document_linker.py
  - scripts/graph_stages/cosmos_db_client.py
  - scripts/process_resolutions.py
  - scripts/supabase_clear_database.py
  - scripts/test_vector_search.py
  - scripts/stages/acceleration_utils.py
  - city_clerk_documents/graph_json/debug/linking_report_01_09_2024.json
  - scripts/clear_meeting_data.py
  - scripts/clear_database_sync.py
  - city_clerk_documents/graph_json/debug/linked_documents.json
  - scripts/stages/__init__.py


================================================================================


################################################################################
# File: scripts/stages/embed_vectors.py
################################################################################

# File: scripts/stages/embed_vectors.py

#!/usr/bin/env python3
"""
Stage 7 — Optimized embedding with rate limiting, deduplication, and conservative batching.
"""
from __future__ import annotations
import argparse, json, logging, os, sys, time
from datetime import datetime
from typing import Any, Dict, List, Optional, Set
import asyncio
import aiohttp
from dotenv import load_dotenv
from openai import OpenAI
from supabase import create_client
from tqdm import tqdm
from tqdm.asyncio import tqdm as async_tqdm
import hashlib

# Try to import tiktoken for accurate token counting
try:
    import tiktoken
    TIKTOKEN_AVAILABLE = True
except ImportError:
    TIKTOKEN_AVAILABLE = False
    logging.warning("tiktoken not available - using conservative token estimation")

load_dotenv()
SUPABASE_URL  = os.getenv("SUPABASE_URL")
SUPABASE_KEY  = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
OPENAI_API_KEY= os.getenv("OPENAI_API_KEY")
EMBEDDING_MODEL="text-embedding-ada-002"
MODEL_TOKEN_LIMIT=8192
TOKEN_GUARD=200

# 🎯 DYNAMIC BATCHING - Token limits with safety margins
MAX_BATCH_TOKENS = 7692  # Conservative limit (8192 - 500 safety margin)
MAX_CHUNK_TOKENS = 6000  # Individual chunk limit
MIN_BATCH_TOKENS = 100   # Minimum viable batch size

MAX_TOTAL_TOKENS=MAX_BATCH_TOKENS  # Use dynamic batch limit instead

# Conservative defaults for rate limiting
DEFAULT_BATCH_ROWS=200  # Reduced from 5000
DEFAULT_COMMIT_ROWS=10  # Reduced from 100
DEFAULT_MAX_CONCURRENT=3  # Reduced from 50
MAX_RETRIES=5
RETRY_DELAY=2
RATE_LIMIT_DELAY=0.3  # 300ms between API calls
MAX_CALLS_PER_MINUTE=150  # Conservative limit

openai_client=OpenAI(api_key=OPENAI_API_KEY)
logging.basicConfig(level=logging.INFO,
    format="%(asctime)s — %(levelname)s — %(message)s")
log=logging.getLogger(__name__)

# Global variables for rate limiting
call_timestamps = []
total_tokens_used = 0

class AsyncEmbedder:
    """Async embedding client with strict rate limiting and connection pooling."""
    
    def __init__(self, api_key: str, max_concurrent: int = DEFAULT_MAX_CONCURRENT):
        self.api_key = api_key
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.session = None
        self.call_count = 0
        self.start_time = time.time()
        
        # Initialize tiktoken encoder if available
        if TIKTOKEN_AVAILABLE:
            self.encoder = tiktoken.encoding_for_model(EMBEDDING_MODEL)
        else:
            self.encoder = None
    
    async def __aenter__(self):
        timeout = aiohttp.ClientTimeout(total=300)
        connector = aiohttp.TCPConnector(limit=20, limit_per_host=10)  # Reduced limits
        self.session = aiohttp.ClientSession(
            headers={"Authorization": f"Bearer {self.api_key}"},
            timeout=timeout,
            connector=connector
        )
        return self
    
    async def __aexit__(self, *args):
        await self.session.close()
    
    def count_tokens(self, text: str) -> int:
        """Count tokens accurately using tiktoken if available."""
        if self.encoder:
            return len(self.encoder.encode(text))
        else:
            # Conservative fallback
            return int(len(text.split()) * 0.75) + 50
    
    async def rate_limit_check(self):
        """Enforce rate limiting."""
        current_time = time.time()
        
        # Remove timestamps older than 1 minute
        call_timestamps[:] = [ts for ts in call_timestamps if current_time - ts < 60]
        
        # Check if we're approaching the rate limit
        if len(call_timestamps) >= MAX_CALLS_PER_MINUTE - 5:
            sleep_time = 60 - (current_time - call_timestamps[0])
            if sleep_time > 0:
                log.info(f"Rate limit protection: sleeping {sleep_time:.1f}s")
                await asyncio.sleep(sleep_time)
        
        # Always enforce minimum delay between calls
        await asyncio.sleep(RATE_LIMIT_DELAY)
        
        # Record this call
        call_timestamps.append(current_time)
    
    async def embed_batch_async(self, texts: List[str]) -> List[List[float]]:
        """Embed a batch of texts asynchronously with rate limiting and token validation."""
        global total_tokens_used
        
        async with self.semaphore:
            await self.rate_limit_check()
            
            # 🛡️ FINAL TOKEN VALIDATION - Last safety check before API call
            batch_tokens = sum(self.count_tokens(text) for text in texts)
            
            if batch_tokens > MAX_BATCH_TOKENS:
                error_msg = f"🚨 CRITICAL: Batch tokens {batch_tokens} exceed limit {MAX_BATCH_TOKENS}"
                log.error(error_msg)
                raise RuntimeError(error_msg)
            
            total_tokens_used += batch_tokens
            
            log.info(f"🎯 API call: {len(texts)} texts (~{batch_tokens} tokens) - WITHIN LIMITS ✅")
            log.info(f"📊 Total tokens used so far: ~{total_tokens_used}")
            
            for attempt in range(MAX_RETRIES):
                try:
                    async with self.session.post(
                        "https://api.openai.com/v1/embeddings",
                        json={
                            "model": EMBEDDING_MODEL,
                            "input": texts
                        }
                    ) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            self.call_count += 1
                            log.info(f"✅ API call successful: {len(texts)} embeddings generated")
                            return [item["embedding"] for item in data["data"]]
                        elif resp.status == 400:  # Bad request - likely token limit
                            error = await resp.text()
                            log.error(f"🚨 API error 400 (likely token limit): {error}")
                            log.error(f"🚨 Batch details: {len(texts)} texts, {batch_tokens} tokens")
                            raise RuntimeError(f"Token limit API error: {error}")
                        elif resp.status == 429:  # Rate limit error
                            error = await resp.text()
                            log.warning(f"Rate limit hit: {error}")
                            # Exponential backoff for rate limits
                            sleep_time = (2 ** attempt) * 2
                            log.info(f"Exponential backoff: sleeping {sleep_time}s")
                            await asyncio.sleep(sleep_time)
                        else:
                            error = await resp.text()
                            log.warning(f"API error {resp.status}: {error}")
                            await asyncio.sleep(RETRY_DELAY)
                except Exception as e:
                    log.warning(f"Attempt {attempt + 1} failed: {e}")
                    await asyncio.sleep(RETRY_DELAY * (attempt + 1))
            
            raise RuntimeError("Failed to embed batch after retries")

def deduplicate_chunks(chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Remove duplicate text chunks based on content hash."""
    seen_hashes: Set[str] = set()
    unique_chunks = []
    duplicates_removed = 0
    
    for chunk in chunks:
        text = chunk.get("text", "").strip()
        if not text:
            continue
            
        # Create hash of the text content
        text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()
        
        if text_hash not in seen_hashes:
            seen_hashes.add(text_hash)
            unique_chunks.append(chunk)
        else:
            duplicates_removed += 1
    
    if duplicates_removed > 0:
        log.info(f"Deduplication: removed {duplicates_removed} duplicate chunks")
    
    return unique_chunks

async def process_chunks_async(
    sb,
    chunks: List[Dict[str, Any]],
    embedder: AsyncEmbedder,
    commit_size: int = DEFAULT_COMMIT_ROWS
) -> int:
    """Process chunks with async embedding and batch updates."""
    # Deduplicate chunks first
    unique_chunks = deduplicate_chunks(chunks)
    log.info(f"📊 CHUNK PROCESSING PROGRESS:")
    log.info(f"   📄 Original chunks: {len(chunks)}")
    log.info(f"   📄 Unique chunks: {len(unique_chunks)}")
    if len(chunks) != len(unique_chunks):
        log.info(f"   🔄 Duplicates removed: {len(chunks) - len(unique_chunks)}")
    
    # Create token-safe slices
    slices = safe_slices(unique_chunks, commit_size)
    total_embedded = 0
    total_slices = len(slices)
    
    if total_slices == 0:
        log.warning(f"📊 NO SLICES TO PROCESS - All chunks were filtered out")
        return 0
    
    log.info(f"📊 EMBEDDING SLICES PROGRESS:")
    log.info(f"   🎯 Total slices to process: {total_slices}")
    log.info(f"   📄 Total chunks to embed: {sum(len(slice_data) for slice_data in slices)}")
    
    # Process each slice
    for i, slice_data in enumerate(slices):
        slice_num = i + 1
        slice_progress = (slice_num / total_slices * 100)
        
        log.info(f"📊 SLICE {slice_num}/{total_slices} ({slice_progress:.1f}%):")
        log.info(f"   📄 Processing {len(slice_data)} chunks in this slice")
        
        texts = [row["text"] for row in slice_data]
        
        try:
            # Get embeddings asynchronously
            log.info(f"   🔄 Calling OpenAI API for {len(texts)} embeddings...")
            embeddings = await embedder.embed_batch_async(texts)
            log.info(f"   ✅ Received {len(embeddings)} embeddings from API")
            
            # Update database in batch
            log.info(f"   💾 Updating database for {len(slice_data)} chunks...")
            update_tasks = []
            for row, emb in zip(slice_data, embeddings):
                # ✅ GUARANTEED SKIP LOGIC - Layer 3: Final update verification
                def update_with_verification(r, e):
                    # Final check before updating
                    final_check = sb.table("documents_chunks").select("embedding")\
                        .eq("id", r["id"]).execute()
                    
                    if final_check.data and final_check.data[0].get("embedding"):
                        log.info(f"🛡️ Chunk {r['id']} already has embedding - SKIPPING update")
                        return {"skipped": True}
                    
                    # Safe to update
                    return sb.table("documents_chunks")\
                        .update({"embedding": e})\
                        .eq("id", r["id"])\
                        .execute()
                
                # Use thread pool for database updates
                loop = asyncio.get_event_loop()
                task = loop.run_in_executor(None, update_with_verification, row, emb)
                update_tasks.append(task)
            
            # Wait for all updates
            log.info(f"   ⏳ Waiting for {len(update_tasks)} database updates...")
            results = await asyncio.gather(*update_tasks, return_exceptions=True)
            successful = sum(1 for r in results if not isinstance(r, Exception) and not (isinstance(r, dict) and r.get("skipped")))
            skipped = sum(1 for r in results if isinstance(r, dict) and r.get("skipped"))
            failed = sum(1 for r in results if isinstance(r, Exception))
            total_embedded += successful
            
            # 📊 SLICE COMPLETION PROGRESS
            log.info(f"📊 SLICE {slice_num} COMPLETE:")
            log.info(f"   ✅ Successful updates: {successful}")
            if skipped > 0:
                log.info(f"   ⏭️  Skipped (already embedded): {skipped}")
            if failed > 0:
                log.info(f"   ❌ Failed updates: {failed}")
            log.info(f"   📈 Total embedded so far: {total_embedded}")
            
        except Exception as e:
            log.error(f"❌ SLICE {slice_num} FAILED: {e}")
            log.error(f"   📄 Chunks in failed slice: {len(slice_data)}")
    
    # Final summary
    log.info(f"📊 CHUNK PROCESSING COMPLETE:")
    log.info(f"   ✅ Total chunks embedded: {total_embedded}")
    log.info(f"   📊 Slices processed: {total_slices}")
    log.info(f"   📈 Success rate: {(total_embedded/len(unique_chunks)*100):.1f}%" if unique_chunks else "0%")
    
    return total_embedded

async def main_async(batch_size: int = None, commit_size: int = None, max_concurrent: int = None):
    """Async main function for embedding."""
    global EMBEDDING_MODEL, MODEL_TOKEN_LIMIT, MAX_TOTAL_TOKENS, total_tokens_used
    
    # Use provided parameters or conservative defaults
    batch_size = batch_size or DEFAULT_BATCH_ROWS
    commit_size = commit_size or DEFAULT_COMMIT_ROWS
    max_concurrent = max_concurrent or DEFAULT_MAX_CONCURRENT
    
    log.info(f"Starting with conservative settings:")
    log.info(f"  Batch size: {batch_size}")
    log.info(f"  Commit size: {commit_size}")
    log.info(f"  Max concurrent: {max_concurrent}")
    log.info(f"  Rate limit delay: {RATE_LIMIT_DELAY}s")
    
    # 🎯 Dynamic batching status
    log.info(f"🎯 DYNAMIC BATCHING ENABLED:")
    log.info(f"   Max batch tokens: {MAX_BATCH_TOKENS} (with safety margin)")
    log.info(f"   Max chunk tokens: {MAX_CHUNK_TOKENS}")
    log.info(f"   Min batch tokens: {MIN_BATCH_TOKENS}")
    log.info(f"   ✅ GUARANTEED: No token limit API errors")
    
    # ✅ Check tiktoken availability for accurate token counting
    if not TIKTOKEN_AVAILABLE:
        log.warning("⚠️  tiktoken not available - using conservative token estimation")
        log.warning("⚠️  For accurate token counting, install: pip install tiktoken")
    else:
        log.info("✅ tiktoken available - using accurate token counting")
    
    if not OPENAI_API_KEY:
        log.error("OPENAI_API_KEY missing")
        sys.exit(1)
    
    sb = init_supabase()
    existing_count = count_processed_chunks(sb)
    log.info(f"Rows already embedded: {existing_count}")
    
    # ✅ GUARANTEED SKIP LOGIC - Status check
    total_chunks_res = sb.table("documents_chunks").select("id", count="exact")\
        .eq("chunking_strategy", "token_window").execute()
    total_chunks = total_chunks_res.count or 0
    
    pending_chunks = total_chunks - existing_count
    
    log.info("📊 Current status:")
    log.info(f"   Chunks already embedded: {existing_count}")
    log.info(f"   Chunks needing embedding: {pending_chunks}")
    log.info(f"   Total chunks: {total_chunks}")
    
    if pending_chunks == 0:
        log.info("✅ All chunks already have embeddings - nothing to do!")
        return
    
    async with AsyncEmbedder(OPENAI_API_KEY, max_concurrent) as embedder:
        loop, total = 0, 0
        
        # 📊 PROGRESS TRACKING - Initialize counters
        total_chunks_to_process = pending_chunks
        chunks_processed = 0
        chunks_skipped = 0
        
        log.info(f"📊 EMBEDDING PROGRESS TRACKING INITIALIZED:")
        log.info(f"   🎯 Total chunks to embed: {total_chunks_to_process}")
        log.info(f"   🎯 Starting embedding process...")
        
        while True:
            loop += 1
            rows = fetch_unprocessed_chunks(sb, limit=batch_size, offset=0)
            
            if not rows:
                log.info("✨ Done — no more rows.")
                break
            
            # 📊 PROGRESS: Show current status before processing
            remaining_chunks = total_chunks_to_process - chunks_processed
            progress_percent = (chunks_processed / total_chunks_to_process * 100) if total_chunks_to_process > 0 else 0
            
            log.info(f"📊 EMBEDDING PROGRESS - Loop {loop}:")
            log.info(f"   📄 Fetched: {len(rows)} chunks")
            log.info(f"   ✅ Processed: {chunks_processed}/{total_chunks_to_process} ({progress_percent:.1f}%)")
            log.info(f"   ⏳ Remaining: {remaining_chunks} chunks")
            if chunks_skipped > 0:
                log.info(f"   ⚠️  Skipped: {chunks_skipped} oversized chunks")
            
            # Process chunks asynchronously
            embedded = await process_chunks_async(sb, rows, embedder, commit_size)
            total += embedded
            chunks_processed += embedded
            
            # Update skipped count (this will be calculated in process_chunks_async)
            current_chunk_count = count_processed_chunks(sb)
            actual_embedded_this_loop = current_chunk_count - (existing_count + chunks_processed - embedded)
            
            # 📊 PROGRESS: Show results after processing
            final_progress_percent = (chunks_processed / total_chunks_to_process * 100) if total_chunks_to_process > 0 else 0
            log.info(f"📊 LOOP {loop} COMPLETE:")
            log.info(f"   ✅ This loop: {embedded} chunks embedded")
            log.info(f"   📈 Total progress: {chunks_processed}/{total_chunks_to_process} ({final_progress_percent:.1f}%)")
            log.info(f"   🔄 API calls made: {embedder.call_count}")
            log.info(f"   📊 Total tokens used: ~{total_tokens_used}")
            
            # Check if we're making progress or stuck
            if embedded == 0 and len(rows) > 0:
                chunks_skipped += len(rows)
                log.warning(f"⚠️  WARNING: No chunks embedded in this loop - all {len(rows)} chunks were skipped")
                log.warning(f"⚠️  Total skipped so far: {chunks_skipped} chunks")
                
                # Prevent infinite loop by limiting consecutive failed attempts
                if loop > 5 and embedded == 0:
                    log.error(f"🚨 STUCK: {loop} consecutive loops with no progress - stopping to prevent infinite loop")
                    break
    
    # Save report
    ts = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    report = {
        "timestamp": ts,
        "batch_size": batch_size,
        "commit": commit_size,
        "max_concurrent": max_concurrent,
        "total_embedded": total,
        "total_with_embeddings": count_processed_chunks(sb),
        "total_tokens_used": total_tokens_used,
        "api_calls_made": embedder.call_count if 'embedder' in locals() else 0
    }
    
    # Create reports directory if it doesn't exist
    reports_dir = "reports/embedding"
    os.makedirs(reports_dir, exist_ok=True)
    
    fname = f"{reports_dir}/supabase_embedding_report_{ts}.json"
    with open(fname, "w") as fp:
        json.dump(report, fp, indent=2)
    
    log.info(f"Report saved to {fname}")

async def main_async_cli():
    """CLI version with argument parsing."""
    global EMBEDDING_MODEL, MODEL_TOKEN_LIMIT, MAX_TOTAL_TOKENS
    
    ap = argparse.ArgumentParser()
    ap.add_argument("--batch-size", type=int, default=DEFAULT_BATCH_ROWS)
    ap.add_argument("--commit", type=int, default=DEFAULT_COMMIT_ROWS)
    ap.add_argument("--skip", type=int, default=0)
    ap.add_argument("--model", default=EMBEDDING_MODEL)
    ap.add_argument("--model-limit", type=int, default=MODEL_TOKEN_LIMIT)
    ap.add_argument("--max-concurrent", type=int, default=DEFAULT_MAX_CONCURRENT,
                   help="Maximum concurrent API calls (default: 3 for rate limiting)")
    args = ap.parse_args()
    
    EMBEDDING_MODEL = args.model
    MODEL_TOKEN_LIMIT = args.model_limit
    MAX_TOTAL_TOKENS = MODEL_TOKEN_LIMIT - TOKEN_GUARD
    
    await main_async(args.batch_size, args.commit, args.max_concurrent)

# Keep original interface for compatibility
def main() -> None:
    """Original synchronous interface."""
    asyncio.run(main_async_cli())

# Keep all existing helper functions unchanged...
def init_supabase():
    if not SUPABASE_URL or not SUPABASE_KEY:
        log.error("SUPABASE creds missing"); sys.exit(1)
    return create_client(SUPABASE_URL,SUPABASE_KEY)

def count_processed_chunks(sb)->int:
    res=sb.table("documents_chunks").select("id",count="exact").not_.is_("embedding","null").execute()
    return res.count or 0

def fetch_unprocessed_chunks(sb,*,limit:int,offset:int=0)->List[Dict[str,Any]]:
    first,last=offset,offset+limit-1
    res=sb.table("documents_chunks").select("id,text,token_start,token_end")\
        .eq("chunking_strategy","token_window").is_("embedding","null")\
        .range(first,last).execute()
    
    chunks = res.data or []
    
    if chunks:
        # ✅ GUARANTEED SKIP LOGIC - Layer 2: Double-check verification
        chunk_ids = [chunk["id"] for chunk in chunks]
        verification = sb.table("documents_chunks").select("id")\
            .in_("id", chunk_ids).not_.is_("embedding", "null").execute()
        
        already_embedded_ids = {row["id"] for row in verification.data or []}
        
        if already_embedded_ids:
            log.warning(f"🛡️ Found {len(already_embedded_ids)} chunks that already have embeddings - SKIPPING them")
            chunks = [chunk for chunk in chunks if chunk["id"] not in already_embedded_ids]
        
        log.info(f"✅ GUARANTEED: Fetched {len(chunks)} chunks WITHOUT embeddings")
        
        if not chunks:
            log.info("✅ All chunks already have embeddings - nothing to do!")
    
    return chunks

def generate_embedding_batch(texts:List[str])->List[List[float]]:
    attempt=0
    while attempt<MAX_RETRIES:
        try:
            resp=openai_client.embeddings.create(model=EMBEDDING_MODEL,input=texts)
            return [d.embedding for d in resp.data]
        except Exception as exc:
            attempt+=1; log.warning("Embedding batch %s/%s failed: %s",attempt,MAX_RETRIES,exc)
            time.sleep(RETRY_DELAY)
    raise RuntimeError("OpenAI embedding batch failed after retries")

def chunk_tokens(row:Dict[str,Any])->int:
    try:
        t=int(row["token_end"])-int(row["token_start"])+1
        if 0<t<=16384: return t
    except: pass
    
    # Use tiktoken if available for more accurate counting
    text = row.get("text", "")
    if TIKTOKEN_AVAILABLE:
        try:
            encoder = tiktoken.encoding_for_model(EMBEDDING_MODEL)
            return len(encoder.encode(text))
        except:
            pass
    
    # Conservative fallback
    approx=int(len(text.split())*0.75)+50  # More conservative estimate
    return min(max(1,approx),MODEL_TOKEN_LIMIT)

def safe_slices(rows:List[Dict[str,Any]],max_rows:int)->List[List[Dict[str,Any]]]:
    """🎯 DYNAMIC BATCH SIZING - Guarantees no token limit errors."""
    return dynamic_batch_slices(rows)

def dynamic_batch_slices(rows: List[Dict[str, Any]]) -> List[List[Dict[str, Any]]]:
    """
    🎯 DYNAMIC BATCH SIZING - Creates variable-sized batches that GUARANTEE no API token errors.
    
    Protection Layers:
    1. Individual chunk validation (max 6,000 tokens)
    2. Dynamic batch sizing (max 7,692 tokens total)
    3. Conservative safety margins
    4. Multiple validation checks
    """
    batches = []
    current_batch = []
    current_tokens = 0
    skipped_chunks = 0
    
    # Initialize token encoder for accurate counting
    encoder = None
    if TIKTOKEN_AVAILABLE:
        try:
            encoder = tiktoken.encoding_for_model(EMBEDDING_MODEL)
        except:
            pass
    
    def count_tokens_accurate(text: str) -> int:
        """Count tokens with maximum accuracy."""
        if encoder:
            return len(encoder.encode(text))
        else:
            # Conservative fallback estimation
            return int(len(text.split()) * 0.75) + 50
    
    log.info(f"🎯 Starting dynamic batch creation with {len(rows)} chunks")
    log.info(f"🎯 Limits: {MAX_CHUNK_TOKENS} tokens/chunk, {MAX_BATCH_TOKENS} tokens/batch")
    
    for i, row in enumerate(rows):
        # Clean text and validate
        text = (row.get("text") or "").replace("\x00", "").strip()
        if not text:
            log.warning(f"🎯 Skipping empty chunk {row.get('id', 'unknown')}")
            continue
        
        # ️ PROTECTION LAYER 1: Individual chunk validation
        chunk_tokens = count_tokens_accurate(text)
        
        if chunk_tokens > MAX_CHUNK_TOKENS:
            log.warning(f"🎯 Skipping oversized chunk {row.get('id', 'unknown')}: {chunk_tokens} tokens (max: {MAX_CHUNK_TOKENS})")
            skipped_chunks += 1
            continue
        
        # 🛡️ PROTECTION LAYER 2: Dynamic batch sizing
        would_exceed = current_tokens + chunk_tokens > MAX_BATCH_TOKENS
        
        if would_exceed and current_batch:
            # Finalize current batch
            log.info(f"🎯 Created batch with {len(current_batch)} chunks (~{current_tokens} tokens)")
            batches.append(current_batch)
            current_batch = []
            current_tokens = 0
        
        # Add chunk to current batch
        current_batch.append({"id": row["id"], "text": text})
        current_tokens += chunk_tokens
        
        # 🛡️ PROTECTION LAYER 3: Safety validation
        if current_tokens > MAX_BATCH_TOKENS:
            log.error(f"🚨 CRITICAL: Batch exceeded limit! {current_tokens} > {MAX_BATCH_TOKENS}")
            # Emergency fallback - remove last chunk and finalize batch
            if len(current_batch) > 1:
                current_batch.pop()
                current_tokens -= chunk_tokens
                log.info(f"🎯 Emergency: Created batch with {len(current_batch)} chunks (~{current_tokens} tokens)")
                batches.append(current_batch)
                current_batch = [{"id": row["id"], "text": text}]
                current_tokens = chunk_tokens
            else:
                log.error(f"🚨 Single chunk too large: {chunk_tokens} tokens")
                current_batch = []
                current_tokens = 0
                skipped_chunks += 1
    
    # Add final batch if not empty
    if current_batch and current_tokens >= MIN_BATCH_TOKENS:
        log.info(f"🎯 Created final batch with {len(current_batch)} chunks (~{current_tokens} tokens)")
        batches.append(current_batch)
    elif current_batch:
        log.warning(f"🎯 Skipping tiny final batch: {current_tokens} tokens < {MIN_BATCH_TOKENS} minimum")
    
    # 🛡️ PROTECTION LAYER 4: Final validation
    total_chunks = sum(len(batch) for batch in batches)
    max_batch_tokens = max((sum(count_tokens_accurate(chunk["text"]) for chunk in batch) for batch in batches), default=0)
    
    log.info(f"🎯 DYNAMIC BATCHING COMPLETE:")
    log.info(f"   📊 {len(batches)} batches created")
    log.info(f"   📊 {total_chunks} chunks processed")
    log.info(f"   📊 {skipped_chunks} chunks skipped")
    log.info(f"   📊 Largest batch: {max_batch_tokens} tokens (limit: {MAX_BATCH_TOKENS})")
    log.info(f"   ✅ GUARANTEED: No batch exceeds {MAX_BATCH_TOKENS} tokens")
    
    if max_batch_tokens > MAX_BATCH_TOKENS:
        log.error(f"🚨 CRITICAL ERROR: Batch validation failed!")
        raise RuntimeError(f"Batch token validation failed: {max_batch_tokens} > {MAX_BATCH_TOKENS}")
    
    return batches

def embed_slice(sb,slice_rows:List[Dict[str,Any]])->int:
    embeds=generate_embedding_batch([r["text"] for r in slice_rows])
    ok=0
    for row,emb in zip(slice_rows,embeds):
        attempt=0
        while attempt<MAX_RETRIES:
            res=sb.table("documents_chunks").update({"embedding":emb}).eq("id",row["id"]).execute()
            if getattr(res,"error",None):
                attempt+=1; log.warning("Update %s failed (%s/%s): %s",row["id"],attempt,MAX_RETRIES,res.error)
                time.sleep(RETRY_DELAY)
            else: ok+=1; break
    return ok

if __name__=="__main__": 
    asyncio.run(main_async_cli())


================================================================================


################################################################################
# File: scripts/stages/extract_clean.py
################################################################################

# File: scripts/stages/extract_clean.py

#!/usr/bin/env python3
"""
Stage 1-2 — *Extract PDF → clean text → logical sections*  
Optimized version with concurrent processing support.
Outputs `<n>.json` (+ a pretty `.txt`) in `city_clerk_documents/{json,txt}/`.
This file is **fully self-contained** – no import from `stages.common`.
"""
from __future__ import annotations

import json, logging, os, pathlib, re, sys
from collections import Counter
from datetime import datetime
from textwrap import dedent
from typing import Any, Dict, List, Sequence, Optional
import asyncio
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing as mp

# ─── helpers formerly in common.py ──────────────────────────────────
_LATIN1_REPLACEMENTS = {  # windows-1252 fallback
    0x82: "‚",
    0x83: "ƒ",
    0x84: "„",
    0x85: "…",
    0x86: "†",
    0x87: "‡",
    0x88: "ˆ",
    0x89: "‰",
    0x8A: "Š",
    0x8B: "‹",
    0x8C: "Œ",
    0x8E: "Ž",
    0x91: "'",
    0x92: "'",
    0x93: '"',
    0x94: '"',
    0x95: "•",
    0x96: "–",
    0x97: "—",
    0x98: "˜",
    0x99: "™",
    0x9A: "š",
    0x9B: "›",
    0x9C: "œ",
    0x9E: "ž",
    0x9F: "Ÿ",
}
_TRANSLATE_LAT1 = str.maketrans(_LATIN1_REPLACEMENTS)
def latin1_scrub(txt:str)->str: return txt.translate(_TRANSLATE_LAT1)

_WS_RE = re.compile(r"[ \t]+\n")
def normalize_ws(txt:str)->str:
    return re.sub(r"[ \t]{2,}", " ", _WS_RE.sub("\n", txt)).strip()

def pct_ascii_letters(txt:str)->float:
    letters=sum(ch.isascii() and ch.isalpha() for ch in txt)
    return letters/max(1,len(txt))

def needs_ocr(txt:str)->bool:
    return (not txt.strip()) or ("\x00" in txt) or (pct_ascii_letters(txt)<0.15)

def scrub_nuls(obj:Any)->Any:
    if isinstance(obj,str):  return obj.replace("\x00","")
    if isinstance(obj,list): return [scrub_nuls(x) for x in obj]
    if isinstance(obj,dict): return {k:scrub_nuls(v) for k,v in obj.items()}
    return obj

def _make_converter()->DocumentConverter:
    opts = PdfPipelineOptions()
    opts.do_ocr = True
    return DocumentConverter({InputFormat.PDF: PdfFormatOption(pipeline_options=opts)})
# ───────────────────────────────────────────────────────────────────

from dotenv import load_dotenv
from openai import OpenAI
from supabase import create_client            # only needed if you later push rows
from tqdm import tqdm

# ─── optional heavy deps ----------------------------------------------------
try:
    import PyPDF2                             # raw fallback
    from unstructured.partition.pdf import partition_pdf
    from docling.datamodel.base_models import InputFormat
    from docling.datamodel.pipeline_options import PdfPipelineOptions
    from docling.document_converter import DocumentConverter, PdfFormatOption
except ImportError as exc:                    # pragma: no cover
    sys.exit(f"Missing dependency → {exc}.  Run `pip install -r requirements.txt`.")

# ─── env / paths ------------------------------------------------------------
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

REPO_ROOT  = pathlib.Path(__file__).resolve().parents[2]
TXT_DIR    = REPO_ROOT / "city_clerk_documents" / "txt"
JSON_DIR   = REPO_ROOT / "city_clerk_documents" / "json"
TXT_DIR.mkdir(parents=True, exist_ok=True)
JSON_DIR.mkdir(parents=True, exist_ok=True)

GPT_META_MODEL = "gpt-4.1-mini-2025-04-14"

log = logging.getLogger(__name__)

# ╔══════════════════════════════════════════════════════════════════════════╗
# ║                           helper utilities                              ║
# ╚══════════════════════════════════════════════════════════════════════════╝

_LATIN = {0x91:"'",0x92:"'",0x93:'"',0x94:'"',0x95:"•",0x96:"–",0x97:"—"}
_DOI_RE = re.compile(r"\b10\.\d{4,9}/[-._;()/:A-Z0-9]+", re.I)
_JOURNAL_RE = re.compile(r"(Journal|Proceedings|Annals|Psychiatry|Psychology|Nature|Science)[^\n]{0,120}", re.I)
_ABSTRACT_RE = re.compile(r"(?<=\bAbstract\b[:\s])(.{50,2000}?)(?:\n[A-Z][^\n]{0,60}\n|\Z)", re.S)
_KEYWORDS_RE = re.compile(r"\bKey\s*words?\b[:\s]*(.+)", re.I)
_HEADING_TYPES = {"title","heading","header","subtitle","subheading"}

# ─── minimal bib helpers ───────────────────────────────────────────────────
def _authors(val)->List[str]:
    if val is None: return []
    if isinstance(val,list): return [str(x).strip() for x in val if x]
    return re.split(r"\s*,\s*|\s+and\s+", str(val).strip())

_DEF_META = {
    "document_type": None,
    "title": None,
    "date": None,
    "year": None,
    "month": None,
    "day": None,
    "mayor": None,
    "vice_mayor": None,
    "commissioners": [],
    "city_attorney": None,
    "city_manager": None,
    "city_clerk": None,
    "public_works_director": None,
    "agenda": None,
    "keywords": [],
}

def merge_meta(*sources:Dict[str,Any])->Dict[str,Any]:
    m=_DEF_META.copy()
    for src in sources:
        for k,v in src.items():
            if v not in (None,"",[],{}): m[k]=v
    m["commissioners"]=m["commissioners"] or []
    m["keywords"]=m["keywords"] or []
    return m

def bib_from_filename(pdf:pathlib.Path)->Dict[str,Any]:
    s=pdf.stem
    m=re.search(r"\b(19|20)\d{2}\b",s)
    yr=int(m.group(0)) if m else None
    if m:
        title=s[m.end():].strip(" -_")
    else:
        parts=s.split(" ",1); title=parts[1] if len(parts)==2 else s
    return {"year":yr,"title":title}

def bib_from_header(txt:str)->Dict[str,Any]:
    md={}
    if m:=_DOI_RE.search(txt): md["doi"]=m.group(0)
    if m:=_JOURNAL_RE.search(txt): md["journal"]=" ".join(m.group(0).split())
    if m:=_ABSTRACT_RE.search(txt): md["abstract"]=" ".join(m.group(1).split())
    if m:=_KEYWORDS_RE.search(txt):
        kws=[k.strip(" ;.,") for k in re.split(r"[;,]",m.group(1)) if k.strip()]
        md["keywords"]=kws
    return md

# ─── GPT enrichment ────────────────────────────────────────────────────────
def _first_words(txt:str,n:int=3000)->str: return " ".join(txt.split()[:n])

def gpt_metadata(text:str)->Dict[str,Any]:
    if not OPENAI_API_KEY: return {}
    cli=OpenAI(api_key=OPENAI_API_KEY)
    prompt=dedent(f"""
        Extract metadata from this city clerk document and return a JSON object with these fields:
        - document_type: one of [Resolution, Ordinance, Proclamation, Contract, Meeting Minutes, Agenda]
        - title: document title
        - date: full date string
        - year: numeric year
        - month: numeric month (1-12)
        - day: numeric day
        - mayor: name only (e.g., "John Smith")
        - vice_mayor: name only (e.g., "Jane Doe")
        - commissioners: array of commissioner names only (e.g., ["Robert Brown", "Sarah Johnson", "Michael Davis"])
        - city_attorney: name only
        - city_manager: name only
        - city_clerk: name only
        - public_works_director: name only
        - agenda: agenda items or summary if present
        - keywords: array of relevant keywords (e.g., ["budget", "zoning", "public safety"])
        
        Text:
        {text}
    """)
    rsp=cli.chat.completions.create(model=GPT_META_MODEL,temperature=0,
        messages=[{"role":"system","content":"Structured metadata extractor"},
                  {"role":"user","content":prompt}])
    txt=rsp.choices[0].message.content
    json_txt=re.search(r"{[\s\S]*}",txt)
    return json.loads(json_txt.group(0) if json_txt else "{}")        # type: ignore[arg-type]

# ╔══════════════════════════════════════════════════════════════════════════╗
# ║                         core extraction logic                            ║
# ╚══════════════════════════════════════════════════════════════════════════╝

def _docling_elements(doc)->List[Dict[str,Any]]:
    p:Dict[int,List[Dict[str,str]]]={}
    for it,_ in doc.iterate_items():
        pg=getattr(it.prov[0],"page_no",1)
        lbl=(getattr(it,"label","") or "").upper()
        typ="heading" if lbl in ("TITLE","SECTION_HEADER","HEADER") else \
            "list_item" if lbl=="LIST_ITEM" else \
            "table" if lbl=="TABLE" else "paragraph"
        p.setdefault(pg,[]).append({"type":typ,"text":str(getattr(it,"text",it)).strip()})
    out=[]
    for pn in sorted(p):
        out.append({"section":f"Page {pn}","page_number":pn,
                    "text":"\n".join(el["text"] for el in p[pn]),
                    "elements":p[pn]})
    return out

def _unstructured_elements(pdf:pathlib.Path)->List[Dict[str,Any]]:
    els=partition_pdf(str(pdf),strategy="hi_res")
    pages:Dict[int,List[Dict[str,str]]]={}
    for el in els:
        pn=getattr(el.metadata,"page_number",1)
        pages.setdefault(pn,[]).append({"type":el.category or "paragraph",
                                        "text":normalize_ws(str(el))})
    return [{"section":f"Page {pn}","page_number":pn,
             "text":"\n".join(e["text"] for e in it),"elements":it}
            for pn,it in sorted(pages.items())]

def _pypdf_elements(pdf:pathlib.Path)->List[Dict[str,Any]]:
    out=[]
    with open(pdf,"rb") as fh:
        for pn,pg in enumerate(PyPDF2.PdfReader(fh).pages,1):
            raw=normalize_ws(pg.extract_text() or "")
            paras=[p for p in re.split(r"\n{2,}",raw) if p.strip()]
            out.append({"section":f"Page {pn}","page_number":pn,
                        "text":"\n".join(paras),
                        "elements":[{"type":"paragraph","text":p} for p in paras]})
    return out

def _group_by_headings(page_secs:Sequence[Dict[str,Any]])->List[Dict[str,Any]]:
    out=[]; cur=None; last=1
    for s in page_secs:
        pn=s.get("page_number",last); last=pn
        for el in s.get("elements",[]):
            kind=(el.get("type")or"").lower(); txt=el.get("text","").strip()
            if kind in _HEADING_TYPES and txt:
                if cur: out.append(cur)
                cur={"section":txt,"page_start":pn,"page_end":pn,"elements":[el.copy()]}
            else:
                if not cur:
                    cur={"section":"(untitled)","page_start":pn,"page_end":pn,"elements":[]}
                cur["elements"].append(el.copy()); cur["page_end"]=pn
    if cur and not out: out.append(cur)
    return out

def _sections_md(sections:List[Dict[str,Any]])->str:
    md=[]
    for s in sections:
        md.append(f"# {s.get('section','(Untitled)')}")
        for el in s.get("elements",[]): md.append(el.get("text",""))
        md.append("")
    return "\n".join(md).strip()

# ╔══════════════════════════════════════════════════════════════════════════╗
# ║                         Inline extract_pdf from common                   ║
# ╚══════════════════════════════════════════════════════════════════════════╝

def extract_pdf(
    pdf: pathlib.Path,
    txt_dir: pathlib.Path,
    json_dir: pathlib.Path,
    conv: DocumentConverter,
    *,
    overwrite: bool = False,
    ocr_lang: str = "eng",
    keep_markup: bool = True,
    docling_only: bool = False,
    stats: Counter | None = None,
    enrich_llm: bool = True,
) -> pathlib.Path:
    """Return path to JSON payload ready for downstream stages."""
    txt_path = txt_dir / f"{pdf.stem}.txt"
    json_path = json_dir / f"{pdf.stem}.json"
    if not overwrite and txt_path.exists() and json_path.exists():
        return json_path

    # –– 1. Docling
    page_secs: List[Dict[str, Any]] = []
    bundle = None
    try:
        bundle = conv.convert(str(pdf))
        if keep_markup:
            page_secs = _docling_elements(bundle.document)
        else:
            full = bundle.document.export_to_text(page_break_marker="\f")
            page_secs = [{
                "section": "Full document",
                "page_number": 1,
                "text": full,
                "elements": [{"type": "paragraph", "text": full}],
            }]
    except Exception as exc:
        log.warning("Docling failed on %s → %s", pdf.name, exc)

    # –– 2. Unstructured fallback
    if not page_secs and not docling_only:
        try:
            page_secs = _unstructured_elements(pdf)
        except Exception as exc:
            log.warning("unstructured failed on %s → %s", pdf.name, exc)

    # –– 3. PyPDF last resort
    if not page_secs and not docling_only:
        log.info("PyPDF2 fallback on %s", pdf.name)
        page_secs = _pypdf_elements(pdf)

    if not page_secs:
        raise RuntimeError("No text extracted from PDF")

    # Latin-1 scrub + OCR repair
    for sec in page_secs:
        sec["text"] = latin1_scrub(sec.get("text", ""))
        for el in sec.get("elements", []):
            el["text"] = latin1_scrub(el.get("text", ""))
        pn = sec.get("page_number")
        need_ocr_before = bool(pn and needs_ocr(sec["text"]))
        if need_ocr_before:
            try:
                from pdfplumber import open as pdfopen
                import pytesseract

                with pdfopen(str(pdf)) as doc:
                    pil = doc.pages[pn - 1].to_image(resolution=300).original
                ocr_txt = normalize_ws(
                    pytesseract.image_to_string(pil, lang=ocr_lang)
                )
                if ocr_txt:
                    sec["text"] = ocr_txt
                    sec["elements"] = [
                        {"type": "paragraph", "text": p}
                        for p in re.split(r"\n{2,}", ocr_txt)
                        if p.strip()
                    ]
                    if stats is not None:
                        stats["ocr_pages"] += 1
            except Exception:
                pass

    # when markup is disabled we already have final sections
    logical_secs = (
        _group_by_headings(page_secs) if keep_markup else page_secs
    )

    # CRITICAL: Ensure EVERY section has a 'text' field
    # This is required by the chunking stage
    for sec in logical_secs:
        if 'elements' in sec:
            # Build text from elements if not already present
            element_texts = []
            for el in sec.get("elements", []):
                el_text = el.get("text", "")
                # Skip internal docling metadata that starts with self_ref=
                if el_text and not el_text.startswith('self_ref='):
                    element_texts.append(el_text)
            
            # Always set text field (overwrite if exists to ensure consistency)
            sec["text"] = "\n".join(element_texts)
        elif 'text' not in sec:
            # Fallback for sections without elements
            sec["text"] = ""
    
    # Also ensure page_secs have text (for debugging/consistency)
    for sec in page_secs:
        if 'text' not in sec and 'elements' in sec:
            element_texts = []
            for el in sec.get("elements", []):
                el_text = el.get("text", "")
                if el_text and not el_text.startswith('self_ref='):
                    element_texts.append(el_text)
            sec["text"] = "\n".join(element_texts)

    header_txt = " ".join(s["text"] for s in page_secs[:2])[:8000]
    heuristic_meta = merge_meta(
        bundle.document.metadata.model_dump()
        if "bundle" in locals() and hasattr(bundle.document, "metadata")
        else {},
        bib_from_filename(pdf),
        bib_from_header(header_txt),
    )

    # single GPT call for **all** metadata fields -----------------------
    llm_meta: Dict[str, Any] = {}
    if enrich_llm and OPENAI_API_KEY:
        try:
            oa = OpenAI(api_key=OPENAI_API_KEY)
            llm_meta = gpt_metadata(
                _first_words(" ".join(s["text"] for s in logical_secs))
            )
        except Exception as exc:
            log.warning("LLM metadata extraction failed on %s → %s", pdf.name, exc)

    meta = merge_meta(heuristic_meta, llm_meta)

    payload = {
        **meta,
        "created_at": datetime.utcnow().isoformat() + "Z",
        "source_pdf": str(pdf.resolve()),
        "sections": logical_secs,
    }

    json_path.parent.mkdir(parents=True, exist_ok=True)
    txt_path.parent.mkdir(parents=True, exist_ok=True)
    json_path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), "utf-8")

    # --- Markdown serialisation (rich) ---
    md_text = _sections_md(logical_secs) if keep_markup else "\n".join(
        "# " + s.get("section", "(No title)") + "\n\n" + s.get("text", "") for s in logical_secs
    )
    txt_path.write_text(md_text, "utf-8")

    if stats is not None:
        stats["processed"] += 1

    return json_path

# New: Thread-safe converter pool
class ConverterPool:
    """Thread-safe pool of DocumentConverter instances."""
    def __init__(self, size: int = None):
        self.size = size or mp.cpu_count()
        self._converters = []
        self._lock = mp.Lock()
        self._initialized = False
    
    def get(self):
        """Get a converter from the pool."""
        with self._lock:
            if not self._initialized:
                self._initialize()
            return self._converters[mp.current_process()._identity[0] % self.size]
    
    def _initialize(self):
        """Lazy initialize converters."""
        for _ in range(self.size):
            self._converters.append(_make_converter())
        self._initialized = True

# Global converter pool
_converter_pool = ConverterPool()

def extract_pdf_concurrent(
    pdf: pathlib.Path,
    txt_dir: pathlib.Path,
    json_dir: pathlib.Path,
    *,
    overwrite: bool = False,
    ocr_lang: str = "eng",
    keep_markup: bool = True,
    docling_only: bool = False,
    stats: Counter | None = None,
    enrich_llm: bool = True,
) -> pathlib.Path:
    """Thread-safe version of extract_pdf that uses converter pool."""
    conv = _converter_pool.get()
    return extract_pdf(
        pdf, txt_dir, json_dir, conv,
        overwrite=overwrite,
        ocr_lang=ocr_lang,
        keep_markup=keep_markup,
        docling_only=docling_only,
        stats=stats,
        enrich_llm=enrich_llm
    )

async def extract_batch_async(
    pdfs: List[pathlib.Path],
    *,
    overwrite: bool = False,
    keep_markup: bool = True,
    ocr_lang: str = "eng",
    enrich_llm: bool = True,
    max_workers: Optional[int] = None,
) -> List[pathlib.Path]:
    """Extract multiple PDFs concurrently."""
    from .acceleration_utils import hardware
    
    stats = Counter()
    results = []
    
    # Use process pool for CPU-bound extraction
    with hardware.get_process_pool(max_workers) as executor:
        # Submit all tasks
        future_to_pdf = {
            executor.submit(
                extract_pdf_concurrent,
                pdf, TXT_DIR, JSON_DIR,
                overwrite=overwrite,
                ocr_lang=ocr_lang,
                keep_markup=keep_markup,
                enrich_llm=enrich_llm,
                stats=stats
            ): pdf
            for pdf in pdfs
        }
        
        # Process completed tasks
        from tqdm import tqdm
        for future in tqdm(as_completed(future_to_pdf), total=len(pdfs), desc="Extracting PDFs"):
            pdf = future_to_pdf[future]
            try:
                json_path = future.result()
                results.append(json_path)
            except Exception as exc:
                log.error(f"Failed to extract {pdf.name}: {exc}")
                
    log.info(f"Extraction complete: {stats['processed']} processed, {stats['ocr_pages']} OCR pages")
    return results

# ╔══════════════════════════════════════════════════════════════════════════╗
# ║                          public entry-point                             ║
# ╚══════════════════════════════════════════════════════════════════════════╝
# ---------------------------------------------------------------------------
#  Stage-1/2 wrapper (now paper-thin)
# ---------------------------------------------------------------------------
def run_one(
    pdf: pathlib.Path,
    *,
    overwrite: bool = False,
    keep_markup: bool = True,
    ocr_lang: str = "eng",
    enrich_llm: bool = True,
    conv=None,
    stats: Counter | None = None,
) -> pathlib.Path:
    conv = conv or _make_converter()
    out = extract_pdf(
        pdf,
        TXT_DIR,
        JSON_DIR,
        conv,
        overwrite=overwrite,
        ocr_lang=ocr_lang,
        keep_markup=keep_markup,
        enrich_llm=enrich_llm,
        stats=stats,
    )
    return out

def json_path_for(pdf:pathlib.Path)->pathlib.Path:
    """Helper if Stage 1 is disabled but its artefact exists."""
    return JSON_DIR / f"{pdf.stem}.json"

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format="%(message)s")
    import argparse
    p=argparse.ArgumentParser(); p.add_argument("pdf",type=pathlib.Path)
    args=p.parse_args()
    print(run_one(args.pdf))


================================================================================


################################################################################
# File: scripts/graph_pipeline.py
################################################################################

# File: scripts/graph_pipeline.py

"""
City Clerk Graph Pipeline - UPDATED FOR SUBDIRECTORY STRUCTURE
Orchestrates the complete pipeline from PDF extraction to graph building.
"""

import asyncio
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
import json
from datetime import datetime
import argparse

# Import pipeline components
from graph_stages.agenda_pdf_extractor import AgendaPDFExtractor
from graph_stages.ontology_extractor import OntologyExtractor
from graph_stages.agenda_graph_builder import AgendaGraphBuilder
from graph_stages.cosmos_db_client import CosmosGraphClient
from graph_stages.enhanced_document_linker import EnhancedDocumentLinker

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
log = logging.getLogger('graph_pipeline')

# Pipeline control flags (similar to RAG pipeline)
RUN_EXTRACT_PDF = True
RUN_EXTRACT_ONTOLOGY = True
RUN_LINK_DOCUMENTS = True
RUN_BUILD_GRAPH = True
RUN_VALIDATE_LINKS = True
CLEAR_GRAPH_FIRST = False  # Warning: This will delete all existing data!
UPSERT_EXISTING_NODES = True  # Update existing nodes instead of failing
UPSERT_MODE = True  # Enable upsert functionality
SKIP_EXISTING_EDGES = True  # Don't recreate existing edges


class CityClerkGraphPipeline:
    """Main pipeline orchestrator for city clerk document graph."""
    
    def __init__(self, 
                 base_dir: Path = Path("city_clerk_documents/global"),
                 output_dir: Path = Path("city_clerk_documents/graph_json"),
                 upsert_mode: bool = True):
        self.base_dir = base_dir
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.upsert_mode = upsert_mode
        
        # Define subdirectory structure
        self.city_dir = self.base_dir / "City Comissions 2024"
        self.agenda_dir = self.city_dir / "Agendas"
        self.ordinances_dir = self.city_dir / "Ordinances" / "2024"
        self.resolutions_dir = self.city_dir / "Resolutions"
        
        # Initialize components
        self.pdf_extractor = AgendaPDFExtractor(output_dir)
        self.ontology_extractor = OntologyExtractor(output_dir=output_dir)
        self.document_linker = EnhancedDocumentLinker()
        self.cosmos_client = None
        self.graph_builder = None
        
        # Enhanced statistics
        self.stats = {
            "agendas_processed": 0,
            "ordinances_linked": 0,
            "resolutions_linked": 0,
            "missing_links": 0,
            "errors": 0,
            "nodes_created": 0,
            "nodes_updated": 0,
            "edges_created": 0,
            "edges_skipped": 0
        }
        
        # Store all missing items for final report
        self.all_missing_items = {}
    
    async def initialize(self):
        """Initialize async components."""
        # Initialize Cosmos DB client
        self.cosmos_client = CosmosGraphClient()
        await self.cosmos_client.connect()
        
        # Initialize graph builder with upsert mode
        self.graph_builder = AgendaGraphBuilder(self.cosmos_client, upsert_mode=self.upsert_mode)
        
        log.info(f"✅ Pipeline initialized (Upsert mode: {'ON' if self.upsert_mode else 'OFF'})")
    
    async def process_agenda(self, agenda_path: Path) -> Dict[str, Any]:
        """Process a single agenda through all pipeline stages."""
        log.info(f"\n{'='*60}")
        log.info(f"📋 Processing agenda: {agenda_path.name}")
        log.info(f"{'='*60}")
        
        result = {
            "agenda": agenda_path.name,
            "status": "pending",
            "stages": {}
        }
        
        try:
            # Stage 1: Extract PDF
            if RUN_EXTRACT_PDF:
                log.info("📄 Stage 1: Extracting PDF content...")
                extracted_data = self.pdf_extractor.extract_agenda(agenda_path)
                result["stages"]["pdf_extraction"] = {
                    "status": "success",
                    "sections": len(extracted_data.get("sections", [])),
                    "hyperlinks": len(extracted_data.get("hyperlinks", {}))
                }
            else:
                log.info("⏭️  Skipping PDF extraction (RUN_EXTRACT_PDF=False)")
                # Load existing extracted data
                extracted_path = self.output_dir / f"{agenda_path.stem}_extracted.json"
                if extracted_path.exists():
                    with open(extracted_path, 'r') as f:
                        extracted_data = json.load(f)
                else:
                    raise FileNotFoundError(f"No extracted data found for {agenda_path.name}")
            
            # Stage 2: Extract Ontology
            if RUN_EXTRACT_ONTOLOGY:
                log.info("🧠 Stage 2: Extracting rich ontology with LLM...")
                ontology = self.ontology_extractor.extract_ontology(agenda_path)
                
                # Verify ontology extraction
                num_sections = len(ontology.get('sections', []))
                num_entities = len(ontology.get('entities', []))
                total_items = sum(len(s.get('items', [])) for s in ontology.get('sections', []))
                
                log.info(f"✅ Ontology extracted: {num_sections} sections, {total_items} items, {num_entities} entities")
                
                result["stages"]["ontology_extraction"] = {
                    "status": "success",
                    "meeting_date": ontology.get("meeting_date"),
                    "sections": num_sections,
                    "agenda_items": total_items,
                    "entities": num_entities,
                    "relationships": len(ontology.get('relationships', []))
                }
            else:
                log.info("⏭️  Skipping ontology extraction (RUN_EXTRACT_ONTOLOGY=False)")
                # Load existing ontology
                ontology_path = self.output_dir / f"{agenda_path.stem}_ontology.json"
                if ontology_path.exists():
                    with open(ontology_path, 'r') as f:
                        ontology = json.load(f)
                else:
                    raise FileNotFoundError(f"No ontology found for {agenda_path.name}")
            
            # Stage 3: Link Documents
            linked_docs = {}
            if RUN_LINK_DOCUMENTS:
                log.info("🔗 Stage 3: Linking ordinance AND resolution documents...")
                meeting_date = ontology.get("meeting_date")
                
                # Pass both directories to the enhanced document linker
                linked_docs = await self.document_linker.link_documents_for_meeting(
                    meeting_date,
                    self.ordinances_dir,      # Ordinances directory
                    self.resolutions_dir      # Resolutions directory
                )
                
                total_linked = len(linked_docs.get("ordinances", [])) + len(linked_docs.get("resolutions", []))
                self.stats["ordinances_linked"] += len(linked_docs.get("ordinances", []))
                self.stats["resolutions_linked"] = self.stats.get("resolutions_linked", 0) + len(linked_docs.get("resolutions", []))
                
                result["stages"]["document_linking"] = {
                    "status": "success",
                    "ordinances_found": len(linked_docs.get("ordinances", [])),
                    "resolutions_found": len(linked_docs.get("resolutions", [])),
                    "total_linked": total_linked
                }
            else:
                log.info("⏭️  Skipping document linking (RUN_LINK_DOCUMENTS=False)")
            
            # Stage 4: Build Graph
            if RUN_BUILD_GRAPH:
                log.info("🏗️  Stage 4: Building enhanced graph representation...")
                graph_data = await self.graph_builder.build_graph_from_ontology(
                    ontology, 
                    agenda_path,
                    linked_docs
                )
                
                # Update stats from graph builder
                self.stats["nodes_created"] += self.graph_builder.stats.get("nodes_created", 0)
                self.stats["nodes_updated"] += self.graph_builder.stats.get("nodes_updated", 0)
                self.stats["edges_created"] += self.graph_builder.stats.get("edges_created", 0)
                self.stats["edges_skipped"] += self.graph_builder.stats.get("edges_skipped", 0)
                
                result["stages"]["graph_building"] = {
                    "status": "success",
                    "sections": graph_data.get("statistics", {}).get("sections", 0),
                    "agenda_items": graph_data.get("statistics", {}).get("items", 0),
                    "entities": graph_data.get("statistics", {}).get("entities", 0),
                    "relationships": graph_data.get("statistics", {}).get("relationships", 0),
                    "nodes_created": self.graph_builder.stats.get("nodes_created", 0),
                    "edges_created": self.graph_builder.stats.get("edges_created", 0)
                }
            else:
                log.info("⏭️  Skipping graph building (RUN_BUILD_GRAPH=False)")
            
            # Stage 5: Validate Links
            if RUN_VALIDATE_LINKS:
                log.info("✅ Stage 5: Validating document links...")
                validation_results = await self._validate_links(ontology, linked_docs)
                result["stages"]["link_validation"] = validation_results
            else:
                log.info("⏭️  Skipping link validation (RUN_VALIDATE_LINKS=False)")
            
            result["status"] = "success"
            self.stats["agendas_processed"] += 1
            
        except Exception as e:
            log.error(f"❌ Error processing {agenda_path.name}: {e}")
            import traceback
            traceback.print_exc()
            result["status"] = "error"
            result["error"] = str(e)
            self.stats["errors"] += 1
        
        return result
    
    async def _validate_links(self, ontology: Dict, linked_docs: Dict) -> Dict[str, Any]:
        """Validate that all agenda items are properly linked."""
        validation_results = {
            "status": "success",
            "total_items": 0,
            "linked_items": 0,
            "unlinked_items": []
        }
        
        # Get all agenda items from sections
        all_items = []
        for section in ontology.get("sections", []):
            all_items.extend(section.get("items", []))
        
        validation_results["total_items"] = len(all_items)
        
        # Get all linked document item codes
        linked_item_codes = set()
        for doc_type in ["ordinances", "resolutions"]:
            for doc in linked_docs.get(doc_type, []):
                if doc.get("item_code"):
                    normalized_code = self.graph_builder.normalize_item_code(doc["item_code"])
                    linked_item_codes.add(normalized_code)
        
        # Check each agenda item
        for item in all_items:
            normalized_code = self.graph_builder.normalize_item_code(item.get("item_code", ""))
            if normalized_code in linked_item_codes:
                validation_results["linked_items"] += 1
            else:
                validation_results["unlinked_items"].append({
                    "item_code": item.get("item_code"),
                    "title": item.get("title", "Unknown"),
                    "document_reference": item.get("document_reference")
                })
        
        return validation_results
    
    async def run(self, agenda_pattern: str = "Agenda*.pdf"):
        """Run the complete pipeline."""
        log.info("🚀 Starting City Clerk Graph Pipeline")
        log.info(f"📁 Base directory: {self.base_dir}")
        log.info(f"📁 Agenda directory: {self.agenda_dir}")
        log.info(f"📁 Ordinances directory: {self.ordinances_dir}")
        log.info(f"📁 Resolutions directory: {self.resolutions_dir}")
        
        # Check directories exist
        if not self.agenda_dir.exists():
            log.error(f"❌ Agenda directory does not exist: {self.agenda_dir}")
            log.info(f"💡 Please ensure the directory structure exists")
            return
        
        if not self.ordinances_dir.exists():
            log.warning(f"⚠️  Ordinances directory does not exist: {self.ordinances_dir}")
            log.info(f"💡 Document linking may fail without ordinances")
        
        if not self.resolutions_dir.exists():
            log.warning(f"⚠️  Resolutions directory does not exist: {self.resolutions_dir}")
            log.info(f"💡 Resolution linking may fail without resolutions directory")
        
        log.info(f"📊 Pipeline stages enabled:")
        log.info(f"   - Extract PDF: {RUN_EXTRACT_PDF}")
        log.info(f"   - Extract Ontology: {RUN_EXTRACT_ONTOLOGY}")
        log.info(f"   - Link Documents: {RUN_LINK_DOCUMENTS}")
        log.info(f"   - Build Graph: {RUN_BUILD_GRAPH}")
        log.info(f"   - Validate Links: {RUN_VALIDATE_LINKS}")
        
        # Initialize components
        await self.initialize()
        
        # Clear graph if requested
        if CLEAR_GRAPH_FIRST and self.cosmos_client:
            log.warning("🗑️  Clearing existing graph data...")
            await self.cosmos_client.clear_graph()
        
        # Find agenda files
        agenda_files = sorted(self.agenda_dir.glob(agenda_pattern))
        log.info(f"📋 Found {len(agenda_files)} agenda files")
        
        if agenda_files:
            log.info("📄 Agenda files found:")
            for f in agenda_files[:5]:
                log.info(f"   - {f.name}")
            if len(agenda_files) > 5:
                log.info(f"   ... and {len(agenda_files) - 5} more")
        
        # Process each agenda
        results = []
        for agenda_path in agenda_files:
            result = await self.process_agenda(agenda_path)
            results.append(result)
        
        # Generate summary report
        await self._generate_report(results)
        
        # Generate consolidated missing items report
        if self.all_missing_items:
            await self._generate_missing_items_report()
        
        # Cleanup
        if self.cosmos_client:
            await self.cosmos_client.close()
        
        log.info("✅ Pipeline complete!")
    
    async def _generate_report(self, results: List[Dict[str, Any]]):
        """Generate pipeline execution report."""
        report = {
            "pipeline_run": datetime.utcnow().isoformat(),
            "statistics": self.stats,
            "agenda_results": results
        }
        
        # Save report
        report_path = self.output_dir / f"pipeline_report_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        # Print summary
        log.info(f"\n{'='*60}")
        log.info("📊 Pipeline Summary:")
        log.info(f"   - Agendas processed: {self.stats['agendas_processed']}")
        log.info(f"   - Ordinances linked: {self.stats['ordinances_linked']}")
        log.info(f"   - Resolutions linked: {self.stats['resolutions_linked']}")
        log.info(f"   - Missing links: {self.stats['missing_links']}")
        log.info(f"   - Errors: {self.stats['errors']}")
        log.info(f"   - Report saved to: {report_path.name}")
        log.info(f"{'='*60}")
    
    async def _generate_missing_items_report(self):
        """Generate consolidated report of missing agenda items."""
        report = {
            "generated_at": datetime.utcnow().isoformat(),
            "summary": {
                "total_agendas_processed": self.stats["agendas_processed"],
                "total_missing_items": self.stats["missing_links"],
                "agendas_with_missing_items": len(self.all_missing_items)
            },
            "by_agenda": self.all_missing_items
        }
        
        report_path = self.output_dir / "consolidated_missing_items_report.json"
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        log.info(f"📋 Missing items report saved to: {report_path.name}")


async def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(description="City Clerk Graph Pipeline")
    parser.add_argument(
        "--base-dir", 
        type=Path,
        default=Path("city_clerk_documents/global"),
        help="Base directory containing City Comissions folder"
    )
    parser.add_argument(
        "--pattern",
        default="Agenda*.pdf",
        help="File pattern for agenda PDFs"
    )
    parser.add_argument(
        "--clear-graph",
        action="store_true",
        help="Clear existing graph before processing"
    )
    parser.add_argument(
        "--no-upsert",
        action="store_true",
        help="Disable upsert mode (fail on existing nodes)"
    )
    
    # Pipeline stage controls
    parser.add_argument("--skip-pdf-extract", action="store_true", help="Skip PDF extraction")
    parser.add_argument("--skip-ontology", action="store_true", help="Skip ontology extraction")
    parser.add_argument("--skip-linking", action="store_true", help="Skip document linking")
    parser.add_argument("--skip-graph", action="store_true", help="Skip graph building")
    parser.add_argument("--skip-validation", action="store_true", help="Skip link validation")
    
    args = parser.parse_args()
    
    # Override global flags based on arguments
    global CLEAR_GRAPH_FIRST, RUN_EXTRACT_PDF, RUN_EXTRACT_ONTOLOGY, RUN_LINK_DOCUMENTS, RUN_BUILD_GRAPH, RUN_VALIDATE_LINKS
    
    if args.clear_graph:
        CLEAR_GRAPH_FIRST = True
    
    if args.skip_pdf_extract:
        RUN_EXTRACT_PDF = False
    if args.skip_ontology:
        RUN_EXTRACT_ONTOLOGY = False
    if args.skip_linking:
        RUN_LINK_DOCUMENTS = False
    if args.skip_graph:
        RUN_BUILD_GRAPH = False
    if args.skip_validation:
        RUN_VALIDATE_LINKS = False
    
    # Create and run pipeline
    pipeline = CityClerkGraphPipeline(
        base_dir=args.base_dir,
        upsert_mode=not args.no_upsert  # Default is True unless --no-upsert
    )
    await pipeline.run(agenda_pattern=args.pattern)


if __name__ == "__main__":
    asyncio.run(main())


================================================================================


################################################################################
# File: scripts/debug_missing_resolutions.py
################################################################################

# File: scripts/debug_missing_resolutions.py

#!/usr/bin/env python3
"""
Debug script to trace why certain resolutions are not being added to the graph.
"""

import asyncio
import logging
from pathlib import Path
from typing import Dict, List, Optional
import json
import re
from datetime import datetime

from graph_stages.enhanced_document_linker import EnhancedDocumentLinker
from graph_stages.cosmos_db_client import CosmosGraphClient
from graph_stages.agenda_graph_builder import AgendaGraphBuilder

# Set up detailed logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
log = logging.getLogger('debug_resolutions')


class ResolutionDebugger:
    """Debug tool for tracing resolution processing issues."""
    
    def __init__(self):
        self.document_linker = EnhancedDocumentLinker()
        self.cosmos_client = None
        self.graph_builder = None
        self.debug_data = {
            "timestamp": datetime.now().isoformat(),
            "target_resolutions": ["2024-04", "2024-05", "2024-06", "2024-07"],
            "meeting_date": "01.09.2024",
            "findings": {}
        }
    
    async def initialize(self):
        """Initialize connections."""
        self.cosmos_client = CosmosGraphClient()
        await self.cosmos_client.connect()
        self.graph_builder = AgendaGraphBuilder(self.cosmos_client, upsert_mode=True)
        log.info("✅ Debugger initialized")
    
    async def debug_full_pipeline(self, 
                                  meeting_date: str,
                                  resolutions_dir: Path,
                                  agenda_json_dir: Path):
        """Run full debugging trace."""
        log.info(f"🔍 Starting debug for meeting date: {meeting_date}")
        log.info(f"📁 Resolutions directory: {resolutions_dir}")
        log.info(f"📁 Agenda JSON directory: {agenda_json_dir}")
        
        # Step 1: Check file system
        await self._debug_filesystem(meeting_date, resolutions_dir)
        
        # Step 2: Check agenda extraction
        await self._debug_agenda_extraction(meeting_date, agenda_json_dir)
        
        # Step 3: Check document linking
        await self._debug_document_linking(meeting_date, resolutions_dir)
        
        # Step 4: Check graph state
        await self._debug_graph_state(meeting_date)
        
        # Step 5: Generate report
        self._generate_debug_report()
    
    async def _debug_filesystem(self, meeting_date: str, resolutions_dir: Path):
        """Debug step 1: Check if files exist in filesystem."""
        log.info("\n📂 STEP 1: Checking filesystem...")
        
        # Convert date format
        date_underscore = meeting_date.replace(".", "_")
        
        # List all files in resolutions directory
        all_files = []
        if resolutions_dir.exists():
            all_files = list(resolutions_dir.rglob("*.pdf"))
            log.info(f"Total PDF files in resolutions directory: {len(all_files)}")
        else:
            log.error(f"❌ Resolutions directory does not exist: {resolutions_dir}")
            self.debug_data["findings"]["filesystem"] = {
                "error": "Resolutions directory does not exist",
                "path": str(resolutions_dir)
            }
            return
        
        # Check for target resolutions
        target_found = {}
        for target in self.debug_data["target_resolutions"]:
            found_files = []
            for file in all_files:
                if target in file.name and date_underscore in file.name:
                    found_files.append(str(file))
                    log.info(f"✅ Found {target}: {file.name} at {file.parent}")
            
            if not found_files:
                # Try alternative patterns
                alt_patterns = [
                    f"{target}*{date_underscore}*.pdf",
                    f"{target}*{meeting_date.replace('.', '-')}*.pdf",
                    f"{target}*.pdf"
                ]
                for pattern in alt_patterns:
                    matches = list(resolutions_dir.rglob(pattern))
                    if matches:
                        found_files.extend([str(m) for m in matches])
                        log.info(f"✅ Found {target} with pattern {pattern}: {[m.name for m in matches]}")
                        break
            
            target_found[target] = found_files
            if not found_files:
                log.warning(f"❌ NOT FOUND: {target} for date {meeting_date}")
        
        self.debug_data["findings"]["filesystem"] = {
            "total_files": len(all_files),
            "target_resolutions_found": target_found,
            "sample_filenames": [f.name for f in all_files[:10]]
        }
    
    async def _debug_agenda_extraction(self, meeting_date: str, agenda_json_dir: Path):
        """Debug step 2: Check if resolutions appear in extracted agenda."""
        log.info("\n📋 STEP 2: Checking agenda extraction...")
        
        # Find ontology file
        ontology_files = list(agenda_json_dir.glob(f"*{meeting_date.replace('.', '*')}*_ontology.json"))
        
        if not ontology_files:
            log.error(f"❌ No ontology file found for date {meeting_date}")
            self.debug_data["findings"]["agenda_extraction"] = {
                "error": "No ontology file found"
            }
            return
        
        ontology_file = ontology_files[0]
        log.info(f"📄 Using ontology file: {ontology_file.name}")
        
        # Load and analyze ontology
        with open(ontology_file, 'r') as f:
            ontology = json.load(f)
        
        # Find all F-section items (resolutions)
        f_items = []
        all_items = []
        
        for section in ontology.get('sections', []):
            for item in section.get('items', []):
                all_items.append(item)
                if item.get('item_code', '').startswith('F'):
                    f_items.append(item)
                    log.info(f"📌 Found F-item: {item['item_code']} - {item.get('document_reference', 'NO_REF')}")
        
        # Check for our target resolutions
        target_items = {}
        for target in self.debug_data["target_resolutions"]:
            found = False
            for item in all_items:
                if target in str(item.get('document_reference', '')):
                    target_items[target] = item
                    found = True
                    log.info(f"✅ Found {target} in agenda as item {item['item_code']}")
                    break
            
            if not found:
                log.warning(f"❌ {target} NOT in agenda items")
                # Check if it might be in the text but not extracted
                self._search_in_agenda_text(target, ontology_file.parent)
        
        self.debug_data["findings"]["agenda_extraction"] = {
            "ontology_file": ontology_file.name,
            "total_items": len(all_items),
            "f_items_count": len(f_items),
            "f_items": [{"code": item['item_code'], "ref": item.get('document_reference')} for item in f_items],
            "target_items_found": {k: v.get('item_code', 'NOT_FOUND') for k, v in target_items.items()}
        }
    
    def _search_in_agenda_text(self, target: str, agenda_dir: Path):
        """Search for resolution reference in raw agenda text."""
        # Look for extracted text file
        text_files = list(agenda_dir.glob("*_extracted.json"))
        for text_file in text_files:
            try:
                with open(text_file, 'r') as f:
                    data = json.load(f)
                    full_text = data.get('full_text', '')
                    if target in full_text:
                        # Find context
                        index = full_text.find(target)
                        context = full_text[max(0, index-200):index+200]
                        log.info(f"🔍 Found {target} in raw text but not extracted as item:")
                        log.info(f"   Context: ...{context}...")
            except:
                pass
    
    async def _debug_document_linking(self, meeting_date: str, resolutions_dir: Path):
        """Debug step 3: Trace document linking process."""
        log.info("\n🔗 STEP 3: Debugging document linking...")
        
        # Run the document linker with debug logging
        debug_dir = Path("city_clerk_documents/graph_json/debug")
        debug_dir.mkdir(exist_ok=True)
        
        # Mock ordinances directory for the enhanced linker
        dummy_ordinances = Path("dummy")
        
        try:
            linked_docs = await self.document_linker.link_documents_for_meeting(
                meeting_date,
                dummy_ordinances,
                resolutions_dir
            )
            
            resolutions = linked_docs.get("resolutions", [])
            log.info(f"📊 Document linker found {len(resolutions)} resolutions")
            
            # Check our targets
            target_linking = {}
            for target in self.debug_data["target_resolutions"]:
                found = None
                for res in resolutions:
                    if target == res.get('document_number'):
                        found = res
                        log.info(f"✅ {target} linked with item code: {res.get('item_code', 'NONE')}")
                        break
                
                if not found:
                    log.warning(f"❌ {target} NOT linked by document linker")
                
                target_linking[target] = {
                    "found": found is not None,
                    "item_code": found.get('item_code') if found else None,
                    "title": found.get('title', '')[:100] if found else None
                }
            
            self.debug_data["findings"]["document_linking"] = {
                "total_resolutions_linked": len(resolutions),
                "target_linking": target_linking,
                "all_linked": [{"num": r['document_number'], "code": r.get('item_code')} 
                              for r in resolutions]
            }
            
        except Exception as e:
            log.error(f"❌ Document linking failed: {e}")
            import traceback
            traceback.print_exc()
            self.debug_data["findings"]["document_linking"] = {
                "error": str(e)
            }
    
    async def _debug_graph_state(self, meeting_date: str):
        """Debug step 4: Check current graph state."""
        log.info("\n🕸️ STEP 4: Checking graph state...")
        
        # Convert date format for graph IDs
        date_dashes = meeting_date.replace('.', '-')
        
        # Check for resolution nodes
        target_nodes = {}
        for target in self.debug_data["target_resolutions"]:
            node_id = f"resolution-{target}"
            exists = await self.cosmos_client.vertex_exists(node_id)
            
            if exists:
                # Get node details
                node = await self.cosmos_client.get_vertex(node_id)
                log.info(f"✅ Node exists: {node_id}")
                log.info(f"   Properties: {node}")
                
                # Check edges
                edges_query = f"g.V('{node_id}').bothE().count()"
                edge_count = await self.cosmos_client._execute_query(edges_query)
                log.info(f"   Edge count: {edge_count[0] if edge_count else 0}")
                
                target_nodes[target] = {
                    "exists": True,
                    "properties": dict(node) if node else {},
                    "edge_count": edge_count[0] if edge_count else 0
                }
            else:
                log.warning(f"❌ Node NOT FOUND: {node_id}")
                target_nodes[target] = {"exists": False}
        
        # Check for agenda items that should link to these resolutions
        log.info("\n🔍 Checking agenda items in graph...")
        for target in self.debug_data["target_resolutions"]:
            # Try different item code patterns
            possible_codes = [f"F-{target.split('-')[1]}", f"F.-{target.split('-')[1]}."]
            for code in possible_codes:
                item_id = f"item-{date_dashes}-{code.replace('.', '')}"
                exists = await self.cosmos_client.vertex_exists(item_id)
                if exists:
                    log.info(f"✅ Agenda item exists: {item_id}")
                    break
                else:
                    log.info(f"❓ Agenda item not found: {item_id}")
        
        self.debug_data["findings"]["graph_state"] = {
            "target_nodes": target_nodes,
            "date_format_used": date_dashes
        }
    
    def _generate_debug_report(self):
        """Generate comprehensive debug report."""
        report_path = Path("city_clerk_documents/graph_json/debug/resolution_debug_report.json")
        report_path.parent.mkdir(exist_ok=True)
        
        with open(report_path, 'w') as f:
            json.dump(self.debug_data, f, indent=2)
        
        log.info(f"\n📊 Debug report saved to: {report_path}")
        
        # Print summary
        log.info("\n" + "="*60)
        log.info("DEBUGGING SUMMARY")
        log.info("="*60)
        
        for step, findings in self.debug_data["findings"].items():
            log.info(f"\n{step.upper()}:")
            if "error" in findings:
                log.info(f"  ❌ ERROR: {findings['error']}")
            else:
                log.info(f"  ✅ Completed")
    
    async def cleanup(self):
        """Clean up resources."""
        if self.cosmos_client:
            await self.cosmos_client.close()


async def main():
    """Run the debugger."""
    import argparse
    
    parser = argparse.ArgumentParser(description="Debug missing resolutions")
    parser.add_argument("--meeting-date", default="01.09.2024", 
                       help="Meeting date in MM.DD.YYYY format")
    parser.add_argument("--resolutions-dir", type=Path,
                       default=Path("city_clerk_documents/global/City Comissions 2024/Resolutions"),
                       help="Resolutions directory")
    parser.add_argument("--agenda-dir", type=Path,
                       default=Path("city_clerk_documents/graph_json"),
                       help="Directory with extracted agenda JSONs")
    
    args = parser.parse_args()
    
    debugger = ResolutionDebugger()
    await debugger.initialize()
    
    try:
        await debugger.debug_full_pipeline(
            args.meeting_date,
            args.resolutions_dir,
            args.agenda_dir
        )
    finally:
        await debugger.cleanup()


if __name__ == "__main__":
    asyncio.run(main())


================================================================================


################################################################################
# File: scripts/pipeline_modular_optimized.py
################################################################################

# File: scripts/pipeline_modular_optimized.py

#!/usr/bin/env python3
"""
Optimized pipeline orchestrator with full parallelization.
Maintains compatibility with original pipeline_modular.py interface.
"""
from __future__ import annotations
import argparse, logging, pathlib, random
from collections import Counter
from rich.console import Console
import asyncio
from typing import List, Optional
import multiprocessing as mp

# Import stages
from stages import extract_clean, llm_enrich, chunk_text, db_upsert, embed_vectors
from stages.acceleration_utils import hardware

# Keep existing toggles
RUN_EXTRACT    = True
RUN_LLM_ENRICH = True
RUN_CHUNK      = True
RUN_DB         = True
RUN_EMBED      = True

log = logging.getLogger("pipeline-modular-optimized")

class OptimizedPipeline:
    """Optimized pipeline with parallel processing and rate limiting."""
    
    def __init__(self, batch_size: int = 15, max_api_concurrent: int = 3):  # 🛡️ Rate limited: was 50, 20
        self.batch_size = batch_size
        self.max_api_concurrent = max_api_concurrent
        self.stats = Counter()
        
        # 🛡️ Log rate limiting settings
        log.info("🛡️  Rate-limited pipeline initialized:")
        log.info(f"   Batch size: {batch_size} (was 50)")
        log.info(f"   Max concurrent: {max_api_concurrent} (was 20)")
        log.info("   Target: <800K tokens/minute (safe margin)")
    
    async def process_batch(self, pdfs: List[pathlib.Path], start_doc_num: int = 1, total_docs: int = None) -> None:
        """Process a batch of PDFs through all stages with individual document progress tracking."""
        
        if total_docs is None:
            total_docs = len(pdfs)
        
        batch_size = len(pdfs)
        log.info(f"📊 BATCH PROCESSING START:")
        log.info(f"   📄 Documents in this batch: {batch_size}")
        log.info(f"   🎯 Document range: {start_doc_num} to {start_doc_num + batch_size - 1}")
        log.info(f"   📈 Overall progress: {start_doc_num-1}/{total_docs} completed ({((start_doc_num-1)/total_docs*100):.1f}%)")
        
        # Stage 1-2: Extract & Clean (CPU-bound, use process pool)
        json_docs = []
        if RUN_EXTRACT:
            log.info(f"🔄 EXTRACTION STAGE - Processing {len(pdfs)} PDFs...")
            for i, pdf in enumerate(pdfs):
                doc_num = start_doc_num + i
                doc_progress = ((doc_num-1) / total_docs * 100)
                log.info(f"📄 [{doc_num}/{total_docs}] ({doc_progress:.1f}%) Extracting: {pdf.name}")
            
            json_docs = await extract_clean.extract_batch_async(
                pdfs, 
                enrich_llm=False  # We'll do LLM enrichment separately
            )
            
            log.info(f"📊 EXTRACTION STAGE COMPLETE:")
            for i, pdf in enumerate(pdfs):
                doc_num = start_doc_num + i
                log.info(f"   ✅ [{doc_num}/{total_docs}] Extracted: {pdf.name}")
        else:
            # Use existing JSON files
            json_docs = [extract_clean.json_path_for(pdf) for pdf in pdfs]
            log.info(f"📊 EXTRACTION STAGE SKIPPED - Using existing JSON files")

        # Stage 4: LLM Enrich (I/O-bound, use async)
        if RUN_LLM_ENRICH and json_docs:
            log.info(f"🔄 ENRICHMENT STAGE - Processing {len(json_docs)} documents...")
            for i, json_doc in enumerate(json_docs):
                doc_num = start_doc_num + i
                doc_progress = ((doc_num-1) / total_docs * 100)
                doc_name = pathlib.Path(json_doc).stem.replace('_clean', '')
                log.info(f"📄 [{doc_num}/{total_docs}] ({doc_progress:.1f}%) Enriching: {doc_name}")
            
            await llm_enrich.enrich_batch_async(
                json_docs,
                max_concurrent=self.max_api_concurrent
            )
            
            log.info(f"📊 ENRICHMENT STAGE COMPLETE:")
            for i, json_doc in enumerate(json_docs):
                doc_num = start_doc_num + i
                doc_name = pathlib.Path(json_doc).stem.replace('_clean', '')
                log.info(f"   ✅ [{doc_num}/{total_docs}] Enriched: {doc_name}")

        # Stage 5: Chunk (CPU-bound, use process pool)
        chunks_map = {}
        if RUN_CHUNK and json_docs:
            log.info(f"🔄 CHUNKING STAGE - Processing {len(json_docs)} documents...")
            for i, json_doc in enumerate(json_docs):
                doc_num = start_doc_num + i
                doc_progress = ((doc_num-1) / total_docs * 100)
                doc_name = pathlib.Path(json_doc).stem.replace('_clean', '')
                log.info(f"📄 [{doc_num}/{total_docs}] ({doc_progress:.1f}%) Chunking: {doc_name}")
            
            chunks_map = await chunk_text.chunk_batch_async(json_docs)
            
            log.info(f"📊 CHUNKING STAGE COMPLETE:")
            total_chunks_created = 0
            for i, json_doc in enumerate(json_docs):
                doc_num = start_doc_num + i
                doc_name = pathlib.Path(json_doc).stem.replace('_clean', '')
                chunk_count = len(chunks_map.get(json_doc, []))
                total_chunks_created += chunk_count
                log.info(f"   ✅ [{doc_num}/{total_docs}] Chunked: {doc_name} ({chunk_count} chunks)")
            log.info(f"   📊 Total chunks created in this batch: {total_chunks_created}")

        # Stage 6: DB Upsert (I/O-bound, use async)
        if RUN_DB and chunks_map:
            log.info(f"🔄 DATABASE UPSERT STAGE - Processing {len(chunks_map)} documents...")
            
            # Show progress before upserting
            total_chunks_to_upsert = sum(len(chunks) for chunks in chunks_map.values())
            log.info(f"📊 DATABASE UPSERT PROGRESS:")
            log.info(f"   💾 Total chunks to upsert: {total_chunks_to_upsert}")
            
            for i, (json_path, chunks) in enumerate(chunks_map.items()):
                doc_num = start_doc_num + i
                doc_progress = ((doc_num-1) / total_docs * 100)
                doc_name = pathlib.Path(json_path).stem.replace('_clean', '')
                log.info(f"📄 [{doc_num}/{total_docs}] ({doc_progress:.1f}%) Upserting: {doc_name} ({len(chunks)} chunks)")
            
            documents = [
                {
                    "json_path": json_path,
                    "chunks": chunks,
                    "do_embed": False  # We'll embed in batch later
                }
                for json_path, chunks in chunks_map.items()
            ]
            await db_upsert.upsert_batch_async(documents)
            
            log.info(f"📊 DATABASE UPSERT STAGE COMPLETE:")
            total_chunks_upserted = 0
            for i, (json_path, chunks) in enumerate(chunks_map.items()):
                doc_num = start_doc_num + i
                doc_name = pathlib.Path(json_path).stem.replace('_clean', '')
                total_chunks_upserted += len(chunks)
                log.info(f"   ✅ [{doc_num}/{total_docs}] Upserted: {doc_name}")
            log.info(f"   📊 Total chunks upserted: {total_chunks_upserted}")

        # Update stats
        self.stats["ok"] += len([c for c in chunks_map.values() if c])
        self.stats["fail"] += len([c for c in chunks_map.values() if not c])
        
        # Final batch summary
        end_doc_num = start_doc_num + batch_size - 1
        overall_progress = (end_doc_num / total_docs * 100)
        log.info(f"📊 BATCH COMPLETE:")
        log.info(f"   ✅ Documents processed: {start_doc_num}-{end_doc_num}")
        log.info(f"   📈 Overall progress: {end_doc_num}/{total_docs} ({overall_progress:.1f}%)")
        log.info(f"   📊 Successful documents: {self.stats['ok']}")
        log.info(f"   ❌ Failed documents: {self.stats['fail']}")
    
    async def run(self, src: pathlib.Path, selection: str = "sequential", cap: int = 0):
        """Run the optimized pipeline."""
        Console().rule("[bold cyan]Misophonia PDF → Vector pipeline (optimized)")
        
        # Get PDF list
        pdfs = [src] if src.is_file() else sorted(src.rglob("*.pdf"))
        if cap:
            pdfs = random.sample(pdfs, cap) if selection == "random" else pdfs[:cap]
        
        total_pdfs = len(pdfs)
        log.info(f"Processing {total_pdfs} PDFs in batches of {self.batch_size}")
        
        # Track PDF-level progress
        pdfs_processed = 0
        
        # Process in batches
        for i in range(0, len(pdfs), self.batch_size):
            batch = pdfs[i:i + self.batch_size]
            batch_num = i//self.batch_size + 1
            total_batches = (len(pdfs) + self.batch_size - 1)//self.batch_size
            
            # Calculate the starting document number for this batch
            start_doc_num = pdfs_processed + 1
            
            # Enhanced progress logging with both batch and PDF-level progress
            log.info(f"📄 Processing batch {batch_num}/{total_batches} ({len(batch)} PDFs)")
            log.info(f"📊 Overall progress: {pdfs_processed}/{total_pdfs} PDFs completed ({pdfs_processed/total_pdfs*100:.1f}%)")
            log.info(f"🔢 Document range: {start_doc_num}-{start_doc_num + len(batch) - 1} of {total_pdfs}")
            
            await self.process_batch(batch, start_doc_num, total_pdfs)
            
            # Update PDF progress counter
            pdfs_processed += len(batch)
            
            # Log completion of this batch
            log.info(f"✅ Batch {batch_num} complete - Total PDFs processed: {pdfs_processed}/{total_pdfs} ({pdfs_processed/total_pdfs*100:.1f}%)")
        
        # Final progress summary
        log.info(f"🎉 All PDFs processed: {pdfs_processed}/{total_pdfs} ({pdfs_processed/total_pdfs*100:.1f}%)")
        
        # Stage 7: Batch embed all at once with conservative settings
        if RUN_EMBED:
            log.info("🔄 STARTING EMBEDDING STAGE...")
            log.info("📊 EMBEDDING STAGE PROGRESS:")
            log.info("   🎯 Running batch embedding with rate limiting...")
            log.info("   🎯 This stage will process all upserted chunks for embedding")
            
            # 🛡️ Pass conservative embedding parameters (consistent with embed_vectors.py defaults)
            await embed_vectors.main_async(
                batch_size=200,     # 🛡️ Rate limited: fetch 200 chunks at once (conservative default)
                commit_size=10,     # 🛡️ Rate limited: 10 chunks per API call (conservative default) 
                max_concurrent=3    # 🛡️ Rate limited: max 3 concurrent embedding calls (conservative default)
            )
            
            log.info("📊 EMBEDDING STAGE COMPLETE ✅")
        
        Console().rule("[green]Finished")
        log.info("📊 PIPELINE COMPLETE - FINAL SUMMARY:")
        log.info("🛡️  Rate limiting successful - no API limits hit")
        log.info(f"📊 Total documents processed: {self.stats['ok']}")
        log.info(f"📊 Total documents failed: {self.stats['fail']}")
        log.info(f"📊 Success rate: {(self.stats['ok']/(self.stats['ok']+self.stats['fail'])*100):.1f}%" if (self.stats['ok']+self.stats['fail']) > 0 else "100%")

def main(src: pathlib.Path, selection: str = "sequential", cap: int = 0) -> None:
    """Main entry point compatible with original pipeline_modular.py"""
    # Set multiprocessing start method for macOS
    mp.set_start_method('spawn', force=True)
    
    # Create and run pipeline with conservative rate limiting
    pipeline = OptimizedPipeline(
        batch_size=15,  # 🛡️ Rate limited: process 15 PDFs at a time (was 50)
        max_api_concurrent=3  # 🛡️ Rate limited: max 3 concurrent API requests (was 20)
    )
    
    # Run async pipeline
    asyncio.run(pipeline.run(src, selection, cap))

if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s — %(levelname)s — %(message)s"
    )
    
    p = argparse.ArgumentParser()
    p.add_argument("src", type=pathlib.Path,
                   default=pathlib.Path("city_clerk_documents/global"), nargs="?")
    p.add_argument("--selection", choices=["sequential", "random"],
                   default="sequential")
    p.add_argument("--cap", type=int, default=0)
    p.add_argument("--batch-size", type=int, default=15,  # 🛡️ Rate limited: default to 15 (was 50)
                   help="Number of PDFs to process in parallel")
    p.add_argument("--api-concurrent", type=int, default=3,  # 🛡️ Rate limited: default to 3 (was 20)
                   help="Max concurrent API calls")
    args = p.parse_args()
    
    # Override batch size if specified
    if args.batch_size:
        pipeline = OptimizedPipeline(
            batch_size=args.batch_size,
            max_api_concurrent=args.api_concurrent
        )
        asyncio.run(pipeline.run(args.src, args.selection, args.cap))
    else:
        main(args.src, args.selection, args.cap)


================================================================================


################################################################################
# File: scripts/graph_stages/pdf_extractor.py
################################################################################

# scripts/graph_stages/pdf_extractor.py

import os
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import logging
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.datamodel.base_models import InputFormat, DocumentStream
from docling.datamodel.pipeline_options import PdfPipelineOptions
import json
from io import BytesIO

# Set up logging
logging.basicConfig(level=logging.INFO)
log = logging.getLogger(__name__)

class PDFExtractor:
    """Extract text from PDFs using Docling for accurate OCR and text extraction."""
    
    def __init__(self, pdf_dir: Path, output_dir: Optional[Path] = None):
        """Initialize the PDF extractor with Docling."""
        self.pdf_dir = Path(pdf_dir)
        self.output_dir = output_dir or Path("city_clerk_documents/extracted_text")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Create debug directory
        self.debug_dir = self.output_dir / "debug"
        self.debug_dir.mkdir(exist_ok=True)
        
        # Initialize Docling converter with OCR enabled
        # Configure for better OCR and structure detection
        pipeline_options = PdfPipelineOptions()
        pipeline_options.do_ocr = True  # Enable OCR for better text extraction
        pipeline_options.do_table_structure = True  # Better table extraction
        
        self.converter = DocumentConverter(
            format_options={
                InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
            }
        )
    
    def extract_text_from_pdf(self, pdf_path: Path) -> Tuple[str, List[Dict[str, str]]]:
        """
        Extract text from PDF using Docling with OCR support.
        
        Returns:
            Tuple of (full_text, pages) where pages is a list of dicts with 'text' and 'page_num'
        """
        log.info(f"📄 Extracting text from: {pdf_path.name}")
        
        # Convert PDF with Docling - just pass the path directly
        result = self.converter.convert(str(pdf_path))
        
        # Get the document
        doc = result.document
        
        # Get the markdown representation which preserves structure better
        full_markdown = doc.export_to_markdown()
        
        # Extract pages if available
        pages = []
        
        # Try to extract page-level content from the document structure
        if hasattr(doc, 'pages') and doc.pages:
            for page_num, page in enumerate(doc.pages, 1):
                # Get page text
                page_text = ""
                if hasattr(page, 'text'):
                    page_text = page.text
                elif hasattr(page, 'get_text'):
                    page_text = page.get_text()
                else:
                    # Try to extract from elements
                    page_elements = []
                    if hasattr(page, 'elements'):
                        for element in page.elements:
                            if hasattr(element, 'text'):
                                page_elements.append(element.text)
                    page_text = "\n".join(page_elements)
                
                if page_text:
                    pages.append({
                        'text': page_text,
                        'page_num': page_num
                    })
        
        # If we couldn't extract pages, create one page with all content
        if not pages and full_markdown:
            pages = [{
                'text': full_markdown,
                'page_num': 1
            }]
        
        # Use markdown as the full text (better structure preservation)
        full_text = full_markdown if full_markdown else ""
        
        # Save debug information
        debug_info = {
            'file': pdf_path.name,
            'total_pages': len(pages),
            'total_characters': len(full_text),
            'extraction_method': 'docling',
            'has_markdown': bool(full_markdown),
            'doc_attributes': list(dir(doc)) if doc else []
        }
        
        debug_file = self.debug_dir / f"{pdf_path.stem}_extraction_debug.json"
        with open(debug_file, 'w', encoding='utf-8') as f:
            json.dump(debug_info, f, indent=2)
        
        # Save the full extracted text for inspection
        text_file = self.debug_dir / f"{pdf_path.stem}_full_text.txt"
        with open(text_file, 'w', encoding='utf-8') as f:
            f.write(full_text)
        
        # Save markdown version separately for debugging
        if full_markdown:
            markdown_file = self.debug_dir / f"{pdf_path.stem}_markdown.md"
            with open(markdown_file, 'w', encoding='utf-8') as f:
                f.write(full_markdown)
        
        log.info(f"✅ Extracted {len(pages)} pages, {len(full_text)} characters")
        
        return full_text, pages
    
    def extract_all_pdfs(self) -> Dict[str, Dict[str, any]]:
        """Extract text from all PDFs in the directory."""
        pdf_files = list(self.pdf_dir.glob("*.pdf"))
        log.info(f"📚 Found {len(pdf_files)} PDF files to process")
        
        extracted_data = {}
        
        for pdf_path in pdf_files:
            try:
                full_text, pages = self.extract_text_from_pdf(pdf_path)
                
                extracted_data[pdf_path.stem] = {
                    'full_text': full_text,
                    'pages': pages,
                    'metadata': {
                        'filename': pdf_path.name,
                        'num_pages': len(pages),
                        'total_chars': len(full_text),
                        'extraction_method': 'docling'
                    }
                }
                
                # Save extracted text
                output_file = self.output_dir / f"{pdf_path.stem}_extracted.json"
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(extracted_data[pdf_path.stem], f, indent=2, ensure_ascii=False)
                
                log.info(f"✅ Saved extracted text to: {output_file}")
                
            except Exception as e:
                log.error(f"❌ Failed to process {pdf_path.name}: {e}")
                import traceback
                traceback.print_exc()
                # No fallback - if Docling fails, we fail
                raise
        
        return extracted_data

# Add this to requirements.txt:
# unstructured[pdf]==0.10.30
# pytesseract>=0.3.10
# pdf2image>=1.16.3
# poppler-utils (system dependency - install with: brew install poppler)


================================================================================


################################################################################
# File: scripts/stages/llm_enrich.py
################################################################################

# File: scripts/stages/llm_enrich.py

#!/usr/bin/env python3
"""
Stage 4 — LLM metadata enrichment with concurrent API calls.
"""
from __future__ import annotations
import json, logging, pathlib, re, os
from textwrap import dedent
from typing import Any, Dict, List
import asyncio
from concurrent.futures import ThreadPoolExecutor
import aiofiles

# ─── minimal shared helpers ────────────────────────────────────────
def _authors(val:Any)->List[str]:
    if val is None: return []
    if isinstance(val,list): return [str(a).strip() for a in val if a]
    return re.split(r"\s*,\s*|\s+and\s+", str(val).strip())

META_FIELDS_CORE = [
    "document_type", "title", "date", "year", "month", "day",
    "mayor", "vice_mayor", "commissioners",
    "city_attorney", "city_manager", "city_clerk", "public_works_director",
    "agenda", "keywords"
]
EXTRA_MD_FIELDS = ["peer_reviewed","open_access","license","open_access_status"]
META_FIELDS     = META_FIELDS_CORE+EXTRA_MD_FIELDS
_DEF_META_TEMPLATE = {**{k:None for k in META_FIELDS_CORE},
                      "doc_type":"scientific paper",
                      "authors":[], "keywords":[], "research_topics":[],
                      "peer_reviewed":None,"open_access":None,
                      "license":None,"open_access_status":None}

def merge_meta(*sources:Dict[str,Any])->Dict[str,Any]:
    merged=_DEF_META_TEMPLATE.copy()
    for src in sources:
        for k in META_FIELDS:
            v=src.get(k)
            if v not in (None,"",[],{}): merged[k]=v
    merged["authors"]=_authors(merged["authors"])
    merged["keywords"]=merged["keywords"] or []
    merged["research_topics"]=merged["research_topics"] or []
    return merged
# ───────────────────────────────────────────────────────────────────

from dotenv import load_dotenv
from openai import OpenAI
load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
MODEL          = "gpt-4.1-mini-2025-04-14"
log            = logging.getLogger(__name__)

def _first_words(txt:str,n:int=3000)->str: return " ".join(txt.split()[:n])

def _gpt(text:str)->Dict[str,Any]:
    if not OPENAI_API_KEY: return {}
    cli=OpenAI(api_key=OPENAI_API_KEY)
    prompt=dedent(f"""
        Extract all metadata fields from this city clerk document. Return ONE JSON object with these fields:
        - document_type: must be one of [Resolution, Ordinance, Proclamation, Contract, Meeting Minutes, Agenda]
        - title: the document title
        - date: full date string as found in document
        - year: numeric year (YYYY)
        - month: numeric month (1-12)
        - day: numeric day of month
        - mayor: name only (e.g., "John Smith") - single person
        - vice_mayor: name only (e.g., "Jane Doe") - single person
        - commissioners: array of commissioner names only (e.g., ["Robert Brown", "Sarah Johnson", "Michael Davis"])
        - city_attorney: name only (e.g., "Emily Wilson")
        - city_manager: name only
        - city_clerk: name only
        - public_works_director: name only
        - agenda: agenda items or meeting topics if present
        - keywords: array of relevant keywords or topics (e.g., ["budget", "zoning", "infrastructure"])
        
        Text:
        {text}
    """)
    rsp=cli.chat.completions.create(model=MODEL,temperature=0,
            messages=[{"role":"system","content":"metadata extractor"},
                      {"role":"user","content":prompt}])
    raw=rsp.choices[0].message.content
    m=re.search(r"{[\s\S]*}",raw)
    return json.loads(m.group(0) if m else "{}")

# Async version of GPT call
async def _gpt_async(text: str, semaphore: asyncio.Semaphore) -> Dict[str, Any]:
    """Async GPT call with rate limiting."""
    if not OPENAI_API_KEY:
        return {}
    
    async with semaphore:  # Rate limiting
        loop = asyncio.get_event_loop()
        # Run synchronous OpenAI call in thread pool
        return await loop.run_in_executor(None, _gpt, text)

async def enrich_async(json_path: pathlib.Path, semaphore: asyncio.Semaphore) -> None:
    """Async version of enrich."""
    # Read file asynchronously
    async with aiofiles.open(json_path, 'r') as f:
        content = await f.read()
        data = json.loads(content)
    
    # Reconstruct body text
    full = " ".join(
        el.get("text", "") for sec in data["sections"] 
        for el in sec.get("elements", [])
    )
    
    # Make async GPT call
    new_meta = await _gpt_async(_first_words(full), semaphore)
    
    # Merge metadata
    data.update(new_meta)
    
    # Write back asynchronously
    async with aiofiles.open(json_path, 'w') as f:
        await f.write(json.dumps(data, indent=2, ensure_ascii=False))
    
    log.info("✓ metadata enriched → %s", json_path.name)

async def enrich_batch_async(
    json_paths: List[pathlib.Path],
    max_concurrent: int = 10
) -> None:
    """Enrich multiple documents concurrently with rate limiting."""
    semaphore = asyncio.Semaphore(max_concurrent)
    
    tasks = [enrich_async(path, semaphore) for path in json_paths]
    
    from tqdm.asyncio import tqdm_asyncio
    await tqdm_asyncio.gather(*tasks, desc="Enriching metadata")

# Keep original interface for compatibility
def enrich(json_path: pathlib.Path) -> None:
    """Original synchronous interface."""
    data = json.loads(json_path.read_text())
    full = " ".join(
        el.get("text", "") for sec in data["sections"] 
        for el in sec.get("elements", [])
    )
    new_meta = _gpt(_first_words(full))
    data.update(new_meta)
    json_path.write_text(json.dumps(data, indent=2, ensure_ascii=False), 'utf-8')
    log.info("✓ metadata enriched → %s", json_path.name)

if __name__ == "__main__":
    import argparse, logging
    logging.basicConfig(level=logging.INFO, format="%(message)s")
    p=argparse.ArgumentParser(); p.add_argument("json",type=pathlib.Path)
    enrich(p.parse_args().json)


================================================================================


################################################################################
# File: scripts/topic_filter_and_title.py
################################################################################

#!/usr/bin/env python3
# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------
"""
Stage 3 — *Topic filter + title extraction*

Loop over every TXT file in **city_clerk_documents/txt/**, send a fixed-length
prefix to GPT-4 to extract a title and decide if it's relevant to misophonia.

The script writes **two helper files in the same */scripts/* folder** (the
"script library", as requested):

  1.  not_about_<topic>.txt    — one TXT filename per line (safe to delete)
  2.  rename_map_<topic>.tsv   — "current_filename<TAB>inferred_title"

Change *TOPIC*, *MAX_WORDS* or *MODEL* below as needed.
"""
from __future__ import annotations

import json
import logging
import os
import time
from pathlib import Path
from typing import Any, Dict, List

from dotenv import load_dotenv
from openai import OpenAI

# ────────────────────────── user-tunable settings ───────────────────────────

TOPIC: str = "Misophonia"            # ← change for other topics
MAX_WORDS: int = 300                 # ≈ two paragraphs
MODEL: str = "gpt-4.1-mini-2025-04-14"
RATE_LIMIT_SLEEP: float = 1.2        # s between API calls to stay polite

# ───────────────────────────── paths & client ───────────────────────────────

SCRIPT_DIR = Path(__file__).resolve().parent            # /scripts/
REPO_ROOT  = SCRIPT_DIR.parent                          # project root
TXT_DIR    = REPO_ROOT / "documents" / "research" / "txt"

OUT_NO_TOPIC = SCRIPT_DIR / f"not_about_{TOPIC.lower()}.txt"
OUT_RENAME   = SCRIPT_DIR / f"rename_map_{TOPIC.lower()}.tsv"

load_dotenv()  # picks up OPENAI_API_KEY from .env or environment
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ─────────────────────────── helper functions ──────────────────────────────

def read_excerpt(txt_path: Path, max_words: int = MAX_WORDS) -> str:
    """
    Return the first *max_words* words of the TXT file.
    """
    text = txt_path.read_text(encoding="utf-8", errors="ignore")
    words = text.split()
    return " ".join(words[:max_words])

def query_llm(content: str) -> Dict[str, Any]:
    """
    Ask GPT-4 whether the paper is about *TOPIC* and get its title.
    Returns a dict like:  { "relevant": true/false, "title": "…" }
    """
    system_msg = (
        "You are a scholarly assistant. You will receive an excerpt from a "
        f"scientific paper. Decide whether the paper is about the topic "
        f"'{TOPIC}'. Respond **ONLY** with valid JSON containing two keys:\n"
        '  "relevant": true or false\n'
        '  "title":    full paper title if present, else ""'
    )
    user_msg = f"Excerpt:\n\"\"\"\n{content}\n\"\"\""

    chat = client.chat.completions.create(
        model=MODEL,
        temperature=0,            # deterministic
        top_p=1,
        max_tokens=256,
        stream=False,
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user",   "content": user_msg},
        ],
        response_format={"type": "json_object"},
    )
    return json.loads(chat.choices[0].message.content)

# ──────────────────────────────── main loop ─────────────────────────────────

def main() -> None:
    logging.basicConfig(level=logging.INFO, format="%(message)s")

    if not TXT_DIR.exists():
        logging.error(f"TXT source folder not found: {TXT_DIR}")
        return

    txt_files = sorted(TXT_DIR.glob("*.txt"))
    if not txt_files:
        logging.error(f"No TXT files in {TXT_DIR}")
        return

    not_about: List[str] = []
    rename_rows: List[str] = []

    for fp in txt_files:
        logging.info(f"→ {fp.name}")
        excerpt = read_excerpt(fp)

        # default values in case of any exception
        relevant = False
        title = ""

        try:
            resp = query_llm(excerpt)
            relevant = bool(resp.get("relevant"))
            title    = (resp.get("title") or "").strip()
        except Exception as e:
            logging.warning(f"  ⚠️  LLM error ({e}); treating as NOT relevant")

        if not relevant:
            not_about.append(fp.name)
        else:
            rename_rows.append(f"{fp.name}\t{title}")

        time.sleep(RATE_LIMIT_SLEEP)   # simple client-side rate-limit

    # ───────────────────────────── write outputs ────────────────────────────
    if not_about:
        OUT_NO_TOPIC.write_text("\n".join(not_about) + "\n", encoding="utf-8")
        logging.info(f"✍️  {len(not_about)} non-topic files → {OUT_NO_TOPIC.name}")

    if rename_rows:
        OUT_RENAME.write_text("\n".join(rename_rows) + "\n", encoding="utf-8")
        logging.info(f"✍️  {len(rename_rows)} rename rows → {OUT_RENAME.name}")

    if not not_about and not rename_rows:
        logging.info("✅ No files processed (empty dataset?).")
    else:
        logging.info("✅ Finished.")

# ───────────────────────── entry point ──────────────────────────────────────

if __name__ == "__main__":
    main()


================================================================================


################################################################################
# File: scripts/debug_ordinance_2024_02.py
################################################################################

# File: scripts/debug_ordinance_2024_02.py

#!/usr/bin/env python3
"""
Debug script to analyze ordinance 2024-02 and find why agenda item extraction failed.
"""

import PyPDF2
from pathlib import Path
import re

def analyze_ordinance_2024_02():
    """Analyze the problematic ordinance document."""
    
    # Find the ordinance file
    ordinance_dir = Path("city_clerk_documents/global/City Comissions 2024/Ordinances/2024")
    ordinance_files = list(ordinance_dir.glob("2024-02*.pdf"))
    
    if not ordinance_files:
        print("❌ Could not find ordinance 2024-02")
        return
    
    ordinance_path = ordinance_files[0]
    print(f"📄 Analyzing: {ordinance_path.name}")
    
    # Extract text
    try:
        with open(ordinance_path, 'rb') as f:
            reader = PyPDF2.PdfReader(f)
            full_text = ""
            
            print(f"📊 Document has {len(reader.pages)} pages")
            
            # Extract all text
            for i, page in enumerate(reader.pages):
                page_text = page.extract_text()
                full_text += page_text
                
                # Check last few pages for agenda items
                if i >= len(reader.pages) - 3:
                    print(f"\n--- Page {i+1} (checking for agenda items) ---")
                    # Look for agenda item patterns
                    agenda_patterns = [
                        r'\(Agenda Item[:\s]*([A-Z][\.-]?\d+)\)',
                        r'Agenda Item[:\s]*([A-Z][\.-]?\d+)',
                        r'Item[:\s]*([A-Z][\.-]?\d+)',
                        r'([A-Z][\.-]?\d+[\.-]?)\s*\n',  # E.-2. at end of line
                    ]
                    
                    for pattern in agenda_patterns:
                        matches = re.findall(pattern, page_text, re.IGNORECASE)
                        if matches:
                            print(f"🎯 Found potential agenda items: {matches}")
            
            # Check the last 2000 characters
            print(f"\n--- Last 2000 characters of document ---")
            print(full_text[-2000:])
            
            # Search entire document for agenda patterns
            print(f"\n--- Searching entire document for agenda patterns ---")
            print(f"Total document length: {len(full_text)} characters")
            
            # More comprehensive search
            comprehensive_patterns = [
                r'\(Agenda\s+Item[:\s]*([A-Z][\.-]?\d+[\.-]?)\)',
                r'Agenda\s+Item[:\s]*([A-Z][\.-]?\d+[\.-]?)',
                r'Item[:\s]*([A-Z][\.-]?\d+[\.-]?)',
                r'([A-Z])\.-(\d+)\.',  # E.-2.
                r'([A-Z])-(\d+)',      # E-2
            ]
            
            for pattern in comprehensive_patterns:
                matches = re.findall(pattern, full_text, re.IGNORECASE | re.MULTILINE)
                if matches:
                    print(f"Pattern '{pattern}' found: {matches}")
            
            # Save full text for manual inspection
            debug_dir = Path("city_clerk_documents/graph_json/debug")
            debug_dir.mkdir(exist_ok=True)
            
            with open(debug_dir / "ordinance_2024_02_full_text.txt", 'w', encoding='utf-8') as f:
                f.write(full_text)
            
            print(f"\n✅ Full text saved to: {debug_dir / 'ordinance_2024_02_full_text.txt'}")
            
    except Exception as e:
        print(f"❌ Error analyzing document: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    analyze_ordinance_2024_02()


================================================================================


################################################################################
# File: scripts/debug_agenda_items.py
################################################################################

# File: scripts/debug_agenda_items.py

#!/usr/bin/env python3
"""
Debug script to check what agenda items were extracted from the agenda.
"""

import json
from pathlib import Path

def check_agenda_items():
    """Check what agenda items were extracted."""
    
    # Find the ontology file
    ontology_dir = Path("city_clerk_documents/graph_json")
    ontology_files = list(ontology_dir.glob("*01.9.2024*_ontology.json"))
    
    if not ontology_files:
        print("❌ No ontology file found for 01.09.2024")
        return
    
    ontology_file = ontology_files[0]
    print(f"📄 Checking ontology: {ontology_file.name}")
    
    # Load the ontology
    with open(ontology_file, 'r') as f:
        ontology = json.load(f)
    
    print(f"\n📅 Meeting date: {ontology.get('meeting_date')}")
    
    # Extract all agenda items
    all_items = []
    agenda_structure = ontology.get('agenda_structure', [])
    
    print(f"\n📑 Found {len(agenda_structure)} sections:")
    
    for section in agenda_structure:
        section_name = section.get('section_name', 'Unknown')
        items = section.get('items', [])
        print(f"\n  Section: {section_name}")
        print(f"  Items in section: {len(items)}")
        
        for item in items:
            item_code = item.get('item_code', 'NO_CODE')
            title = item.get('title', 'No title')[:80]
            all_items.append({
                'code': item_code,
                'title': title,
                'section': section_name
            })
            print(f"    - {item_code}: {title}...")
    
    print(f"\n📊 Total agenda items found: {len(all_items)}")
    
    # Check specifically for E items
    e_items = [item for item in all_items if item['code'].startswith('E')]
    print(f"\n🔍 E-section items found:")
    for item in sorted(e_items, key=lambda x: x['code']):
        print(f"  - {item['code']}: {item['title'][:60]}...")
    
    # Check if E-2 exists
    e2_exists = any(item['code'] in ['E-2', 'E.-2.', 'E.-2', 'E.2'] for item in all_items)
    print(f"\n❓ Does E-2 exist in agenda? {e2_exists}")
    
    # Save all items for inspection
    debug_dir = Path("city_clerk_documents/graph_json/debug")
    debug_dir.mkdir(exist_ok=True)
    
    with open(debug_dir / "all_agenda_items.json", 'w') as f:
        json.dump(all_items, f, indent=2)
    
    print(f"\n✅ All agenda items saved to: {debug_dir / 'all_agenda_items.json'}")

if __name__ == "__main__":
    check_agenda_items()


================================================================================


################################################################################
# File: scripts/test_graph_pipeline.py
################################################################################

# File: scripts/test_graph_pipeline.py

"""
Test script for City Clerk Graph Pipeline
"""
import asyncio
import os
from pathlib import Path
from dotenv import load_dotenv

load_dotenv()

async def test_pipeline():
    """Test basic pipeline functionality."""
    # Check environment variables
    required_vars = ['COSMOS_ENDPOINT', 'COSMOS_KEY', 'GROQ_API_KEY']
    missing = [var for var in required_vars if not os.getenv(var)]
    
    if missing:
        print(f"❌ Missing environment variables: {missing}")
        print("Please set these in your .env file")
        return
    
    # Test imports
    try:
        from graph_stages.cosmos_db_client import CosmosGraphClient
        from graph_stages.agenda_pdf_extractor import AgendaPDFExtractor
        from graph_stages.agenda_ontology_extractor import CityClerkOntologyExtractor
        from graph_stages.document_linker import DocumentLinker
        from graph_stages.agenda_graph_builder import AgendaGraphBuilder
        print("✅ All imports successful")
    except ImportError as e:
        print(f"❌ Import error: {e}")
        return
    
    # Test Cosmos connection
    try:
        cosmos = CosmosGraphClient()
        await cosmos.connect()
        print("✅ Cosmos DB connection successful")
        await cosmos.close()
    except Exception as e:
        print(f"❌ Cosmos DB connection failed: {e}")
        return
    
    # Test PDF extraction
    test_dir = Path("city_clerk_documents/global")
    if test_dir.exists():
        pdfs = list(test_dir.glob("Agenda*.pdf"))
        if pdfs:
            print(f"✅ Found {len(pdfs)} agenda PDFs")
            
            # Test extraction on first PDF
            extractor = AgendaPDFExtractor()
            result = extractor.extract(pdfs[0])
            print(f"✅ Extracted {len(result.get('sections', []))} sections from {pdfs[0].name}")
        else:
            print("⚠️  No agenda PDFs found in test directory")
    else:
        print(f"⚠️  Test directory not found: {test_dir}")
    
    print("\n✅ Pipeline components are functional!")

if __name__ == "__main__":
    asyncio.run(test_pipeline())


================================================================================


################################################################################
# File: scripts/clear_database.py
################################################################################

#!/usr/bin/env python3
"""
Clear Cosmos DB Graph Database
This script will clear all vertices and edges from the graph database.
"""

import asyncio
import sys
import os
from pathlib import Path

# Add scripts directory to path
script_dir = Path(__file__).parent
sys.path.append(str(script_dir))

from graph_stages.cosmos_db_client import CosmosGraphClient


async def clear_database():
    """Clear the entire graph database."""
    print('🗑️  Clearing Cosmos DB graph database...')
    print('⚠️  This will delete ALL vertices and edges!')
    
    client = None
    try:
        # Create and connect client
        client = CosmosGraphClient()
        await client.connect()
        
        print('🗑️  Clearing entire graph...')
        await client.clear_graph()
        
        print('✅ Graph database cleared successfully!')
        return True
        
    except Exception as e:
        print(f'❌ Error clearing database: {e}')
        return False
    
    finally:
        # Ensure proper cleanup
        if client:
            await client.close()


def main():
    """Main entry point with proper async handling."""
    # Create new event loop
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    
    try:
        # Run the async function
        success = loop.run_until_complete(clear_database())
        
        # Exit with appropriate code
        if not success:
            sys.exit(1)
            
    finally:
        # Clean up the loop
        loop.close()


if __name__ == "__main__":
    main()


================================================================================


################################################################################
# File: graph_clear_database.py
################################################################################

# File: graph_clear_database.py

#!/usr/bin/env python3
"""
Clear Cosmos DB Graph Database
This script will clear all vertices and edges from the graph database.
"""

import asyncio
import sys
import os
from pathlib import Path

# Add scripts directory to path
script_dir = Path(__file__).parent / 'scripts'
sys.path.append(str(script_dir))

from graph_stages.cosmos_db_client import CosmosGraphClient

async def clear_database():
    """Clear the entire graph database."""
    print('🗑️  Clearing Cosmos DB graph database...')
    print('⚠️  This will delete ALL vertices and edges!')
    
    try:
        async with CosmosGraphClient() as client:
            await client.clear_graph()
        print('✅ Graph database cleared successfully!')
        return True
        
    except Exception as e:
        print(f'❌ Error clearing database: {e}')
        return False

if __name__ == "__main__":
    success = asyncio.run(clear_database())
    if not success:
        sys.exit(1)


================================================================================


################################################################################
# File: scripts/graph_stages/__init__.py
################################################################################

# File: scripts/graph_stages/__init__.py

"""
Graph Pipeline Stages
=====================
Components for building city clerk document knowledge graph.
"""

from .cosmos_db_client import CosmosGraphClient
from .agenda_pdf_extractor import AgendaPDFExtractor
from .agenda_ontology_extractor import CityClerkOntologyExtractor
from .enhanced_document_linker import EnhancedDocumentLinker
from .agenda_graph_builder import AgendaGraphBuilder

__all__ = [
    'CosmosGraphClient',
    'AgendaPDFExtractor',
    'CityClerkOntologyExtractor',
    'EnhancedDocumentLinker',
    'AgendaGraphBuilder'
]


================================================================================

