# Concatenated Project Code - Part 1 of 3
# Generated: 2025-06-12 12:11:13
# Root Directory: /Users/gianmariatroiani/Documents/knologi/graph_database
================================================================================

# Directory Structure
################################################################################
├── .gitignore (939.0B, no ext)
├── city_clerk_documents/
│   └── [EXCLUDED] 6 items: .DS_Store (excluded file), extracted_markdown (excluded dir), extracted_text (excluded dir)
│       ... and 3 more excluded items
├── config.py (1.7KB, .py)
├── extract_documents_for_graphrag.py (3.5KB, .py)
├── graph_clear_database.py (963.0B, .py)
├── graph_visualizer.py (25.1KB, .py)
├── graphrag_query_ui.py (30.0KB, .py)
├── graphrag_visualization.html (6.1MB, .html)
├── ontology_model.txt (6.6KB, .txt)
├── ontology_modelv2.txt (10.2KB, .txt)
├── pipeline_summary.py (7.9KB, .py)
├── repository_directory_structure.txt (5.7KB, .txt)
├── requirements.txt (1.2KB, .txt)
├── run_city_clerk_pipeline.sh (440.0B, .sh)
├── run_graph_pipeline.sh (1.3KB, .sh)
├── run_graph_visualizer.sh (3.6KB, .sh)
├── run_graphrag.sh (5.6KB, .sh)
├── run_query_ui.sh (761.0B, .sh)
├── scripts/
│   ├── extract_all_to_markdown.py (8.2KB, .py)
│   ├── graph_pipeline.py (21.9KB, .py)
│   ├── graph_stages/
│   │   ├── __init__.py (647.0B, .py)
│   │   ├── agenda_graph_builder.py (63.2KB, .py)
│   │   ├── agenda_ontology_extractor.py (31.4KB, .py)
│   │   ├── agenda_pdf_extractor.py (32.8KB, .py)
│   │   ├── cosmos_db_client.py (10.2KB, .py)
│   │   ├── document_linker.py (13.8KB, .py)
│   │   ├── enhanced_document_linker.py (26.6KB, .py)
│   │   ├── ontology_extractor.py (42.3KB, .py)
│   │   ├── pdf_extractor.py (6.5KB, .py)
│   │   ├── verbatim_transcript_linker.py (19.9KB, .py)
│   │   ├── [EXCLUDED] 1 items: __pycache__ (excluded dir)
│   ├── microsoft_framework/
│   │   ├── README.md (6.9KB, .md)
│   │   ├── __init__.py (1.3KB, .py)
│   │   ├── city_clerk_settings_template.yaml (2.1KB, .yaml)
│   │   ├── cosmos_synchronizer.py (3.5KB, .py)
│   │   ├── document_adapter.py (26.0KB, .py)
│   │   ├── enhanced_entity_deduplicator.py (56.5KB, .py)
│   │   ├── graph_visualizer_microsoft_framework.py (12.1KB, .py)
│   │   ├── graphrag_initializer.py (5.1KB, .py)
│   │   ├── graphrag_output_processor.py (7.7KB, .py)
│   │   ├── graphrag_pipeline.py (2.0KB, .py)
│   │   ├── incremental_processor.py (1.9KB, .py)
│   │   ├── prompt_tuner.py (7.9KB, .py)
│   │   ├── query_engine.py (64.0KB, .py)
│   │   ├── query_router.py (16.1KB, .py)
│   │   ├── run_graphrag.sh (5.6KB, .sh)
│   │   ├── run_graphrag_pipeline.py (20.7KB, .py)
│   │   ├── source_tracker.py (2.6KB, .py)
│   │   ├── test_queries.py (5.7KB, .py)
│   │   ├── [EXCLUDED] 1 items: __pycache__ (excluded dir)
│   ├── [EXCLUDED] 4 items: pipeline_modular_optimized.py (excluded file), rag_local_web_app.py (excluded file), stages (excluded dir)
│       ... and 1 more excluded items
├── settings.yaml (3.4KB, .yaml)
├── [EXCLUDED] 13 items: .DS_Store (excluded file), .env (excluded file), .git (excluded dir)
    ... and 10 more excluded items


================================================================================


# File Index - Which Files Are in Which Parts
################################################################################

## Part 1 (10 files):
  - scripts/microsoft_framework/query_engine.py
  - scripts/graph_stages/agenda_ontology_extractor.py
  - graphrag_query_ui.py
  - scripts/microsoft_framework/query_router.py
  - scripts/graph_stages/document_linker.py
  - scripts/microsoft_framework/README.md
  - scripts/microsoft_framework/cosmos_synchronizer.py
  - extract_documents_for_graphrag.py
  - scripts/microsoft_framework/city_clerk_settings_template.yaml
  - config.py

## Part 2 (11 files):
  - scripts/graph_stages/agenda_graph_builder.py
  - scripts/graph_stages/agenda_pdf_extractor.py
  - scripts/graph_stages/enhanced_document_linker.py
  - scripts/microsoft_framework/run_graphrag_pipeline.py
  - scripts/graph_stages/cosmos_db_client.py
  - scripts/microsoft_framework/graphrag_output_processor.py
  - scripts/microsoft_framework/graphrag_initializer.py
  - settings.yaml
  - scripts/microsoft_framework/graphrag_pipeline.py
  - scripts/microsoft_framework/__init__.py
  - scripts/graph_stages/__init__.py

## Part 3 (10 files):
  - scripts/microsoft_framework/enhanced_entity_deduplicator.py
  - scripts/graph_stages/ontology_extractor.py
  - scripts/microsoft_framework/document_adapter.py
  - scripts/graph_stages/verbatim_transcript_linker.py
  - scripts/extract_all_to_markdown.py
  - scripts/microsoft_framework/prompt_tuner.py
  - scripts/graph_stages/pdf_extractor.py
  - scripts/microsoft_framework/source_tracker.py
  - scripts/microsoft_framework/incremental_processor.py
  - requirements.txt


================================================================================


################################################################################
# File: scripts/microsoft_framework/query_engine.py
################################################################################

# File: scripts/microsoft_framework/query_engine.py

import subprocess
import sys
import os
import json
import re
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Set
from enum import Enum
import logging
from .query_router import SmartQueryRouter, QueryIntent
from .source_tracker import SourceTracker

logger = logging.getLogger(__name__)

class QueryType(Enum):
    LOCAL = "local"
    GLOBAL = "global"
    DRIFT = "drift"

class CityClerkQueryEngine:
    """Enhanced query engine with inline source citations."""
    
    def __init__(self, graphrag_root: Path):
        self.graphrag_root = Path(graphrag_root)
        # Check for deduplicated data and use it if available
        output_dir = self.graphrag_root / "output"
        dedup_dir = output_dir / "deduplicated"
        if dedup_dir.exists() and list(dedup_dir.glob("*.parquet")):
            self.output_dir = dedup_dir
            logger.info("Using deduplicated GraphRAG data")
            
            # Load aliases for better query matching
            import pandas as pd
            entities_df = pd.read_parquet(dedup_dir / "entities.parquet")
            if 'aliases' in entities_df.columns:
                self.entity_aliases = {}
                for idx, row in entities_df.iterrows():
                    if row.get('aliases'):
                        for alias in row['aliases'].split('|'):
                            self.entity_aliases[alias.lower()] = row['title']
        else:
            self.output_dir = output_dir
            self.entity_aliases = {}
        self.source_tracker = SourceTracker()  # New component
        
    def _get_python_executable(self):
        """Get the correct Python executable."""
        from pathlib import Path
        
        current_file = Path(__file__)
        project_root = current_file.parent.parent.parent
        
        venv_python = project_root / "venv" / "bin" / "python3"
        if venv_python.exists():
            return str(venv_python)
        
        return sys.executable
        
    async def query(self, query: str, method: Optional[str] = None, **kwargs) -> Dict[str, Any]:
        """Execute query with source tracking and inline citations."""
        
        # Enable source tracking
        kwargs['track_sources'] = True
        
        # Route query
        if not method:
            router = SmartQueryRouter()
            route_info = router.determine_query_method(query)
            method = route_info['method']
            kwargs.update(route_info.get('params', {}))
        
        # Execute query with source tracking
        if method == 'local':
            result = await self._local_search_with_sources(query, **kwargs)
        elif method == 'global':
            result = await self._global_search_with_sources(query, **kwargs)
        elif method == 'drift':
            result = await self._drift_search_with_sources(query, **kwargs)
        else:
            raise ValueError(f"Unknown method: {method}")
        
        # Clean up any JSON artifacts from the answer
        result['answer'] = self._clean_json_artifacts(result['answer'])
        
        # Process answer to add inline citations
        result['answer'] = self._add_inline_citations(result['answer'], result['sources_used'])
        
        return result
    
    async def _local_search_with_sources(self, query: str, **kwargs) -> Dict[str, Any]:
        """Local search with comprehensive source tracking."""
        
        # Use the existing working local search implementation
        result = await self._execute_local_query(query, kwargs)
        
        # Extract sources from the GraphRAG response
        sources_used = self._extract_sources_from_local_response(result['answer'])
        
        # Add source tracking to the result
        result['sources_used'] = sources_used
        result['data_sources'] = self._format_data_sources(sources_used)
        
        return result
    
    async def _global_search_with_sources(self, query: str, **kwargs) -> Dict[str, Any]:
        """Global search with source tracking."""
        
        # Use the existing working global search implementation
        result = await self._execute_global_query(query, kwargs)
        
        # Extract sources from the GraphRAG response
        sources_used = self._extract_sources_from_global_response(result['answer'])
        
        # Add source tracking to the result
        result['sources_used'] = sources_used
        result['data_sources'] = self._format_data_sources(sources_used)
        
        return result
    
    async def _drift_search_with_sources(self, query: str, **kwargs) -> Dict[str, Any]:
        """DRIFT search with source tracking."""
        
        # Use the existing working drift search implementation
        result = await self._execute_drift_query(query, kwargs)
        
        # Extract sources from the GraphRAG response
        sources_used = self._extract_sources_from_drift_response(result['answer'])
        
        # Add source tracking to the result
        result['sources_used'] = sources_used
        result['data_sources'] = self._format_data_sources(sources_used)
        
        return result
    
    def _clean_json_artifacts(self, answer: str) -> str:
        """Clean JSON artifacts and metadata from GraphRAG response."""
        if not answer:
            return answer
            
        import re
        import json
        
        # Remove JSON blocks that might appear in the response
        # Pattern 1: Remove standalone JSON objects
        json_pattern = r'\{[^{}]*"[^"]*":\s*[^{}]*\}'
        answer = re.sub(json_pattern, '', answer)
        
        # Pattern 2: Remove array-like structures
        array_pattern = r'\[[^\[\]]*"[^"]*"[^\[\]]*\]'
        answer = re.sub(array_pattern, '', answer)
        
        # Pattern 3: Remove configuration-like strings
        config_patterns = [
            r'"[^"]*":\s*"[^"]*"',  # "key": "value"
            r'"[^"]*":\s*\d+',      # "key": 123
            r'"[^"]*":\s*true|false', # "key": true/false
            r'"[^"]*":\s*null',     # "key": null
        ]
        
        for pattern in config_patterns:
            answer = re.sub(pattern, '', answer)
        
        # Remove metadata headers that sometimes appear
        metadata_patterns = [
            r'SUCCESS:\s*.*?\n',
            r'INFO:\s*.*?\n',
            r'DEBUG:\s*.*?\n',
            r'WARNING:\s*.*?\n',
            r'ERROR:\s*.*?\n',
            r'METADATA:\s*.*?\n',
            r'RESPONSE:\s*',
            r'QUERY:\s*.*?\n',
        ]
        
        for pattern in metadata_patterns:
            answer = re.sub(pattern, '', answer, flags=re.IGNORECASE)
        
        # Remove any lines that look like JSON structure
        lines = answer.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            # Skip lines that are purely JSON-like
            if (line.startswith('{') and line.endswith('}')) or \
               (line.startswith('[') and line.endswith(']')) or \
               (line.startswith('"') and line.endswith('"') and ':' in line):
                continue
            # Skip empty lines created by removal
            if line:
                cleaned_lines.append(line)
        
        # Rejoin and clean up extra whitespace
        cleaned_answer = '\n'.join(cleaned_lines)
        
        # Remove multiple consecutive newlines
        cleaned_answer = re.sub(r'\n\s*\n\s*\n', '\n\n', cleaned_answer)
        
        # Remove leading/trailing whitespace
        cleaned_answer = cleaned_answer.strip()
        
        return cleaned_answer
    
    def _add_inline_citations(self, answer: str, sources_used: Dict[str, Any]) -> str:
        """Add inline citations to answer text."""
        
        # Extract entity and relationship IDs for citation
        entity_ids = list(sources_used.get('entities', {}).keys())
        rel_ids = list(sources_used.get('relationships', {}).keys())
        source_ids = list(sources_used.get('sources', {}).keys())
        
        # Split answer into paragraphs
        paragraphs = answer.split('\n\n')
        cited_paragraphs = []
        
        for para in paragraphs:
            if not para.strip():
                cited_paragraphs.append(para)
                continue
            
            # Determine which sources are relevant to this paragraph
            relevant_entities = []
            relevant_rels = []
            relevant_sources = []
            
            # Simple relevance check based on entity mentions
            para_lower = para.lower()
            
            for eid, entity in sources_used.get('entities', {}).items():
                if entity['title'].lower() in para_lower or \
                   any(word in para_lower for word in entity.get('description', '').lower().split()[:10]):
                    relevant_entities.append(str(eid))
            
            for rid, rel in sources_used.get('relationships', {}).items():
                if any(word in para_lower for word in rel.get('description', '').lower().split()[:10]):
                    relevant_rels.append(str(rid))
            
            # Add generic source references
            if relevant_entities or relevant_rels:
                relevant_sources = source_ids[:3]  # Use first few sources
            
            # Build citation
            if relevant_entities or relevant_rels or relevant_sources:
                citation_parts = []
                
                if relevant_sources:
                    citation_parts.append(f"Sources ({', '.join(map(str, relevant_sources[:5]))})")
                
                if relevant_entities:
                    citation_parts.append(f"Entities ({', '.join(relevant_entities[:7])})")
                
                if relevant_rels:
                    citation_parts.append(f"Relationships ({', '.join(relevant_rels[:5])})")
                
                citation = f" Data: {'; '.join(citation_parts)}."
                cited_paragraphs.append(para + citation)
            else:
                cited_paragraphs.append(para)
        
        return '\n\n'.join(cited_paragraphs)
    
    def _format_data_sources(self, sources_used: Dict[str, Any]) -> Dict[str, List[Any]]:
        """Format sources for display."""
        return {
            'entities': list(sources_used.get('entities', {}).values()),
            'relationships': list(sources_used.get('relationships', {}).values()),
            'sources': list(sources_used.get('sources', {}).values()),
            'text_units': list(sources_used.get('text_units', {}).values())
        }
    
    def _extract_sources_from_global_response(self, response: str) -> Dict[str, Any]:
        """Extract sources from global search response with actual data lookup."""
        sources_used = {
            'entities': {},
            'relationships': {},
            'sources': {},
            'text_units': {}
        }
        
        try:
            import pandas as pd
            
            # Load GraphRAG output files
            entities_path = self.graphrag_root / "output" / "entities.parquet"
            community_reports_path = self.graphrag_root / "output" / "community_reports.parquet"
            
            entities_df = None
            reports_df = None
            
            if entities_path.exists():
                entities_df = pd.read_parquet(entities_path)
            if community_reports_path.exists():
                reports_df = pd.read_parquet(community_reports_path)
            
            # Parse community references and entity IDs from response
            import re
            entity_matches = re.findall(r'Entities\s*\(([^)]+)\)', response)
            
            for match in entity_matches:
                ids = [id.strip() for id in match.split(',') if id.strip().replace('+more', '').strip().isdigit()]
                for entity_id_str in ids:
                    entity_id = int(entity_id_str)
                    
                    # Look up actual entity data
                    if entities_df is not None and entity_id in entities_df.index:
                        entity_row = entities_df.loc[entity_id]
                        sources_used['entities'][entity_id] = {
                            'id': entity_id,
                            'title': entity_row.get('title', f'Entity {entity_id}'),
                            'type': entity_row.get('type', 'Unknown'),
                            'description': entity_row.get('description', 'No description available')[:200]
                        }
                    else:
                        # Fallback if not found
                        sources_used['entities'][entity_id] = {
                            'id': entity_id,
                            'title': f'Entity {entity_id}',
                            'type': 'Unknown',
                            'description': 'Entity not found in output'
                        }
            
            # Parse community report references
            report_matches = re.findall(r'Reports\s*\(([^)]+)\)', response)
            
            for match in report_matches:
                ids = [id.strip() for id in match.split(',') if id.strip().replace('+more', '').strip().isdigit()]
                for report_id_str in ids:
                    report_id = int(report_id_str)
                    
                    # Look up actual community report data
                    if reports_df is not None and report_id in reports_df.index:
                        report_row = reports_df.loc[report_id]
                        sources_used['sources'][report_id] = {
                            'id': report_id,
                            'title': f"Community Report #{report_id}",
                            'type': 'community_report',
                            'text_preview': report_row.get('summary', 'No summary available')[:100]
                        }
                    else:
                        # Fallback if not found
                        sources_used['sources'][report_id] = {
                            'id': report_id,
                            'title': f'Community Report {report_id}',
                            'type': 'community_report',
                            'text_preview': 'Report not found in output'
                        }
        
        except Exception as e:
            logger.error(f"Error loading GraphRAG data for global source extraction: {e}")
            import traceback
            traceback.print_exc()
        
        return sources_used
    
    def _extract_sources_from_drift_response(self, response: str) -> Dict[str, Any]:
        """Extract sources from DRIFT search response with actual data lookup."""
        sources_used = {
            'entities': {},
            'relationships': {},
            'sources': {},
            'text_units': {}
        }
        
        try:
            import pandas as pd
            
            # Load GraphRAG output files
            entities_path = self.graphrag_root / "output" / "entities.parquet"
            relationships_path = self.graphrag_root / "output" / "relationships.parquet"
            text_units_path = self.graphrag_root / "output" / "text_units.parquet"
            
            entities_df = None
            relationships_df = None
            text_units_df = None
            
            if entities_path.exists():
                entities_df = pd.read_parquet(entities_path)
            if relationships_path.exists():
                relationships_df = pd.read_parquet(relationships_path)
            if text_units_path.exists():
                text_units_df = pd.read_parquet(text_units_path)
            
            # Parse entity references from DRIFT response
            import re
            entity_matches = re.findall(r'Entities\s*\(([^)]+)\)', response)
            
            for match in entity_matches:
                ids = [id.strip() for id in match.split(',') if id.strip().replace('+more', '').strip().isdigit()]
                for entity_id_str in ids:
                    entity_id = int(entity_id_str)
                    
                    # Look up actual entity data
                    if entities_df is not None and entity_id in entities_df.index:
                        entity_row = entities_df.loc[entity_id]
                        sources_used['entities'][entity_id] = {
                            'id': entity_id,
                            'title': entity_row.get('title', f'Entity {entity_id}'),
                            'type': entity_row.get('type', 'Unknown'),
                            'description': entity_row.get('description', 'No description available')[:200]
                        }
                    else:
                        # Fallback if not found
                        sources_used['entities'][entity_id] = {
                            'id': entity_id,
                            'title': f'Entity {entity_id}',
                            'type': 'Unknown',
                            'description': 'Entity not found in output'
                        }
            
            # Parse relationship references
            rel_matches = re.findall(r'Relationships\s*\(([^)]+)\)', response)
            
            for match in rel_matches:
                ids = [id.strip() for id in match.split(',') if id.strip().replace('+more', '').strip().isdigit()]
                for rel_id_str in ids:
                    rel_id = int(rel_id_str)
                    
                    # Look up actual relationship data
                    if relationships_df is not None and rel_id in relationships_df.index:
                        rel_row = relationships_df.loc[rel_id]
                        sources_used['relationships'][rel_id] = {
                            'id': rel_id,
                            'source': rel_row.get('source', f'Relationship {rel_id}'),
                            'target': rel_row.get('target', 'Unknown'),
                            'description': rel_row.get('description', 'No description available')[:200],
                            'weight': rel_row.get('weight', 0.0)
                        }
                    else:
                        # Fallback if not found
                        sources_used['relationships'][rel_id] = {
                            'id': rel_id,
                            'source': f'Relationship {rel_id}',
                            'target': 'Unknown',
                            'description': 'Relationship not found in output',
                            'weight': 0.0
                        }
            
            # Parse source references (text units)
            source_matches = re.findall(r'Sources\s*\(([^)]+)\)', response)
            
            for match in source_matches:
                ids = [id.strip() for id in match.split(',') if id.strip().replace('+more', '').strip().isdigit()]
                for source_id_str in ids:
                    source_id = int(source_id_str)
                    
                    # Look up actual text unit data
                    if text_units_df is not None and source_id in text_units_df.index:
                        source_row = text_units_df.loc[source_id]
                        sources_used['sources'][source_id] = {
                            'id': source_id,
                            'title': source_row.get('document_ids', f'Source {source_id}'),
                            'type': 'text_unit',
                            'text_preview': source_row.get('text', 'No text available')[:100]
                        }
                    else:
                        # Fallback if not found
                        sources_used['sources'][source_id] = {
                            'id': source_id,
                            'title': f'Source {source_id}',
                            'type': 'document',
                            'text_preview': 'Source not found in output'
                        }
        
        except Exception as e:
            logger.error(f"Error loading GraphRAG data for DRIFT source extraction: {e}")
            import traceback
            traceback.print_exc()
        
        return sources_used
    
    def _extract_sources_from_local_response(self, response: str) -> Dict[str, Any]:
        """Extract sources from local search response with actual data lookup."""
        sources_used = {
            'entities': {},
            'relationships': {},
            'sources': {},
            'text_units': {}
        }
        
        try:
            import pandas as pd
            
            # Load GraphRAG output files
            entities_path = self.graphrag_root / "output" / "entities.parquet"
            relationships_path = self.graphrag_root / "output" / "relationships.parquet"
            text_units_path = self.graphrag_root / "output" / "text_units.parquet"
            
            entities_df = None
            relationships_df = None
            text_units_df = None
            
            if entities_path.exists():
                entities_df = pd.read_parquet(entities_path)
            if relationships_path.exists():
                relationships_df = pd.read_parquet(relationships_path)
            if text_units_path.exists():
                text_units_df = pd.read_parquet(text_units_path)
            
            # Parse entity references from local response
            import re
            entity_matches = re.findall(r'Entities\s*\(([^)]+)\)', response)
            
            for match in entity_matches:
                ids = [id.strip() for id in match.split(',') if id.strip().replace('+more', '').strip().isdigit()]
                for entity_id_str in ids:
                    entity_id = int(entity_id_str)
                    
                    # Look up actual entity data
                    if entities_df is not None and entity_id in entities_df.index:
                        entity_row = entities_df.loc[entity_id]
                        sources_used['entities'][entity_id] = {
                            'id': entity_id,
                            'title': entity_row.get('title', f'Entity {entity_id}'),
                            'type': entity_row.get('type', 'Unknown'),
                            'description': entity_row.get('description', 'No description available')[:200]
                        }
                    else:
                        # Fallback if not found
                        sources_used['entities'][entity_id] = {
                            'id': entity_id,
                            'title': f'Entity {entity_id}',
                            'type': 'Unknown',
                            'description': 'Entity not found in output'
                        }
            
            # Parse relationship references
            rel_matches = re.findall(r'Relationships\s*\(([^)]+)\)', response)
            
            for match in rel_matches:
                ids = [id.strip() for id in match.split(',') if id.strip().replace('+more', '').strip().isdigit()]
                for rel_id_str in ids:
                    rel_id = int(rel_id_str)
                    
                    # Look up actual relationship data
                    if relationships_df is not None and rel_id in relationships_df.index:
                        rel_row = relationships_df.loc[rel_id]
                        sources_used['relationships'][rel_id] = {
                            'id': rel_id,
                            'source': rel_row.get('source', f'Relationship {rel_id}'),
                            'target': rel_row.get('target', 'Unknown'),
                            'description': rel_row.get('description', 'No description available')[:200],
                            'weight': rel_row.get('weight', 0.0)
                        }
                    else:
                        # Fallback if not found
                        sources_used['relationships'][rel_id] = {
                            'id': rel_id,
                            'source': f'Relationship {rel_id}',
                            'target': 'Unknown',
                            'description': 'Relationship not found in output',
                            'weight': 0.0
                        }
            
            # Parse source references (text units)
            source_matches = re.findall(r'Sources\s*\(([^)]+)\)', response)
            
            for match in source_matches:
                ids = [id.strip() for id in match.split(',') if id.strip().replace('+more', '').strip().isdigit()]
                for source_id_str in ids:
                    source_id = int(source_id_str)
                    
                    # Look up actual text unit data
                    if text_units_df is not None and source_id in text_units_df.index:
                        source_row = text_units_df.loc[source_id]
                        sources_used['sources'][source_id] = {
                            'id': source_id,
                            'title': source_row.get('document_ids', f'Source {source_id}'),
                            'type': 'text_unit',
                            'text_preview': source_row.get('text', 'No text available')[:100]
                        }
                    else:
                        # Fallback if not found
                        sources_used['sources'][source_id] = {
                            'id': source_id,
                            'title': f'Source {source_id}',
                            'type': 'document',
                            'text_preview': 'Source not found in output'
                        }
            
        except Exception as e:
            logger.error(f"Error loading GraphRAG data for source extraction: {e}")
            import traceback
            traceback.print_exc()
        
        return sources_used
    

    
    async def _execute_query(self, question: str, method: str, **kwargs) -> Dict[str, Any]:
        """Execute the actual query with original functionality."""
        
        # Auto-route if method not specified
        if method is None:
            route_info = self.router.determine_query_method(question)
            method = route_info['method']
            params = route_info['params']
            intent = route_info['intent']
            
            # Log the routing decision
            logger.info(f"Query: {question}")
            logger.info(f"Routed to: {method} (intent: {intent.value})")
            
            # Check if multiple entities detected
            if 'multiple_entities' in params:
                entity_count = len(params['multiple_entities'])
                logger.info(f"Detected {entity_count} entities in query")
                logger.info(f"Query focus: {'comparison' if params.get('comparison_mode') else 'specific' if params.get('strict_entity_focus') else 'contextual'}")
        else:
            params = kwargs
            intent = None
        
        # Execute query based on method
        if method == "global":
            result = await self._execute_global_query(question, params)
        elif method == "local":
            result = await self._execute_local_query(question, params)
        elif method == "drift":
            result = await self._execute_drift_query(question, params)
        else:
            raise ValueError(f"Unknown query method: {method}")
        
        # Add routing metadata to result
        result['routing_metadata'] = {
            'detected_intent': self._get_intent_type(params),
            'community_context_enabled': params.get('include_community_context', True),
            'query_method': method,
            'entity_count': len(params.get('multiple_entities', [])) if 'multiple_entities' in params else 1 if 'entity_filter' in params else 0
        }
        
        # Extract comprehensive source information
        sources_info = self._extract_sources_from_response(
            result.get('answer', ''), 
            result.get('query_type', method)
        )
        
        # Add both the sources info and the entity chunks if this is a local search
        result['sources_info'] = sources_info
        
        # For local searches, also get the actual entity chunks that were used
        if method == "local" or result.get('query_type') == 'local':
            result['entity_chunks'] = await self._get_entity_chunks(question, params)
        
        return result
    
    async def _extract_data_sources(self, result: Dict[str, Any], method: str) -> Dict[str, Any]:
        """Extract entities, relationships, and sources from query result."""
        data_sources = {
            'entities': [],
            'relationships': [],
            'sources': [],
            'communities': [],
            'text_units': []
        }
        
        try:
            # For local search
            if method == 'local' and 'context_data' in result:
                context = result['context_data']
                
                # Extract entity IDs and details
                if 'entities' in context:
                    entities_df = context['entities']
                    data_sources['entities'] = [
                        {
                            'id': idx,
                            'title': row.get('title', 'Unknown'),
                            'type': row.get('type', 'Unknown'),
                            'description': row.get('description', '')[:100] + '...' if len(row.get('description', '')) > 100 else row.get('description', '')
                        }
                        for idx, row in entities_df.iterrows()
                    ]
                
                # Extract relationship IDs and details
                if 'relationships' in context:
                    relationships_df = context['relationships']
                    data_sources['relationships'] = [
                        {
                            'id': idx,
                            'source': row.get('source', ''),
                            'target': row.get('target', ''),
                            'description': row.get('description', '')[:100] + '...',
                            'weight': row.get('weight', 0)
                        }
                        for idx, row in relationships_df.iterrows()
                    ]
                
                # Extract source documents
                if 'sources' in context:
                    sources_df = context['sources']
                    data_sources['sources'] = [
                        {
                            'id': idx,
                            'title': row.get('title', 'Unknown'),
                            'chunk_id': row.get('chunk_id', ''),
                            'document_type': row.get('document_type', 'Unknown')
                        }
                        for idx, row in sources_df.iterrows()
                    ]
            
            # For global search
            elif method == 'global' and 'context_data' in result:
                context = result['context_data']
                
                # Extract community information
                if 'communities' in context:
                    communities = context['communities']
                    data_sources['communities'] = [
                        {
                            'id': comm.get('id', ''),
                            'title': comm.get('title', 'Community'),
                            'level': comm.get('level', 0),
                            'entity_count': len(comm.get('entities', []))
                        }
                        for comm in communities
                    ]
                
                # Extract entities from communities
                for comm in context.get('communities', []):
                    for entity_id in comm.get('entities', []):
                        # Load entity details
                        entity = await self._get_entity_by_id(entity_id)
                        if entity:
                            data_sources['entities'].append({
                                'id': entity_id,
                                'title': entity.get('title', 'Unknown'),
                                'type': entity.get('type', 'Unknown'),
                                'from_community': comm.get('id', '')
                            })
            
            # Extract text units if available
            if 'text_units' in result.get('context_data', {}):
                text_units = result['context_data']['text_units']
                data_sources['text_units'] = [
                    {
                        'id': unit.get('id', ''),
                        'chunk_id': unit.get('chunk_id', ''),
                        'document': unit.get('document', ''),
                        'text_preview': unit.get('text', '')[:100] + '...'
                    }
                    for unit in text_units[:10]  # Limit to first 10
                ]
                
        except Exception as e:
            logger.error(f"Error extracting data sources: {e}")
            import traceback
            traceback.print_exc()
        
        return data_sources
    
    async def _get_entity_by_id(self, entity_id: int):
        """Get entity details by ID."""
        try:
            entities_path = self.graphrag_root / "output" / "entities.parquet"
            if entities_path.exists():
                import pandas as pd
                entities_df = pd.read_parquet(entities_path)
                if entity_id in entities_df.index:
                    return entities_df.loc[entity_id].to_dict()
        except Exception as e:
            logger.error(f"Error loading entity {entity_id}: {e}")
        return None

    async def _extract_local_context(self, query: str, **kwargs) -> Dict[str, Any]:
        """Manually extract context data for local search."""
        context = {
            'entities': None,
            'relationships': None,
            'sources': None,
            'text_units': []
        }
        
        try:
            import pandas as pd
            
            # Load data files
            entities_path = self.graphrag_root / "output" / "entities.parquet"
            relationships_path = self.graphrag_root / "output" / "relationships.parquet"
            text_units_path = self.graphrag_root / "output" / "text_units.parquet"
            
            # Get top-k entities
            if entities_path.exists():
                entities_df = pd.read_parquet(entities_path)
                
                # Filter based on query relevance (simple keyword matching for now)
                query_terms = query.lower().split()
                relevant_entities = []
                
                for idx, entity in entities_df.iterrows():
                    title = str(entity.get('title', '')).lower()
                    description = str(entity.get('description', '')).lower()
                    
                    # Check if any query term matches
                    if any(term in title or term in description for term in query_terms):
                        relevant_entities.append(idx)
                
                # Get top-k relevant entities
                top_k = kwargs.get('top_k_entities', 10)
                context['entities'] = entities_df.loc[relevant_entities[:top_k]]
                
                # Get relationships for these entities
                if relationships_path.exists() and len(relevant_entities) > 0:
                    relationships_df = pd.read_parquet(relationships_path)
                    
                    # Filter relationships involving our entities
                    entity_set = set(relevant_entities[:top_k])
                    relevant_rels = relationships_df[
                        relationships_df['source'].isin(entity_set) | 
                        relationships_df['target'].isin(entity_set)
                    ]
                    
                    context['relationships'] = relevant_rels
            
            # Get text units
            if text_units_path.exists():
                text_units_df = pd.read_parquet(text_units_path)
                
                # Get relevant text units (simplified - in practice would use embeddings)
                relevant_units = []
                for idx, unit in text_units_df.iterrows():
                    text = str(unit.get('text', '')).lower()
                    if any(term in text for term in query.lower().split()):
                        relevant_units.append({
                            'id': idx,
                            'text': unit.get('text', ''),
                            'chunk_id': unit.get('chunk_id', ''),
                            'document': unit.get('document', '')
                        })
                
                context['text_units'] = relevant_units[:10]
            
            # Extract source documents
            if context['entities'] is not None and not context['entities'].empty:
                # Get unique source documents from entities
                sources = []
                for idx, entity in context['entities'].iterrows():
                    if 'source_document' in entity:
                        sources.append({
                            'id': len(sources),
                            'title': entity['source_document'],
                            'document_type': entity.get('document_type', 'Unknown'),
                            'chunk_id': entity.get('chunk_id', '')
                        })
                
                # Deduplicate sources
                seen = set()
                unique_sources = []
                for source in sources:
                    key = source['title']
                    if key not in seen:
                        seen.add(key)
                        unique_sources.append(source)
                
                context['sources'] = pd.DataFrame(unique_sources)
                
        except Exception as e:
            logger.error(f"Error extracting context: {e}")
            import traceback
            traceback.print_exc()
        
        return context

    def _get_intent_type(self, params: Dict) -> str:
        """Determine intent type from parameters."""
        if params.get('comparison_mode'):
            return 'comparison'
        elif params.get('strict_entity_focus'):
            return 'specific_entity'
        elif params.get('focus_on_relationships'):
            return 'relationships'
        else:
            return 'contextual'
    
    async def _execute_global_query(self, question: str, params: Dict) -> Dict[str, Any]:
        """Execute a global search query."""
        cmd = [
            self._get_python_executable(),
            "-m", "graphrag", "query",
            "--root", str(self.graphrag_root),
            "--method", "global"
        ]
        
        if "community_level" in params:
            cmd.extend(["--community-level", str(params["community_level"])])
        
        cmd.extend(["--query", question])
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        # Clean JSON artifacts from the response
        answer = self._clean_json_artifacts(result.stdout)
        
        return {
            "query": question,
            "query_type": "global",
            "answer": answer,
            "context": self._extract_context(result.stdout),
            "parameters": params
        }
    
    async def _execute_local_query(self, question: str, params: Dict) -> Dict[str, Any]:
        """Execute a local search query with available GraphRAG options."""
        
        # Handle multiple entities
        if "multiple_entities" in params:
            return await self._execute_multi_entity_query(question, params)
        
        # Single entity query - use available GraphRAG options
        cmd = [
            self._get_python_executable(),
            "-m", "graphrag", "query",
            "--root", str(self.graphrag_root),
            "--method", "local"
        ]
        
        # Use community-level to control context (available option)
        if params.get("disable_community", False):
            # Use highest community level to get most specific results
            cmd.extend(["--community-level", "3"])  
            logger.info("Using high community level (3) for specific entity query")
        else:
            # Use default community level for broader context
            cmd.extend(["--community-level", "2"])
            logger.info("Using default community level (2) for contextual query")
        
        # If we have entity filtering request, modify the query to be more specific
        if "entity_filter" in params:
            filter_info = params["entity_filter"]
            entity_type = filter_info['type'].replace('_', ' ').lower()
            entity_value = filter_info['value']
            
            if params.get("strict_entity_focus", False):
                # Make query more specific to focus on just this entity
                enhanced_question = f"Tell me specifically about {entity_type} {entity_value}. Focus only on {entity_value} and do not include information about other items."
                logger.info(f"Enhanced query for strict focus on {entity_value}")
            else:
                # Keep original query but mention the entity
                enhanced_question = f"{question} (specifically about {entity_type} {entity_value})"
                logger.info(f"Enhanced query for contextual information about {entity_value}")
            
            question = enhanced_question
        
        cmd.extend(["--query", question])
        
        logger.debug(f"Executing command: {' '.join(cmd)}")
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        answer = result.stdout
        
        # Clean JSON artifacts from the response
        answer = self._clean_json_artifacts(answer)
        
        # Post-process if strict entity focus is requested
        if params.get("strict_entity_focus", False) and "entity_filter" in params:
            answer = self._filter_to_specific_entity(answer, params["entity_filter"]["value"])
        
        return {
            "query": question,
            "query_type": "local",
            "answer": answer,
            "context": self._extract_context(answer),
            "parameters": params,
            "intent_detection": {
                "specific_entity_focus": params.get("strict_entity_focus", False),
                "community_level_used": 3 if params.get("disable_community") else 2,
                "query_enhanced": "entity_filter" in params
            }
        }
    
    async def _execute_multi_entity_query(self, question: str, params: Dict) -> Dict[str, Any]:
        """Execute queries for multiple entities using available GraphRAG options."""
        entities = params["multiple_entities"]
        all_results = []
        
        # Determine query strategy based on intent
        if params.get("aggregate_results") and params.get("strict_entity_focus"):
            # Query each entity separately with high community level
            logger.info(f"Executing separate queries for {len(entities)} entities")
            
            for entity in entities:
                cmd = [
                    self._get_python_executable(),
                    "-m", "graphrag", "query",
                    "--root", str(self.graphrag_root),
                    "--method", "local",
                    "--community-level", "3"  # High level for specific results
                ]
                
                # Create entity-specific query
                entity_type = entity['type'].replace('_', ' ').lower()
                entity_query = f"Tell me specifically about {entity_type} {entity['value']}. Focus only on {entity['value']}."
                cmd.extend(["--query", entity_query])
                
                result = subprocess.run(cmd, capture_output=True, text=True)
                all_results.append({
                    "entity": entity,
                    "answer": result.stdout
                })
            
            # Combine results
            combined_answer = self._format_multiple_entity_results(all_results, params)
            
        elif params.get("comparison_mode"):
            # Query with all entities for comparison
            logger.info(f"Executing comparison query for entities: {[e['value'] for e in entities]}")
            
            entity_values = [e['value'] for e in entities]
            comparison_query = f"Compare and contrast {' and '.join(entity_values)}. What are the similarities and differences between these items?"
            
            cmd = [
                self._get_python_executable(),
                "-m", "graphrag", "query",
                "--root", str(self.graphrag_root),
                "--method", "local",
                "--community-level", "2",  # Medium level for comparison context
                "--query", comparison_query
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True)
            combined_answer = self._format_comparison_results(result.stdout, entities)
            
        else:
            # Query for relationships between entities
            logger.info(f"Executing relationship query for entities: {[e['value'] for e in entities]}")
            
            entity_values = [e['value'] for e in entities]
            relationship_query = f"How do {' and '.join(entity_values)} relate to each other? What connections exist between these items?"
            
            cmd = [
                self._get_python_executable(),
                "-m", "graphrag", "query",
                "--root", str(self.graphrag_root),
                "--method", "local",
                "--community-level", "1",  # Lower level for broader relationships
                "--query", relationship_query
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True)
            combined_answer = result.stdout
        
        # Clean JSON artifacts from the combined answer
        combined_answer = self._clean_json_artifacts(combined_answer)
        
        return {
            "query": question,
            "query_type": "local",
            "answer": combined_answer,
            "context": self._extract_context(combined_answer),
            "parameters": params,
            "intent_detection": {
                "multi_entity_query": True,
                "entity_count": len(entities),
                "query_mode": "comparison" if params.get("comparison_mode") else "aggregate" if params.get("aggregate_results") else "relationships"
            }
        }
    
    def _format_multiple_entity_results(self, results: List[Dict], params: Dict) -> str:
        """Format results from multiple individual entity queries."""
        formatted = []
        
        formatted.append(f"Information about {len(results)} requested items:\n")
        
        for i, result in enumerate(results, 1):
            entity = result['entity']
            answer = result['answer'].strip()
            
            formatted.append(f"\n{i}. {entity['type'].replace('_', ' ').title()} {entity['value']}:")
            formatted.append("-" * 50)
            
            # Clean and format the answer
            if answer:
                # Remove any GraphRAG metadata/headers and JSON artifacts
                clean_answer = self._clean_graphrag_output(answer)
                clean_answer = self._clean_json_artifacts(clean_answer)
                formatted.append(clean_answer)
            else:
                formatted.append(f"No information found for {entity['value']}")
        
        return "\n".join(formatted)
    
    def _format_comparison_results(self, raw_answer: str, entities: List[Dict]) -> str:
        """Format comparison results to highlight differences and similarities."""
        # Clean JSON artifacts from the raw answer
        clean_answer = self._clean_json_artifacts(raw_answer)
        
        # This could be enhanced with more sophisticated formatting
        formatted = [f"Comparison of {', '.join([e['value'] for e in entities])}:\n"]
        formatted.append(clean_answer)
        
        return "\n".join(formatted)
    
    def _clean_graphrag_output(self, output: str) -> str:
        """Remove GraphRAG metadata and format output cleanly."""
        # Remove common GraphRAG headers/footers
        lines = output.split('\n')
        cleaned_lines = []
        
        for line in lines:
            # Skip metadata lines
            if line.startswith('INFO:') or line.startswith('WARNING:') or line.startswith('DEBUG:'):
                continue
            # Skip empty lines at start/end
            if not line.strip() and (not cleaned_lines or len(cleaned_lines) == len(lines) - 1):
                continue
            cleaned_lines.append(line)
        
        return '\n'.join(cleaned_lines).strip()
    
    async def _execute_drift_query(self, question: str, params: Dict) -> Dict[str, Any]:
        """Execute a DRIFT search query."""
        cmd = [
            self._get_python_executable(),
            "-m", "graphrag", "query",
            "--root", str(self.graphrag_root),
            "--method", "drift"
        ]
        
        cmd.extend(["--query", question])
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        # Clean JSON artifacts from the response
        answer = self._clean_json_artifacts(result.stdout)
        
        return {
            "query": question,
            "query_type": "drift",
            "answer": answer,
            "context": self._extract_context(result.stdout),
            "parameters": params
        }
    
    def _filter_to_specific_entity(self, response: str, target_entity: str) -> str:
        """Aggressively filter response to ONLY information about the target entity."""
        if not target_entity or not response:
            return response
        
        # Split response into sentences
        sentences = response.split('.')
        filtered_sentences = []
        
        for sentence in sentences:
            # Only keep sentences that explicitly mention the target entity
            if target_entity in sentence:
                # Check if any other entity codes are mentioned
                other_entities = re.findall(r'\b[A-Z]-\d+\b', sentence)
                other_entities = [e for e in other_entities if e != target_entity]
                
                # Only keep if no other entities are mentioned
                if not other_entities:
                    filtered_sentences.append(sentence.strip())
        
        if filtered_sentences:
            filtered_response = '. '.join(filtered_sentences) + '.'
            filtered_response = f"Information specifically about {target_entity}:\n\n{filtered_response}"
        else:
            filtered_response = f"Specific information about {target_entity} only."
        
        return filtered_response
    
    def _is_paragraph_about_target(self, paragraph: str, target: str, all_entities: set) -> bool:
        """Determine if a paragraph should be kept in filtered response."""
        if target not in paragraph:
            return False
        
        other_entities = all_entities - {target}
        if not any(entity in paragraph for entity in other_entities):
            return True
        
        target_count = paragraph.count(target)
        other_counts = sum(paragraph.count(entity) for entity in other_entities)
        
        return target_count >= other_counts
    
    def _extract_sources_from_response(self, response: str, method: str) -> Dict[str, Any]:
        """Extract and resolve all source references from GraphRAG response."""
        sources_info = {
            'entities': [],
            'reports': [],
            'raw_references': {},
            'resolved_sources': []
        }
        
        import re
        
        # Parse all reference patterns
        entities_pattern = r'Entities\s*\(([^)]+)\)'
        reports_pattern = r'Reports\s*\(([^)]+)\)'
        sources_pattern = r'Sources\s*\(([^)]+)\)'
        data_pattern = r'Data:\s*(?:Sources\s*\([^)]+\);\s*)?(?:Entities\s*\([^)]+\)|Reports\s*\([^)]+\))'
        
        # Extract all matches
        entities_matches = re.findall(entities_pattern, response)
        reports_matches = re.findall(reports_pattern, response)
        sources_matches = re.findall(sources_pattern, response)
        
        # Store raw references
        sources_info['raw_references'] = {
            'entities': entities_matches,
            'reports': reports_matches,
            'sources': sources_matches
        }
        
        # Parse entity IDs
        all_entity_ids = []
        for match in entities_matches:
            ids = [id.strip() for id in match.split(',') if id.strip().replace('+more', '').strip().isdigit()]
            all_entity_ids.extend(ids)
        
        # Parse report IDs
        all_report_ids = []
        for match in reports_matches:
            ids = [id.strip() for id in match.split(',') if id.strip().replace('+more', '').strip().isdigit()]
            all_report_ids.extend(ids)
        
        # Load and resolve entities
        if all_entity_ids:
            try:
                entities_path = self.graphrag_root / "output/entities.parquet"
                if entities_path.exists():
                    import pandas as pd
                    entities_df = pd.read_parquet(entities_path)
                    
                    for entity_id in all_entity_ids:
                        try:
                            entity_idx = int(entity_id)
                            if entity_idx in entities_df.index:
                                entity = entities_df.loc[entity_idx]
                                sources_info['entities'].append({
                                    'id': entity_idx,
                                    'title': entity['title'],
                                    'type': entity['type'],
                                    'description': entity.get('description', '')[:300]
                                })
                        except Exception as e:
                            logger.error(f"Failed to resolve entity {entity_id}: {e}")
            except Exception as e:
                logger.error(f"Failed to load entities: {e}")
        
        # Load and resolve community reports
        if all_report_ids:
            try:
                reports_path = self.graphrag_root / "output/community_reports.parquet"
                if reports_path.exists():
                    import pandas as pd
                    reports_df = pd.read_parquet(reports_path)
                    
                    for report_id in all_report_ids:
                        try:
                            report_idx = int(report_id)
                            if report_idx in reports_df.index:
                                report = reports_df.loc[report_idx]
                                sources_info['reports'].append({
                                    'id': report_idx,
                                    'title': f"Community Report #{report_idx}",
                                    'summary': report.get('summary', '')[:300] if 'summary' in report else str(report)[:300],
                                    'level': report.get('level', 'unknown') if 'level' in report else 'unknown'
                                })
                        except Exception as e:
                            logger.error(f"Failed to resolve report {report_id}: {e}")
            except Exception as e:
                logger.error(f"Failed to load reports: {e}")
        
        # Create resolved sources list combining everything
        sources_info['resolved_sources'] = sources_info['entities'] + sources_info['reports']
        
        return sources_info

    async def _get_retrieved_entities(self, query: str, params: Dict) -> List[Dict]:
        """Get the actual entities that GraphRAG retrieved for this query."""
        source_entities = []
        
        try:
            entities_path = self.graphrag_root / "output/entities.parquet"
            text_units_path = self.graphrag_root / "output/text_units.parquet"
            relationships_path = self.graphrag_root / "output/relationships.parquet"
            
            if entities_path.exists():
                import pandas as pd
                entities_df = pd.read_parquet(entities_path)
                
                # If we have an entity filter, find that specific entity
                if 'entity_filter' in params:
                    filter_info = params['entity_filter']
                    entity_value = filter_info['value']
                    entity_type = filter_info['type']
                    
                    # Find matching entities by type and value
                    matches = entities_df[
                        (entities_df['type'].str.upper() == entity_type.upper()) &
                        (entities_df['title'].str.contains(entity_value, case=False, na=False))
                    ]
                    
                    # Also find related entities through relationships
                    if relationships_path.exists() and not matches.empty:
                        relationships_df = pd.read_parquet(relationships_path)
                        
                        for _, entity in matches.iterrows():
                            entity_id = entity.name  # Index is the entity ID
                            
                            # Find all relationships involving this entity
                            related_rels = relationships_df[
                                (relationships_df['source'] == entity_id) | 
                                (relationships_df['target'] == entity_id)
                            ]
                            
                            # Add the main entity
                            source_entities.append({
                                'entity_id': entity_id,
                                'title': entity['title'],
                                'type': entity['type'],
                                'description': entity.get('description', '')[:500],
                                'is_primary': True,
                                'source_document': self._trace_entity_to_document(entity_id, entity['title'])
                            })
                            
                            # Add related entities
                            for _, rel in related_rels.iterrows():
                                other_id = rel['target'] if rel['source'] == entity_id else rel['source']
                                if other_id in entities_df.index:
                                    related_entity = entities_df.loc[other_id]
                                    source_entities.append({
                                        'entity_id': other_id,
                                        'title': related_entity['title'],
                                        'type': related_entity['type'],
                                        'description': related_entity.get('description', '')[:300],
                                        'is_primary': False,
                                        'relationship': rel['description'],
                                        'source_document': self._trace_entity_to_document(other_id, related_entity['title'])
                                    })
        
        except Exception as e:
            logger.error(f"Failed to get retrieved entities: {e}")
        
        return source_entities

    async def _get_entity_chunks(self, query: str, params: Dict) -> List[Dict]:
        """Get the actual text chunks/entities that were retrieved."""
        chunks = []
        
        try:
            # Load entities and text units
            entities_path = self.graphrag_root / "output/entities.parquet"
            text_units_path = self.graphrag_root / "output/text_units.parquet"
            
            if entities_path.exists():
                import pandas as pd
                entities_df = pd.read_parquet(entities_path)
                
                # If text units exist, load them too
                text_units_df = None
                if text_units_path.exists():
                    text_units_df = pd.read_parquet(text_units_path)
                
                # Get entities based on the query parameters
                if 'entity_filter' in params:
                    filter_info = params['entity_filter']
                    entity_value = filter_info['value']
                    entity_type = filter_info['type']
                    
                    # Find all matching entities
                    matches = entities_df[
                        (entities_df['type'].str.upper() == entity_type.upper()) & 
                        (entities_df['title'].str.contains(entity_value, case=False, na=False))
                    ]
                    
                    # Also get related entities by looking at descriptions
                    related = entities_df[
                        entities_df['description'].str.contains(entity_value, case=False, na=False)
                    ]
                    
                    all_matches = pd.concat([matches, related]).drop_duplicates()
                    
                    # Convert to chunks format
                    for _, entity in all_matches.iterrows():
                        chunk = {
                            'entity_id': entity.name,
                            'type': entity['type'],
                            'title': entity['title'],
                            'description': entity.get('description', ''),
                            'source': self._trace_entity_to_document(entity.name, entity['title'])
                        }
                        chunks.append(chunk)
        
        except Exception as e:
            logger.error(f"Failed to get entity chunks: {e}")
        
        return chunks

    def _trace_entity_to_document(self, entity_id, entity_title: str) -> Dict:
        """Trace an entity back to its source document."""
        try:
            csv_path = self.graphrag_root / "city_clerk_documents.csv"
            if csv_path.exists():
                import pandas as pd
                docs_df = pd.read_csv(csv_path)
                
                # Extract potential item code from entity title
                import re
                item_match = re.search(r'([A-Z]-\d+)', entity_title)
                doc_match = re.search(r'(\d{4}-\d+)', entity_title)
                
                # Try to find matching document
                for _, doc in docs_df.iterrows():
                    # Check if entity matches document identifiers
                    if (item_match and item_match.group(1) == doc.get('item_code')) or \
                       (doc_match and doc_match.group(1) in str(doc.get('document_number', ''))) or \
                       (entity_title.lower() in str(doc.get('title', '')).lower()):
                        return {
                            'document_id': doc['id'],
                            'title': doc.get('title', ''),
                            'type': doc.get('document_type', ''),
                            'meeting_date': doc.get('meeting_date', ''),
                            'source_file': doc.get('source_file', '')
                        }
        except Exception as e:
            logger.error(f"Failed to trace entity to document: {e}")
        
        return {}
    
    def _extract_context(self, response: str) -> List[Dict]:
        """Extract context and sources from response."""
        context = []
        # Parse response for entity references and sources
        return context

# Legacy compatibility class
class CityClerkGraphRAGQuery(CityClerkQueryEngine):
    """Legacy compatibility wrapper."""
    
    async def query(self, 
                    question: str, 
                    query_type: QueryType = QueryType.LOCAL,
                    community_level: int = 0) -> Dict[str, Any]:
        """Legacy query method for backward compatibility."""
        return await super().query(
            question=question,
            method=query_type.value,
            community_level=community_level
        )

# Example usage function
async def handle_user_query(question: str, graphrag_root: Path = None):
    """Handle user query with intelligent routing."""
    if graphrag_root is None:
        graphrag_root = Path("./graphrag_data")
    
    engine = CityClerkQueryEngine(graphrag_root)
    result = await engine.query(question)
    
    print(f"Query: {question}")
    print(f"Selected method: {result['query_type']}")
    print(f"Detected entities: {result['routing_metadata'].get('entity_count', 0)}")
    print(f"Query intent: {result['routing_metadata'].get('detected_intent', 'unknown')}")
    
    return result


================================================================================


################################################################################
# File: scripts/graph_stages/agenda_ontology_extractor.py
################################################################################

# File: scripts/graph_stages/agenda_ontology_extractor.py

"""
City Clerk Ontology Extractor - FIXED VERSION
Uses OpenAI LLM to extract structured data from city agenda documents.
"""

import logging
import json
import re
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
import os
from groq import Groq
from dotenv import load_dotenv
import asyncio

load_dotenv()

log = logging.getLogger('ontology_extractor')


class CityClerkOntologyExtractor:
    """Extract structured ontology from city clerk documents using LLM."""
    
    def __init__(self, 
                 openai_api_key: Optional[str] = None,
                 model: str = "gpt-4.1-mini-2025-04-14",
                 output_dir: Optional[Path] = None,
                 max_tokens: int = 32768):
        """Initialize the extractor with OpenAI client."""
        self.api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY not found in environment")
        
        self.client = Groq()
        self.model = model
        self.max_tokens = max_tokens
        self.output_dir = output_dir or Path("city_clerk_documents/graph_json")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Create debug directory
        self.debug_dir = self.output_dir / "debug"
        self.debug_dir.mkdir(exist_ok=True)
    
    def extract(self, pdf_path: Path) -> Dict[str, Any]:
        """Extract complete ontology from agenda PDF."""
        log.info(f"🧠 Extracting ontology from {pdf_path.name}")
        
        # First, load the extracted text
        extracted_path = self.output_dir / f"{pdf_path.stem}_extracted.json"
        if extracted_path.exists():
            with open(extracted_path, 'r') as f:
                extracted_data = json.load(f)
        else:
            raise FileNotFoundError(f"No extracted data found for {pdf_path.name}. Run PDF extraction first.")
        
        # Get full text from sections
        full_text = "\n".join(section.get("text", "") for section in extracted_data.get("sections", []))
        
        # Save full text for debugging
        with open(self.debug_dir / f"{pdf_path.stem}_full_text.txt", 'w', encoding='utf-8') as f:
            f.write(full_text)
        
        # Extract meeting date from filename first (more reliable)
        meeting_date = self._extract_meeting_date_from_filename(pdf_path.stem)
        if not meeting_date:
            meeting_date = self._extract_meeting_date(pdf_path.stem, full_text[:1000])
        
        log.info(f"📅 Extracted meeting date: {meeting_date}")
        
        # Step 1: Extract meeting information
        meeting_info = self._extract_meeting_info(full_text[:4000])
        
        # Step 2: Extract complete agenda structure
        agenda_structure = self._extract_agenda_structure(full_text)
        
        # Step 3: Extract entities
        entities = self._extract_entities(full_text[:15000])
        
        # Step 4: Extract relationships between items
        relationships = self._extract_relationships(agenda_structure)
        
        # Build complete ontology
        ontology = {
            "meeting_date": meeting_date,
            "meeting_info": meeting_info,
            "agenda_structure": agenda_structure,
            "entities": entities,
            "relationships": relationships,
            "hyperlinks": extracted_data.get("hyperlinks", {}),
            "metadata": {
                "source_pdf": str(pdf_path.absolute()),
                "extraction_date": datetime.utcnow().isoformat() + "Z",
                "model": self.model
            }
        }
        
        # Save ontology
        output_path = self.output_dir / f"{pdf_path.stem}_ontology.json"
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(ontology, f, indent=2, ensure_ascii=False)
        
        log.info(f"✅ Ontology extraction complete: {len(agenda_structure)} sections, {sum(len(s.get('items', [])) for s in agenda_structure)} items")
        return ontology
    
    def _extract_meeting_date_from_filename(self, filename: str) -> Optional[str]:
        """Extract meeting date from filename like 'Agenda 01.9.2024'."""
        # Try to extract date from filename
        date_patterns = [
            r'(\d{1,2})\.(\d{1,2})\.(\d{4})',  # 01.9.2024
            r'(\d{1,2})-(\d{1,2})-(\d{4})',    # 01-9-2024
            r'(\d{1,2})_(\d{1,2})_(\d{4})',    # 01_9_2024
        ]
        
        for pattern in date_patterns:
            match = re.search(pattern, filename)
            if match:
                month, day, year = match.groups()
                return f"{month.zfill(2)}.{day.zfill(2)}.{year}"
        
        return None
    
    def _extract_meeting_date(self, filename: str, text: str) -> str:
        """Extract meeting date in MM.DD.YYYY format."""
        # Try filename first
        date = self._extract_meeting_date_from_filename(filename)
        if date:
            return date
        
        # Try MM/DD/YYYY format in text
        date_match = re.search(r'(\d{1,2})/(\d{1,2})/(\d{4})', text)
        if date_match:
            month, day, year = date_match.groups()
            return f"{month.zfill(2)}.{day.zfill(2)}.{year}"
        
        # Default fallback
        return "01.09.2024"  # Based on the actual filename
    
    def _clean_json_response(self, json_text: str) -> str:
        """Clean and fix common JSON formatting issues from LLM responses."""
        # Remove markdown code blocks
        json_text = re.sub(r'```json\s*', '', json_text)
        json_text = re.sub(r'\s*```', '', json_text)
        
        # Remove any text before the first { or [
        json_start = json_text.find('{')
        array_start = json_text.find('[')
        
        if json_start == -1 and array_start == -1:
            return json_text
        
        if json_start == -1:
            start_pos = array_start
        elif array_start == -1:
            start_pos = json_start
        else:
            start_pos = min(json_start, array_start)
        
        json_text = json_text[start_pos:]
        
        # Fix common escape issues
        json_text = json_text.replace('\\n', ' ')
        json_text = json_text.replace('\\"', '"')
        json_text = json_text.replace('\\/', '/')
        
        # Fix truncated strings by closing them
        # Count quotes to detect unclosed strings
        in_string = False
        escape_next = False
        cleaned_chars = []
        
        for i, char in enumerate(json_text):
            if escape_next:
                escape_next = False
                cleaned_chars.append(char)
                continue
                
            if char == '\\':
                escape_next = True
                cleaned_chars.append(char)
                continue
                
            if char == '"':
                in_string = not in_string
                
            cleaned_chars.append(char)
        
        # If we end while still in a string, close it
        if in_string:
            cleaned_chars.append('"')
            # Also close any open braces/brackets
            open_braces = cleaned_chars.count('{') - cleaned_chars.count('}')
            open_brackets = cleaned_chars.count('[') - cleaned_chars.count(']')
            
            if open_braces > 0:
                cleaned_chars.append('}' * open_braces)
            if open_brackets > 0:
                cleaned_chars.append(']' * open_brackets)
        
        return ''.join(cleaned_chars)
    
    def _extract_meeting_info(self, text: str) -> Dict[str, Any]:
        """Extract meeting metadata using LLM."""
        prompt = """Analyze this city commission meeting agenda and extract meeting details.

Text:
{text}

IMPORTANT: Return ONLY the JSON object below. Do not include any other text, markdown formatting, or code blocks.

{{
    "meeting_type": "Regular Meeting or Special Meeting or Workshop",
    "meeting_time": "time if mentioned",
    "location": {{
        "name": "venue name",
        "address": "full address"
    }},
    "officials_present": {{
        "mayor": "name or null",
        "vice_mayor": "name or null",
        "commissioners": ["names"],
        "city_attorney": "name or null",
        "city_manager": "name or null",
        "city_clerk": "name or null"
    }}
}}""".format(text=text)
        
        try:
            response = self.client.chat.completions.create(
                model="meta-llama/llama-4-maverick-17b-128e-instruct",
                messages=[
                    {"role": "system", "content": "You are a JSON extractor. Return only valid JSON, no markdown or other formatting."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_completion_tokens=8192,
                top_p=1,
                stream=False,
                stop=None
            )
            
            # Save LLM response for debugging
            raw_response = response.choices[0].message.content.strip()
            with open(self.debug_dir / "meeting_info_llm_response.txt", 'w', encoding='utf-8') as f:
                f.write(raw_response)
            
            # Clean and parse JSON
            json_text = self._clean_json_response(raw_response)
            result = json.loads(json_text)
            
            # Save parsed result
            with open(self.debug_dir / "meeting_info_parsed.json", 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2)
            
            return result
            
        except json.JSONDecodeError as e:
            log.error(f"JSON decode error in meeting info: {e}")
            log.error(f"Raw response saved to debug/meeting_info_llm_response.txt")
            return self._default_meeting_info()
        except Exception as e:
            log.error(f"Failed to extract meeting info: {e}")
            return self._default_meeting_info()
    
    def _extract_agenda_structure(self, text: str) -> List[Dict[str, Any]]:
        """Extract complete agenda structure with all items."""
        # Split text into smaller chunks to avoid token limits
        max_chunk_size = 30000  # characters
        
        if len(text) > max_chunk_size:
            # Process in chunks
            chunks = [text[i:i+max_chunk_size] for i in range(0, len(text), max_chunk_size-1000)]
            all_sections = []
            
            for i, chunk in enumerate(chunks):
                log.info(f"Processing chunk {i+1}/{len(chunks)} for agenda structure")
                sections = self._extract_agenda_structure_chunk(chunk, i)
                all_sections.extend(sections)
            
            return all_sections
        else:
            return self._extract_agenda_structure_chunk(text, 0)
    
    def _extract_agenda_structure_chunk(self, text: str, chunk_num: int) -> List[Dict[str, Any]]:
        """Extract agenda structure from a text chunk."""
        prompt = """Extract the complete agenda structure from this city commission agenda.

Extract ALL sections and their items, including:
- PRESENTATIONS AND PROTOCOL DOCUMENTS
- APPROVAL OF MINUTES
- PUBLIC COMMENTS
- CONSENT AGENDA
- PUBLIC HEARINGS
- RESOLUTIONS
- CITY MANAGER ITEMS
- CITY ATTORNEY ITEMS
- BOARDS AND COMMITTEES
- DISCUSSION ITEMS

Text:
{text}

Return ONLY a JSON array. Each section should have this structure:
[
    {{
        "section_name": "PRESENTATIONS AND PROTOCOL DOCUMENTS",
        "section_type": "PRESENTATIONS",
        "order": 1,
        "items": [
            {{
                "item_code": "A.-1.",
                "document_reference": "23-6764",
                "title": "Presentation of a Proclamation declaring...",
                "item_type": "Presentation"
            }}
        ]
    }},
    {{
        "section_name": "CONSENT AGENDA",
        "section_type": "CONSENT",
        "order": 4,
        "items": [
            {{
                "item_code": "D.-1.",
                "document_reference": "23-6830",
                "title": "A Resolution of the City Commission appointing...",
                "item_type": "Resolution"
            }}
        ]
    }}
]""".format(text=text)
        
        try:
            response = self.client.chat.completions.create(
                model="meta-llama/llama-4-maverick-17b-128e-instruct",
                messages=[
                    {"role": "system", "content": "Extract ALL agenda items. Do not skip any items in the sequence. If you see E-1 and E-3, look carefully for E-2."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_completion_tokens=8192,
                top_p=1,
                stream=False,
                stop=None
            )
            
            raw_response = response.choices[0].message.content.strip()
            
            # Save LLM response for debugging
            with open(self.debug_dir / f"agenda_structure_chunk{chunk_num}_llm_response.txt", 'w', encoding='utf-8') as f:
                f.write(raw_response)
            
            # For OpenAI, just clean the JSON response
            json_text = self._clean_json_response(raw_response)
            
            # Try to parse
            try:
                agenda_structure = json.loads(json_text)
                
                # Validate for missing items in sequence
                all_items = []
                for section in agenda_structure:
                    all_items.extend(section.get('items', []))
                
                # Check for missing E items
                e_items = sorted([item['item_code'] for item in all_items if item['item_code'].startswith('E')])
                if e_items:
                    log.info(f"Found E-section items: {e_items}")
                    # Check for gaps
                    for i in range(len(e_items) - 1):
                        current = e_items[i]
                        next_item = e_items[i + 1]
                        # Extract numbers
                        current_num = int(re.search(r'\d+', current).group())
                        next_num = int(re.search(r'\d+', next_item).group())
                        if next_num - current_num > 1:
                            log.warning(f"⚠️  Gap detected: {current} -> {next_item}. Missing items in between!")
                
            except json.JSONDecodeError:
                # If parsing fails, try to extract items manually from the text
                log.warning(f"Failed to parse LLM response, extracting items manually")
                agenda_structure = self._extract_items_manually(text)
            
            # Save parsed result
            with open(self.debug_dir / f"agenda_structure_chunk{chunk_num}_parsed.json", 'w', encoding='utf-8') as f:
                json.dump(agenda_structure, f, indent=2)
            
            return agenda_structure
            
        except Exception as e:
            log.error(f"Failed to extract agenda structure chunk {chunk_num}: {e}")
            # Try manual extraction as fallback
            return self._extract_items_manually(text)
    
    def _extract_items_manually(self, text: str) -> List[Dict[str, Any]]:
        """Manually extract agenda items using regex patterns."""
        log.info("Attempting manual extraction of agenda items")
        
        sections = []
        current_section = None
        
        # Define section headers (no letter mapping needed)
        section_patterns = [
            (r'^(PRESENTATIONS AND PROTOCOL DOCUMENTS)', 'PRESENTATIONS'),
            (r'^(PUBLIC COMMENTS?)', 'PUBLIC_COMMENTS'),
            (r'^(CONSENT AGENDA)', 'CONSENT'),
            (r'^(ORDINANCES.*)', 'ORDINANCES'),
            (r'^(RESOLUTIONS)', 'RESOLUTIONS'),
            (r'^(BOARDS?/COMMITTEES? ITEMS?)', 'BOARDS_COMMITTEES'),
            (r'^(CITY MANAGER ITEMS?)', 'CITY_MANAGER'),
            (r'^(CITY ATTORNEY ITEMS?)', 'CITY_ATTORNEY'),
            (r'^(CITY CLERK ITEMS?)', 'CITY_CLERK'),
            (r'^(DISCUSSION ITEMS?)', 'DISCUSSION'),
            (r'^([A-Z])\.\s+(.+)$', 'SECTION'),  # Letter-prefixed sections
            (r'^(APPROVAL OF MINUTES)', 'MINUTES'),
            (r'^(PUBLIC HEARINGS)', 'PUBLIC_HEARINGS'),
            (r'^(BOARDS AND COMMITTEES)', 'BOARDS'),
            # Also match letter-prefixed sections like "A. PRESENTATIONS..."
        ]
        
        lines = text.split('\n')
        
        for i, line in enumerate(lines):
            line_stripped = line.strip()
            
            # Skip empty lines
            if not line_stripped:
                continue
            
            # Check for section headers
            section_found = False
            for pattern, section_type in section_patterns:
                match = re.match(pattern, line_stripped, re.IGNORECASE)
                if match:
                    # Save previous section if exists
                    if current_section and current_section["items"]:
                        sections.append(current_section)
                    
                    # Extract section name
                    if section_type == 'SECTION':
                        section_name = match.group(1)
                    else:
                        section_name = match.group(1)
                    
                    current_section = {
                        "section_name": section_name.strip(),
                        "section_type": section_type,
                        "order": len(sections) + 1,
                        "items": []
                    }
                    section_found = True
                    break
            
            if section_found:
                continue
            
            # Check for agenda items
            # Patterns: A.-1. 23-6764, D.-1. 23-6830, 1.-1. 23-6797
            item_patterns = [
                r'^([A-Z]\.-\d+\.?)\s+(\d{2}-\d{4,5})\s+(.+)$',  # A.-1. 23-6764
                r'^(\d+\.-\d+\.?)\s+(\d{2}-\d{4,5})\s+(.+)$',    # 1.-1. 23-6797
                r'^([A-Z]-\d+)\s+(\d{2}-\d{4,5})\s+(.+)$',       # E-1 23-6784
            ]
            
            for pattern in item_patterns:
                match = re.match(pattern, line_stripped)
                if match and current_section:
                    item_code = match.group(1)
                    doc_ref = match.group(2)
                    title = match.group(3).strip()
                    
                    # Determine item type based on title or section
                    item_type = self._determine_item_type(title, current_section.get("section_type", ""))
                    
                    item = {
                        "item_code": item_code,
                        "document_reference": doc_ref,
                        "title": title[:300],
                        "item_type": item_type
                    }
                    
                    current_section["items"].append(item)
                    log.info(f"Extracted {item_type}: {item_code} - {doc_ref}")
                    break
        
        # Don't forget the last section
        if current_section and current_section["items"]:
            sections.append(current_section)
        
        total_items = sum(len(s['items']) for s in sections)
        log.info(f"Manual extraction complete: {total_items} items in {len(sections)} sections")
        
        return sections

    def _normalize_agenda_item_code(self, code: str) -> str:
        """Normalize agenda item code to consistent format for agenda display."""
        # Remove all spaces
        code = code.strip()
        
        # Ensure we have the letter part
        match = re.match(r'([A-Z])\.?-?(\d+)\.?', code)
        if match:
            letter = match.group(1)
            number = match.group(2)
            # Return in consistent format: "E.-9."
            return f"{letter}.-{number}."
        
        return code
    
    def _determine_item_type(self, title: str, section_type: str) -> str:
        """Determine item type from title and section."""
        title_lower = title.lower()
        
        # Check title first for explicit type
        if 'an ordinance' in title_lower:
            return 'Ordinance'
        elif 'a resolution' in title_lower:  # Changed: more specific
            return 'Resolution'
        elif 'proclamation' in title_lower:
            return 'Proclamation'
        elif 'recognition' in title_lower:
            return 'Recognition'
        elif 'congratulations' in title_lower:
            return 'Recognition'
        elif 'presentation' in title_lower:
            return 'Presentation'
        elif section_type == 'PRESENTATIONS':
            return 'Presentation'
        elif section_type == 'MINUTES':
            return 'Minutes Approval'
        else:
            return 'Agenda Item'  # Generic fallback

    def _determine_section_type(self, section_name: str) -> str:
        """Determine section type from section name."""
        section_name_upper = section_name.upper()
        if "RESOLUTION" in section_name_upper:
            return "RESOLUTION"
        elif "ORDINANCE" in section_name_upper:
            return "ORDINANCE"
        elif "COMMISSION" in section_name_upper:
            return "COMMISSION"
        elif "CONSENT" in section_name_upper:
            return "CONSENT"
        else:
            return "GENERAL"
    
    async def extract_entities(self, content: str, context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Extract entities with parallel processing for chunks."""
        # Split content into chunks for parallel processing
        chunk_size = 4000  # Adjust based on model limits
        chunks = [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]
        
        if len(chunks) > 1:
            # Process chunks in parallel
            max_concurrent = min(len(chunks), 5)
            semaphore = asyncio.Semaphore(max_concurrent)
            
            async def extract_from_chunk(chunk, idx):
                async with semaphore:
                    return await self._extract_entities_from_chunk(chunk, context, idx)
            
            # Execute parallel extraction
            chunk_results = await asyncio.gather(
                *[extract_from_chunk(chunk, idx) for idx, chunk in enumerate(chunks)],
                return_exceptions=True
            )
            
            # Merge results
            all_entities = []
            for result in chunk_results:
                if isinstance(result, Exception):
                    log.error(f"Chunk extraction error: {result}")
                elif result:
                    all_entities.extend(result)
            
            # Deduplicate entities
            return self._deduplicate_entities(all_entities)
        else:
            # Single chunk, process normally
            return await self._extract_entities_from_chunk(content, context, 0)

    # Add helper method
    async def _extract_entities_from_chunk(self, chunk: str, context: Dict[str, Any], chunk_idx: int) -> List[Dict[str, Any]]:
        """Extract entities from a single chunk."""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            None,
            self._extract_entities_sync,
            chunk,
            context,
            chunk_idx
        )
    
    def _extract_entities_sync(self, chunk: str, context: Dict[str, Any], chunk_idx: int) -> List[Dict[str, Any]]:
        """Synchronous entity extraction for executor."""
        prompt = """Extract entities from this city agenda document.

Text:
{text}

IMPORTANT: Return ONLY the JSON object below. No markdown, no code blocks, no other text.

{{
    "people": [
        {{"name": "John Smith", "role": "Mayor", "context": "presiding"}}
    ],
    "organizations": [
        {{"name": "City Commission", "type": "government", "context": "governing body"}}
    ],
    "locations": [
        {{"name": "City Hall", "address": "405 Biltmore Way", "type": "government building"}}
    ],
    "monetary_amounts": [
        {{"amount": "$100,000", "purpose": "budget allocation", "context": "parks improvement"}}
    ],
    "dates": [
        {{"date": "01/23/2024", "event": "meeting date", "type": "meeting"}}
    ],
    "legal_references": [
        {{"type": "Resolution", "number": "2024-01", "title": "Budget Amendment"}}
    ]
}}""".format(text=chunk[:10000])  # Limit text to avoid token issues
        
        try:
            response = self.client.chat.completions.create(
                model="meta-llama/llama-4-maverick-17b-128e-instruct",
                messages=[
                    {"role": "system", "content": "Extract entities. Return only JSON, no formatting."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_completion_tokens=8192,
                top_p=1,
                stream=False,
                stop=None
            )
            
            raw_response = response.choices[0].message.content.strip()
            
            # Save LLM response for debugging
            with open(self.debug_dir / f"entities_chunk_{chunk_idx}_llm_response.txt", 'w', encoding='utf-8') as f:
                f.write(raw_response)
            
            # Clean and parse JSON
            json_text = self._clean_json_response(raw_response)
            entities_dict = json.loads(json_text)
            
            # Convert to list format
            entities = []
            for entity_type, entity_list in entities_dict.items():
                for entity in entity_list:
                    entity['type'] = entity_type
                    entities.append(entity)
            
            # Save parsed result
            with open(self.debug_dir / f"entities_chunk_{chunk_idx}_parsed.json", 'w', encoding='utf-8') as f:
                json.dump(entities, f, indent=2)
            
            return entities
            
        except json.JSONDecodeError as e:
            log.error(f"JSON decode error in entities chunk {chunk_idx}: {e}")
            return []
        except Exception as e:
            log.error(f"Failed to extract entities from chunk {chunk_idx}: {e}")
            return []
    
    def _deduplicate_entities(self, entities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Remove duplicate entities based on name/identifier."""
        seen = set()
        unique_entities = []
        
        for entity in entities:
            # Create identifier based on name or other unique field
            identifier = entity.get('name', entity.get('amount', entity.get('date', str(entity))))
            if identifier not in seen:
                seen.add(identifier)
                unique_entities.append(entity)
        
        return unique_entities
    
    def _extract_entities(self, text: str) -> Dict[str, List[Dict[str, Any]]]:
        """Extract all entities mentioned in the document (legacy sync method)."""
        # This is the original synchronous method for backward compatibility
        prompt = """Extract entities from this city agenda document.

Text:
{text}

IMPORTANT: Return ONLY the JSON object below. No markdown, no code blocks, no other text.

{{
    "people": [
        {{"name": "John Smith", "role": "Mayor", "context": "presiding"}}
    ],
    "organizations": [
        {{"name": "City Commission", "type": "government", "context": "governing body"}}
    ],
    "locations": [
        {{"name": "City Hall", "address": "405 Biltmore Way", "type": "government building"}}
    ],
    "monetary_amounts": [
        {{"amount": "$100,000", "purpose": "budget allocation", "context": "parks improvement"}}
    ],
    "dates": [
        {{"date": "01/23/2024", "event": "meeting date", "type": "meeting"}}
    ],
    "legal_references": [
        {{"type": "Resolution", "number": "2024-01", "title": "Budget Amendment"}}
    ]
}}""".format(text=text[:10000])  # Limit text to avoid token issues
        
        try:
            response = self.client.chat.completions.create(
                model="meta-llama/llama-4-maverick-17b-128e-instruct",
                messages=[
                    {"role": "system", "content": "Extract entities. Return only JSON, no formatting."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_completion_tokens=8192,
                top_p=1,
                stream=False,
                stop=None
            )
            
            raw_response = response.choices[0].message.content.strip()
            
            # Save LLM response for debugging
            with open(self.debug_dir / "entities_llm_response.txt", 'w', encoding='utf-8') as f:
                f.write(raw_response)
            
            # Clean and parse JSON
            json_text = self._clean_json_response(raw_response)
            entities = json.loads(json_text)
            
            # Save parsed result
            with open(self.debug_dir / "entities_parsed.json", 'w', encoding='utf-8') as f:
                json.dump(entities, f, indent=2)
            
            return entities
            
        except json.JSONDecodeError as e:
            log.error(f"JSON decode error in entities: {e}")
            log.error(f"Raw response saved to debug/entities_llm_response.txt")
            return self._default_entities()
        except Exception as e:
            log.error(f"Failed to extract entities: {e}")
            return self._default_entities()
    
    def _extract_relationships(self, agenda_structure: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Extract relationships between agenda items."""
        relationships = []
        
        # Find related items based on common references
        all_items = []
        for section in agenda_structure:
            for item in section.get("items", []):
                item["section"] = section.get("section_name")
                all_items.append(item)
        
        # Save all items for debugging
        with open(self.debug_dir / "all_agenda_items.json", 'w', encoding='utf-8') as f:
            json.dump(all_items, f, indent=2)
        
        # Look for items that reference each other
        for i, item1 in enumerate(all_items):
            for j, item2 in enumerate(all_items[i+1:], i+1):
                # Check if items share document references
                if (item1.get("document_reference") and 
                    item1.get("document_reference") == item2.get("document_reference")):
                    relationships.append({
                        "from_code": item1.get("item_code"),
                        "to_code": item2.get("item_code"),
                        "relationship_type": "REFERENCES_SAME_DOCUMENT",
                        "description": f"Both reference document {item1.get('document_reference')}"
                    })
        
        return relationships
    
    def _default_meeting_info(self) -> Dict[str, Any]:
        """Return default meeting info structure."""
        return {
            "meeting_type": "Regular Meeting",
            "meeting_time": "9:00 a.m.",
            "location": {
                "name": "City Hall, Commission Chambers",
                "address": "405 Biltmore Way, Coral Gables, FL 33134"
            },
            "officials_present": {
                "mayor": None,
                "vice_mayor": None,
                "commissioners": [],
                "city_attorney": None,
                "city_manager": None,
                "city_clerk": None
            }
        }
    
    def _default_entities(self) -> Dict[str, List]:
        """Return default empty entities structure."""
        return {
            "people": [],
            "organizations": [],
            "locations": [],
            "monetary_amounts": [],
            "dates": [],
            "legal_references": []
        }


================================================================================


################################################################################
# File: graphrag_query_ui.py
################################################################################

# File: graphrag_query_ui.py

#!/usr/bin/env python3
"""
GraphRAG Query UI
A web interface for querying the City Clerk GraphRAG knowledge base.
"""

import dash
from dash import dcc, html, Input, Output, State, ctx
import dash_bootstrap_components as dbc
from dash.exceptions import PreventUpdate
import asyncio
from pathlib import Path
import sys
import json
from datetime import datetime
import logging
from typing import Dict, Any


# Add project root to path
# Handle both cases: script in root or in a subdirectory
current_file = Path(__file__).resolve()
if current_file.parent.name == "scripts" or current_file.parent.name == "ui":
    project_root = current_file.parent.parent
else:
    project_root = current_file.parent
sys.path.append(str(project_root))

from scripts.microsoft_framework import (
    CityClerkQueryEngine,
    SmartQueryRouter,
    QueryIntent
)

# Set up logging
logging.basicConfig(level=logging.INFO)
log = logging.getLogger(__name__)

# Initialize the app with a nice theme
app = dash.Dash(
    __name__, 
    external_stylesheets=[dbc.themes.BOOTSTRAP],
    title="GraphRAG City Clerk Query System"
)

# Initialize query engine and router
GRAPHRAG_ROOT = project_root / "graphrag_data"
query_engine = None
query_router = SmartQueryRouter()

# Store query history
query_history = []

# Define the layout
app.layout = dbc.Container([
    dbc.Row([
        dbc.Col([
            html.H1("🏛️ City Clerk GraphRAG Query System", className="text-center mb-4"),
            html.Hr(),
        ])
    ]),
    
    # Query Input Section
    dbc.Row([
        dbc.Col([
            dbc.Card([
                dbc.CardHeader(html.H4("🔍 Enter Your Query")),
                dbc.CardBody([
                    dbc.Textarea(
                        id="query-input",
                        placeholder="Ask about agenda items, ordinances, resolutions, or city proceedings...\n\nExamples:\n- What is agenda item E-1?\n- Tell me about ordinance 2024-01\n- What are the main development themes?\n- How has zoning policy evolved?",
                        style={"height": "150px"},
                        className="mb-3"
                    ),
                    
                    dbc.Row([
                        dbc.Col([
                            html.Label("Query Method:", className="fw-bold"),
                            dbc.RadioItems(
                                id="query-method",
                                options=[
                                    {"label": "🤖 Auto-Select (Recommended)", "value": "auto"},
                                    {"label": "🎯 Local Search", "value": "local"},
                                    {"label": "🌐 Global Search", "value": "global"},
                                    {"label": "🔄 DRIFT Search", "value": "drift"}
                                ],
                                value="auto",
                                inline=False
                            ),
                        ], md=6),
                        
                        dbc.Col([
                            html.Label("Query Options:", className="fw-bold"),
                            dbc.Checklist(
                                id="query-options",
                                options=[
                                    {"label": "Include community context", "value": "community"},
                                    {"label": "Show routing details", "value": "routing"},
                                    {"label": "Show data sources", "value": "sources"},
                                    {"label": "Verbose results", "value": "verbose"}
                                ],
                                value=["community", "routing", "sources"],
                                inline=False
                            ),
                        ], md=6),
                    ]),
                    
                    dbc.Row([
                        dbc.Col([
                            dbc.Button(
                                "🚀 Submit Query",
                                id="submit-query",
                                color="primary",
                                size="lg",
                                className="w-100 mt-3",
                                n_clicks=0
                            ),
                        ], md=6),
                        dbc.Col([
                            dbc.Button(
                                "🧹 Clear",
                                id="clear-all",
                                color="secondary",
                                size="lg",
                                className="w-100 mt-3",
                                n_clicks=0
                            ),
                        ], md=6),
                    ]),
                ])
            ], className="mb-4"),
        ])
    ]),
    
    # Loading indicator
    dcc.Loading(
        id="loading",
        type="default",
        children=[
            html.Div(id="loading-output")
        ]
    ),
    
    # Routing Information
    dbc.Row([
        dbc.Col([
            dbc.Collapse(
                dbc.Card([
                    dbc.CardHeader(html.H5("🎯 Query Routing Analysis")),
                    dbc.CardBody(id="routing-info")
                ], className="mb-4"),
                id="routing-collapse",
                is_open=False
            ),
        ])
    ]),
    
    # Results Section
    dbc.Row([
        dbc.Col([
            dbc.Card([
                dbc.CardHeader(html.H4("📊 Query Results")),
                dbc.CardBody(id="query-results", style={"min-height": "300px"})
            ], className="mb-4"),
        ])
    ]),
    
    # Query History
    dbc.Row([
        dbc.Col([
            dbc.Card([
                dbc.CardHeader([
                    html.H5("📜 Query History", className="d-inline"),
                    dbc.Button(
                        "Clear History",
                        id="clear-history",
                        color="danger",
                        size="sm",
                        className="float-end",
                        n_clicks=0
                    )
                ]),
                dbc.CardBody(id="query-history")
            ]),
        ])
    ]),
    
    # Hidden div for storing state
    html.Div(id="query-state", style={"display": "none"})
    
], fluid=True, className="p-4")

def create_data_sources_display(data_sources):
    """Create a formatted display of data sources used in the query."""
    
    # Handle empty or None data_sources
    if not data_sources:
        return html.Div([
            html.Hr(style={'margin': '20px 0', 'border-top': '1px solid #e0e0e0'}),
            html.Div([
                html.H4("📊 Data Sources", style={'margin-bottom': '10px', 'color': '#333'}),
                html.P("No data sources tracked for this query.", style={
                    'background-color': '#f5f5f5',
                    'padding': '10px',
                    'border-radius': '5px',
                    'color': '#666'
                })
            ])
        ])
    
    # Get lists of items
    entities = data_sources.get('entities', [])
    relationships = data_sources.get('relationships', [])
    sources = data_sources.get('sources', [])
    text_units = data_sources.get('text_units', [])
    
    # Create summary
    summary_parts = []
    
    if entities:
        entity_ids = [str(e.get('id', 'Unknown')) for e in entities[:10]]
        ids_str = ', '.join(entity_ids)
        if len(entities) > 10:
            ids_str += f", ... +{len(entities) - 10} more"
        summary_parts.append(f"Entities ({ids_str})")
    
    if relationships:
        rel_ids = [str(r.get('id', 'Unknown')) for r in relationships[:10]]
        ids_str = ', '.join(rel_ids)
        if len(relationships) > 10:
            ids_str += f", ... +{len(relationships) - 10} more"
        summary_parts.append(f"Relationships ({ids_str})")
    
    if sources:
        source_ids = [str(s.get('id', 'Unknown')) for s in sources[:10]]
        ids_str = ', '.join(source_ids)
        if len(sources) > 10:
            ids_str += f", ... +{len(sources) - 10} more"
        summary_parts.append(f"Sources ({ids_str})")
    
    summary_text = "Data: " + "; ".join(summary_parts) + "." if summary_parts else "Data: No sources tracked."
    
    # Create expandable sections
    details_sections = []
    
    # Entities section with better formatting
    if entities:
        entity_items = []
        for entity in entities[:20]:
            entity_items.append(
                html.Li([
                    html.Div([
                        html.Strong(f"[{entity.get('id', 'Unknown')}] {entity.get('title', 'Unknown')}"),
                        html.Span(f" ({entity.get('type', 'Unknown')})", 
                                style={'color': '#666', 'font-size': '0.9em'})
                    ]),
                    html.P(entity.get('description', 'No description available')[:200] + '...' 
                          if len(entity.get('description', '')) > 200 else entity.get('description', ''),
                          style={'margin': '5px 0 0 20px', 'color': '#555', 'font-size': '0.9em'})
                ], style={'margin-bottom': '10px', 'list-style': 'none'})
            )
        
        if len(entities) > 20:
            entity_items.append(
                html.Li(f"... and {len(entities) - 20} more entities", 
                       style={'font-style': 'italic', 'color': '#666'})
            )
        
        details_sections.append(
            html.Details([
                html.Summary([
                    html.Span("📊 ", style={'font-size': '1.2em'}),
                    f"Entities Used ({len(entities)})"
                ], style={'cursor': 'pointer', 'font-weight': 'bold', 'padding': '10px 0'}),
                html.Ul(entity_items, style={'padding-left': '20px', 'margin-top': '10px'})
            ], style={'margin': '15px 0', 'border-left': '3px solid #4a90e2', 'padding-left': '10px'})
        )
    
    # Relationships section
    if relationships:
        rel_items = []
        for rel in relationships[:15]:
            rel_items.append(
                html.Li([
                    html.Div([
                        html.Strong(f"[{rel.get('id', 'Unknown')}] "),
                        html.Span(f"{rel.get('source', 'Unknown')} → {rel.get('target', 'Unknown')}", 
                                style={'color': '#2c5aa0'})
                    ]),
                    html.P([
                        html.Em(rel.get('description', 'No description')[:150] + '...' 
                               if len(rel.get('description', '')) > 150 else rel.get('description', '')),
                        html.Span(f" (weight: {rel.get('weight', 0):.2f})", 
                                style={'color': '#666', 'font-size': '0.9em'})
                    ], style={'margin': '5px 0 0 20px', 'color': '#555', 'font-size': '0.9em'})
                ], style={'margin-bottom': '10px', 'list-style': 'none'})
            )
        
        if len(relationships) > 15:
            rel_items.append(
                html.Li(f"... and {len(relationships) - 15} more relationships", 
                       style={'font-style': 'italic', 'color': '#666'})
            )
        
        details_sections.append(
            html.Details([
                html.Summary([
                    html.Span("🔗 ", style={'font-size': '1.2em'}),
                    f"Relationships Used ({len(relationships)})"
                ], style={'cursor': 'pointer', 'font-weight': 'bold', 'padding': '10px 0'}),
                html.Ul(rel_items, style={'padding-left': '20px', 'margin-top': '10px'})
            ], style={'margin': '15px 0', 'border-left': '3px solid #e24a4a', 'padding-left': '10px'})
        )
    
    # Sources section
    if sources:
        source_items = []
        for source in sources[:10]:
            source_items.append(
                html.Li([
                    html.Div([
                        html.Strong(f"[{source.get('id', 'Unknown')}] {source.get('title', 'Unknown')}"),
                        html.Span(f" ({source.get('type', 'document')})", 
                                style={'color': '#666', 'font-size': '0.9em'})
                    ]),
                    html.P(source.get('text_preview', '')[:150] + '...' 
                          if len(source.get('text_preview', '')) > 150 else source.get('text_preview', ''),
                          style={'margin': '5px 0 0 20px', 'color': '#555', 'font-size': '0.9em', 'font-style': 'italic'})
                ], style={'margin-bottom': '10px', 'list-style': 'none'})
            )
        
        if len(sources) > 10:
            source_items.append(
                html.Li(f"... and {len(sources) - 10} more sources", 
                       style={'font-style': 'italic', 'color': '#666'})
            )
        
        details_sections.append(
            html.Details([
                html.Summary([
                    html.Span("📄 ", style={'font-size': '1.2em'}),
                    f"Source Documents ({len(sources)})"
                ], style={'cursor': 'pointer', 'font-weight': 'bold', 'padding': '10px 0'}),
                html.Ul(source_items, style={'padding-left': '20px', 'margin-top': '10px'})
            ], style={'margin': '15px 0', 'border-left': '3px solid #4ae255', 'padding-left': '10px'})
        )
    
    # Combine everything
    return html.Div([
        html.Hr(style={'margin': '20px 0', 'border-top': '1px solid #e0e0e0'}),
        html.Div([
            html.H4("📊 Data Sources", style={'margin-bottom': '15px', 'color': '#333'}),
            html.Div(summary_text, style={
                'background-color': '#f5f5f5',
                'padding': '12px',
                'border-radius': '5px',
                'font-family': 'monospace',
                'font-size': '14px',
                'color': '#333',
                'border': '1px solid #ddd'
            }),
            html.Div(details_sections, style={'margin-top': '20px'})
        ], style={
            'background-color': '#fafafa',
            'padding': '20px',
            'border-radius': '8px',
            'border': '1px solid #e0e0e0'
        })
    ])

# Callback for handling queries
@app.callback(
    [Output("query-results", "children"),
     Output("routing-info", "children"),
     Output("routing-collapse", "is_open"),
     Output("query-history", "children"),
     Output("loading-output", "children")],
    [Input("submit-query", "n_clicks"),
     Input("clear-all", "n_clicks"),
     Input("clear-history", "n_clicks")],
    [State("query-input", "value"),
     State("query-method", "value"),
     State("query-options", "value")]
)
def handle_query(submit_clicks, clear_clicks, clear_history_clicks, query_text, method, options):
    global query_history
    
    # Determine which button was clicked
    triggered = ctx.triggered_id
    
    if triggered == "clear-all":
        return "", "", False, render_query_history(), ""
    
    if triggered == "clear-history":
        query_history = []
        return dash.no_update, dash.no_update, dash.no_update, render_query_history(), ""
    
    if triggered != "submit-query" or not query_text:
        raise PreventUpdate
    
    # Initialize query engine if needed
    global query_engine
    if query_engine is None:
        try:
            query_engine = CityClerkQueryEngine(GRAPHRAG_ROOT)
        except Exception as e:
            return render_error(f"Failed to initialize query engine: {e}"), "", False, dash.no_update, ""
    
    # Show loading message
    loading_msg = html.Div([
        html.H5("🔄 Processing your query..."),
        html.P(f"Query: {query_text[:100]}..."),
        html.P(f"Method: {method}")
    ])
    
    try:
        # Determine method
        if method == "auto":
            # Use router to determine method
            route_info = query_router.determine_query_method(query_text)
            actual_method = route_info['method']
            routing_details = route_info
        else:
            actual_method = method
            routing_details = {"method": method, "params": {}}
        
        # Run query asynchronously
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        # Add options to params
        params = routing_details.get('params', {})
        if "community" not in options:
            params['include_community_context'] = False
        
        # Run the query
        result = loop.run_until_complete(
            query_engine.query(
                query=query_text,
                method=actual_method if method != "auto" else None,
                **params
            )
        )
        
        # Extract data sources
        data_sources = result.get('data_sources', result.get('context_data', {}))
        
        # Format the main answer with proper markdown
        answer_content = dcc.Markdown(
            result.get('answer', 'No response generated.'),
            style={
                'padding': '20px',
                'backgroundColor': '#f8f9fa',
                'borderRadius': '8px',
                'lineHeight': '1.6',
                'whiteSpace': 'pre-wrap'  # Preserve formatting
            }
        )
        
        # Create data sources display if requested
        sources_display = html.Div()
        if "sources" in options:
            sources_display = create_data_sources_display(data_sources)
        
        # Combine results
        results_content = html.Div([
            html.H3("Answer:", style={'marginBottom': '15px'}),
            answer_content,
            sources_display
        ])
        
        # Add to history
        query_history.insert(0, {
            "timestamp": datetime.now(),
            "query": query_text,
            "method": actual_method,
            "auto_routed": method == "auto"
        })
        
        # Limit history to 10 items
        query_history = query_history[:10]
        
        routing_content = render_routing_info(routing_details, actual_method) if "routing" in options else ""
        show_routing = "routing" in options
        
        return results_content, routing_content, show_routing, render_query_history(), ""
        
    except Exception as e:
        log.error(f"Query failed: {e}")
        return render_error(f"Query failed: {str(e)}"), "", False, dash.no_update, ""

def render_results(result, options):
    """Render query results with all source information."""
    
    answer = result.get('answer', 'No answer available')
    sources_info = result.get('sources_info', {})
    entity_chunks = result.get('entity_chunks', [])
    metadata = result.get('routing_metadata', {})
    
    # Clean up the answer (remove metadata lines)
    if isinstance(answer, str):
        lines = answer.split('\n')
        cleaned_lines = [line for line in lines if not line.startswith(('INFO:', 'WARNING:', 'DEBUG:', 'SUCCESS:'))]
        answer = '\n'.join(cleaned_lines).strip()
    
    content = [
        html.H5("📝 Answer:", className="mb-3"),
        dcc.Markdown(answer, className="p-3 bg-light rounded"),
    ]
    
    # Show data sources if requested
    if "sources" in options and sources_info:
        content.extend([
            html.Hr(),
            html.H5("📊 Data Sources:", className="mb-3"),
            render_all_sources(sources_info, entity_chunks)
        ])
    
    # Show verbose metadata if requested
    if "verbose" in options and metadata:
        content.extend([
            html.Hr(),
            html.H6("🔍 Query Metadata:"),
            html.Pre(json.dumps(metadata, indent=2), className="bg-dark text-light p-3 rounded")
        ])
    
    return html.Div(content)

def render_all_sources(sources_info, entity_chunks):
    """Render all source information comprehensively."""
    content = []
    
    # Show raw references first
    raw_refs = sources_info.get('raw_references', {})
    if any(raw_refs.values()):
        ref_text = []
        if raw_refs.get('entities'):
            ref_text.append(f"Entities: {', '.join(raw_refs['entities'])}")
        if raw_refs.get('reports'):
            ref_text.append(f"Reports: {', '.join(raw_refs['reports'])}")
        if raw_refs.get('sources'):
            ref_text.append(f"Sources: {', '.join(raw_refs['sources'])}")
        
        content.append(
            dbc.Alert([
                html.Strong("📋 References in Answer: "),
                html.Br(),
                html.Code(' | '.join(ref_text))
            ], color="info", className="mb-3")
        )
    
    # Show resolved reports (for GLOBAL search)
    reports = sources_info.get('reports', [])
    if reports:
        content.append(html.H6("📑 Community Reports Used:", className="mb-2"))
        for report in reports[:10]:  # Limit to first 10
            content.append(
                dbc.Card([
                    dbc.CardBody([
                        html.Strong(f"Report #{report['id']}"),
                        html.Span(f" (Level {report.get('level', '?')})", className="text-muted"),
                        html.P(report.get('summary', 'No summary available'), 
                               className="small text-muted mt-1 mb-0")
                    ])
                ], className="mb-2", color="light", outline=True)
            )
        if len(reports) > 10:
            content.append(html.P(f"... and {len(reports) - 10} more reports", className="text-muted"))
    
    # Show resolved entities (for LOCAL search)
    entities = sources_info.get('entities', [])
    if entities:
        content.append(html.H6("🎯 Entities Referenced:", className="mb-2 mt-3"))
        for entity in entities[:10]:  # Limit to first 10
            content.append(
                dbc.Card([
                    dbc.CardBody([
                        html.Div([
                            html.Span(entity['type'].upper(), 
                                     className=f"badge bg-{get_entity_color(entity['type'])} me-2"),
                            html.Strong(entity['title']),
                            html.Span(f" (#{entity['id']})", className="text-muted small")
                        ]),
                        html.P(entity.get('description', ''), 
                               className="small text-muted mt-1 mb-0")
                    ])
                ], className="mb-2", color="light", outline=True)
            )
        if len(entities) > 10:
            content.append(html.P(f"... and {len(entities) - 10} more entities", className="text-muted"))
    
    # Show entity chunks (the actual retrieved content)
    if entity_chunks:
        content.append(html.H6("📄 Retrieved Content Chunks:", className="mb-2 mt-3"))
        for chunk in entity_chunks[:5]:  # Show first 5 chunks
            source_info = chunk.get('source', {})
            content.append(
                dbc.Card([
                    dbc.CardBody([
                        html.Div([
                            html.Span(chunk['type'].upper(), 
                                     className=f"badge bg-{get_entity_color(chunk['type'])} me-2"),
                            html.Strong(chunk['title'])
                        ]),
                        html.P(chunk.get('description', '')[:200] + "..." 
                               if len(chunk.get('description', '')) > 200 
                               else chunk.get('description', ''), 
                               className="small mt-2"),
                        html.Hr(className="my-2"),
                        html.Small([
                            html.Strong("Source: "),
                            f"{source_info.get('type', 'Unknown')} - {source_info.get('meeting_date', 'N/A')}",
                            html.Br(),
                            html.Strong("File: "),
                            html.Code(source_info.get('source_file', 'Unknown'), className="small")
                        ], className="text-muted")
                    ])
                ], className="mb-2")
            )
        if len(entity_chunks) > 5:
            content.append(html.P(f"... and {len(entity_chunks) - 5} more chunks", className="text-muted"))
    
    return html.Div(content)

def get_entity_color(entity_type):
    """Get color for entity type badge."""
    color_map = {
        'AGENDA_ITEM': 'primary',
        'ORDINANCE': 'success', 
        'RESOLUTION': 'warning',
        'PERSON': 'info',
        'ORGANIZATION': 'secondary',
        'MEETING': 'danger',
        'DOCUMENT': 'dark'
    }
    return color_map.get(entity_type.upper(), 'light')

def render_entity_card(entity, highlight=False, is_related=False):
    """Render a single entity card with proper formatting."""
    
    # Determine card color based on entity type
    color_map = {
        'AGENDA_ITEM': 'primary',
        'ORDINANCE': 'success',
        'RESOLUTION': 'warning',
        'PERSON': 'info',
        'ORGANIZATION': 'secondary',
        'referenced_entity': 'danger'
    }
    
    border_color = color_map.get(entity.get('entity_type', entity.get('type', '')), 'light')
    
    card_content = [
        html.H6([
            html.Span(
                entity.get('entity_type', entity.get('type', '')).upper(), 
                className=f"badge bg-{border_color} me-2"
            ),
            entity['title'],
            html.Span(
                f" (Entity #{entity.get('id', entity.get('entity_id', ''))})",
                className="text-muted small"
            ) if entity.get('id') or entity.get('entity_id') else ""
        ]),
        html.P(
            entity.get('description', ''), 
            className="text-muted small mb-2",
            style={"maxHeight": "100px", "overflow": "auto"}
        ),
    ]
    
    # Add relationship info if this is a related entity
    if is_related and entity.get('relationship'):
        card_content.insert(1, html.P([
            html.Strong("Relationship: "),
            html.Em(entity['relationship'][:100] + "..." if len(entity['relationship']) > 100 else entity['relationship'])
        ], className="small"))
    
    # Add source document info if available
    source_doc = entity.get('source_document', {})
    if source_doc:
        card_content.append(
            html.Div([
                html.Hr(className="my-2"),
                html.Small([
                    html.Strong("Source: "),
                    f"{source_doc.get('type', 'Document')} - {source_doc.get('meeting_date', 'N/A')}",
                    html.Br(),
                    html.Strong("File: "),
                    html.Code(source_doc.get('source_file', 'Unknown'), className="small")
                ], className="text-muted")
            ])
        )
    
    return dbc.Card(
        dbc.CardBody(card_content),
        className="mb-2",
        color=border_color if highlight else None,
        outline=True,
        style={"border-width": "2px"} if highlight else {}
    )

def render_routing_info(routing_details, actual_method):
    """Render routing analysis information."""
    
    intent = routing_details.get('intent')
    params = routing_details.get('params', {})
    
    content = [
        html.P([
            html.Strong("Selected Method: "),
            html.Span(actual_method.upper(), className="badge bg-primary")
        ]),
    ]
    
    if intent:
        content.append(html.P([
            html.Strong("Detected Intent: "),
            html.Span(intent.value if hasattr(intent, 'value') else str(intent))
        ]))
    
    # Show detected entities
    if 'entity_filter' in params:
        entity = params['entity_filter']
        content.append(html.P([
            html.Strong("Primary Entity: "),
            html.Code(f"{entity['type']}: {entity['value']}")
        ]))
    
    if 'multiple_entities' in params:
        entities = params['multiple_entities']
        content.append(html.P([
            html.Strong("Detected Entities: "),
            html.Ul([
                html.Li(html.Code(f"{e['type']}: {e['value']}"))
                for e in entities
            ])
        ]))
    
    # Show key parameters
    key_params = ['top_k_entities', 'community_level', 'comparison_mode', 'strict_entity_focus']
    param_list = []
    for param in key_params:
        if param in params:
            param_list.append(html.Li(f"{param}: {params[param]}"))
    
    if param_list:
        content.append(html.Div([
            html.Strong("Parameters:"),
            html.Ul(param_list)
        ]))
    
    return html.Div(content)

def render_query_history():
    """Render the query history."""
    if not query_history:
        return html.P("No queries yet", className="text-muted")
    
    history_items = []
    for item in query_history:
        badge_color = "success" if item['auto_routed'] else "info"
        history_items.append(
            html.Li([
                html.Small(item['timestamp'].strftime("%H:%M:%S"), className="text-muted me-2"),
                html.Span(item['method'].upper(), className=f"badge bg-{badge_color} me-2"),
                html.Span(item['query'][:100] + "..." if len(item['query']) > 100 else item['query'])
            ], className="mb-2")
        )
    
    return html.Ul(history_items, className="list-unstyled")

def render_error(error_msg):
    """Render an error message."""
    return dbc.Alert([
        html.H5("❌ Error", className="alert-heading"),
        html.P(error_msg)
    ], color="danger")

# Add some custom CSS
app.index_string = '''
<!DOCTYPE html>
<html>
    <head>
        {%metas%}
        <title>{%title%}</title>
        {%favicon%}
        {%css%}
        <style>
            body {
                background-color: #f8f9fa;
            }
            .card {
                box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            }
            .card-header {
                background-color: #e9ecef;
            }
            pre {
                white-space: pre-wrap;
                word-wrap: break-word;
            }
        </style>
    </head>
    <body>
        {%app_entry%}
        <footer>
            {%config%}
            {%scripts%}
            {%renderer%}
        </footer>
    </body>
</html>
'''

if __name__ == "__main__":
    print("🚀 Starting GraphRAG Query UI...")
    print(f"📁 GraphRAG Root: {GRAPHRAG_ROOT}")
    print("🌐 Open http://localhost:8050 in your browser")
    print("Press Ctrl+C to stop")
    
    app.run(debug=True, host='0.0.0.0', port=8050)


================================================================================


################################################################################
# File: scripts/microsoft_framework/query_router.py
################################################################################

# File: scripts/microsoft_framework/query_router.py

from enum import Enum
from typing import Dict, Any, Optional, List, Tuple
import re
import logging

logger = logging.getLogger(__name__)

class QueryIntent(Enum):
    ENTITY_SPECIFIC = "entity_specific"  # Use Local
    HOLISTIC = "holistic"               # Use Global  
    EXPLORATORY = "exploratory"         # Use DRIFT
    TEMPORAL = "temporal"               # Use DRIFT

class QueryFocus(Enum):
    SPECIFIC_ENTITY = "specific_entity"          # User wants info about ONE specific item
    MULTIPLE_SPECIFIC = "multiple_specific"      # User wants info about MULTIPLE specific items
    COMPARISON = "comparison"                    # User wants to compare entities
    CONTEXTUAL = "contextual"                    # User wants relationships/context
    GENERAL = "general"                          # No specific entity mentioned

class SmartQueryRouter:
    """Automatically route queries to the optimal search method with intelligent intent detection."""
    
    def __init__(self):
        # Entity extraction patterns
        self.entity_patterns = {
            'agenda_item': [
                r'(?:agenda\s+)?(?:item|items)\s+([A-Z]-?\d+)',
                r'(?:item|items)\s+([A-Z]-?\d+)',
                r'([A-Z]-\d+)(?:\s+agenda)?',
                r'\b([A-Z]-\d+)\b'  # Just the code itself
            ],
            'ordinance': [
                r'ordinance(?:\s+(?:number|no\.?|#))?\s*(\d{4}-\d+|\d+)',
                r'(?:city\s+)?ordinance\s+(\d{4}-\d+|\d+)',
                r'\b(\d{4}-\d+)\b(?=.*ordinance)',
                r'ordinance\s+(\w+)'
            ],
            'resolution': [
                r'resolution(?:\s+(?:number|no\.?|#))?\s*(\d{4}-\d+|\d+)',
                r'(?:city\s+)?resolution\s+(\d{4}-\d+|\d+)',
                r'\b(\d{4}-\d+)\b(?=.*resolution)',
                r'resolution\s+(\w+)'
            ]
        }
        
        # Intent indicators
        self.specific_indicators = {
            'singular_determiners': ['the', 'this', 'that', 'a', 'an'],
            'identity_verbs': ['is', 'are', 'was', 'were', 'means', 'mean', 'refers to', 'refer to', 'concerns', 'concern', 'about'],
            'detail_nouns': ['details', 'information', 'content', 'text', 'provision', 'provisions', 'summary', 'summaries', 'description', 'descriptions'],
            'specific_question_words': ['what', 'which', 'show', 'tell', 'explain', 'describe', 'list'],
            'limiting_adverbs': ['only', 'just', 'specifically', 'exactly', 'precisely', 'individually', 'separately']
        }
        
        self.comparison_indicators = {
            'comparison_verbs': ['compare', 'contrast', 'differ', 'differentiate', 'distinguish'],
            'comparison_words': ['versus', 'vs', 'against', 'compared to', 'difference', 'differences', 'similarity', 'similarities'],
            'comparison_phrases': ['how do', 'what is the difference', 'what are the differences']
        }
        
        self.contextual_indicators = {
            'plural_forms': ['items', 'ordinances', 'resolutions', 'documents'],
            'relationship_words': ['related', 'connected', 'associated', 'linked', 'relationship', 
                                  'connections', 'references', 'mentions', 'together', 'context',
                                  'affects', 'impacts', 'influences', 'between', 'among'],
            'exploration_verbs': ['explore', 'analyze', 'understand', 'investigate'],
            'scope_expanders': ['all', 'other', 'various', 'multiple', 'several', 'any']
        }
        
        # Holistic patterns for global search
        self.holistic_patterns = [
            r"what are the (?:main|top|key) (themes|topics|issues)",
            r"summarize (?:the|all) (.*)",
            r"overall (.*)",
            r"trends in (.*)",
            r"patterns across (.*)"
        ]
        
        # Temporal patterns for drift search
        self.temporal_patterns = [
            r"how has (.*) (?:changed|evolved)",
            r"timeline of (.*)",
            r"history of (.*)",
            r"development of (.*) over time",
            r"evolution of (.*)",
            r"changes in (.*)"
        ]
    
    def determine_query_method(self, query: str) -> Dict[str, Any]:
        """Determine query method with source tracking enabled."""
        query_lower = query.lower()
        
        # First check for holistic queries (global search)
        for pattern in self.holistic_patterns:
            if re.search(pattern, query_lower):
                result = {
                    "method": "global",
                    "intent": QueryIntent.HOLISTIC,
                    "params": {
                        "community_level": self._determine_community_level(query),
                        "response_type": "multiple paragraphs"
                    }
                }
                # Add source tracking to params
                result['params']['track_sources'] = True
                result['params']['include_source_metadata'] = True
                result['params']['citation_style'] = 'inline'
                return result
        
        # Check for temporal/exploratory queries (drift search)
        for pattern in self.temporal_patterns:
            if re.search(pattern, query_lower):
                result = {
                    "method": "drift",
                    "intent": QueryIntent.TEMPORAL,
                    "params": {
                        "initial_community_level": 2,
                        "max_follow_ups": 5
                    }
                }
                # Add source tracking to params
                result['params']['track_sources'] = True
                result['params']['include_source_metadata'] = True
                result['params']['citation_style'] = 'inline'
                return result
        
        # Extract ALL entity references
        all_entities = self._extract_all_entities(query)
        
        if not all_entities:
            # No specific entity found - use local search as default
            result = {
                "method": "local",
                "intent": QueryIntent.EXPLORATORY,
                "params": {
                    "top_k_entities": 10,
                    "include_community_context": True
                }
            }
            # Add source tracking to params
            result['params']['track_sources'] = True
            result['params']['include_source_metadata'] = True
            result['params']['citation_style'] = 'inline'
            return result
        
        # Handle single entity
        if len(all_entities) == 1:
            entity_info = all_entities[0]
            query_focus = self._determine_single_entity_focus(query_lower, entity_info)
            
            if query_focus == QueryFocus.SPECIFIC_ENTITY:
                result = {
                    "method": "local",
                    "intent": QueryIntent.ENTITY_SPECIFIC,
                    "params": {
                        "entity_filter": {
                            "type": entity_info['type'].upper(),
                            "value": entity_info['value']
                        },
                        "top_k_entities": 1,
                        "include_community_context": False,
                        "strict_entity_focus": True,
                        "disable_community": True
                    }
                }
            else:  # CONTEXTUAL
                result = {
                    "method": "local",
                    "intent": QueryIntent.ENTITY_SPECIFIC,
                    "params": {
                        "entity_filter": {
                            "type": entity_info['type'].upper(),
                            "value": entity_info['value']
                        },
                        "top_k_entities": 10,
                        "include_community_context": True,
                        "strict_entity_focus": False,
                        "disable_community": False
                    }
                }
            
            # Add source tracking to params
            result['params']['track_sources'] = True
            result['params']['include_source_metadata'] = True
            result['params']['citation_style'] = 'inline'
            return result
        
        # Handle multiple entities
        else:
            query_focus = self._determine_multi_entity_focus(query_lower, all_entities)
            
            if query_focus == QueryFocus.MULTIPLE_SPECIFIC:
                # User wants specific info about each entity separately
                result = {
                    "method": "local",
                    "intent": QueryIntent.ENTITY_SPECIFIC,
                    "params": {
                        "multiple_entities": all_entities,
                        "top_k_entities": 1,
                        "include_community_context": False,
                        "strict_entity_focus": True,
                        "disable_community": True,
                        "aggregate_results": True  # Combine results for each entity
                    }
                }
            elif query_focus == QueryFocus.COMPARISON:
                # User wants to compare entities
                result = {
                    "method": "local",
                    "intent": QueryIntent.ENTITY_SPECIFIC,
                    "params": {
                        "multiple_entities": all_entities,
                        "top_k_entities": 5,
                        "include_community_context": True,  # Need context for comparison
                        "strict_entity_focus": False,
                        "disable_community": False,
                        "comparison_mode": True
                    }
                }
            else:  # CONTEXTUAL
                # User wants relationships between entities
                result = {
                    "method": "local",
                    "intent": QueryIntent.ENTITY_SPECIFIC,
                    "params": {
                        "multiple_entities": all_entities,
                        "top_k_entities": 10,
                        "include_community_context": True,
                        "strict_entity_focus": False,
                        "disable_community": False,
                        "focus_on_relationships": True
                    }
                }
            
            # Add source tracking to params
            result['params']['track_sources'] = True
            result['params']['include_source_metadata'] = True
            result['params']['citation_style'] = 'inline'
            return result
    
    def _extract_all_entities(self, query: str) -> List[Dict[str, str]]:
        """Extract ALL entity references from query."""
        query_lower = query.lower()
        entities = []
        found_positions = {}  # Track positions to avoid duplicates
        
        for entity_type, patterns in self.entity_patterns.items():
            for pattern in patterns:
                for match in re.finditer(pattern, query_lower, re.IGNORECASE):
                    value = match.group(1)
                    position = match.start()
                    
                    # Normalize value
                    if entity_type == 'agenda_item':
                        value = value.upper()
                        if not '-' in value and len(value) > 1:
                            value = f"{value[0]}-{value[1:]}"
                    
                    # Check if we already found an entity at this position
                    if position not in found_positions:
                        found_positions[position] = True
                        entities.append({
                            'type': entity_type,
                            'value': value,
                            'position': position
                        })
        
        # Sort by position and remove position info
        entities = sorted(entities, key=lambda x: x['position'])
        for entity in entities:
            del entity['position']
        
        # Remove duplicates while preserving order
        seen = set()
        unique_entities = []
        for entity in entities:
            key = f"{entity['type']}:{entity['value']}"
            if key not in seen:
                seen.add(key)
                unique_entities.append(entity)
        
        return unique_entities
    
    def _determine_single_entity_focus(self, query_lower: str, entity_info: Dict) -> QueryFocus:
        """Determine focus for single entity queries."""
        specific_score = 0
        contextual_score = 0
        
        tokens = query_lower.split()
        
        # Check for limiting words
        for word in self.specific_indicators['limiting_adverbs']:
            if word in tokens:
                specific_score += 3
        
        # Check for relationship words
        for word in self.contextual_indicators['relationship_words']:
            if word in tokens:
                contextual_score += 3
        
        # Simple "what is X" patterns
        if re.match(r'^(what|whats|what\'s)\s+(is|are)\s+', query_lower):
            specific_score += 2
        
        # Very short queries tend to be specific
        if len(tokens) <= 3:
            specific_score += 2
        
        # Check for detail-seeking patterns
        for noun in self.specific_indicators['detail_nouns']:
            if noun in query_lower:
                specific_score += 1
        
        logger.info(f"Single entity focus scores - Specific: {specific_score}, Contextual: {contextual_score}")
        
        return QueryFocus.SPECIFIC_ENTITY if specific_score > contextual_score else QueryFocus.CONTEXTUAL
    
    def _determine_multi_entity_focus(self, query_lower: str, entities: List[Dict]) -> QueryFocus:
        """Determine focus for multi-entity queries."""
        tokens = query_lower.split()
        
        # Check for comparison indicators
        comparison_score = 0
        for verb in self.comparison_indicators['comparison_verbs']:
            if verb in tokens:
                comparison_score += 3
        
        for word in self.comparison_indicators['comparison_words']:
            if word in query_lower:
                comparison_score += 2
        
        for phrase in self.comparison_indicators['comparison_phrases']:
            if phrase in query_lower:
                comparison_score += 2
        
        # Check for specific information indicators
        specific_score = 0
        
        # "What are E-1 and E-2?" suggests wanting specific info
        if re.match(r'^(what|whats|what\'s)\s+(are|is)\s+', query_lower):
            specific_score += 2
        
        # Check for "separately" or "individually"
        if any(word in tokens for word in ['separately', 'individually', 'each']):
            specific_score += 3
        
        # Check for detail nouns with plural entities
        for noun in self.specific_indicators['detail_nouns']:
            if noun in query_lower:
                specific_score += 1
        
        # Check for contextual/relationship indicators
        contextual_score = 0
        for word in self.contextual_indicators['relationship_words']:
            if word in tokens:
                contextual_score += 2
        
        # Check for "and" patterns that suggest relationships
        # e.g., "relationship between E-1 and E-2"
        if re.search(r'between.*and', query_lower):
            contextual_score += 3
        
        logger.info(f"Multi-entity focus scores - Comparison: {comparison_score}, Specific: {specific_score}, Contextual: {contextual_score}")
        
        # Determine focus based on highest score
        if comparison_score >= specific_score and comparison_score >= contextual_score:
            return QueryFocus.COMPARISON
        elif specific_score > contextual_score:
            return QueryFocus.MULTIPLE_SPECIFIC
        else:
            return QueryFocus.CONTEXTUAL
    
    def _determine_community_level(self, query: str) -> int:
        """Determine optimal community level based on query scope."""
        if any(word in query.lower() for word in ["entire", "all", "overall", "whole"]):
            return 0  # Highest level
        elif any(word in query.lower() for word in ["department", "district", "area"]):
            return 1  # Mid level
        else:
            return 2  # Lower level for more specific summaries


================================================================================


################################################################################
# File: scripts/graph_stages/document_linker.py
################################################################################

# File: scripts/graph_stages/document_linker.py

"""
Document Linker
Links ordinance documents to their corresponding agenda items.
"""

import logging
import re
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import json
from datetime import datetime
import PyPDF2
from groq import Groq
import os
from dotenv import load_dotenv
import asyncio
from concurrent.futures import ThreadPoolExecutor
import multiprocessing

load_dotenv()

log = logging.getLogger('document_linker')


class DocumentLinker:
    """Links ordinance and resolution documents to agenda items."""
    
    def __init__(self,
                 openai_api_key: Optional[str] = None,
                 model: str = "gpt-4.1-mini-2025-04-14",
                 agenda_extraction_max_tokens: int = 32768):
        """Initialize the document linker."""
        self.api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY not found in environment")
        
        self.client = Groq()
        self.model = model
        self.agenda_extraction_max_tokens = agenda_extraction_max_tokens
    
    async def link_documents_for_meeting(self, 
                                       meeting_date: str,
                                       documents_dir: Path) -> Dict[str, List[Dict]]:
        """Find and link all documents for a specific meeting date."""
        log.info(f"🔗 Linking documents for meeting date: {meeting_date}")
        log.info(f"📁 Looking for ordinances in: {documents_dir}")
        
        # Create debug directory
        debug_dir = Path("city_clerk_documents/graph_json/debug")
        debug_dir.mkdir(exist_ok=True)
        
        # Convert date format: "01.09.2024" -> "01_09_2024"
        date_underscore = meeting_date.replace(".", "_")
        
        # Log what we're searching for
        with open(debug_dir / "document_search_info.txt", 'w') as f:
            f.write(f"Meeting Date: {meeting_date}\n")
            f.write(f"Date with underscores: {date_underscore}\n")
            f.write(f"Search directory: {documents_dir}\n")
            f.write(f"Search pattern: *{date_underscore}.pdf\n")
        
        # Find all matching ordinance/resolution files
        # Pattern: YYYY-## - MM_DD_YYYY.pdf
        pattern = f"*{date_underscore}.pdf"
        matching_files = list(documents_dir.glob(pattern))
        
        # Also try without spaces in case filenames vary
        pattern2 = f"*{date_underscore}*.pdf"
        additional_files = [f for f in documents_dir.glob(pattern2) if f not in matching_files]
        matching_files.extend(additional_files)
        
        # List all files in directory for debugging
        all_files = list(documents_dir.glob("*.pdf"))
        with open(debug_dir / "all_ordinance_files.txt", 'w') as f:
            f.write(f"Total files in {documents_dir}: {len(all_files)}\n")
            for file in sorted(all_files):
                f.write(f"  - {file.name}\n")
        
        log.info(f"📄 Found {len(matching_files)} documents for date {meeting_date}")
        
        if matching_files:
            log.info("📄 Documents found:")
            for f in matching_files[:5]:
                log.info(f"   - {f.name}")
            if len(matching_files) > 5:
                log.info(f"   ... and {len(matching_files) - 5} more")
        
        # Save matched files list
        with open(debug_dir / "matched_documents.txt", 'w') as f:
            f.write(f"Documents matching date {meeting_date}:\n")
            for file in matching_files:
                f.write(f"  - {file.name}\n")
        
        # Process documents in parallel
        linked_documents = {
            "ordinances": [],
            "resolutions": []
        }
        
        if matching_files:
            # Use asyncio.gather for parallel processing
            max_concurrent = min(multiprocessing.cpu_count() * 2, 10)
            semaphore = asyncio.Semaphore(max_concurrent)
            
            async def process_with_semaphore(doc_path):
                async with semaphore:
                    return await self._process_document(doc_path, meeting_date)
            
            # Process all documents concurrently
            results = await asyncio.gather(
                *[process_with_semaphore(doc_path) for doc_path in matching_files],
                return_exceptions=True
            )
            
            # Categorize results
            for doc_info, doc_path in zip(results, matching_files):
                if isinstance(doc_info, Exception):
                    log.error(f"Error processing {doc_path.name}: {doc_info}")
                    continue
                if doc_info:
                    if "ordinance" in doc_info.get("title", "").lower():
                        linked_documents["ordinances"].append(doc_info)
                    else:
                        linked_documents["resolutions"].append(doc_info)
        
        # Save linked documents info
        with open(debug_dir / "linked_documents.json", 'w') as f:
            json.dump(linked_documents, f, indent=2)
        
        log.info(f"✅ Linked {len(linked_documents['ordinances'])} ordinances, {len(linked_documents['resolutions'])} resolutions")
        return linked_documents
    
    async def _process_document(self, doc_path: Path, meeting_date: str) -> Optional[Dict[str, Any]]:
        """Process a single document to extract agenda item reference."""
        try:
            # Extract document number from filename (e.g., "2024-04" from "2024-04 - 01_23_2024.pdf")
            doc_match = re.match(r'^(\d{4}-\d{2})', doc_path.name)
            if not doc_match:
                log.warning(f"Could not parse document number from {doc_path.name}")
                return None
            
            document_number = doc_match.group(1)
            
            # Extract text from PDF
            text = self._extract_pdf_text(doc_path)
            if not text:
                log.warning(f"No text extracted from {doc_path.name}")
                return None
            
            # Extract agenda item code using LLM
            item_code = await self._extract_agenda_item_code(text, document_number)
            
            # Extract additional metadata
            parsed_data = self._parse_document_metadata(text)
            
            doc_info = {
                "path": str(doc_path),
                "filename": doc_path.name,
                "document_number": document_number,
                "item_code": item_code,
                "document_type": "Ordinance" if "ordinance" in text[:500].lower() else "Resolution",
                "title": self._extract_title(text),
                "parsed_data": parsed_data
            }
            
            log.info(f"📄 Processed {doc_path.name}: Item {item_code or 'NOT_FOUND'}")
            return doc_info
            
        except Exception as e:
            log.error(f"Error processing {doc_path.name}: {e}")
            return None
    
    def _extract_pdf_text(self, pdf_path: Path) -> str:
        """Extract text from PDF file."""
        try:
            with open(pdf_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                text_parts = []
                
                # Extract text from all pages
                for page in reader.pages:
                    text = page.extract_text()
                    if text:
                        text_parts.append(text)
                
                return "\n".join(text_parts)
        except Exception as e:
            log.error(f"Failed to extract text from {pdf_path.name}: {e}")
            return ""
    
    async def _extract_agenda_item_code(self, text: str, document_number: str) -> Optional[str]:
        """Extract agenda item code from document text using LLM."""
        # Create debug directory if it doesn't exist
        debug_dir = Path("city_clerk_documents/graph_json/debug")
        debug_dir.mkdir(exist_ok=True)
        
        # Send the entire document to qwen-32b
        text_excerpt = text
        
        # Save the full text being sent to LLM for debugging
        with open(debug_dir / f"llm_input_{document_number}.txt", 'w', encoding='utf-8') as f:
            f.write(f"Document: {document_number}\n")
            f.write(f"Text length: {len(text)} characters\n")
            f.write(f"Using model: {self.model}\n")
            f.write(f"Max tokens: {self.agenda_extraction_max_tokens}\n")
            f.write("\n--- FULL DOCUMENT SENT TO LLM ---\n")
            f.write(text_excerpt)
        
        log.info(f"📄 Sending full document to LLM for {document_number}: {len(text)} characters")
        
        prompt = f"""You are analyzing a City of Coral Gables ordinance document (Document #{document_number}).

Your task is to find the AGENDA ITEM CODE referenced in this document.

IMPORTANT: The agenda item can appear ANYWHERE in the document - on page 3, at the end, or anywhere else. Search the ENTIRE document carefully.

The agenda item typically appears in formats like:
- (Agenda Item: E-1)
- Agenda Item: E-3)
- (Agenda Item E-1)
- Item H-3
- H.-3. (with periods and dots)
- E.-2. (with dots)
- E-2 (without dots)
- Item E-2

Full document text:
{text_excerpt}

Respond in this EXACT format:
AGENDA_ITEM: [code] or AGENDA_ITEM: NOT_FOUND

Important: Return the code as it appears (e.g., E-2, not E.-2.)

Examples:
- If you find "(Agenda Item: E-2)" → respond: AGENDA_ITEM: E-2
- If you find "Item E-2" → respond: AGENDA_ITEM: E-2
- If you find "H.-3." → respond: AGENDA_ITEM: H-3
- If no agenda item found → respond: AGENDA_ITEM: NOT_FOUND"""
        
        try:
            response = self.client.chat.completions.create(
                model="meta-llama/llama-4-maverick-17b-128e-instruct",
                messages=[
                    {"role": "system", "content": "You are a precise data extractor. Find and extract only the agenda item code. Search the ENTIRE document thoroughly."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_completion_tokens=8192,
                top_p=1,
                stream=False,
                stop=None
            )
            
            raw_response = response.choices[0].message.content.strip()
            
            # Save raw LLM response for debugging
            with open(debug_dir / f"llm_response_{document_number}_raw.txt", 'w', encoding='utf-8') as f:
                f.write(raw_response)
            
            # Parse response directly (no qwen parsing needed)
            result = raw_response
            
            # Log the response for debugging
            log.info(f"LLM response for {document_number} (first 200 chars): {result[:200]}")
            
            # Parse the response
            if "AGENDA_ITEM:" in result:
                code = result.split("AGENDA_ITEM:")[1].strip()
                if code != "NOT_FOUND":
                    # Normalize the code (remove dots, ensure format)
                    code = self._normalize_item_code(code)
                    log.info(f"✅ Found agenda item code for {document_number}: {code}")
                    return code
                else:
                    log.warning(f"❌ LLM could not find agenda item in {document_number}")
            else:
                log.error(f"❌ Invalid LLM response format for {document_number}: {result[:100]}")
            
            return None
            
        except Exception as e:
            log.error(f"Failed to extract agenda item for {document_number}: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _normalize_item_code(self, code: str) -> str:
        """Normalize item code to consistent format."""
        if not code:
            return code
        
        # Remove trailing dots and spaces
        code = code.rstrip('. ')
        
        # Remove dots between letter and dash: "E.-1" -> "E-1"
        code = re.sub(r'([A-Z])\.(-)', r'\1\2', code)
        
        # Handle cases without dash: "E.1" -> "E-1"
        code = re.sub(r'([A-Z])\.(\d)', r'\1-\2', code)
        
        # Remove any remaining dots
        code = code.replace('.', '')
        
        return code
    
    def _extract_title(self, text: str) -> str:
        """Extract document title from text."""
        # Look for "AN ORDINANCE" or "A RESOLUTION" pattern
        title_match = re.search(r'(AN?\s+(ORDINANCE|RESOLUTION)[^.]+\.)', text[:2000], re.IGNORECASE)
        if title_match:
            return title_match.group(1).strip()
        
        # Fallback to first substantive line
        lines = text.split('\n')
        for line in lines[:20]:
            if len(line) > 20 and not line.isdigit():
                return line.strip()[:200]
        
        return "Untitled Document"
    
    def _parse_document_metadata(self, text: str) -> Dict[str, Any]:
        """Parse additional metadata from document."""
        metadata = {}
        
        # Extract date passed
        date_match = re.search(r'day\s+of\s+(\w+),?\s+(\d{4})', text)
        if date_match:
            metadata["date_passed"] = date_match.group(0)
        
        # Extract vote information
        vote_match = re.search(r'PASSED\s+AND\s+ADOPTED.*?(\d+).*?(\d+)', text, re.IGNORECASE | re.DOTALL)
        if vote_match:
            metadata["vote_details"] = {
                "ayes": vote_match.group(1),
                "nays": vote_match.group(2) if len(vote_match.groups()) > 1 else "0"
            }
        
        # Extract motion information
        motion_match = re.search(r'motion\s+(?:was\s+)?made\s+by\s+([^,]+)', text, re.IGNORECASE)
        if motion_match:
            metadata["motion"] = {"moved_by": motion_match.group(1).strip()}
        
        # Extract mayor signature
        mayor_match = re.search(r'Mayor[:\s]+([^\n]+)', text[-1000:])
        if mayor_match:
            metadata["signatories"] = {"mayor": mayor_match.group(1).strip()}
        
        return metadata


================================================================================


################################################################################
# File: scripts/microsoft_framework/README.md
################################################################################

<!-- 
File: scripts/microsoft_framework/README.md
 -->

# City Clerk GraphRAG System

Microsoft GraphRAG integration for city clerk document processing with advanced entity extraction, community detection, and intelligent query routing.

## 🚀 Quick Start

### 1. Set up your environment:
```bash
export OPENAI_API_KEY='your-api-key-here'
```

### 2. Run the complete pipeline:
```bash
./run_graphrag.sh run
```

### 3. Test queries interactively:
```bash
./run_graphrag.sh query
```

## 📋 Prerequisites

1. **Environment Variables**:
   ```bash
   export OPENAI_API_KEY='your-openai-api-key'
   ```

2. **Extracted Documents**: 
   - City clerk documents should be extracted as JSON files in `city_clerk_documents/extracted_text/`
   - Run your existing document extraction pipeline first

3. **Dependencies**:
   - Python 3.8+
   - GraphRAG library
   - All dependencies in `requirements.txt`

## 🛠️ Installation & Setup

### Option 1: Using the Shell Script (Recommended)
```bash
# Setup environment and dependencies
./run_graphrag.sh setup

# Run the complete pipeline
./run_graphrag.sh run
```

### Option 2: Manual Setup
```bash
# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Run pipeline
python3 scripts/microsoft_framework/run_graphrag_pipeline.py
```

## 🔍 Query System

The system supports three types of queries with automatic routing:

### 1. 🎯 Local Search (Entity-Specific)
Best for specific entities and their immediate relationships:
```
"Who is Commissioner Smith?"
"What is ordinance 2024-01?"
"Tell me about agenda item E-1"
```

### 2. 🌐 Global Search (Holistic)
Best for broad themes and dataset-wide analysis:
```
"What are the main themes in city development?"
"Summarize all budget discussions"
"Overall trends in housing policy"
```

### 3. 🔄 DRIFT Search (Temporal/Complex)
Best for temporal changes and complex exploratory queries:
```
"How has the waterfront project evolved?"
"Timeline of budget decisions"
"Development of housing policy over time"
```

## 📊 Pipeline Steps

The GraphRAG pipeline includes:

1. **🔧 Environment Setup** - Initialize GraphRAG with city clerk configuration
2. **📄 Document Adaptation** - Convert extracted JSON documents to GraphRAG format
3. **🎯 Prompt Tuning** - Auto-tune prompts for city government domain
4. **🏗️ GraphRAG Indexing** - Extract entities, relationships, and communities
5. **📊 Results Processing** - Load and summarize GraphRAG outputs
6. **🔍 Query Testing** - Test with example queries
7. **🌐 Cosmos DB Sync** - Optionally sync to existing Cosmos DB

## 📁 Output Structure

After running the pipeline, you'll find:

```
graphrag_data/
├── settings.yaml           # GraphRAG configuration
├── city_clerk_documents.csv # Input documents in GraphRAG format
├── prompts/                # Auto-tuned prompts
│   ├── entity_extraction.txt
│   └── community_report.txt
└── output/                 # GraphRAG results
    ├── entities.parquet    # Extracted entities
    ├── relationships.parquet # Entity relationships
    ├── communities.parquet # Community clusters
    └── community_reports.parquet # Community summaries
```

## 🎮 Usage Examples

### Run Complete Pipeline
```bash
./run_graphrag.sh run
```

### Interactive Query Session
```bash
./run_graphrag.sh query
```

### View Results Summary
```bash
./run_graphrag.sh results
```

### Clean Up Data
```bash
./run_graphrag.sh clean
```

### Example Queries to Try

**Entity-specific (Local Search):**
- "Who is Mayor Johnson?"
- "What is resolution 2024-15?"
- "Tell me about the parks department"

**Holistic (Global Search):**
- "What are the main development themes?"
- "Summarize all transportation discussions"
- "Overall budget allocation patterns"

**Temporal (DRIFT Search):**
- "How has zoning policy evolved?"
- "Timeline of infrastructure projects"
- "Development of affordable housing initiatives"

## ⚙️ Configuration

### Model Settings
The system is configured to use:
- **Model**: `gpt-4.1-mini-2025-04-14`
- **Max Tokens**: `32768`
- **Temperature**: `0` (deterministic)

### Entity Types
Configured for city government entities:
- `person`, `organization`, `location`
- `document`, `meeting`, `agenda_item`
- `money`, `project`, `ordinance`, `resolution`, `contract`

### Query Configuration
- **Global Search**: Community-level analysis with dynamic selection
- **Local Search**: Top-K entity retrieval with community context
- **DRIFT Search**: Iterative exploration with follow-up expansion

## 🔧 Advanced Usage

### Python API
```python
from scripts.microsoft_framework import CityClerkQueryEngine

# Initialize query engine
engine = CityClerkQueryEngine("./graphrag_data")

# Auto-routed query
result = await engine.query("What are the main budget themes?")

# Specific method
result = await engine.query(
    "Who is Commissioner Smith?", 
    method="local",
    top_k_entities=15
)
```

### Custom Query Routing
```python
from scripts.microsoft_framework import SmartQueryRouter

router = SmartQueryRouter()
route_info = router.determine_query_method("Your query here")
print(f"Recommended method: {route_info['method']}")
```

### Cosmos DB Integration
```python
from scripts.microsoft_framework import GraphRAGCosmosSync

sync = GraphRAGCosmosSync("./graphrag_data/output")
await sync.sync_to_cosmos()
```

## 📈 Performance Tips

1. **Incremental Processing**: Use `IncrementalGraphRAGProcessor` for new documents
2. **Community Levels**: Adjust community levels for different query scopes
3. **Query Optimization**: Use specific entity names and agenda codes when known
4. **Batch Processing**: Process documents in batches for large datasets

## 🐛 Troubleshooting

### Common Issues

**GraphRAG not found:**
```bash
pip install graphrag
```

**No documents found:**
- Ensure documents are in `city_clerk_documents/extracted_text/`
- Run document extraction pipeline first

**API Key issues:**
```bash
export OPENAI_API_KEY='your-key-here'
```

**Memory issues:**
- Reduce `max_tokens` in settings
- Process documents in smaller batches

### Debug Mode
```bash
# Run with verbose output
python3 scripts/microsoft_framework/run_graphrag_pipeline.py --verbose

# Check GraphRAG logs
tail -f graphrag_data/logs/*.log
```

## 🤝 Integration with Existing System

This GraphRAG system integrates seamlessly with your existing infrastructure:

- **Reuses**: Docling PDF extraction, URL preservation, Cosmos DB client
- **Extends**: Adds advanced entity extraction and community detection
- **Maintains**: Existing graph schema and document processing pipeline
- **Enhances**: Query capabilities with intelligent routing

## 📚 More Information

- [Microsoft GraphRAG Documentation](https://microsoft.github.io/graphrag/)
- [Query Configuration Guide](./city_clerk_settings_template.yaml)
- [Entity Types and Prompts](./prompt_tuner.py)

---

For issues or questions, check the troubleshooting section or review the pipeline logs in `graphrag_data/logs/`.


================================================================================


################################################################################
# File: scripts/microsoft_framework/cosmos_synchronizer.py
################################################################################

# File: scripts/microsoft_framework/cosmos_synchronizer.py

from pathlib import Path
import pandas as pd
import json
from typing import Dict, List, Any
import asyncio
from scripts.graph_stages.cosmos_db_client import CosmosGraphClient

class GraphRAGCosmosSync:
    """Synchronize GraphRAG output with Cosmos DB."""
    
    def __init__(self, graphrag_output_dir: Path):
        self.output_dir = Path(graphrag_output_dir)
        self.cosmos_client = CosmosGraphClient()
        
    async def sync_to_cosmos(self):
        """Sync GraphRAG data to Cosmos DB."""
        await self.cosmos_client.connect()
        
        try:
            # Load GraphRAG artifacts
            entities_df = pd.read_parquet(self.output_dir / "entities.parquet")
            relationships_df = pd.read_parquet(self.output_dir / "relationships.parquet")
            communities_df = pd.read_parquet(self.output_dir / "communities.parquet")
            
            # Sync entities
            print(f"📤 Syncing {len(entities_df)} entities to Cosmos DB...")
            for _, entity in entities_df.iterrows():
                await self._sync_entity(entity)
            
            # Sync relationships
            print(f"🔗 Syncing {len(relationships_df)} relationships...")
            for _, rel in relationships_df.iterrows():
                await self._sync_relationship(rel)
            
            # Sync communities as properties
            print(f"🏘️ Syncing {len(communities_df)} communities...")
            for _, community in communities_df.iterrows():
                await self._sync_community(community)
                
        finally:
            await self.cosmos_client.close()
    
    async def _sync_entity(self, entity: pd.Series):
        """Sync a GraphRAG entity to Cosmos DB."""
        # Map GraphRAG entity to Cosmos vertex
        vertex_id = f"graphrag_entity_{entity['id']}"
        
        properties = {
            'name': entity['name'],
            'type': entity['type'],
            'description': entity['description'],
            'graphrag_id': entity['id'],
            'community_ids': json.dumps(entity.get('community_ids', [])),
            'has_graphrag': True
        }
        
        # Map to appropriate label based on type
        label_map = {
            'person': 'Person',
            'organization': 'Organization',
            'location': 'Location',
            'document': 'Document',
            'meeting': 'Meeting',
            'agenda_item': 'AgendaItem',
            'project': 'Project'
        }
        
        label = label_map.get(entity['type'].lower(), 'Entity')
        
        await self.cosmos_client.upsert_vertex(
            label=label,
            vertex_id=vertex_id,
            properties=properties
        )
    
    async def _sync_relationship(self, rel: pd.Series):
        """Sync a GraphRAG relationship to Cosmos DB."""
        from_id = f"graphrag_entity_{rel['source']}"
        to_id = f"graphrag_entity_{rel['target']}"
        
        properties = {
            'description': rel['description'],
            'weight': rel.get('weight', 1.0),
            'graphrag_rel_id': rel['id']
        }
        
        await self.cosmos_client.create_edge_if_not_exists(
            from_id=from_id,
            to_id=to_id,
            edge_type=rel['type'].upper(),
            properties=properties
        )
    
    async def _sync_community(self, community: pd.Series):
        """Sync a GraphRAG community as metadata to relevant entities."""
        # Communities can be stored as properties on entities
        # or as separate vertices depending on your schema preference
        pass


================================================================================


################################################################################
# File: extract_documents_for_graphrag.py
################################################################################

# File: extract_documents_for_graphrag.py

#!/usr/bin/env python3
"""
Extract documents using enhanced PDF extractor for GraphRAG processing.
This script uses the intelligent metadata header functionality.
"""

import sys
from pathlib import Path
import asyncio

# Add current directory to path
sys.path.append('.')

from scripts.graph_stages.pdf_extractor import PDFExtractor

def extract_documents_for_graphrag():
    """Extract a few sample documents for GraphRAG testing."""
    
    print("🚀 Starting document extraction for GraphRAG testing")
    print("="*60)
    
    # Define source and output directories
    pdf_dir = Path("city_clerk_documents/global copy/City Comissions 2024/Agendas")
    output_dir = Path("city_clerk_documents/extracted_text")
    markdown_dir = Path("city_clerk_documents/extracted_markdown")
    
    # Create output directories
    output_dir.mkdir(parents=True, exist_ok=True)
    markdown_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"📁 Source directory: {pdf_dir}")
    print(f"📁 JSON output directory: {output_dir}")
    print(f"📁 Markdown output directory: {markdown_dir}")
    
    # Find agenda PDFs (limit to a few for testing)
    agenda_files = sorted(pdf_dir.glob("Agenda*.pdf"))[:3]  # Just first 3 for testing
    
    if not agenda_files:
        print("❌ No agenda PDFs found!")
        return False
    
    print(f"📄 Found {len(agenda_files)} agenda files to process:")
    for pdf in agenda_files:
        print(f"   - {pdf.name}")
    
    # Initialize extractor
    extractor = PDFExtractor(pdf_dir, output_dir)
    
    # Process each PDF
    for pdf_path in agenda_files:
        try:
            print(f"\n📄 Processing: {pdf_path.name}")
            
            # Extract with intelligent metadata headers
            markdown_path, enhanced_content = extractor.extract_and_save_with_metadata(
                pdf_path, markdown_dir
            )
            print(f"✅ Enhanced markdown saved to: {markdown_path}")
            
            # Also extract regular JSON for compatibility
            full_text, pages = extractor.extract_text_from_pdf(pdf_path)
            extracted_data = {
                'full_text': full_text,
                'pages': pages,
                'document_type': extractor._determine_doc_type(pdf_path.name),
                'metadata': {
                    'filename': pdf_path.name,
                    'num_pages': len(pages),
                    'total_chars': len(full_text),
                    'extraction_method': 'docling_enhanced'
                }
            }
            
            # Save JSON
            import json
            json_path = output_dir / f"{pdf_path.stem}_extracted.json"
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(extracted_data, f, indent=2, ensure_ascii=False)
            print(f"✅ JSON saved to: {json_path}")
            
        except Exception as e:
            print(f"❌ Error processing {pdf_path.name}: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    print(f"\n✅ Document extraction complete!")
    print(f"📊 Extracted {len(agenda_files)} documents")
    print(f"📁 Files available in:")
    print(f"   - JSON: {output_dir}")
    print(f"   - Enhanced Markdown: {markdown_dir}")
    
    return True

if __name__ == "__main__":
    success = extract_documents_for_graphrag()
    if success:
        print("\n🎯 Ready for GraphRAG pipeline!")
    else:
        print("\n❌ Extraction failed")
        sys.exit(1)


================================================================================


################################################################################
# File: scripts/microsoft_framework/city_clerk_settings_template.yaml
################################################################################

# File: scripts/microsoft_framework/city_clerk_settings_template.yaml

llm:
  api_type: "openai"
  model: "gpt-4.1-mini-2025-04-14"
  api_key: "${OPENAI_API_KEY}"
  max_tokens: 32768
  temperature: 0
  
chunks:
  size: 1200
  overlap: 200
  group_by_columns: ["document_type", "meeting_date", "item_code"]
  
entity_extraction:
  prompt: "prompts/city_clerk_entity_extraction.txt"
  entity_types: ["person", "organization", "location", "document", 
                 "meeting", "money", "project", "agenda_item",
                 "ordinance", "resolution", "contract"]
  max_gleanings: 2
  
claim_extraction:
  enabled: true
  prompt: "prompts/city_clerk_claims.txt"
  description: "Extract voting records, motions, and decisions"
  
community_reports:
  prompt: "prompts/city_clerk_community_report.txt"
  max_length: 2000
  max_input_length: 32768
  
embeddings:
  model: "text-embedding-3-small"
  batch_size: 16
  batch_max_tokens: 2048
  
cluster_graph:
  max_cluster_size: 10
  
storage:
  type: "file"
  base_dir: "./output/artifacts"

# Query configuration section
query:
  # Global search settings
  global_search:
    community_level: 2  # Which hierarchical level to use
    max_tokens: 32768
    temperature: 0.0
    top_p: 1.0
    n: 1
    use_dynamic_community_selection: true
    relevance_score_threshold: 0.7
    rate_relevancy_model: "gpt-4.1-mini-2025-04-14"  # Same model for consistency
    
  # Local search settings  
  local_search:
    text_unit_prop: 0.5  # Proportion of context window for text units
    community_prop: 0.1  # Proportion for community summaries
    conversation_history_max_turns: 5
    top_k_entities: 10  # Number of related entities to retrieve
    top_k_relationships: 10
    max_tokens: 32768
    temperature: 0.0
    
  # DRIFT search settings
  drift_search:
    initial_community_level: 2
    max_iterations: 5
    follow_up_expansion: 3
    relevance_threshold: 0.7
    max_tokens: 32768
    temperature: 0.0
    primer_queries: 3  # Initial community queries
    follow_up_depth: 5  # Max recursion depth
    similarity_threshold: 0.8
    termination_strategy: "convergence"  # or "max_depth"
    include_global_context: true


================================================================================


################################################################################
# File: config.py
################################################################################

# File: config.py

#!/usr/bin/env python3
"""
Configuration file for graph database visualization
Manages database credentials and connection settings
"""

import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Azure Cosmos DB Gremlin Configuration
COSMOS_ENDPOINT = os.getenv("COSMOS_ENDPOINT", "wss://aida-graph-db.gremlin.cosmos.azure.com:443")
COSMOS_KEY = os.getenv("COSMOS_KEY", "")  # This will be set from .env file
DATABASE = os.getenv("COSMOS_DATABASE", "cgGraph")
CONTAINER = os.getenv("COSMOS_CONTAINER", "cityClerk")
PARTITION_KEY = os.getenv("COSMOS_PARTITION_KEY", "partitionKey")
PARTITION_VALUE = os.getenv("COSMOS_PARTITION_VALUE", "demo")

def validate_config():
    """Validate that all required configuration is available"""
    required_vars = {
        "COSMOS_KEY": COSMOS_KEY,
        "COSMOS_ENDPOINT": COSMOS_ENDPOINT,
        "DATABASE": DATABASE,
        "CONTAINER": CONTAINER
    }
    
    missing_vars = [var for var, value in required_vars.items() if not value]
    
    if missing_vars:
        print("❌ Missing required configuration:")
        for var in missing_vars:
            print(f"   - {var}")
        print("\n🔧 Please create a .env file with your credentials:")
        print("   COSMOS_KEY=your_actual_cosmos_key_here")
        print("   COSMOS_ENDPOINT=wss://aida-graph-db.gremlin.cosmos.azure.com:443")
        print("   COSMOS_DATABASE=cgGraph")
        print("   COSMOS_CONTAINER=cityClerk")
        return False
    
    return True

if __name__ == "__main__":
    print("🔧 Configuration Check:")
    if validate_config():
        print("✅ All configuration variables are set!")
    else:
        print("❌ Configuration incomplete!")


================================================================================

