# Concatenated Project Code - Part 2 of 3
# Generated: 2025-06-04 18:42:53
# Root Directory: /Users/gianmariatroiani/Documents/knologi̊/graph_database
================================================================================

# File Index - Which Files Are in Which Parts
################################################################################

## Part 1 (5 files):
  - scripts/graph_stages/agenda_pdf_extractor.py
  - scripts/graph_stages/cosmos_db_client.py
  - scripts/graph_stages/pdf_extractor.py
  - graph_clear_database.py
  - requirements.txt

## Part 2 (6 files):
  - scripts/graph_stages/enhanced_document_linker.py
  - scripts/graph_stages/document_linker.py
  - city_clerk_documents/graph_json/debug/enhanced_linked_documents.json
  - config.py
  - city_clerk_documents/graph_json/debug/linking_report_01_09_2024.json
  - debug/meeting_info_parsed.json

## Part 3 (4 files):
  - debug/extracted_items.json
  - scripts/graph_stages/verbatim_transcript_linker.py
  - city_clerk_documents/graph_json/debug/verbatim/verbatim_linking_report_01_09_2024.json
  - scripts/graph_stages/__init__.py


================================================================================


################################################################################
# File: scripts/graph_stages/enhanced_document_linker.py
################################################################################

# File: scripts/graph_stages/enhanced_document_linker.py

"""
Enhanced Document Linker
Links both ordinance and resolution documents to their corresponding agenda items.
"""

import logging
import re
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import json
from datetime import datetime
import PyPDF2
from openai import OpenAI
import os
from dotenv import load_dotenv

load_dotenv()

log = logging.getLogger('enhanced_document_linker')


class EnhancedDocumentLinker:
    """Links ordinance and resolution documents to agenda items."""
    
    def __init__(self,
                 openai_api_key: Optional[str] = None,
                 model: str = "gpt-4.1-mini-2025-04-14",
                 agenda_extraction_max_tokens: int = 32768):
        """Initialize the enhanced document linker."""
        self.api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY not found in environment")
        
        self.client = OpenAI(api_key=self.api_key)
        self.model = model
        self.agenda_extraction_max_tokens = agenda_extraction_max_tokens
    
    async def link_documents_for_meeting(self, 
                                       meeting_date: str,
                                       ordinances_dir: Path,
                                       resolutions_dir: Path) -> Dict[str, List[Dict]]:
        """Find and link all documents (ordinances AND resolutions) for a specific meeting date."""
        log.info(f"🔗 Enhanced linking: documents for meeting date: {meeting_date}")
        log.info(f"📁 Ordinances directory: {ordinances_dir}")
        log.info(f"📁 Resolutions directory: {resolutions_dir}")
        
        # Create debug directory
        debug_dir = Path("city_clerk_documents/graph_json/debug")
        debug_dir.mkdir(exist_ok=True)
        
        # Convert date format: "01.09.2024" -> "01_09_2024"
        date_underscore = meeting_date.replace(".", "_")
        
        # Initialize results
        linked_documents = {
            "ordinances": [],
            "resolutions": []
        }
        
        # Process ordinances
        if ordinances_dir.exists():
            ordinance_files = self._find_matching_files(ordinances_dir, date_underscore)
            log.info(f"📄 Found {len(ordinance_files)} ordinance files")
            
            for doc_path in ordinance_files:
                doc_info = await self._process_document(doc_path, meeting_date, "ordinance")
                if doc_info:
                    linked_documents["ordinances"].append(doc_info)
        else:
            log.warning(f"⚠️  Ordinances directory not found: {ordinances_dir}")
        
        # Process resolutions - NEW LOGIC
        if resolutions_dir.exists():
            # Check for year subdirectory first
            year = meeting_date.split('.')[-1]  # Extract year from date
            year_dir = resolutions_dir / year
            
            if year_dir.exists():
                resolution_files = self._find_matching_files(year_dir, date_underscore)
                log.info(f"📄 Found {len(resolution_files)} resolution files in {year} directory")
            else:
                # Fall back to main resolutions directory
                resolution_files = self._find_matching_files(resolutions_dir, date_underscore)
                log.info(f"📄 Found {len(resolution_files)} resolution files in main directory")
            
            for doc_path in resolution_files:
                doc_info = await self._process_document(doc_path, meeting_date, "resolution")
                if doc_info:
                    linked_documents["resolutions"].append(doc_info)
        else:
            log.warning(f"⚠️  Resolutions directory not found: {resolutions_dir}")
        
        # Save enhanced linked documents info
        with open(debug_dir / "enhanced_linked_documents.json", 'w') as f:
            json.dump(linked_documents, f, indent=2)
        
        # Log summary
        total_linked = len(linked_documents['ordinances']) + len(linked_documents['resolutions'])
        log.info(f"✅ Enhanced linking complete:")
        log.info(f"   📄 Ordinances: {len(linked_documents['ordinances'])}")
        log.info(f"   📄 Resolutions: {len(linked_documents['resolutions'])}")
        log.info(f"   📄 Total linked: {total_linked}")
        
        # Save detailed report
        self._generate_linking_report(meeting_date, linked_documents, debug_dir)
        
        return linked_documents
    
    def _find_matching_files(self, directory: Path, date_pattern: str) -> List[Path]:
        """Find all PDF files matching the date pattern."""
        # Pattern: YYYY-## - MM_DD_YYYY.pdf
        pattern = f"*{date_pattern}.pdf"
        matching_files = list(directory.glob(pattern))
        
        # Also try without spaces in case filenames vary
        pattern2 = f"*{date_pattern}*.pdf"
        additional_files = [f for f in directory.glob(pattern2) if f not in matching_files]
        matching_files.extend(additional_files)
        
        # Also check for variations in date format
        # Some files might use dashes instead of underscores
        date_dash = date_pattern.replace("_", "-")
        pattern3 = f"*{date_dash}*.pdf"
        more_files = [f for f in directory.glob(pattern3) if f not in matching_files]
        matching_files.extend(more_files)
        
        return sorted(matching_files)
    
    async def _process_document(self, doc_path: Path, meeting_date: str, doc_type: str) -> Optional[Dict[str, Any]]:
        """Process a single document to extract agenda item reference."""
        try:
            # Extract document number from filename
            doc_match = re.match(r'^(\d{4}-\d{2,3})', doc_path.name)
            if not doc_match:
                log.warning(f"Could not parse document number from {doc_path.name}")
                return None
            
            document_number = doc_match.group(1)
            
            # Extract text from PDF
            text = self._extract_pdf_text(doc_path)
            if not text:
                log.warning(f"No text extracted from {doc_path.name}")
                return None
            
            # Extract agenda item code using LLM
            item_code = await self._extract_agenda_item_code(text, document_number, doc_type)
            
            # Extract additional metadata
            parsed_data = self._parse_document_metadata(text, doc_type)
            
            doc_info = {
                "path": str(doc_path),
                "filename": doc_path.name,
                "document_number": document_number,
                "item_code": item_code,
                "document_type": doc_type.capitalize(),
                "title": self._extract_title(text, doc_type),
                "parsed_data": parsed_data
            }
            
            log.info(f"📄 Processed {doc_type} {doc_path.name}: Item {item_code or 'NOT_FOUND'}")
            return doc_info
            
        except Exception as e:
            log.error(f"Error processing {doc_path.name}: {e}")
            return None
    
    def _extract_pdf_text(self, pdf_path: Path) -> str:
        """Extract text from PDF file."""
        try:
            with open(pdf_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                text_parts = []
                
                # Extract text from all pages
                for page in reader.pages:
                    text = page.extract_text()
                    if text:
                        text_parts.append(text)
                
                return "\n".join(text_parts)
        except Exception as e:
            log.error(f"Failed to extract text from {pdf_path.name}: {e}")
            return ""
    
    async def _extract_agenda_item_code(self, text: str, document_number: str, doc_type: str) -> Optional[str]:
        """Extract agenda item code from document text using LLM."""
        debug_dir = Path("city_clerk_documents/graph_json/debug")
        debug_dir.mkdir(exist_ok=True)
        
        # Try regex patterns first for better accuracy
        patterns = [
            r'Item\s+([A-Z]\.-?\d+\.?)',  # Item D.-1.
            r'Agenda\s+Item[:\s]+([A-Z]\.-?\d+\.?)',  # Agenda Item: D.-1.
            r'Section\s+([A-Z])[,\s]+Item\s+(\d+)',  # Section D, Item 1
            r'consent\s+agenda.*item\s+([A-Z]\.-?\d+\.?)',  # Consent Agenda ... Item D.-1.
            r'\b([A-Z]\.-\d+\.?)\s+\d{2}-\d{4}',  # D.-1. 23-6830 pattern
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                if len(match.groups()) == 2:  # Section X, Item Y format
                    code = f"{match.group(1)}-{match.group(2)}"
                else:
                    code = match.group(1)
                normalized_code = self._normalize_item_code(code)
                log.info(f"✅ Found agenda item code via regex for {document_number}: {normalized_code}")
                return normalized_code
        
        # Customize prompt based on document type
        if doc_type == "resolution":
            doc_type_text = "resolution"
            typical_sections = "F items (e.g., F-1, F-2, F-3)"
        else:
            doc_type_text = "ordinance"
            typical_sections = "E items (e.g., E-1, E-2, E-3)"
        
        prompt = f"""You are analyzing a City of Coral Gables {doc_type_text} document (Document #{document_number}).

Your task is to find the AGENDA ITEM CODE referenced in this document.

CRITICAL INSTRUCTIONS:
1. Search the ENTIRE document for agenda item references
2. Return ONLY the code in this format: AGENDA_ITEM: [code]
3. The code should be ONLY the letter and number (e.g., E-2, F-10, H-1)
4. Do NOT include any explanations, reasoning, or additional text
5. If no agenda item is found, return: AGENDA_ITEM: NOT_FOUND

Examples of valid responses:
- AGENDA_ITEM: E-2
- AGENDA_ITEM: F-10
- AGENDA_ITEM: H-1
- AGENDA_ITEM: NOT_FOUND

DO NOT RETURN ANYTHING ELSE. NO EXPLANATIONS.

Full document text:
{text}"""
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": f"You are a precise data extractor for {doc_type_text} documents. Find and extract only the agenda item code. Search the ENTIRE document thoroughly."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_tokens=self.agenda_extraction_max_tokens
            )
            
            raw_response = response.choices[0].message.content.strip()
            
            # Save raw LLM response for debugging
            with open(debug_dir / f"llm_response_{doc_type}_{document_number}_raw.txt", 'w', encoding='utf-8') as f:
                f.write(raw_response)
            
            # Parse response directly (no qwen parsing needed)
            result = raw_response
            
            # Save cleaned response
            with open(debug_dir / f"llm_response_{doc_type}_{document_number}_cleaned.txt", 'w', encoding='utf-8') as f:
                f.write(result)
            
            # Parse the response
            if "AGENDA_ITEM:" in result:
                # Extract just the code part, stopping at first space or newline after the code
                parts = result.split("AGENDA_ITEM:")[1].strip()
                
                # Extract just the code pattern (letter-number)
                code_match = re.match(r'^([A-Z]-?\d+)', parts)
                if code_match:
                    code = code_match.group(1)
                    if code != "NOT_FOUND":
                        code = self._normalize_item_code(code)
                        log.info(f"✅ Found agenda item code for {document_number}: {code}")
                        return code
                elif parts.startswith("NOT_FOUND"):
                    log.warning(f"❌ LLM could not find agenda item in {document_number}")
                else:
                    # Try to extract code from a messy response
                    code_pattern = r'\b([A-Z]-?\d+)\b'
                    match = re.search(code_pattern, parts)
                    if match:
                        code = self._normalize_item_code(match.group(1))
                        log.info(f"✅ Extracted agenda item code for {document_number}: {code} (from messy response)")
                        return code
                    log.error(f"❌ Could not parse item code from response: {parts[:100]}")
            else:
                log.error(f"❌ Invalid LLM response format for {document_number}: {result[:100]}")
            
            return None
            
        except Exception as e:
            log.error(f"Failed to extract agenda item for {doc_type} {document_number}: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _normalize_item_code(self, code: str) -> str:
        """Normalize item code to consistent format."""
        if not code:
            return code
        
        # Remove trailing dots and spaces
        code = code.rstrip('. ')
        
        # Remove dots between letter and dash: "E.-1" -> "E-1"
        code = re.sub(r'([A-Z])\.(-)', r'\1\2', code)
        
        # Handle cases without dash: "E.1" -> "E-1"
        code = re.sub(r'([A-Z])\.(\d)', r'\1-\2', code)
        
        # Remove any remaining dots
        code = code.replace('.', '')
        
        # Ensure we have a dash between letter and number
        code = re.sub(r'([A-Z])(\d)', r'\1-\2', code)
        
        return code
    
    def _extract_title(self, text: str, doc_type: str) -> str:
        """Extract document title from text."""
        # Look for "AN ORDINANCE" or "A RESOLUTION" pattern
        if doc_type == "resolution":
            pattern = r'(A\s+RESOLUTION[^.]+\.)'
        else:
            pattern = r'(AN?\s+ORDINANCE[^.]+\.)'
            
        title_match = re.search(pattern, text[:2000], re.IGNORECASE)
        if title_match:
            return title_match.group(1).strip()
        
        # Fallback to first substantive line
        lines = text.split('\n')
        for line in lines[:20]:
            if len(line) > 20 and not line.isdigit():
                return line.strip()[:200]
        
        return f"Untitled {doc_type.capitalize()}"
    
    def _parse_document_metadata(self, text: str, doc_type: str) -> Dict[str, Any]:
        """Parse additional metadata from document."""
        metadata = {
            "document_type": doc_type
        }
        
        # Extract date passed
        date_match = re.search(r'day\s+of\s+(\w+),?\s+(\d{4})', text)
        if date_match:
            metadata["date_passed"] = date_match.group(0)
        
        # Extract vote information
        vote_match = re.search(r'PASSED\s+AND\s+ADOPTED.*?(\d+).*?(\d+)', text, re.IGNORECASE | re.DOTALL)
        if vote_match:
            metadata["vote_details"] = {
                "ayes": vote_match.group(1),
                "nays": vote_match.group(2) if len(vote_match.groups()) > 1 else "0"
            }
        
        # Extract motion information
        motion_match = re.search(r'motion\s+(?:was\s+)?made\s+by\s+([^,]+)', text, re.IGNORECASE)
        if motion_match:
            metadata["motion"] = {"moved_by": motion_match.group(1).strip()}
        
        # Extract mayor signature
        mayor_match = re.search(r'Mayor[:\s]+([^\n]+)', text[-1000:])
        if mayor_match:
            metadata["signatories"] = {"mayor": mayor_match.group(1).strip()}
        
        # Resolution-specific metadata
        if doc_type == "resolution":
            # Look for resolution-specific patterns
            purpose_match = re.search(r'(?:WHEREAS|PURPOSE)[:\s]+([^.]+)', text, re.IGNORECASE)
            if purpose_match:
                metadata["purpose"] = purpose_match.group(1).strip()
        
        return metadata
    
    def _generate_linking_report(self, meeting_date: str, linked_documents: Dict, debug_dir: Path):
        """Generate a detailed report of the linking process."""
        report = {
            "meeting_date": meeting_date,
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total_ordinances": len(linked_documents["ordinances"]),
                "total_resolutions": len(linked_documents["resolutions"]),
                "ordinances_with_items": len([d for d in linked_documents["ordinances"] if d.get("item_code")]),
                "resolutions_with_items": len([d for d in linked_documents["resolutions"] if d.get("item_code")]),
                "unlinked_ordinances": len([d for d in linked_documents["ordinances"] if not d.get("item_code")]),
                "unlinked_resolutions": len([d for d in linked_documents["resolutions"] if not d.get("item_code")])
            },
            "details": {
                "ordinances": [
                    {
                        "document_number": d["document_number"],
                        "item_code": d.get("item_code", "NOT_FOUND"),
                        "title": d.get("title", "")[:100]
                    }
                    for d in linked_documents["ordinances"]
                ],
                "resolutions": [
                    {
                        "document_number": d["document_number"],
                        "item_code": d.get("item_code", "NOT_FOUND"),
                        "title": d.get("title", "")[:100]
                    }
                    for d in linked_documents["resolutions"]
                ]
            }
        }
        
        # FIXED: Use double quotes for f-string
        report_filename = f"linking_report_{meeting_date.replace('.', '_')}.json"
        report_path = debug_dir / report_filename
        
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        log.info(f"📊 Linking report saved to: {report_path}")
    
    # Add backward compatibility method
    async def link_documents_for_meeting_legacy(self, 
                                               meeting_date: str,
                                               documents_dir: Path) -> Dict[str, List[Dict]]:
        """Legacy method for backward compatibility - ordinances only."""
        return await self.link_documents_for_meeting(
            meeting_date,
            documents_dir,  # Ordinances directory
            Path("dummy")   # No resolutions directory
        )


================================================================================


################################################################################
# File: scripts/graph_stages/document_linker.py
################################################################################

# File: scripts/graph_stages/document_linker.py

"""
Document Linker
Links ordinance documents to their corresponding agenda items.
"""

import logging
import re
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import json
from datetime import datetime
import PyPDF2
from openai import OpenAI
import os
from dotenv import load_dotenv

load_dotenv()

log = logging.getLogger('document_linker')


class DocumentLinker:
    """Links ordinance and resolution documents to agenda items."""
    
    def __init__(self,
                 openai_api_key: Optional[str] = None,
                 model: str = "gpt-4.1-mini-2025-04-14",
                 agenda_extraction_max_tokens: int = 32768):
        """Initialize the document linker."""
        self.api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY not found in environment")
        
        self.client = OpenAI(api_key=self.api_key)
        self.model = model
        self.agenda_extraction_max_tokens = agenda_extraction_max_tokens
    
    async def link_documents_for_meeting(self, 
                                       meeting_date: str,
                                       documents_dir: Path) -> Dict[str, List[Dict]]:
        """Find and link all documents for a specific meeting date."""
        log.info(f"🔗 Linking documents for meeting date: {meeting_date}")
        log.info(f"📁 Looking for ordinances in: {documents_dir}")
        
        # Create debug directory
        debug_dir = Path("city_clerk_documents/graph_json/debug")
        debug_dir.mkdir(exist_ok=True)
        
        # Convert date format: "01.09.2024" -> "01_09_2024"
        date_underscore = meeting_date.replace(".", "_")
        
        # Log what we're searching for
        with open(debug_dir / "document_search_info.txt", 'w') as f:
            f.write(f"Meeting Date: {meeting_date}\n")
            f.write(f"Date with underscores: {date_underscore}\n")
            f.write(f"Search directory: {documents_dir}\n")
            f.write(f"Search pattern: *{date_underscore}.pdf\n")
        
        # Find all matching ordinance/resolution files
        # Pattern: YYYY-## - MM_DD_YYYY.pdf
        pattern = f"*{date_underscore}.pdf"
        matching_files = list(documents_dir.glob(pattern))
        
        # Also try without spaces in case filenames vary
        pattern2 = f"*{date_underscore}*.pdf"
        additional_files = [f for f in documents_dir.glob(pattern2) if f not in matching_files]
        matching_files.extend(additional_files)
        
        # List all files in directory for debugging
        all_files = list(documents_dir.glob("*.pdf"))
        with open(debug_dir / "all_ordinance_files.txt", 'w') as f:
            f.write(f"Total files in {documents_dir}: {len(all_files)}\n")
            for file in sorted(all_files):
                f.write(f"  - {file.name}\n")
        
        log.info(f"📄 Found {len(matching_files)} documents for date {meeting_date}")
        
        if matching_files:
            log.info("📄 Documents found:")
            for f in matching_files[:5]:
                log.info(f"   - {f.name}")
            if len(matching_files) > 5:
                log.info(f"   ... and {len(matching_files) - 5} more")
        
        # Save matched files list
        with open(debug_dir / "matched_documents.txt", 'w') as f:
            f.write(f"Documents matching date {meeting_date}:\n")
            for file in matching_files:
                f.write(f"  - {file.name}\n")
        
        # Process each document
        linked_documents = {
            "ordinances": [],
            "resolutions": []
        }
        
        for doc_path in matching_files:
            # Extract document info
            doc_info = await self._process_document(doc_path, meeting_date)
            
            if doc_info:
                # Categorize by type
                if "ordinance" in doc_info.get("title", "").lower():
                    linked_documents["ordinances"].append(doc_info)
                else:
                    linked_documents["resolutions"].append(doc_info)
        
        # Save linked documents info
        with open(debug_dir / "linked_documents.json", 'w') as f:
            json.dump(linked_documents, f, indent=2)
        
        log.info(f"✅ Linked {len(linked_documents['ordinances'])} ordinances, {len(linked_documents['resolutions'])} resolutions")
        return linked_documents
    
    async def _process_document(self, doc_path: Path, meeting_date: str) -> Optional[Dict[str, Any]]:
        """Process a single document to extract agenda item reference."""
        try:
            # Extract document number from filename (e.g., "2024-04" from "2024-04 - 01_23_2024.pdf")
            doc_match = re.match(r'^(\d{4}-\d{2})', doc_path.name)
            if not doc_match:
                log.warning(f"Could not parse document number from {doc_path.name}")
                return None
            
            document_number = doc_match.group(1)
            
            # Extract text from PDF
            text = self._extract_pdf_text(doc_path)
            if not text:
                log.warning(f"No text extracted from {doc_path.name}")
                return None
            
            # Extract agenda item code using LLM
            item_code = await self._extract_agenda_item_code(text, document_number)
            
            # Extract additional metadata
            parsed_data = self._parse_document_metadata(text)
            
            doc_info = {
                "path": str(doc_path),
                "filename": doc_path.name,
                "document_number": document_number,
                "item_code": item_code,
                "document_type": "Ordinance" if "ordinance" in text[:500].lower() else "Resolution",
                "title": self._extract_title(text),
                "parsed_data": parsed_data
            }
            
            log.info(f"📄 Processed {doc_path.name}: Item {item_code or 'NOT_FOUND'}")
            return doc_info
            
        except Exception as e:
            log.error(f"Error processing {doc_path.name}: {e}")
            return None
    
    def _extract_pdf_text(self, pdf_path: Path) -> str:
        """Extract text from PDF file."""
        try:
            with open(pdf_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                text_parts = []
                
                # Extract text from all pages
                for page in reader.pages:
                    text = page.extract_text()
                    if text:
                        text_parts.append(text)
                
                return "\n".join(text_parts)
        except Exception as e:
            log.error(f"Failed to extract text from {pdf_path.name}: {e}")
            return ""
    
    async def _extract_agenda_item_code(self, text: str, document_number: str) -> Optional[str]:
        """Extract agenda item code from document text using LLM."""
        # Create debug directory if it doesn't exist
        debug_dir = Path("city_clerk_documents/graph_json/debug")
        debug_dir.mkdir(exist_ok=True)
        
        # Send the entire document to qwen-32b
        text_excerpt = text
        
        # Save the full text being sent to LLM for debugging
        with open(debug_dir / f"llm_input_{document_number}.txt", 'w', encoding='utf-8') as f:
            f.write(f"Document: {document_number}\n")
            f.write(f"Text length: {len(text)} characters\n")
            f.write(f"Using model: {self.model}\n")
            f.write(f"Max tokens: {self.agenda_extraction_max_tokens}\n")
            f.write("\n--- FULL DOCUMENT SENT TO LLM ---\n")
            f.write(text_excerpt)
        
        log.info(f"📄 Sending full document to LLM for {document_number}: {len(text)} characters")
        
        prompt = f"""You are analyzing a City of Coral Gables ordinance document (Document #{document_number}).

Your task is to find the AGENDA ITEM CODE referenced in this document.

IMPORTANT: The agenda item can appear ANYWHERE in the document - on page 3, at the end, or anywhere else. Search the ENTIRE document carefully.

The agenda item typically appears in formats like:
- (Agenda Item: E-1)
- Agenda Item: E-3)
- (Agenda Item E-1)
- Item H-3
- H.-3. (with periods and dots)
- E.-2. (with dots)
- E-2 (without dots)
- Item E-2

Full document text:
{text_excerpt}

Respond in this EXACT format:
AGENDA_ITEM: [code] or AGENDA_ITEM: NOT_FOUND

Important: Return the code as it appears (e.g., E-2, not E.-2.)

Examples:
- If you find "(Agenda Item: E-2)" → respond: AGENDA_ITEM: E-2
- If you find "Item E-2" → respond: AGENDA_ITEM: E-2
- If you find "H.-3." → respond: AGENDA_ITEM: H-3
- If no agenda item found → respond: AGENDA_ITEM: NOT_FOUND"""
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a precise data extractor. Find and extract only the agenda item code. Search the ENTIRE document thoroughly."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_tokens=self.agenda_extraction_max_tokens  # Use 100,000 for qwen
            )
            
            raw_response = response.choices[0].message.content.strip()
            
            # Save raw LLM response for debugging
            with open(debug_dir / f"llm_response_{document_number}_raw.txt", 'w', encoding='utf-8') as f:
                f.write(raw_response)
            
            # Parse response directly (no qwen parsing needed)
            result = raw_response
            
            # Log the response for debugging
            log.info(f"LLM response for {document_number} (first 200 chars): {result[:200]}")
            
            # Parse the response
            if "AGENDA_ITEM:" in result:
                code = result.split("AGENDA_ITEM:")[1].strip()
                if code != "NOT_FOUND":
                    # Normalize the code (remove dots, ensure format)
                    code = self._normalize_item_code(code)
                    log.info(f"✅ Found agenda item code for {document_number}: {code}")
                    return code
                else:
                    log.warning(f"❌ LLM could not find agenda item in {document_number}")
            else:
                log.error(f"❌ Invalid LLM response format for {document_number}: {result[:100]}")
            
            return None
            
        except Exception as e:
            log.error(f"Failed to extract agenda item for {document_number}: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _normalize_item_code(self, code: str) -> str:
        """Normalize item code to consistent format."""
        if not code:
            return code
        
        # Remove trailing dots and spaces
        code = code.rstrip('. ')
        
        # Remove dots between letter and dash: "E.-1" -> "E-1"
        code = re.sub(r'([A-Z])\.(-)', r'\1\2', code)
        
        # Handle cases without dash: "E.1" -> "E-1"
        code = re.sub(r'([A-Z])\.(\d)', r'\1-\2', code)
        
        # Remove any remaining dots
        code = code.replace('.', '')
        
        return code
    
    def _extract_title(self, text: str) -> str:
        """Extract document title from text."""
        # Look for "AN ORDINANCE" or "A RESOLUTION" pattern
        title_match = re.search(r'(AN?\s+(ORDINANCE|RESOLUTION)[^.]+\.)', text[:2000], re.IGNORECASE)
        if title_match:
            return title_match.group(1).strip()
        
        # Fallback to first substantive line
        lines = text.split('\n')
        for line in lines[:20]:
            if len(line) > 20 and not line.isdigit():
                return line.strip()[:200]
        
        return "Untitled Document"
    
    def _parse_document_metadata(self, text: str) -> Dict[str, Any]:
        """Parse additional metadata from document."""
        metadata = {}
        
        # Extract date passed
        date_match = re.search(r'day\s+of\s+(\w+),?\s+(\d{4})', text)
        if date_match:
            metadata["date_passed"] = date_match.group(0)
        
        # Extract vote information
        vote_match = re.search(r'PASSED\s+AND\s+ADOPTED.*?(\d+).*?(\d+)', text, re.IGNORECASE | re.DOTALL)
        if vote_match:
            metadata["vote_details"] = {
                "ayes": vote_match.group(1),
                "nays": vote_match.group(2) if len(vote_match.groups()) > 1 else "0"
            }
        
        # Extract motion information
        motion_match = re.search(r'motion\s+(?:was\s+)?made\s+by\s+([^,]+)', text, re.IGNORECASE)
        if motion_match:
            metadata["motion"] = {"moved_by": motion_match.group(1).strip()}
        
        # Extract mayor signature
        mayor_match = re.search(r'Mayor[:\s]+([^\n]+)', text[-1000:])
        if mayor_match:
            metadata["signatories"] = {"mayor": mayor_match.group(1).strip()}
        
        return metadata


================================================================================


################################################################################
# File: city_clerk_documents/graph_json/debug/enhanced_linked_documents.json
################################################################################

{
  "ordinances": [
    {
      "path": "city_clerk_documents/global/City Comissions 2024/Ordinances/2024/2024-01 - 01_09_2024.pdf",
      "filename": "2024-01 - 01_09_2024.pdf",
      "document_number": "2024-01",
      "item_code": "E-1",
      "document_type": "Ordinance",
      "title": "A\nORDINANCE NO.",
      "parsed_data": {
        "document_type": "ordinance"
      }
    },
    {
      "path": "city_clerk_documents/global/City Comissions 2024/Ordinances/2024/2024-02 - 01_09_2024.pdf",
      "filename": "2024-02 - 01_09_2024.pdf",
      "document_number": "2024-02",
      "item_code": "E-2",
      "document_type": "Ordinance",
      "title": "A\nORDINANCE NO.",
      "parsed_data": {
        "document_type": "ordinance"
      }
    },
    {
      "path": "city_clerk_documents/global/City Comissions 2024/Ordinances/2024/2024-03 - 01_09_2024.pdf",
      "filename": "2024-03 - 01_09_2024.pdf",
      "document_number": "2024-03",
      "item_code": "E-3",
      "document_type": "Ordinance",
      "title": "A\nORDINANCE NO.",
      "parsed_data": {
        "document_type": "ordinance"
      }
    }
  ],
  "resolutions": [
    {
      "path": "city_clerk_documents/global/City Comissions 2024/Resolutions/2024/2024-01 - 01_09_2024.pdf",
      "filename": "2024-01 - 01_09_2024.pdf",
      "document_number": "2024-01",
      "item_code": "D-1",
      "document_type": "Resolution",
      "title": "A\nRESOLUTION NO.",
      "parsed_data": {
        "document_type": "resolution"
      }
    },
    {
      "path": "city_clerk_documents/global/City Comissions 2024/Resolutions/2024/2024-02 - 01_09_2024.pdf",
      "filename": "2024-02 - 01_09_2024.pdf",
      "document_number": "2024-02",
      "item_code": "D-2",
      "document_type": "Resolution",
      "title": "A\nRESOLUTION NO.",
      "parsed_data": {
        "document_type": "resolution"
      }
    },
    {
      "path": "city_clerk_documents/global/City Comissions 2024/Resolutions/2024/2024-03 - 01_09_2024.pdf",
      "filename": "2024-03 - 01_09_2024.pdf",
      "document_number": "2024-03",
      "item_code": "F-11",
      "document_type": "Resolution",
      "title": "A\nRESOLUTION NO.",
      "parsed_data": {
        "document_type": "resolution"
      }
    },
    {
      "path": "city_clerk_documents/global/City Comissions 2024/Resolutions/2024/2024-04 - 01_09_2024.pdf",
      "filename": "2024-04 - 01_09_2024.pdf",
      "document_number": "2024-04",
      "item_code": "E-9",
      "document_type": "Resolution",
      "title": "A\nRESOLUTION NO.",
      "parsed_data": {
        "document_type": "resolution"
      }
    },
    {
      "path": "city_clerk_documents/global/City Comissions 2024/Resolutions/2024/2024-05 - 01_09_2024.pdf",
      "filename": "2024-05 - 01_09_2024.pdf",
      "document_number": "2024-05",
      "item_code": "F-10",
      "document_type": "Resolution",
      "title": "A\nRESOLUTION NO.",
      "parsed_data": {
        "document_type": "resolution"
      }
    },
    {
      "path": "city_clerk_documents/global/City Comissions 2024/Resolutions/2024/2024-06 - 01_09_2024.pdf",
      "filename": "2024-06 - 01_09_2024.pdf",
      "document_number": "2024-06",
      "item_code": "H-1",
      "document_type": "Resolution",
      "title": "A\nRESOLUTION NO.",
      "parsed_data": {
        "document_type": "resolution"
      }
    },
    {
      "path": "city_clerk_documents/global/City Comissions 2024/Resolutions/2024/2024-07 - 01_09_2024.pdf",
      "filename": "2024-07 - 01_09_2024.pdf",
      "document_number": "2024-07",
      "item_code": "H-2",
      "document_type": "Resolution",
      "title": "A\nRESOLUTION NO.",
      "parsed_data": {
        "document_type": "resolution"
      }
    }
  ]
}


================================================================================


################################################################################
# File: config.py
################################################################################

# File: config.py

#!/usr/bin/env python3
"""
Configuration file for graph database visualization
Manages database credentials and connection settings
"""

import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Azure Cosmos DB Gremlin Configuration
COSMOS_ENDPOINT = os.getenv("COSMOS_ENDPOINT", "wss://aida-graph-db.gremlin.cosmos.azure.com:443")
COSMOS_KEY = os.getenv("COSMOS_KEY", "")  # This will be set from .env file
DATABASE = os.getenv("COSMOS_DATABASE", "cgGraph")
CONTAINER = os.getenv("COSMOS_CONTAINER", "cityClerk")
PARTITION_KEY = os.getenv("COSMOS_PARTITION_KEY", "partitionKey")
PARTITION_VALUE = os.getenv("COSMOS_PARTITION_VALUE", "demo")

def validate_config():
    """Validate that all required configuration is available"""
    required_vars = {
        "COSMOS_KEY": COSMOS_KEY,
        "COSMOS_ENDPOINT": COSMOS_ENDPOINT,
        "DATABASE": DATABASE,
        "CONTAINER": CONTAINER
    }
    
    missing_vars = [var for var, value in required_vars.items() if not value]
    
    if missing_vars:
        print("❌ Missing required configuration:")
        for var in missing_vars:
            print(f"   - {var}")
        print("\n🔧 Please create a .env file with your credentials:")
        print("   COSMOS_KEY=your_actual_cosmos_key_here")
        print("   COSMOS_ENDPOINT=wss://aida-graph-db.gremlin.cosmos.azure.com:443")
        print("   COSMOS_DATABASE=cgGraph")
        print("   COSMOS_CONTAINER=cityClerk")
        return False
    
    return True

if __name__ == "__main__":
    print("🔧 Configuration Check:")
    if validate_config():
        print("✅ All configuration variables are set!")
    else:
        print("❌ Configuration incomplete!")


================================================================================


################################################################################
# File: city_clerk_documents/graph_json/debug/linking_report_01_09_2024.json
################################################################################

{
  "meeting_date": "01.09.2024",
  "timestamp": "2025-06-03T16:06:08.015221",
  "summary": {
    "total_ordinances": 3,
    "total_resolutions": 7,
    "ordinances_with_items": 3,
    "resolutions_with_items": 7,
    "unlinked_ordinances": 0,
    "unlinked_resolutions": 0
  },
  "details": {
    "ordinances": [
      {
        "document_number": "2024-01",
        "item_code": "E-1",
        "title": "A\nORDINANCE NO."
      },
      {
        "document_number": "2024-02",
        "item_code": "E-2",
        "title": "A\nORDINANCE NO."
      },
      {
        "document_number": "2024-03",
        "item_code": "E-3",
        "title": "A\nORDINANCE NO."
      }
    ],
    "resolutions": [
      {
        "document_number": "2024-01",
        "item_code": "D-1",
        "title": "A\nRESOLUTION NO."
      },
      {
        "document_number": "2024-02",
        "item_code": "D-2",
        "title": "A\nRESOLUTION NO."
      },
      {
        "document_number": "2024-03",
        "item_code": "F-11",
        "title": "A\nRESOLUTION NO."
      },
      {
        "document_number": "2024-04",
        "item_code": "E-9",
        "title": "A\nRESOLUTION NO."
      },
      {
        "document_number": "2024-05",
        "item_code": "F-10",
        "title": "A\nRESOLUTION NO."
      },
      {
        "document_number": "2024-06",
        "item_code": "H-1",
        "title": "A\nRESOLUTION NO."
      },
      {
        "document_number": "2024-07",
        "item_code": "H-2",
        "title": "A\nRESOLUTION NO."
      }
    ]
  }
}


================================================================================


################################################################################
# File: debug/meeting_info_parsed.json
################################################################################

{
  "type": "Regular Meeting",
  "time": "9:00 AM",
  "location": "City Hall, Commission Chambers",
  "commissioners": [
    "Mayor Vince Lago",
    "Vice Mayor Rhonda Anderson",
    "Commissioner Melissa Castro",
    "Commissioner Ariel Fernandez",
    "Commissioner Kirk R. Menendez"
  ],
  "officials": {
    "mayor": "Vince Lago",
    "city_manager": "Peter J. Iglesias, P.E.",
    "city_attorney": "Cristina M. Su\u00e1rez",
    "city_clerk": "Billy Y. Urquia"
  }
}


================================================================================

