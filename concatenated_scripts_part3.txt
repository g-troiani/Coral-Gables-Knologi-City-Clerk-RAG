# Concatenated Project Code - Part 3 of 3
# Generated: 2025-05-30 00:16:01
# Root Directory: /Users/gianmariatroiani/Documents/knologi̊/graph_database
================================================================================

# File Index - Which Files Are in Which Parts
################################################################################

## Part 1 (7 files):
  - scripts/stages/embed_vectors.py
  - scripts/rag_local_web_app.py
  - scripts/pipeline_modular_optimized.py
  - debug_graph.py
  - relationOPENAI.py
  - scripts/graph_stages/agenda_parser.py
  - requirements.txt

## Part 2 (9 files):
  - scripts/stages/extract_clean.py
  - scripts/stages/chunk_text.py
  - scripts/graph_stages/agenda_pdf_extractor.py
  - scripts/graph_stages/cosmos_db_client.py
  - scripts/stages/llm_enrich.py
  - scripts/stages/acceleration_utils.py
  - test_query.py
  - clear_gremlin.py
  - scripts/graph_stages/__init__.py

## Part 3 (8 files):
  - scripts/graph_stages/agenda_graph_builder.py
  - scripts/agenda_structure_pipeline.py
  - scripts/graph_stages/agenda_ontology_extractor.py
  - scripts/clear_database.py
  - scripts/stages/db_upsert.py
  - scripts/graph_stages/entity_deduplicator.py
  - config.py
  - scripts/stages/__init__.py


================================================================================


################################################################################
# File: scripts/graph_stages/agenda_graph_builder.py
################################################################################

# File: scripts/graph_stages/agenda_graph_builder.py

"""
Agenda Graph Builder
===================
Builds graph representation from extracted agenda ontology.
"""
import logging
from typing import Dict, List, Optional
from pathlib import Path
import hashlib

from .cosmos_db_client import CosmosGraphClient

log = logging.getLogger(__name__)


class AgendaGraphBuilder:
    """Build comprehensive graph representation from agenda ontology."""
    
    def __init__(self, cosmos_client: CosmosGraphClient):
        self.cosmos = cosmos_client
        self.entity_id_cache = {}  # Cache for entity IDs
    
    async def build_graph_from_ontology(self, ontology: Dict, agenda_path: Path) -> Dict:
        """Build graph representation from extracted ontology."""
        log.info(f"🔨 Starting graph build for {agenda_path.name}")
        
        graph_data = {
            'nodes': {},
            'edges': [],
            'statistics': {
                'entities': {},
                'relationships': 0
            }
        }
        
        meeting_date = ontology['meeting_date']
        meeting_info = ontology['meeting_info']
        
        # 1. Create Meeting node as the root
        meeting_id = await self._create_meeting_node(meeting_date, meeting_info, agenda_path.name)
        log.info(f"✅ Created meeting node: {meeting_id}")
        
        graph_data['nodes'][meeting_id] = {
            'type': 'Meeting',
            'date': meeting_date,
            'info': meeting_info
        }
        
        # 2. Create nodes for officials present
        await self._create_official_nodes(meeting_info.get('officials_present', {}), meeting_id)
        
        # 3. Process agenda structure
        section_count = 0
        item_count = 0
        
        log.info(f"📑 Processing {len(ontology['agenda_structure'])} sections")
        
        for section_idx, section in enumerate(ontology['agenda_structure']):
            section_count += 1
            section_id = f"section-{meeting_date}-{section_idx}"
            
            # Create AgendaSection node
            await self._create_section_node(section_id, section, section_idx)
            log.info(f"✅ Created section {section_idx}: {section.get('section_name', 'Unknown')}")
            
            graph_data['nodes'][section_id] = {
                'type': 'AgendaSection',
                'name': section['section_name'],
                'order': section_idx
            }
            
            # Link section to meeting
            await self.cosmos.create_edge(
                from_id=meeting_id,
                to_id=section_id,
                edge_type='HAS_SECTION',
                properties={'order': section_idx}
            )
            log.info(f"✅ Created edge: {meeting_id} -> {section_id}")
            
            # Process items in section
            previous_item_id = None
            items = section.get('items', [])
            log.info(f"📌 Processing {len(items)} items in section {section_idx}")
            
            for item_idx, item in enumerate(items):
                if not item.get('item_code'):
                    log.warning(f"Skipping item without code in section {section['section_name']}")
                    continue
                    
                item_count += 1
                item_id = f"item-{meeting_date}-{item['item_code']}"
                
                # Create AgendaItem node with rich metadata
                await self._create_agenda_item_node(item_id, item, section.get('section_type', 'Unknown'))
                log.info(f"✅ Created item {item['item_code']}: {item.get('title', 'Unknown')}")
                
                graph_data['nodes'][item_id] = {
                    'type': 'AgendaItem',
                    'code': item['item_code'],
                    'title': item.get('title', 'Unknown')
                }
                
                # Link item to section
                await self.cosmos.create_edge(
                    from_id=section_id,
                    to_id=item_id,
                    edge_type='CONTAINS_ITEM',
                    properties={'order': item_idx}
                )
                log.info(f"✅ Created edge: {section_id} -> {item_id}")
                
                # Create sequential relationships
                if previous_item_id:
                    await self.cosmos.create_edge(
                        from_id=previous_item_id,
                        to_id=item_id,
                        edge_type='FOLLOWS',
                        properties={'sequence': item_idx}
                    )
                    log.info(f"✅ Created sequential edge: {previous_item_id} -> {item_id}")
                
                previous_item_id = item_id
                
                # Create sponsor relationship if exists
                if item.get('sponsor'):
                    await self._create_sponsor_relationship(item_id, item['sponsor'])
                    log.info(f"✅ Created sponsor relationship for {item_id}")
                
                # Create department relationship if exists
                if item.get('department'):
                    await self._create_department_relationship(item_id, item['department'])
                    log.info(f"✅ Created department relationship for {item_id}")
        
        # 4. Create entity nodes
        log.info(f"👥 Creating entity nodes from extracted entities")
        entity_count = await self._create_entity_nodes(ontology['entities'], meeting_id)
        
        # 5. Create relationships
        relationship_count = 0
        log.info(f"🔗 Creating {len(ontology['relationships'])} relationships")
        
        for rel in ontology['relationships']:
            await self._create_item_relationship(rel, meeting_date)
            relationship_count += 1
        
        # Update statistics
        graph_data['statistics'] = {
            'sections': section_count,
            'items': item_count,
            'entities': entity_count,
            'relationships': relationship_count,
            'meeting_date': meeting_date
        }
        
        log.info(f"🎉 Graph build complete for {agenda_path.name}")
        log.info(f"   - Sections: {section_count}")
        log.info(f"   - Items: {item_count}")
        log.info(f"   - Entities: {entity_count}")
        log.info(f"   - Relationships: {relationship_count}")
        
        return graph_data
    
    async def _create_meeting_node(self, meeting_date: str, meeting_info: Dict, source_file: str = None) -> str:
        """Create Meeting node with comprehensive metadata."""
        meeting_id = f"meeting-{meeting_date.replace('.', '-')}"
        
        # First check if it already exists
        check_query = f"g.V('{meeting_id}')"
        existing = await self.cosmos._execute_query(check_query)
        if existing:
            log.info(f"Meeting {meeting_id} already exists")
            return meeting_id
        
        location = meeting_info.get('location', {})
        if isinstance(location, dict):
            location_str = f"{location.get('name', '')} - {location.get('address', '')}"
        else:
            location_str = "405 Biltmore Way, Coral Gables, FL"
        
        # Escape the location string BEFORE using it in the f-string
        escaped_location = location_str.replace("'", "\\'")
        
        # Simplified query without fold/coalesce
        query = f"""g.addV('Meeting')
            .property('id','{meeting_id}')
            .property('partitionKey','demo')
            .property('nodeType','Meeting')
            .property('date','{meeting_date}')
            .property('type','{meeting_info.get('meeting_type', 'Regular Meeting')}')
            .property('time','{meeting_info.get('meeting_time', '')}')
            .property('location','{escaped_location}')"""
        
        if source_file:
            query += f".property('source_file','{source_file}')"
        
        try:
            result = await self.cosmos._execute_query(query)
            log.info(f"✅ Created Meeting node: {meeting_id}")
            return meeting_id
        except Exception as e:
            log.error(f"❌ Failed to create Meeting node {meeting_id}: {e}")
            raise
    
    async def _create_section_node(self, section_id: str, section: Dict, order: int) -> str:
        """Create AgendaSection node."""
        # Escape strings BEFORE using in f-string
        section_name = section.get('section_name', 'Unknown').replace("'", "\\'")
        section_type = section.get('section_type', 'OTHER').replace("'", "\\'")
        
        query = f"""g.addV('AgendaSection')
           .property('id', '{section_id}')
           .property('partitionKey', 'demo')
           .property('title', '{section_name}')
           .property('type', '{section_type}')
           .property('order', {order})"""
        
        await self.cosmos._execute_query(query)
        return section_id
    
    async def _create_agenda_item_node(self, item_id: str, item: Dict, section_type: str) -> str:
        """Create AgendaItem node with all metadata."""
        # Escape strings BEFORE using in f-string
        title = (item.get('title') or 'Unknown').replace("'", "\\'")
        summary = (item.get('summary') or '').replace("'", "\\'")[:500]
        
        query = f"""g.addV('AgendaItem')
           .property('id', '{item_id}')
           .property('partitionKey', 'demo')
           .property('code', '{item['item_code']}')
           .property('title', '{title}')
           .property('type', '{item.get('item_type', 'Item')}')
           .property('section_type', '{section_type}')"""
        
        if summary:
            query += f".property('summary', '{summary}')"
        
        # Add optional properties
        if item.get('document_reference'):
            query += f".property('document_reference', '{item['document_reference']}')"
        
        await self.cosmos._execute_query(query)
        return item_id
    
    async def _create_official_nodes(self, officials: Dict, meeting_id: str):
        """Create nodes for city officials and link to meeting."""
        if not officials:
            return
            
        roles_mapping = {
            'mayor': 'Mayor',
            'vice_mayor': 'Vice Mayor',
            'city_attorney': 'City Attorney',
            'city_manager': 'City Manager',
            'city_clerk': 'City Clerk'
        }
        
        # Process standard officials
        for key, role in roles_mapping.items():
            if officials.get(key) and officials[key] != 'null':
                person_id = await self._ensure_person_node(officials[key], role)
                await self.cosmos.create_edge(
                    from_id=person_id,
                    to_id=meeting_id,
                    edge_type='ATTENDED',
                    properties={'role': role}
                )
        
        # Process commissioners
        commissioners = officials.get('commissioners', [])
        if isinstance(commissioners, list):
            for idx, commissioner in enumerate(commissioners):
                if commissioner and commissioner != 'null':
                    person_id = await self._ensure_person_node(commissioner, 'Commissioner')
                    await self.cosmos.create_edge(
                        from_id=person_id,
                        to_id=meeting_id,
                        edge_type='ATTENDED',
                        properties={'role': 'Commissioner', 'seat': idx + 1}
                    )
    
    async def _create_entity_nodes(self, entities: Dict[str, List[Dict]], meeting_id: str) -> Dict[str, int]:
        """Create nodes for all extracted entities."""
        entity_counts = {}
        
        # Create Person nodes
        for person in entities.get('people', []):
            if person.get('name'):
                person_id = await self._ensure_person_node(person['name'], person.get('role', 'Participant'))
                # Link to meeting if they're mentioned
                await self.cosmos.create_edge(
                    from_id=person_id,
                    to_id=meeting_id,
                    edge_type='MENTIONED_IN',
                    properties={'context': person.get('context', '')[:100]}
                )
        entity_counts['people'] = len(entities.get('people', []))
        
        # Create Organization nodes
        for org in entities.get('organizations', []):
            if org.get('name'):
                org_id = await self._ensure_organization_node(org['name'], org.get('type', 'Organization'))
                await self.cosmos.create_edge(
                    from_id=org_id,
                    to_id=meeting_id,
                    edge_type='MENTIONED_IN',
                    properties={'context': org.get('context', '')[:100]}
                )
        entity_counts['organizations'] = len(entities.get('organizations', []))
        
        # Create Location nodes
        for location in entities.get('locations', []):
            if location.get('name'):
                loc_id = await self._ensure_location_node(
                    location['name'], 
                    location.get('address', ''),
                    location.get('type', 'Location')
                )
                await self.cosmos.create_edge(
                    from_id=loc_id,
                    to_id=meeting_id,
                    edge_type='REFERENCED_IN',
                    properties={'context': location.get('context', '')[:100]}
                )
        entity_counts['locations'] = len(entities.get('locations', []))
        
        # Create FinancialItem nodes
        for amount in entities.get('monetary_amounts', []):
            if amount.get('amount'):
                fin_id = await self._create_financial_node(
                    amount['amount'],
                    amount.get('purpose', ''),
                    meeting_id
                )
        entity_counts['financial_items'] = len(entities.get('monetary_amounts', []))
        
        return entity_counts
    
    async def _ensure_person_node(self, name: str, role: str) -> str:
        """Create or retrieve person node."""
        clean_name = name.strip()
        # Clean the ID by removing invalid characters
        person_id = f"person-{clean_name.lower().replace(' ', '-').replace('.', '').replace("'", '').replace('"', '').replace('/', '-')}"
        
        # Check cache first
        if person_id in self.entity_id_cache:
            return person_id
        
        # Check if exists in database
        try:
            result = await self.cosmos._execute_query(f"g.V('{person_id}')")
            if result:
                self.entity_id_cache[person_id] = True
                return person_id
        except:
            pass
        
        # Create new person - escape name BEFORE using in f-string
        escaped_name = clean_name.replace("'", "\\'").replace('"', '\\"')
        escaped_role = role.replace("'", "\\'").replace('"', '\\"')
        query = f"""g.addV('Person')
            .property('id', '{person_id}')
            .property('partitionKey', 'demo')
            .property('name', '{escaped_name}')
            .property('roles', '{escaped_role}')"""
        
        await self.cosmos._execute_query(query)
        self.entity_id_cache[person_id] = True
        return person_id
    
    async def _ensure_organization_node(self, name: str, org_type: str) -> str:
        """Create or retrieve organization node."""
        # Clean the ID by removing invalid characters
        org_id = f"org-{name.lower().replace(' ', '-').replace('.', '').replace("'", '').replace('"', '').replace('/', '-').replace(',', '')}"
        
        if org_id in self.entity_id_cache:
            return org_id
        
        escaped_name = name.replace("'", "\\'").replace('"', '\\"')
        escaped_type = org_type.replace("'", "\\'").replace('"', '\\"')
        query = f"""g.V().has('Organization', 'name', '{escaped_name}').fold().coalesce(unfold(),
            addV('Organization')
            .property('id', '{org_id}')
            .property('partitionKey', 'demo')
            .property('name', '{escaped_name}')
            .property('type', '{escaped_type}')
        )"""
        
        await self.cosmos._execute_query(query)
        self.entity_id_cache[org_id] = True
        return org_id
    
    async def _ensure_location_node(self, name: str, address: str, loc_type: str) -> str:
        """Create or retrieve location node."""
        # Clean the ID by removing invalid characters
        loc_id = f"location-{name.lower().replace(' ', '-').replace('.', '').replace("'", '').replace('"', '').replace('/', '-').replace(',', '')}"
        
        if loc_id in self.entity_id_cache:
            return loc_id
        
        escaped_name = name.replace("'", "\\'").replace('"', '\\"')
        escaped_address = address.replace("'", "\\'").replace('"', '\\"')
        escaped_type = loc_type.replace("'", "\\'").replace('"', '\\"')
        
        query = f"""g.V().has('Location', 'name', '{escaped_name}').fold().coalesce(unfold(),
            addV('Location')
            .property('id', '{loc_id}')
            .property('partitionKey', 'demo')
            .property('name', '{escaped_name}')
            .property('address', '{escaped_address}')
            .property('type', '{escaped_type}')
        )"""
        
        await self.cosmos._execute_query(query)
        self.entity_id_cache[loc_id] = True
        return loc_id
    
    async def _create_financial_node(self, amount: str, purpose: str, meeting_id: str) -> str:
        """Create financial item node."""
        fin_id = f"financial-{hashlib.md5(f'{amount}-{purpose}'.encode()).hexdigest()[:8]}"
        
        escaped_purpose = purpose.replace("'", "\\'")
        
        query = f"""g.addV('FinancialItem')
            .property('id', '{fin_id}')
            .property('partitionKey', 'demo')
            .property('amount', '{amount}')
            .property('purpose', '{escaped_purpose}')"""
        
        await self.cosmos._execute_query(query)
        
        # Link to meeting
        await self.cosmos.create_edge(
            from_id=fin_id,
            to_id=meeting_id,
            edge_type='DISCUSSED_IN'
        )
        
        return fin_id
    
    async def _create_sponsor_relationship(self, item_id: str, sponsor_name: str):
        """Create sponsorship relationship."""
        person_id = await self._ensure_person_node(sponsor_name, 'Sponsor')
        await self.cosmos.create_edge(
            from_id=person_id,
            to_id=item_id,
            edge_type='SPONSORS',
            properties={'role': 'sponsor'}
        )
    
    async def _create_department_relationship(self, item_id: str, department_name: str):
        """Create department origination relationship."""
        dept_id = await self._ensure_organization_node(department_name, 'Department')
        await self.cosmos.create_edge(
            from_id=dept_id,
            to_id=item_id,
            edge_type='ORIGINATES',
            properties={'role': 'originating_department'}
        )
    
    async def _create_item_relationship(self, rel: Dict, meeting_date: str):
        """Create relationship between agenda items."""
        from_id = f"item-{meeting_date}-{rel['from_code']}"
        to_id = f"item-{meeting_date}-{rel['to_code']}"
        
        # Check if both items exist
        try:
            from_result = await self.cosmos._execute_query(f"g.V('{from_id}')")
            to_result = await self.cosmos._execute_query(f"g.V('{to_id}')")
            
            if from_result and to_result:
                await self.cosmos.create_edge(
                    from_id=from_id,
                    to_id=to_id,
                    edge_type=rel['relationship_type'],
                    properties={
                        'description': rel.get('description', ''),
                        'strength': rel.get('strength', 'medium')
                    }
                )
        except Exception as e:
            log.warning(f"Could not create relationship {from_id} -> {to_id}: {e}")


================================================================================


################################################################################
# File: scripts/agenda_structure_pipeline.py
################################################################################

# File: scripts/agenda_structure_pipeline.py

#!/usr/bin/env python3
"""
City Clerk Agenda Graph Pipeline - Main Orchestrator
====================================================
Orchestrates the extraction of meaningful entities and relationships from agenda documents.
Now uses dedicated graph pipeline PDF extractor.
"""
import json
import logging
import asyncio
from pathlib import Path
from typing import Dict, List, Optional
from collections import Counter

from dotenv import load_dotenv
from openai import AzureOpenAI
import os

# ============================================================================
# PIPELINE STAGE CONTROLS - Set to False to skip specific stages
# ============================================================================
RUN_PDF_EXTRACT = True      # Stage 1: Extract PDF content with hierarchy
RUN_ONTOLOGY    = True      # Stage 2: Extract ontology using LLM
RUN_BUILD_GRAPH = True      # Stage 3: Build graph from ontology
RUN_CLEAR_GRAPH = False     # Clear existing graph data before processing
RUN_CONN_TEST   = True      # Run connection test before processing
INTERACTIVE     = True      # Ask for user input (set False for automation)

# ============================================================================

# Import graph stages (no longer using RAG pipeline stages)
from graph_stages.agenda_pdf_extractor import AgendaPDFExtractor
from graph_stages.cosmos_db_client import CosmosGraphClient
from graph_stages.agenda_ontology_extractor import CityClerkOntologyExtractor
from graph_stages.agenda_graph_builder import AgendaGraphBuilder

load_dotenv()

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
log = logging.getLogger("agenda_pipeline")

# Azure OpenAI Configuration
AZURE_OPENAI_API_KEY = os.getenv("AZURE_OPENAI_API_KEY")
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_OPENAI_API_VERSION = os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-15-preview")
DEPLOYMENT_NAME = os.getenv("AZURE_OPENAI_DEPLOYMENT", "gpt-4o")

# Initialize Azure OpenAI client
aoai = AzureOpenAI(
    api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    api_version=AZURE_OPENAI_API_VERSION
)
aoai.deployment_name = DEPLOYMENT_NAME  # Add deployment name as attribute


class AgendaPipelineOrchestrator:
    """Main orchestrator for agenda document processing pipeline."""
    
    def __init__(self, cosmos_client: CosmosGraphClient):
        self.cosmos_client = cosmos_client
        self.pdf_extractor = AgendaPDFExtractor()
        self.ontology_extractor = CityClerkOntologyExtractor(aoai)
        self.graph_builder = AgendaGraphBuilder(cosmos_client)
        self.stats = Counter()
        
    async def process_agenda(self, agenda_path: Path) -> Dict:
        """Process a single agenda document through all stages."""
        log.info(f"Processing agenda: {agenda_path.name}")
        
        # Track which stages ran
        stages_run = []
        
        try:
            extracted_data = None
            ontology = None
            graph_data = None
            
            # Stage 1: Extract PDF content with hierarchy preservation
            if RUN_PDF_EXTRACT:
                log.info(f"Stage 1: Extracting PDF content for {agenda_path.name}")
                extracted_data = self.pdf_extractor.extract_agenda(agenda_path)
                
                # Log extraction statistics
                stats = self.pdf_extractor.get_extraction_stats(extracted_data)
                log.info(f"Extraction stats: {stats}")
                stages_run.append("PDF_EXTRACT")
            else:
                log.info("Stage 1: SKIPPED (RUN_PDF_EXTRACT=False)")
                # Try to load existing extraction if available
                json_path = self.pdf_extractor.output_dir / f"{agenda_path.stem}_extracted.json"
                if json_path.exists():
                    log.info(f"Loading existing extraction from: {json_path}")
                    extracted_data = json.loads(json_path.read_text())
                else:
                    log.warning(f"No existing extraction found for {agenda_path.name}")
                    return {'error': 'No extraction available', 'stages_run': stages_run}
            
            # Convert to format expected by ontology extractor
            agenda_data = self._convert_to_agenda_format(extracted_data)
            
            # Stage 2: Extract ontology using LLM
            if RUN_ONTOLOGY:
                log.info(f"Stage 2: Extracting ontology for {agenda_path.name}")
                ontology = await self.ontology_extractor.extract_agenda_ontology(
                    agenda_data, 
                    agenda_path.name
                )
                
                # Save ontology for debugging/reuse
                ontology_path = self.pdf_extractor.output_dir / f"{agenda_path.stem}_ontology.json"
                ontology_path.write_text(json.dumps(ontology, indent=2))
                log.info(f"Saved ontology to: {ontology_path}")
                stages_run.append("ONTOLOGY")
            else:
                log.info("Stage 2: SKIPPED (RUN_ONTOLOGY=False)")
                # Try to load existing ontology
                ontology_path = self.pdf_extractor.output_dir / f"{agenda_path.stem}_ontology.json"
                if ontology_path.exists():
                    log.info(f"Loading existing ontology from: {ontology_path}")
                    ontology = json.loads(ontology_path.read_text())
                else:
                    log.warning(f"No existing ontology found for {agenda_path.name}")
                    return {'error': 'No ontology available', 'stages_run': stages_run}
            
            # Stage 3: Build graph from ontology
            if RUN_BUILD_GRAPH:
                log.info(f"Stage 3: Building graph for {agenda_path.name}")
                graph_data = await self.graph_builder.build_graph_from_ontology(
                    ontology, 
                    agenda_path
                )
                stages_run.append("BUILD_GRAPH")
                
                # Update statistics
                self._update_stats(graph_data)
            else:
                log.info("Stage 3: SKIPPED (RUN_BUILD_GRAPH=False)")
                # Create minimal graph data for statistics
                graph_data = {
                    'statistics': {
                        'sections': len(ontology.get('agenda_structure', [])),
                        'items': len(ontology.get('item_codes', [])),
                        'entities': len(ontology.get('entities', {})),
                        'relationships': len(ontology.get('relationships', []))
                    }
                }
            
            # Add stages run to result
            if graph_data:
                graph_data['stages_run'] = stages_run
            
            return graph_data or {'stages_run': stages_run}
            
        except Exception as e:
            log.error(f"Failed to process {agenda_path.name}: {e}")
            self.stats['failed'] += 1
            raise
    
    def _convert_to_agenda_format(self, extracted_data: Dict) -> Dict:
        """Convert extracted data to format expected by ontology extractor."""
        # Build sections from the extracted hierarchy
        sections = []
        
        # Add title as first section
        if extracted_data.get('title'):
            sections.append({
                'section': 'Title',
                'text': extracted_data['title'],
                'page_number': 1
            })
        
        # Add preamble if exists
        if extracted_data.get('preamble'):
            preamble_text = '\n'.join([
                item['text'] for item in extracted_data['preamble']
            ])
            sections.append({
                'section': 'Preamble',
                'text': preamble_text,
                'page_number': 1
            })
        
        # Convert hierarchical sections
        for section in extracted_data.get('sections', []):
            section_text = section.get('title', '') + '\n\n'
            
            # Add section content
            for content in section.get('content', []):
                section_text += content.get('text', '') + '\n'
            
            # Add subsections
            for subsection in section.get('subsections', []):
                section_text += f"\n{subsection.get('title', '')}\n"
                for content in subsection.get('content', []):
                    section_text += content.get('text', '') + '\n'
            
            sections.append({
                'section': section.get('title', 'Untitled'),
                'text': section_text.strip(),
                'page_start': section.get('page_start', 1),
                'elements': section.get('content', [])
            })
        
        # Include agenda items as a special section
        if extracted_data.get('agenda_items'):
            items_text = "EXTRACTED AGENDA ITEMS:\n\n"
            for item in extracted_data['agenda_items']:
                items_text += f"{item['code']}: {item.get('title', item.get('context', '')[:100])}\n"
            
            sections.append({
                'section': 'Agenda Items Summary',
                'text': items_text
            })
        
        return {
            'sections': sections,
            'metadata': extracted_data.get('metadata', {}),
            'agenda_items': extracted_data.get('agenda_items', [])
        }
    
    async def process_batch(self, agenda_files: List[Path], batch_size: int = 3):
        """Process multiple agenda files in batches."""
        total_files = len(agenda_files)
        
        # Log pipeline configuration
        log.info(f"\n{'='*60}")
        log.info("PIPELINE CONFIGURATION:")
        log.info(f"  PDF Extract: {'ENABLED' if RUN_PDF_EXTRACT else 'DISABLED'}")
        log.info(f"  Ontology:    {'ENABLED' if RUN_ONTOLOGY else 'DISABLED'}")
        log.info(f"  Build Graph: {'ENABLED' if RUN_BUILD_GRAPH else 'DISABLED'}")
        log.info(f"{'='*60}\n")
        
        for i in range(0, total_files, batch_size):
            batch = agenda_files[i:i + batch_size]
            batch_num = i // batch_size + 1
            total_batches = (total_files + batch_size - 1) // batch_size
            
            log.info(f"\n{'='*60}")
            log.info(f"Processing batch {batch_num}/{total_batches} ({len(batch)} files)")
            log.info(f"Overall progress: {i}/{total_files} files completed ({i/total_files*100:.1f}%)")
            log.info(f"{'='*60}")
            
            # Process each file in the batch
            for agenda_file in batch:
                try:
                    result = await self.process_agenda(agenda_file)
                    log.info(f"Completed {agenda_file.name} - Stages run: {result.get('stages_run', [])}")
                except Exception as e:
                    log.error(f"Failed to process {agenda_file.name}: {e}")
                    import traceback
                    traceback.print_exc()
            
            # Log batch statistics
            self._log_batch_stats(batch_num)
    
    def _update_stats(self, graph_data: Dict):
        """Update pipeline statistics."""
        stats = graph_data.get('statistics', {})
        self.stats['meetings'] += 1
        self.stats['sections'] += stats.get('sections', 0)
        self.stats['items'] += stats.get('items', 0)
        self.stats['relationships'] += stats.get('relationships', 0)
        
        # Update entity counts
        for entity_type, count in stats.get('entities', {}).items():
            self.stats[f'entity_{entity_type}'] = self.stats.get(f'entity_{entity_type}', 0) + count
    
    def _log_batch_stats(self, batch_num: int):
        """Log statistics for the current batch."""
        log.info(f"\nBatch {batch_num} Statistics:")
        log.info(f"  - Meetings processed: {self.stats['meetings']}")
        log.info(f"  - Total sections: {self.stats['sections']}")
        log.info(f"  - Total items: {self.stats['items']}")
        log.info(f"  - Total relationships: {self.stats['relationships']}")
        
        # Log entity counts
        entity_stats = {k.replace('entity_', ''): v for k, v in self.stats.items() if k.startswith('entity_')}
        if entity_stats:
            log.info(f"  - Entities: {entity_stats}")
    
    def get_final_stats(self) -> Dict:
        """Get final pipeline statistics."""
        return dict(self.stats)


async def find_agenda_files() -> List[Path]:
    """Find all agenda PDF files in the project."""
    # Start from the project root (parent of scripts directory)
    project_root = Path(__file__).parent.parent
    
    log.info(f"Searching for agenda files from: {project_root}")
    
    # List of possible locations
    possible_paths = [
        project_root / "city_clerk_documents" / "global" / "City Commissions 2024" / "Agendas",
        project_root / "city_clerk_documents" / "Agendas",
        project_root / "Agendas",
    ]
    
    # Also search recursively
    log.info("Searching recursively for Agendas directories...")
    agenda_dirs = list(project_root.rglob("**/Agendas"))
    possible_paths.extend(agenda_dirs)
    
    agenda_files = []
    searched_paths = []
    
    for path in possible_paths:
        searched_paths.append(str(path))
        if path.exists() and path.is_dir():
            # Look for files matching "Agenda *.pdf" pattern
            found_files = sorted(path.glob("Agenda *.pdf"))
            if found_files:
                log.info(f"✅ Found {len(found_files)} agenda files in: {path}")
                agenda_files = found_files
                break
            else:
                # Also try without space
                found_files = sorted(path.glob("Agenda*.pdf"))
                if found_files:
                    log.info(f"✅ Found {len(found_files)} agenda files in: {path}")
                    agenda_files = found_files
                    break
    
    if not agenda_files:
        log.error("❌ No agenda files found! Searched in:")
        for path in searched_paths[:10]:  # Show first 10 paths
            log.error(f"   - {path}")
        
        # Try to find any PDF files to help debug
        all_pdfs = list(project_root.rglob("*.pdf"))
        if all_pdfs:
            log.info(f"\nFound {len(all_pdfs)} total PDF files. Showing some examples:")
            # Show PDFs that might be agendas
            agenda_like = [p for p in all_pdfs if 'agenda' in p.name.lower()]
            if agenda_like:
                log.info(f"Found {len(agenda_like)} PDFs with 'agenda' in name:")
                for pdf in agenda_like[:5]:
                    log.info(f"   - {pdf.relative_to(project_root)}")
            else:
                log.info("First few PDFs found:")
                for pdf in all_pdfs[:5]:
                    log.info(f"   - {pdf.relative_to(project_root)}")
    
    return agenda_files


async def test_cosmos_connection(cosmos_client: CosmosGraphClient):
    """Test basic Cosmos DB operations."""
    log.info("🧪 Testing Cosmos DB connection...")
    
    try:
        # Test 1: Count vertices
        count_query = "g.V().count()"
        result = await cosmos_client._execute_query(count_query)
        log.info(f"✅ Vertex count: {result[0] if result else 0}")
        
        # Test 2: Create a test node
        test_id = "test-node-12345"
        create_query = f"g.addV('TestNode').property('id','{test_id}').property('partitionKey','demo')"
        result = await cosmos_client._execute_query(create_query)
        log.info(f"✅ Created test node")
        
        # Test 3: Query the test node
        query = f"g.V('{test_id}')"
        result = await cosmos_client._execute_query(query)
        log.info(f"✅ Found test node: {len(result)} nodes")
        
        # Clean up
        await cosmos_client._execute_query(f"g.V('{test_id}').drop()")
        
        log.info("✅ All tests passed!")
        
    except Exception as e:
        log.error(f"❌ Test failed: {e}")
        import traceback
        traceback.print_exc()


async def main():
    """Main entry point for the agenda pipeline."""
    # Initialize Cosmos DB client
    cosmos_client = CosmosGraphClient(
        endpoint=os.getenv("COSMOS_ENDPOINT"),
        username=f"/dbs/{os.getenv('COSMOS_DATABASE')}/colls/{os.getenv('COSMOS_CONTAINER')}",
        password=os.getenv("COSMOS_KEY"),
        partition_key="partitionKey",
        partition_value="demo"
    )
    
    await cosmos_client.connect()
    
    try:
        # Run connection test if enabled
        if RUN_CONN_TEST:
            await test_cosmos_connection(cosmos_client)
        else:
            log.info("Connection test SKIPPED (RUN_CONN_TEST=False)")
        
        # Clear existing data if enabled
        if RUN_CLEAR_GRAPH:
            log.info("Clearing existing graph data...")
            await cosmos_client.clear_graph()
        else:
            log.info("Clear graph SKIPPED (RUN_CLEAR_GRAPH=False)")
            
        # Interactive mode check
        if INTERACTIVE and not RUN_CLEAR_GRAPH:
            clear_existing = input("\nClear existing graph data? (y/N): ").lower() == 'y'
            if clear_existing:
                log.info("Clearing existing graph data...")
                await cosmos_client.clear_graph()
        
        # Find agenda files
        agenda_files = await find_agenda_files()
        if not agenda_files:
            return
        
        log.info(f"Found {len(agenda_files)} agenda files to process")
        
        # Determine how many files to process
        num_to_process = 3  # Default
        if INTERACTIVE:
            user_input = input(f"\nHow many files to process? (1-{len(agenda_files)}, default=3): ")
            try:
                num_to_process = int(user_input)
            except:
                pass
        
        num_to_process = min(max(1, num_to_process), len(agenda_files))
        log.info(f"Will process {num_to_process} files")
        
        # Create pipeline orchestrator
        pipeline = AgendaPipelineOrchestrator(cosmos_client)
        
        # Process files in batches
        await pipeline.process_batch(
            agenda_files[:num_to_process],
            batch_size=1  # Process one at a time for better error tracking
        )
        
        # Log final statistics
        final_stats = pipeline.get_final_stats()
        log.info(f"\n{'='*60}")
        log.info("PIPELINE COMPLETE - FINAL STATISTICS")
        log.info(f"{'='*60}")
        log.info("Stages run configuration:")
        log.info(f"  PDF Extract: {'ENABLED' if RUN_PDF_EXTRACT else 'DISABLED'}")
        log.info(f"  Ontology:    {'ENABLED' if RUN_ONTOLOGY else 'DISABLED'}")
        log.info(f"  Build Graph: {'ENABLED' if RUN_BUILD_GRAPH else 'DISABLED'}")
        log.info("")
        log.info("Results:")
        for key, value in sorted(final_stats.items()):
            log.info(f"  {key}: {value}")
        log.info(f"{'='*60}")
        
    finally:
        await cosmos_client.close()


if __name__ == "__main__":
    # Display current configuration at startup
    print("\n" + "="*60)
    print("AGENDA GRAPH PIPELINE - CONFIGURATION")
    print("="*60)
    print(f"PDF Extract:    {'ENABLED' if RUN_PDF_EXTRACT else 'DISABLED'}")
    print(f"Ontology (LLM): {'ENABLED' if RUN_ONTOLOGY else 'DISABLED'}")
    print(f"Build Graph:    {'ENABLED' if RUN_BUILD_GRAPH else 'DISABLED'}")
    print(f"Clear Graph:    {'ENABLED' if RUN_CLEAR_GRAPH else 'DISABLED'}")
    print(f"Connection Test:{'ENABLED' if RUN_CONN_TEST else 'DISABLED'}")
    print(f"Interactive:    {'ENABLED' if INTERACTIVE else 'DISABLED'}")
    print("="*60 + "\n")
    
    asyncio.run(main())


================================================================================


################################################################################
# File: scripts/graph_stages/agenda_ontology_extractor.py
################################################################################

# File: scripts/graph_stages/agenda_ontology_extractor.py

"""
Agenda Ontology Extractor
========================
Extracts city administration ontology from agenda documents using LLM.
"""
import json
import logging
import re
from typing import Dict, List, Any

log = logging.getLogger(__name__)


class CityClerkOntologyExtractor:
    """Extract city administration ontology from agenda documents."""
    
    STANDARD_SECTIONS = [
        "Call to Order",
        "Invocation", 
        "Pledge of Allegiance",
        "Presentations and Protocol Documents",
        "Approval of Minutes",
        "Public Comments",
        "Consent Agenda",
        "Public Hearings",
        "Ordinances on Second Reading",
        "Resolutions",
        "City Commission Items",
        "Board/Committee Items",
        "City Manager Items",
        "City Attorney Items",
        "City Clerk Items",
        "Discussion Items",
        "Adjournment"
    ]
    
    def __init__(self, llm_client):
        self.llm = llm_client
        
    async def extract_agenda_ontology(self, agenda_data: Dict, filename: str) -> Dict:
        """Extract complete ontology from agenda document."""
        
        # Extract meeting date from filename
        meeting_date = self._extract_meeting_date(filename)
        
        # Prepare document text
        full_text = self._prepare_document_text(agenda_data)
        
        ontology = {
            'meeting_date': meeting_date,
            'filename': filename,
            'entities': {},
            'relationships': []
        }
        
        # 1. Extract meeting information
        meeting_info = await self._extract_meeting_info(full_text[:4000])
        ontology['meeting_info'] = meeting_info
        
        # 2. Extract hierarchical agenda structure with ALL items
        agenda_structure = await self._extract_complete_agenda_structure(full_text)
        ontology['agenda_structure'] = agenda_structure
        
        # 3. Extract all entities (people, organizations, locations, etc.)
        entities = await self._extract_entities(full_text)
        ontology['entities'] = entities
        
        # 4. Extract item codes and their metadata
        item_codes = await self._extract_item_codes_and_metadata(full_text)
        ontology['item_codes'] = item_codes
        
        # 5. Extract cross-references and relationships
        relationships = await self._extract_relationships(agenda_structure, item_codes)
        ontology['relationships'] = relationships
        
        return ontology
    
    def _extract_meeting_date(self, filename: str) -> str:
        """Extract date from filename 'Agenda M.DD.YYYY.pdf'"""
        date_match = re.search(r'Agenda\s+(\d{1,2})\.(\d{2})\.(\d{4})', filename)
        if date_match:
            month, day, year = date_match.groups()
            return f"{int(month):02d}.{day}.{year}"
        return "unknown"
    
    def _prepare_document_text(self, agenda_data: Dict) -> str:
        """Prepare clean document text."""
        sections_text = []
        for section in agenda_data.get('sections', []):
            text = section.get('text', '').strip()
            if text and not text.startswith('self_ref='):
                sections_text.append(text)
        return "\n\n".join(sections_text)
    
    def _extract_json_from_response(self, response_content: str) -> Any:
        """Extract JSON from LLM response, handling various formats."""
        if not response_content:
            log.error("Empty response from LLM")
            return None
            
        # Try direct JSON parsing first
        try:
            return json.loads(response_content)
        except json.JSONDecodeError:
            pass
        
        # Try to find JSON in markdown code blocks
        json_match = re.search(r'```(?:json)?\s*(\{.*?\}|\[.*?\])\s*```', response_content, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group(1))
            except json.JSONDecodeError:
                pass
        
        # Try to find raw JSON
        json_match = re.search(r'(\{.*\}|\[.*\])', response_content, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group(1))
            except json.JSONDecodeError:
                pass
        
        log.error(f"Could not parse JSON from response: {response_content[:200]}...")
        return None
    
    async def _extract_meeting_info(self, text: str) -> Dict:
        """Extract detailed meeting information."""
        prompt = f"""Analyze this city commission meeting agenda and extract meeting details.

Text:
{text}

Return a JSON object with these fields:
{{
    "meeting_type": "Regular Meeting or Special Meeting or Workshop",
    "meeting_time": "time if mentioned",
    "location": {{
        "name": "venue name",
        "address": "full address"
    }},
    "officials_present": {{
        "mayor": "name or null",
        "vice_mayor": "name or null",
        "commissioners": ["names"] or [],
        "city_attorney": "name or null",
        "city_manager": "name or null",
        "city_clerk": "name or null",
        "other_officials": []
    }},
    "key_topics": ["main topics"],
    "special_presentations": []
}}

Return ONLY the JSON object, no additional text."""

        try:
            response = self.llm.chat.completions.create(
                model=self.llm.deployment_name if hasattr(self.llm, 'deployment_name') else "gpt-4o",
                temperature=0.0,
                messages=[
                    {"role": "system", "content": "You are a municipal document analyzer. Return only valid JSON."},
                    {"role": "user", "content": prompt}
                ]
            )
            
            content = response.choices[0].message.content
            result = self._extract_json_from_response(content)
            return result or {"meeting_type": "unknown"}
            
        except Exception as e:
            log.error(f"Failed to extract meeting info: {e}")
            return {"meeting_type": "unknown"}
    
    async def _extract_complete_agenda_structure(self, text: str) -> List[Dict]:
        """Extract the complete hierarchical structure of the agenda."""
        # Process in smaller chunks to avoid token limits
        chunks = self._chunk_text(text, 8000)
        all_sections = []
        
        for i, chunk in enumerate(chunks):
            prompt = f"""Extract the agenda structure from this text.

Text:
{chunk}

Find ALL sections and items. Look for patterns like:
- Section headers (e.g., "CONSENT AGENDA", "PUBLIC HEARINGS")
- Item codes (E-1, F-12, G-2, etc.)
- Item titles and descriptions

Return a JSON array like this:
[
    {{
        "section_name": "Consent Agenda",
        "section_type": "CONSENT",
        "order": 1,
        "items": [
            {{
                "item_code": "E-1",
                "title": "Resolution approving...",
                "item_type": "Resolution",
                "document_reference": "2024-66",
                "sponsor": "Commissioner Name",
                "department": "Department Name",
                "summary": "Brief summary"
            }}
        ]
    }}
]

Return ONLY the JSON array."""

            try:
                response = self.llm.chat.completions.create(
                    model=self.llm.deployment_name if hasattr(self.llm, 'deployment_name') else "gpt-4o",
                    temperature=0.0,
                    max_tokens=4000,
                    messages=[
                        {"role": "system", "content": "Extract agenda structure. Return only valid JSON."},
                        {"role": "user", "content": prompt}
                    ]
                )
                
                content = response.choices[0].message.content
                sections = self._extract_json_from_response(content)
                if sections and isinstance(sections, list):
                    all_sections.extend(sections)
                    log.info(f"Extracted {len(sections)} sections from chunk {i+1}")
                    
            except Exception as e:
                log.error(f"Failed to parse chunk {i+1}: {e}")
        
        return self._merge_and_clean_sections(all_sections)
    
    async def _extract_entities(self, text: str) -> Dict[str, List[Dict]]:
        """Extract all named entities from the agenda."""
        chunks = self._chunk_text(text, 6000)
        all_entities = {
            'people': [],
            'organizations': [],
            'locations': [],
            'monetary_amounts': [],
            'dates': [],
            'legal_references': []
        }
        
        for chunk in chunks[:3]:  # Limit to first 3 chunks
            prompt = f"""Extract entities from this agenda text.

Text:
{chunk[:4000]}

Return JSON with:
{{
    "people": [{{"name": "John Smith", "role": "Mayor", "context": "presiding"}}],
    "organizations": [{{"name": "City Commission", "type": "government"}}],
    "locations": [{{"name": "City Hall", "address": "405 Biltmore Way"}}],
    "monetary_amounts": [{{"amount": "$100,000", "purpose": "budget"}}],
    "dates": [{{"date": "01/09/2024", "event": "meeting date"}}],
    "legal_references": [{{"type": "Resolution", "number": "2024-01"}}]
}}

Return ONLY the JSON object."""

            try:
                response = self.llm.chat.completions.create(
                    model=self.llm.deployment_name if hasattr(self.llm, 'deployment_name') else "gpt-4o",
                    temperature=0.0,
                    messages=[
                        {"role": "system", "content": "Extract entities. Return only valid JSON."},
                        {"role": "user", "content": prompt}
                    ]
                )
                
                content = response.choices[0].message.content
                entities = self._extract_json_from_response(content)
                if entities and isinstance(entities, dict):
                    # Merge with existing entities
                    for category, items in entities.items():
                        if category in all_entities and isinstance(items, list):
                            all_entities[category].extend(items)
                            
            except Exception as e:
                log.error(f"Failed to extract entities: {e}")
        
        # Deduplicate entities
        for category in all_entities:
            all_entities[category] = self._deduplicate_entities(all_entities[category])
        
        return all_entities
    
    async def _extract_item_codes_and_metadata(self, text: str) -> Dict[str, Dict]:
        """Extract all item codes and their associated metadata."""
        # Use regex to find item codes first
        item_codes = {}
        
        # Common patterns for item codes
        patterns = [
            r'\b([A-Z])-(\d+)\b',  # E-1, F-12
            r'\b([A-Z])(\d+)\b',   # E1, F12
            r'\b(\d+)-(\d+)\b',    # 2-1, 2-2
        ]
        
        for pattern in patterns:
            matches = re.finditer(pattern, text)
            for match in matches:
                if pattern == r'\b(\d+)-(\d+)\b':
                    code = match.group(0)
                else:
                    code = f"{match.group(1)}-{match.group(2)}"
                
                if code not in item_codes:
                    # Extract context around the code
                    start = max(0, match.start() - 200)
                    end = min(len(text), match.end() + 500)
                    context = text[start:end]
                    
                    # Extract title from context
                    title_match = re.search(rf'{re.escape(code)}[:\s]+([^\n]+)', context)
                    title = title_match.group(1).strip() if title_match else "Unknown"
                    
                    item_codes[code] = {
                        "full_title": title,
                        "type": self._determine_item_type(context),
                        "context": context
                    }
        
        log.info(f"Found {len(item_codes)} item codes via regex")
        return item_codes
    
    def _determine_item_type(self, context: str) -> str:
        """Determine item type from context."""
        context_lower = context.lower()
        if 'resolution' in context_lower:
            return 'Resolution'
        elif 'ordinance' in context_lower:
            return 'Ordinance'
        elif 'contract' in context_lower:
            return 'Contract'
        elif 'proclamation' in context_lower:
            return 'Proclamation'
        elif 'report' in context_lower:
            return 'Report'
        else:
            return 'Item'
    
    async def _extract_relationships(self, agenda_structure: List[Dict], item_codes: Dict) -> List[Dict]:
        """Extract relationships between items."""
        relationships = []
        
        # Create sequential relationships
        all_items = []
        for section in agenda_structure:
            for item in section.get('items', []):
                all_items.append({
                    'code': item['item_code'],
                    'section': section['section_name']
                })
        
        # Sequential relationships within sections
        for i in range(len(all_items) - 1):
            if all_items[i]['section'] == all_items[i+1]['section']:
                relationships.append({
                    'from_code': all_items[i]['code'],
                    'to_code': all_items[i+1]['code'],
                    'relationship_type': 'FOLLOWS',
                    'description': 'Sequential items in same section',
                    'strength': 'strong'
                })
        
        return relationships
    
    def _chunk_text(self, text: str, chunk_size: int) -> List[str]:
        """Split text into overlapping chunks."""
        chunks = []
        overlap = 200
        
        for i in range(0, len(text), chunk_size - overlap):
            chunk = text[i:i + chunk_size]
            chunks.append(chunk)
        
        return chunks
    
    def _merge_and_clean_sections(self, sections: List[Dict]) -> List[Dict]:
        """Merge duplicate sections and ensure all items have codes."""
        merged = {}
        
        for section in sections:
            key = section.get('section_name', 'Unknown')
            
            if key not in merged:
                merged[key] = section
                merged[key]['items'] = section.get('items', [])
            else:
                # Merge items, avoiding duplicates
                existing_codes = {item.get('item_code', '') for item in merged[key].get('items', [])}
                
                for item in section.get('items', []):
                    if item.get('item_code') and item['item_code'] not in existing_codes:
                        merged[key]['items'].append(item)
        
        # Sort by order
        result = list(merged.values())
        result.sort(key=lambda x: x.get('order', 999))
        
        return result
    
    def _deduplicate_entities(self, entities: List[Dict]) -> List[Dict]:
        """Remove duplicate entities."""
        seen = set()
        unique = []
        
        for entity in entities:
            # Create a key based on the entity's main identifier
            if 'name' in entity:
                key = entity['name'].lower().strip()
            elif 'amount' in entity:
                key = entity['amount']
            elif 'date' in entity:
                key = entity['date']
            else:
                continue
            
            if key not in seen:
                seen.add(key)
                unique.append(entity)
        
        return unique


================================================================================


################################################################################
# File: scripts/clear_database.py
################################################################################

# File: scripts/clear_database.py

#!/usr/bin/env python3
"""
Database Clear Utility
======================

Safely clears Supabase database tables for the Misophonia Research system.
This script will delete all data from:
- research_documents table
- documents_chunks table

⚠️  WARNING: This operation is irreversible!
"""
from __future__ import annotations
import os
import sys
import logging
from typing import Optional
from gremlin_python.driver import client, serializer
import asyncio
import argparse

from dotenv import load_dotenv
from supabase import create_client

# Load environment variables
load_dotenv()

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")

# Cosmos DB configuration
COSMOS_ENDPOINT = os.getenv("COSMOS_ENDPOINT", "wss://aida-graph-db.gremlin.cosmos.azure.com:443")
COSMOS_KEY = os.getenv("COSMOS_KEY")
DATABASE = os.getenv("COSMOS_DATABASE", "cgGraph")
CONTAINER = os.getenv("COSMOS_CONTAINER", "cityClerk")

if not SUPABASE_URL or not SUPABASE_KEY:
    sys.exit("❌ Missing SUPABASE_URL or SUPABASE_SERVICE_ROLE_KEY environment variables")

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s — %(levelname)s — %(message)s"
)
log = logging.getLogger(__name__)

def init_supabase():
    """Initialize Supabase client."""
    return create_client(SUPABASE_URL, SUPABASE_KEY)

def get_table_counts(sb) -> dict:
    """Get current row counts for all tables."""
    counts = {}
    
    try:
        # Count documents
        doc_res = sb.table("research_documents").select("id", count="exact").execute()
        counts["research_documents"] = doc_res.count or 0
        
        # Count chunks
        chunk_res = sb.table("documents_chunks").select("id", count="exact").execute()
        counts["documents_chunks"] = chunk_res.count or 0
        
        # Count chunks with embeddings
        embedded_res = sb.table("documents_chunks").select("id", count="exact").not_.is_("embedding", "null").execute()
        counts["chunks_with_embeddings"] = embedded_res.count or 0
        
    except Exception as e:
        log.error(f"Error getting table counts: {e}")
        return {}
    
    return counts

def confirm_deletion() -> bool:
    """Ask user for confirmation before deletion."""
    print("\n" + "="*60)
    print("⚠️  DATABASE CLEAR WARNING")
    print("="*60)
    print("This will permanently delete ALL data from:")
    print("  • research_documents table")
    print("  • documents_chunks table")
    print("  • All embeddings and metadata")
    print("\n❌ This operation CANNOT be undone!")
    print("="*60)
    
    response = input("\nType 'DELETE ALL DATA' to confirm (or anything else to cancel): ")
    return response.strip() == "DELETE ALL DATA"

def clear_table_batch(sb, table_name: str, batch_size: int = 1000) -> int:
    """Clear all rows from a specific table in batches to avoid timeouts."""
    log.info(f"Clearing table: {table_name} (batch size: {batch_size})")
    
    total_deleted = 0
    
    while True:
        try:
            # Get a batch of IDs to delete
            result = sb.table(table_name).select("id").limit(batch_size).execute()
            
            if not result.data or len(result.data) == 0:
                break
            
            ids_to_delete = [row["id"] for row in result.data]
            log.info(f"Deleting batch of {len(ids_to_delete)} rows from {table_name}")
            
            # Delete this batch
            delete_result = sb.table(table_name).delete().in_("id", ids_to_delete).execute()
            
            if hasattr(delete_result, 'error') and delete_result.error:
                log.error(f"Error deleting batch from {table_name}: {delete_result.error}")
                break
            
            batch_deleted = len(delete_result.data) if delete_result.data else 0
            total_deleted += batch_deleted
            log.info(f"✅ Deleted {batch_deleted} rows from {table_name} (total: {total_deleted})")
            
            # If we deleted fewer than the batch size, we're done
            if batch_deleted < batch_size:
                break
                
        except Exception as e:
            log.error(f"Exception deleting batch from {table_name}: {e}")
            break
    
    log.info(f"✅ Total deleted from {table_name}: {total_deleted}")
    return total_deleted

def clear_table(sb, table_name: str) -> int:
    """Clear all rows from a specific table."""
    return clear_table_batch(sb, table_name, batch_size=500)

def init_cosmos():
    """Initialize Cosmos DB Gremlin client."""
    if not COSMOS_KEY:
        log.warning("Cosmos DB credentials not found - skipping Cosmos operations")
        return None
    
    try:
        gremlin_client = client.Client(
            f"{COSMOS_ENDPOINT}/gremlin",
            "g",
            username=f"/dbs/{DATABASE}/colls/{CONTAINER}",
            password=COSMOS_KEY,
            message_serializer=serializer.GraphSONSerializersV2d0()
        )
        return gremlin_client
    except Exception as e:
        log.error(f"Failed to connect to Cosmos DB: {e}")
        return None

def get_cosmos_counts(gremlin_client) -> dict:
    """Get current counts for Cosmos DB graph."""
    if not gremlin_client:
        return {}
    
    counts = {}
    try:
        # Count all vertices
        result = gremlin_client.submit("g.V().count()").all()
        counts["total_vertices"] = result[0] if result else 0
        
        # Count by label
        for label in ["Document", "Person", "Meeting", "Chunk"]:
            result = gremlin_client.submit(f"g.V().hasLabel('{label}').count()").all()
            counts[f"{label.lower()}_nodes"] = result[0] if result else 0
        
        # Count edges
        result = gremlin_client.submit("g.E().count()").all()
        counts["total_edges"] = result[0] if result else 0
        
    except Exception as e:
        log.error(f"Error getting Cosmos DB counts: {e}")
        return {}
    
    return counts

def clear_cosmos_graph(gremlin_client) -> tuple[int, int]:
    """Clear all nodes and edges from Cosmos DB graph."""
    if not gremlin_client:
        return 0, 0
    
    log.info("Clearing Cosmos DB graph...")
    
    try:
        # Get initial counts
        edge_result = gremlin_client.submit("g.E().count()").all()
        edge_count = edge_result[0] if edge_result else 0
        
        vertex_result = gremlin_client.submit("g.V().count()").all()
        vertex_count = vertex_result[0] if vertex_result else 0
        
        # Drop all edges first (required before dropping vertices)
        log.info(f"Dropping {edge_count} edges...")
        gremlin_client.submit("g.E().drop()").all()
        
        # Then drop all vertices
        log.info(f"Dropping {vertex_count} vertices...")
        gremlin_client.submit("g.V().drop()").all()
        
        log.info("✅ Cosmos DB graph cleared")
        return vertex_count, edge_count
        
    except Exception as e:
        log.error(f"Error clearing Cosmos DB graph: {e}")
        return 0, 0

async def main():
    """Main function to clear databases."""
    parser = argparse.ArgumentParser(description="Clear City Clerk databases")
    parser.add_argument("--supabase", action="store_true", help="Clear Supabase tables")
    parser.add_argument("--cosmos", action="store_true", help="Clear Cosmos DB graph")
    parser.add_argument("--all", action="store_true", help="Clear all databases")
    
    args = parser.parse_args()
    
    # If no specific database selected, default to all
    if not args.supabase and not args.cosmos and not args.all:
        args.all = True
    
    print("🗑️  City Clerk Database Clear Utility")
    print("=" * 50)
    
    # Initialize connections
    sb = None
    gremlin_client = None
    
    if args.supabase or args.all:
        sb = init_supabase()
    
    if args.cosmos or args.all:
        gremlin_client = init_cosmos()
    
    # Get current counts
    print("\n📊 Current database status:")
    
    supabase_counts = {}
    cosmos_counts = {}
    
    if sb:
        supabase_counts = get_table_counts(sb)
        if supabase_counts:
            print("  Supabase:")
            print(f"    • Documents: {supabase_counts['research_documents']:,}")
            print(f"    • Chunks: {supabase_counts['documents_chunks']:,}")
            print(f"    • Chunks with embeddings: {supabase_counts['chunks_with_embeddings']:,}")
    
    if gremlin_client:
        cosmos_counts = get_cosmos_counts(gremlin_client)
        if cosmos_counts:
            print("  Cosmos DB Graph:")
            print(f"    • Total vertices: {cosmos_counts['total_vertices']:,}")
            print(f"    • Total edges: {cosmos_counts['total_edges']:,}")
            print(f"    • Documents: {cosmos_counts.get('document_nodes', 0):,}")
            print(f"    • Persons: {cosmos_counts.get('person_nodes', 0):,}")
            print(f"    • Meetings: {cosmos_counts.get('meeting_nodes', 0):,}")
    
    # Check if any data exists
    has_data = False
    if sb and supabase_counts:
        has_data = has_data or (supabase_counts['research_documents'] > 0 or supabase_counts['documents_chunks'] > 0)
    if gremlin_client and cosmos_counts:
        has_data = has_data or (cosmos_counts['total_vertices'] > 0)
    
    if not has_data:
        print("\n✅ Databases are already empty!")
        return
    
    # Get confirmation
    if not confirm_deletion():
        print("\n✅ Operation cancelled. Databases unchanged.")
        return
    
    print("\n🗑️  Starting database clear operation...")
    
    # Clear Supabase if requested
    if sb and (args.supabase or args.all):
        print("\n📋 Clearing Supabase tables...")
        chunks_deleted = clear_table(sb, "documents_chunks")
        docs_deleted = clear_table(sb, "research_documents")
        print(f"✅ Supabase cleared: {docs_deleted:,} documents, {chunks_deleted:,} chunks")
    
    # Clear Cosmos DB if requested
    if gremlin_client and (args.cosmos or args.all):
        print("\n🌐 Clearing Cosmos DB graph...")
        vertices_deleted, edges_deleted = clear_cosmos_graph(gremlin_client)
        print(f"✅ Cosmos DB cleared: {vertices_deleted:,} vertices, {edges_deleted:,} edges")
    
    print("\n✅ Database clear operation completed!")

if __name__ == "__main__":
    asyncio.run(main())


================================================================================


################################################################################
# File: scripts/stages/db_upsert.py
################################################################################

# File: scripts/stages/db_upsert.py

#!/usr/bin/env python3
"""
Stage 6 — Optimized database operations with batching and connection pooling.
"""
from __future__ import annotations
import json, logging, os, pathlib, sys
from datetime import datetime
from typing import Any, Dict, List, Sequence, Optional
import asyncio
from concurrent.futures import ThreadPoolExecutor
import threading

from dotenv import load_dotenv
from supabase import create_client
from tqdm import tqdm

load_dotenv()
SUPABASE_URL  = os.getenv("SUPABASE_URL")
SUPABASE_KEY  = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
OPENAI_API_KEY= os.getenv("OPENAI_API_KEY")

META_FIELDS = [
    "document_type", "title", "date", "year", "month", "day",
    "mayor", "vice_mayor", "commissioners",
    "city_attorney", "city_manager", "city_clerk", "public_works_director",
    "agenda", "keywords"
]

log = logging.getLogger(__name__)

# Thread-safe connection pool
class SupabasePool:
    """Thread-safe Supabase client pool."""
    def __init__(self, size: int = 10):
        self.size = size
        self._clients = []
        self._lock = threading.Lock()
        self._initialized = False
    
    def get(self):
        """Get a client from the pool."""
        with self._lock:
            if not self._initialized:
                self._initialize()
            # Round-robin selection
            import random
            return self._clients[random.randint(0, self.size - 1)]
    
    def _initialize(self):
        """Initialize client pool."""
        for _ in range(self.size):
            self._clients.append(init_supabase())
        self._initialized = True

# Global pool
_sb_pool = SupabasePool()

# ─── Supabase & sanitiser helpers ──────────────────────────────────
def scrub_nuls(obj:Any)->Any:
    if isinstance(obj,str):  return obj.replace("\x00","")
    if isinstance(obj,list): return [scrub_nuls(x) for x in obj]
    if isinstance(obj,dict): return {k:scrub_nuls(v) for k,v in obj.items()}
    return obj

def init_supabase():
    if not (SUPABASE_URL and SUPABASE_KEY):
        sys.exit("⛔  SUPABASE env vars missing")
    return create_client(SUPABASE_URL,SUPABASE_KEY)

def upsert_document(sb,meta:Dict[str,Any])->str:
    meta = scrub_nuls(meta)
    doc_type = meta.get("document_type")
    date = meta.get("date")
    title = meta.get("title")
    if doc_type and date and title:
        existing = (
            sb.table("city_clerk_documents")
            .select("id")
            .eq("document_type", doc_type)
            .eq("date", date)
            .eq("title", title)
            .limit(1)
            .execute()
            .data
        )
        if existing:
            doc_id = existing[0]["id"]
            sb.table("city_clerk_documents").update(meta).eq("id", doc_id).execute()
            return doc_id
    res = sb.table("city_clerk_documents").insert(meta).execute()
    if hasattr(res, "error") and res.error:
        raise RuntimeError(f"Document insert failed: {res.error}")
    return res.data[0]["id"]

# Optimized batch operations
def insert_chunks_optimized(sb, doc_id: str, chunks: Sequence[Dict[str, Any]], 
                          src_json: pathlib.Path, batch_size: int = 1000):
    """Insert chunks with larger batches for better performance."""
    ts = datetime.utcnow().isoformat()
    inserted_ids = []
    
    # Prepare all rows
    rows = [
        {
            "document_id": doc_id,
            "chunk_index": ch["chunk_index"],
            "token_start": ch["token_start"],
            "token_end": ch["token_end"],
            "page_start": ch["page_start"],
            "page_end": ch["page_end"],
            "text": scrub_nuls(ch["text"]),
            "metadata": scrub_nuls(ch.get("metadata", {})),
            "chunking_strategy": "token_window",
            "source_file": str(src_json),
            "created_at": ts,
        }
        for ch in chunks
    ]
    
    # Insert in larger batches
    for i in range(0, len(rows), batch_size):
        batch = rows[i:i + batch_size]
        try:
            res = sb.table("documents_chunks").insert(batch).execute()
            if hasattr(res, "error") and res.error:
                log.error("Batch insert failed: %s", res.error)
                # Fall back to smaller batches
                for j in range(0, len(batch), 100):
                    mini_batch = batch[j:j + 100]
                    mini_res = sb.table("documents_chunks").insert(mini_batch).execute()
                    if hasattr(mini_res, "data"):
                        inserted_ids.extend([r["id"] for r in mini_res.data])
            else:
                inserted_ids.extend([r["id"] for r in res.data])
        except Exception as e:
            log.error(f"Failed to insert batch {i//batch_size + 1}: {e}")
            # Try individual inserts as last resort
            for row in batch:
                try:
                    single_res = sb.table("documents_chunks").insert(row).execute()
                    if hasattr(single_res, "data") and single_res.data:
                        inserted_ids.append(single_res.data[0]["id"])
                except:
                    pass
    
    return inserted_ids

def insert_chunks(sb,doc_id:str,chunks:Sequence[Dict[str,Any]],src_json:pathlib.Path):
    """Original interface using optimized implementation."""
    return insert_chunks_optimized(sb, doc_id, chunks, src_json, batch_size=500)

async def upsert_batch_async(
    documents: List[Dict[str, Any]],
    max_concurrent: int = 5
) -> None:
    """Upsert multiple documents concurrently."""
    semaphore = asyncio.Semaphore(max_concurrent)
    
    async def upsert_one(doc_data):
        async with semaphore:
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(
                None,
                lambda: upsert(
                    doc_data["json_path"],
                    doc_data["chunks"],
                    do_embed=doc_data.get("do_embed", False)
                )
            )
    
    tasks = [upsert_one(doc) for doc in documents]
    
    from tqdm.asyncio import tqdm_asyncio
    await tqdm_asyncio.gather(*tasks, desc="Upserting to database")

# Keep original interface but use optimized implementation
def upsert(json_doc: pathlib.Path, chunks: List[Dict[str, Any]] | None,
           *, do_embed: bool = False) -> None:
    """Original interface with optimized implementation."""
    sb = _sb_pool.get()  # Use connection from pool
    
    data = json.loads(json_doc.read_text())
    row = {k: data.get(k) for k in META_FIELDS} | {
        "source_pdf": data.get("source_pdf", str(json_doc))
    }
    
    # Ensure commissioners is a list
    if "commissioners" in row:
        if isinstance(row["commissioners"], str):
            row["commissioners"] = [row["commissioners"]]
        elif not isinstance(row["commissioners"], list):
            row["commissioners"] = []
    else:
        row["commissioners"] = []
    
    # Ensure keywords is a list
    if "keywords" in row:
        if not isinstance(row["keywords"], list):
            row["keywords"] = []
    else:
        row["keywords"] = []
    
    # Convert agenda from array to text if needed
    if "agenda" in row:
        if isinstance(row["agenda"], list):
            row["agenda"] = "; ".join(str(item) for item in row["agenda"] if item)
        elif row["agenda"] is None:
            row["agenda"] = None
        else:
            row["agenda"] = str(row["agenda"])
    
    # Ensure all text fields are strings or None
    text_fields = ["document_type", "title", "date", "mayor", "vice_mayor", 
                   "city_attorney", "city_manager", "city_clerk", "public_works_director"]
    for field in text_fields:
        if field in row and row[field] is not None:
            row[field] = str(row[field])
    
    # Ensure numeric fields are integers or None
    numeric_fields = ["year", "month", "day"]
    for field in numeric_fields:
        if field in row and row[field] is not None:
            try:
                row[field] = int(row[field])
            except (ValueError, TypeError):
                row[field] = None
    
    doc_id = upsert_document(sb, row)
    
    if not chunks:
        log.info("No chunks to insert for %s", json_doc.stem)
        return
    
    # Use optimized batch insert
    inserted_ids = insert_chunks_optimized(sb, doc_id, chunks, json_doc)
    log.info("↑ %s chunks inserted for %s", len(chunks), json_doc.stem)
    
    if do_embed and inserted_ids:
        from stages import embed_vectors
        embed_vectors.main()

if __name__ == "__main__":
    import argparse
    logging.basicConfig(level=logging.INFO, format="%(message)s")
    p=argparse.ArgumentParser()
    p.add_argument("json",type=pathlib.Path)
    p.add_argument("--chunks",type=pathlib.Path)
    p.add_argument("--embed",action="store_true")
    args=p.parse_args()
    
    chunks = None
    if args.chunks and args.chunks.exists():
        chunks = json.loads(args.chunks.read_text())
    
    upsert(args.json, chunks, do_embed=args.embed)


================================================================================


################################################################################
# File: scripts/graph_stages/entity_deduplicator.py
################################################################################

# File: scripts/graph_stages/entity_deduplicator.py

"""
Entity Deduplication Module
==========================
Handles deduplication of persons and other entities across documents.
"""
import difflib
from typing import Dict, List, Optional, Set, Tuple
import logging

log = logging.getLogger(__name__)

class EntityDeduplicator:
    """Deduplicate entities across documents."""
    
    def __init__(self, similarity_threshold: float = 0.85):
        self.similarity_threshold = similarity_threshold
        self.person_aliases: Dict[str, str] = {}  # alias -> canonical name
        self._load_known_aliases()
    
    def _load_known_aliases(self):
        """Load known name variations for city officials."""
        self.person_aliases.update({
            # Mayors
            "Vince Lago": "Vince Lago",
            "Vincent Lago": "Vince Lago",
            "Mayor Lago": "Vince Lago",
            "Lago": "Vince Lago",  # Handle last name only
            
            # Add more known variations...
        })
    
    def deduplicate_person_name(self, name: str, existing_names: List[str]) -> str:
        """
        Find canonical version of a person name.
        
        Args:
            name: The name to check
            existing_names: List of known canonical names
            
        Returns:
            Canonical name (either existing match or the input name)
        """
        # First check known aliases
        if name in self.person_aliases:
            return self.person_aliases[name]
        
        # Clean the name
        clean_name = self._clean_person_name(name)
        
        # Find best match among existing names
        best_match = self._find_best_match(clean_name, existing_names)
        
        if best_match:
            # Store alias for future use
            self.person_aliases[name] = best_match
            return best_match
        
        # No match found, this is a new canonical name
        return clean_name
    
    def _clean_person_name(self, name: str) -> str:
        """Clean and normalize a person name."""
        # Remove titles
        titles = [
            'Mayor', 'Vice Mayor', 'Commissioner', 'Dr.', 'Mr.', 'Mrs.', 'Ms.',
            'City Attorney', 'City Manager', 'City Clerk'
        ]
        
        clean = name
        for title in titles:
            clean = clean.replace(title, '').strip()
        
        # Remove extra whitespace
        clean = ' '.join(clean.split())
        
        return clean
    
    def _find_best_match(self, name: str, candidates: List[str]) -> Optional[str]:
        """Find best matching name from candidates."""
        if not candidates:
            return None
        
        # Try exact match first
        if name in candidates:
            return name
        
        # Try fuzzy matching
        matches = difflib.get_close_matches(
            name, 
            candidates, 
            n=1, 
            cutoff=self.similarity_threshold
        )
        
        if matches:
            return matches[0]
        
        # Try last name matching for "FirstName LastName" patterns
        name_parts = name.split()
        if len(name_parts) >= 2:
            last_name = name_parts[-1]
            for candidate in candidates:
                if candidate.endswith(last_name):
                    # Additional check: first name initial match
                    if name[0] == candidate[0]:
                        return candidate
        
        return None
    
    def merge_person_roles(self, existing_roles: List[str], new_roles: List[str]) -> List[str]:
        """Merge role lists, maintaining uniqueness and hierarchy."""
        # Role hierarchy (higher number = higher priority)
        role_priority = {
            'Mayor': 10,
            'Vice Mayor': 9,
            'Commissioner': 8,
            'City Attorney': 7,
            'City Manager': 7,
            'City Clerk': 6,
            'Public Works Director': 6,
            'Sponsor': 5,
            'Public Speaker': 4,
        }
        
        all_roles = set(existing_roles + new_roles)
        
        # Sort by priority
        sorted_roles = sorted(
            all_roles,
            key=lambda r: role_priority.get(r, 0),
            reverse=True
        )
        
        return sorted_roles

class MeetingDeduplicator:
    """Deduplicate meeting entities."""
    
    @staticmethod
    def normalize_date(date_str: str) -> str:
        """Normalize date string to consistent format."""
        # Handle various formats:
        # 6.11.2024 -> 06.11.2024
        # 06_11_2024 -> 06.11.2024
        # June 11, 2024 -> 06.11.2024
        
        import re
        
        # Replace underscores with dots
        normalized = date_str.replace('_', '.')
        
        # Handle M.DD.YYYY -> MM.DD.YYYY
        match = re.match(r'^(\d)\.(\d{2})\.(\d{4})$', normalized)
        if match:
            month, day, year = match.groups()
            normalized = f"{int(month):02d}.{day}.{year}"
        
        return normalized


================================================================================


################################################################################
# File: config.py
################################################################################

# File: config.py

#!/usr/bin/env python3
"""
Configuration file for graph database visualization
Manages database credentials and connection settings
"""

import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Azure Cosmos DB Gremlin Configuration
COSMOS_ENDPOINT = os.getenv("COSMOS_ENDPOINT", "wss://aida-graph-db.gremlin.cosmos.azure.com:443")
COSMOS_KEY = os.getenv("COSMOS_KEY", "")  # This will be set from .env file
DATABASE = os.getenv("COSMOS_DATABASE", "cgGraph")
CONTAINER = os.getenv("COSMOS_CONTAINER", "cityClerk")
PARTITION_KEY = os.getenv("COSMOS_PARTITION_KEY", "partitionKey")
PARTITION_VALUE = os.getenv("COSMOS_PARTITION_VALUE", "demo")

def validate_config():
    """Validate that all required configuration is available"""
    required_vars = {
        "COSMOS_KEY": COSMOS_KEY,
        "COSMOS_ENDPOINT": COSMOS_ENDPOINT,
        "DATABASE": DATABASE,
        "CONTAINER": CONTAINER
    }
    
    missing_vars = [var for var, value in required_vars.items() if not value]
    
    if missing_vars:
        print("❌ Missing required configuration:")
        for var in missing_vars:
            print(f"   - {var}")
        print("\n🔧 Please create a .env file with your credentials:")
        print("   COSMOS_KEY=your_actual_cosmos_key_here")
        print("   COSMOS_ENDPOINT=wss://aida-graph-db.gremlin.cosmos.azure.com:443")
        print("   COSMOS_DATABASE=cgGraph")
        print("   COSMOS_CONTAINER=cityClerk")
        return False
    
    return True

if __name__ == "__main__":
    print("🔧 Configuration Check:")
    if validate_config():
        print("✅ All configuration variables are set!")
    else:
        print("❌ Configuration incomplete!")


================================================================================


################################################################################
# File: scripts/stages/__init__.py
################################################################################

# File: scripts/stages/__init__.py

"""Stage helpers live here so `pipeline_integrated` can still be the single-file
reference implementation while every stage can be invoked on its own."""

"""
Namespace package so the stage modules can be imported with
    from stages import <module>
"""
__all__ = [
    "common",
    "extract_clean",
    "llm_enrich",
    "chunk_text",
    "db_upsert",
    "embed_vectors",
]


================================================================================

