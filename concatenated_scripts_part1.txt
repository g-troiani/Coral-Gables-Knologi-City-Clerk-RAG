# Concatenated Project Code - Part 1 of 3
# Generated: 2025-06-03 08:55:23
# Root Directory: /Users/gianmariatroiani/Documents/knologi̊/graph_database
================================================================================

# Directory Structure
################################################################################
├── .gitignore (939.0B, no ext)
├── city_clerk_documents/
│   ├── global copy/
│   │   ├── City Comissions 2024/
│   │   │   ├── Agendas/
│   │   │   │   ├── Agenda 01.23.2024.pdf (155.1KB, .pdf)
│   │   │   │   ├── Agenda 01.9.2024.pdf (151.1KB, .pdf)
│   │   │   │   ├── Agenda 02.13.2024.pdf (155.0KB, .pdf)
│   │   │   │   ├── Agenda 02.27.2024.pdf (152.9KB, .pdf)
│   │   │   │   ├── Agenda 03.12.2024.pdf (166.6KB, .pdf)
│   │   │   │   ├── Agenda 05.07.2024.pdf (167.6KB, .pdf)
│   │   │   │   ├── Agenda 05.21.2024.pdf (172.2KB, .pdf)
│   │   │   │   └── Agenda 06.11.2024.pdf (160.2KB, .pdf)
│   │   │   ├── ExportedFolderContents.zip (62.4MB, .zip)
│   │   │   ├── Ordinances/
│   │   │   │   ├── 2024/
│   │   │   │   │   ├── 2024-01 - 01_09_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-02 - 01_09_2024.pdf (15.1MB, .pdf)
│   │   │   │   │   ├── 2024-03 - 01_09_2024.pdf (11.1MB, .pdf)
│   │   │   │   │   ├── 2024-04 - 01_23_2024.pdf (2.3MB, .pdf)
│   │   │   │   │   ├── 2024-05 - 02_13_2024.pdf (2.0MB, .pdf)
│   │   │   │   │   ├── 2024-06 - 02_13_2024.pdf (1.9MB, .pdf)
│   │   │   │   │   ├── 2024-07 - 02_13_2024.pdf (6.5MB, .pdf)
│   │   │   │   │   ├── 2024-08 - 02_27_2024.pdf (2.5MB, .pdf)
│   │   │   │   │   ├── 2024-09 - 02_27_2024.pdf (1.6MB, .pdf)
│   │   │   │   │   ├── 2024-10 - 03_12_2024.pdf (3.7MB, .pdf)
│   │   │   │   │   ├── 2024-11 - 03_12_2024 - (As Amended).pdf (5.6MB, .pdf)
│   │   │   │   │   ├── 2024-11 - 03_12_2024.pdf (5.5MB, .pdf)
│   │   │   │   │   ├── 2024-12 - 03_12_2024.pdf (1.8MB, .pdf)
│   │   │   │   │   ├── 2024-13 - 03_12_2024.pdf (3.2MB, .pdf)
│   │   │   │   │   ├── 2024-14 - 05_07_2024.pdf (2.8MB, .pdf)
│   │   │   │   │   ├── 2024-15 - 05_07_2024.pdf (3.9MB, .pdf)
│   │   │   │   │   ├── 2024-16 - 05_07_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-17 - 05_07_2024.pdf (4.3MB, .pdf)
│   │   │   │   │   ├── 2024-18 - 05_21_2024.pdf (2.4MB, .pdf)
│   │   │   │   │   ├── 2024-19 - 05_21_2024.pdf (1.7MB, .pdf)
│   │   │   │   │   ├── 2024-20 - 05_21_2024.pdf (1.6MB, .pdf)
│   │   │   │   │   ├── 2024-21 - 06_11_2024.pdf (1.7MB, .pdf)
│   │   │   │   │   ├── 2024-22 - 06_11_2024.pdf (2.0MB, .pdf)
│   │   │   │   │   ├── 2024-23 - 06_11_2024.pdf (1.9MB, .pdf)
│   │   │   │   │   ├── 2024-24 - 06_11_2024.pdf (2.1MB, .pdf)
│   │   │   │   │   └── 2024-25 - 06_11_2024.pdf (1.3MB, .pdf)
│   │   │   │   ├── [EXCLUDED] 1 items: .DS_Store (excluded file)
│   │   │   ├── Resolutions/
│   │   │   │   ├── 2024/
│   │   │   │   │   ├── 2024-01 - 01_09_2024.pdf (448.9KB, .pdf)
│   │   │   │   │   ├── 2024-02 - 01_09_2024.pdf (451.9KB, .pdf)
│   │   │   │   │   ├── 2024-03 - 01_09_2024.pdf (867.3KB, .pdf)
│   │   │   │   │   ├── 2024-04 - 01_09_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-05 - 01_09_2024.pdf (936.9KB, .pdf)
│   │   │   │   │   ├── 2024-06 - 01_09_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-07 - 01_09_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-08 - 01_23_2024.pdf (457.9KB, .pdf)
│   │   │   │   │   ├── 2024-09 - 01_23_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-10 - 01_23_2024.pdf (454.8KB, .pdf)
│   │   │   │   │   ├── 2024-10 - 03_12_2024.pdf (3.7MB, .pdf)
│   │   │   │   │   ├── 2024-100 - 05_21_2024.pdf (3.0MB, .pdf)
│   │   │   │   │   ├── 2024-101 - 05_21_2024.pdf (947.4KB, .pdf)
│   │   │   │   │   ├── 2024-102 - 05_21_2024.pdf (466.0KB, .pdf)
│   │   │   │   │   ├── 2024-103 - 05_21_2024.pdf (991.8KB, .pdf)
│   │   │   │   │   ├── 2024-104 - 05_21_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-105 - 05_21_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-106 - 05_21_2024.pdf (2.4MB, .pdf)
│   │   │   │   │   ├── 2024-107 - 05_21_2024.pdf (6.0MB, .pdf)
│   │   │   │   │   ├── 2024-108 - 05_21_2024.pdf (2.1MB, .pdf)
│   │   │   │   │   ├── 2024-109 - 05_21_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-11 - 01_23_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-110 - 05_21_2024.pdf (921.5KB, .pdf)
│   │   │   │   │   ├── 2024-111 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-112 - 05_21_2024.pdf (6.2MB, .pdf)
│   │   │   │   │   ├── 2024-113 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-114 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-115 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-116 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-117 - 05_21_2024.pdf (6.4MB, .pdf)
│   │   │   │   │   ├── 2024-118 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-119 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-12 - 01_23_2024.pdf (588.3KB, .pdf)
│   │   │   │   │   ├── 2024-120 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-121 - 05_21_2024.pdf (6.4MB, .pdf)
│   │   │   │   │   ├── 2024-122 - 05_21_2024.pdf (6.8MB, .pdf)
│   │   │   │   │   ├── 2024-123 - 05_21_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-124 - 05_21_2024.pdf (11.2MB, .pdf)
│   │   │   │   │   ├── 2024-125 - 05_21_2024.pdf (776.0KB, .pdf)
│   │   │   │   │   ├── 2024-126 - 05_21_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-127 - 05_21_2024.pdf (8.1MB, .pdf)
│   │   │   │   │   ├── 2024-129 - 06_11_2024.pdf (1.3MB, .pdf)
│   │   │   │   │   ├── 2024-13 - 01_23_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-130 - 06_11_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-131 - 06_11_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-132 - 06_11_2024.pdf (458.0KB, .pdf)
│   │   │   │   │   ├── 2024-133 - 06_11_2024 -As Amended.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-134 - 06_11_2024.pdf (884.8KB, .pdf)
│   │   │   │   │   ├── 2024-135 - 06_11_2024.pdf (1022.1KB, .pdf)
│   │   │   │   │   ├── 2024-136 - 06_11_2024.pdf (537.4KB, .pdf)
│   │   │   │   │   ├── 2024-137 - 06_11_2024.pdf (1.7MB, .pdf)
│   │   │   │   │   ├── 2024-138 - 06_11_2024.pdf (1.7MB, .pdf)
│   │   │   │   │   ├── 2024-139 - 06_11_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-14 - 01_23_2024.pdf (996.6KB, .pdf)
│   │   │   │   │   ├── 2024-140 - 06_11_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-141 - 06_11_2024.pdf (13.2MB, .pdf)
│   │   │   │   │   ├── 2024-142 - 06_11_2024.pdf (790.5KB, .pdf)
│   │   │   │   │   ├── 2024-143 -06_11_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-15 - 01_23_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-16 - 01_23_2024.pdf (1.8MB, .pdf)
│   │   │   │   │   ├── 2024-17 - 01_23_2024.pdf (488.0KB, .pdf)
│   │   │   │   │   ├── 2024-18 - 01_23_2024.pdf (849.1KB, .pdf)
│   │   │   │   │   ├── 2024-19 - 02_19_2024.pdf (1019.5KB, .pdf)
│   │   │   │   │   ├── 2024-20 - 02_13_2024.pdf (824.5KB, .pdf)
│   │   │   │   │   ├── 2024-21 - 02_13_2024.pdf (502.6KB, .pdf)
│   │   │   │   │   ├── 2024-22 - 02_13_2024.pdf (450.6KB, .pdf)
│   │   │   │   │   ├── 2024-23 - 02_13_2024.pdf (447.7KB, .pdf)
│   │   │   │   │   ├── 2024-24 - 02_13_2024.pdf (464.1KB, .pdf)
│   │   │   │   │   ├── 2024-25 - 02_13_2024.pdf (458.3KB, .pdf)
│   │   │   │   │   ├── 2024-26 - 02_13_2024.pdf (465.7KB, .pdf)
│   │   │   │   │   ├── 2024-27 - 02_13_2024.pdf (754.2KB, .pdf)
│   │   │   │   │   ├── 2024-28 - 02_13_2024.pdf (761.7KB, .pdf)
│   │   │   │   │   ├── 2024-29 - 02_13_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-30 - 02_13_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-31 - 02_13_2024.pdf (936.6KB, .pdf)
│   │   │   │   │   ├── 2024-32 - 02_13_2024.pdf (1.9MB, .pdf)
│   │   │   │   │   ├── 2024-33 - 02_13_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-34 - 02_13_2024.pdf (1.9MB, .pdf)
│   │   │   │   │   ├── 2024-35 - 02_27_2024.pdf (480.7KB, .pdf)
│   │   │   │   │   ├── 2024-36 - 02_27_2024.pdf (1.3MB, .pdf)
│   │   │   │   │   ├── 2024-37 - 02_27_2024.pdf (940.3KB, .pdf)
│   │   │   │   │   ├── 2024-38 - 02_27_2024.pdf (829.6KB, .pdf)
│   │   │   │   │   ├── 2024-39 - 02_27_2024.pdf (816.5KB, .pdf)
│   │   │   │   │   ├── 2024-41 - 02_27_2024.pdf (833.1KB, .pdf)
│   │   │   │   │   ├── 2024-42 - 02_27_2024.pdf (473.4KB, .pdf)
│   │   │   │   │   ├── 2024-43 - 02_27_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-44 - 02_27_2024.pdf (1.8MB, .pdf)
│   │   │   │   │   ├── 2024-45 - 03_12_2024.pdf (1002.1KB, .pdf)
│   │   │   │   │   ├── 2024-46 - 03_12_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-47 - 03_12_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-48 - 03_12_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-49 - 03_12_2024.pdf (899.6KB, .pdf)
│   │   │   │   │   ├── 2024-50 - 03_12_2024.pdf (462.1KB, .pdf)
│   │   │   │   │   ├── 2024-51 - 03_12_2024.pdf (538.1KB, .pdf)
│   │   │   │   │   ├── 2024-52 - 03_12_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-53 - 03_12_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-54 - 03_12_2024.pdf (2.0MB, .pdf)
│   │   │   │   │   ├── 2024-55 - 03_12_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-56 - 03_12_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-57 - 03_12_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-58 - 03_12_2024.pdf (775.8KB, .pdf)
│   │   │   │   │   ├── 2024-60 - 03_12_2024.pdf (2.3MB, .pdf)
│   │   │   │   │   ├── 2024-61 - 04_16_2024.pdf (6.4MB, .pdf)
│   │   │   │   │   ├── 2024-62 - 04_16_2024.pdf (2.7MB, .pdf)
│   │   │   │   │   ├── 2024-63 -  04_16_2024.pdf (832.3KB, .pdf)
│   │   │   │   │   ├── 2024-64 - 04_16_2024.pdf (452.6KB, .pdf)
│   │   │   │   │   ├── 2024-65 - 04_16_2024.pdf (894.5KB, .pdf)
│   │   │   │   │   ├── 2024-66 - 04_16_2024.pdf (446.6KB, .pdf)
│   │   │   │   │   ├── 2024-67 - 04_16_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-68 - 04_16_2024.pdf (5.6MB, .pdf)
│   │   │   │   │   ├── 2024-69- -04_16_2024.pdf (443.1KB, .pdf)
│   │   │   │   │   ├── 2024-70 - 04_16_2024.pdf (878.9KB, .pdf)
│   │   │   │   │   ├── 2024-71 - 04_16_2024.pdf (951.8KB, .pdf)
│   │   │   │   │   ├── 2024-72 - 04_16_2024.pdf (821.7KB, .pdf)
│   │   │   │   │   ├── 2024-73 - 04_16_2024.pdf (810.8KB, .pdf)
│   │   │   │   │   ├── 2024-74 - 04_16_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-75 - 04_16_2024.pdf (1.6MB, .pdf)
│   │   │   │   │   ├── 2024-76 - 04_16_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-77 - 04_16_2024.pdf (1013.9KB, .pdf)
│   │   │   │   │   ├── 2024-78 - 04_16_2024.pdf (1007.5KB, .pdf)
│   │   │   │   │   ├── 2024-79 - 04_16_2024.pdf (997.7KB, .pdf)
│   │   │   │   │   ├── 2024-80 - 04_16_2024.pdf (525.7KB, .pdf)
│   │   │   │   │   ├── 2024-81 - 04_16_2024.pdf (923.2KB, .pdf)
│   │   │   │   │   ├── 2024-82 - 04_16_2024.pdf (473.6KB, .pdf)
│   │   │   │   │   ├── 2024-83 - 04_16_2024.pdf (915.3KB, .pdf)
│   │   │   │   │   ├── 2024-84 - 05_07_2024.pdf (992.7KB, .pdf)
│   │   │   │   │   ├── 2024-85 - 05_07_2024.pdf (2.0MB, .pdf)
│   │   │   │   │   ├── 2024-86 - 05_07_2024.pdf (503.1KB, .pdf)
│   │   │   │   │   ├── 2024-87 - 05_07_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-88 - 05_07_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-89 - 05_07_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-90 - 05_07_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-91 - 05_07_2024.pdf (1.8MB, .pdf)
│   │   │   │   │   ├── 2024-92 - 05_07_2024.pdf (452.4KB, .pdf)
│   │   │   │   │   ├── 2024-93 - 05_07_2024.pdf (1014.3KB, .pdf)
│   │   │   │   │   ├── 2024-94 - 05_05_2024.pdf (1.3MB, .pdf)
│   │   │   │   │   ├── 2024-95 - 05_07_2024.pdf (974.5KB, .pdf)
│   │   │   │   │   ├── 2024-96 - 05_07_2024.pdf (1.5MB, .pdf)
│   │   │   │   │   ├── 2024-97 - 05_07_2024.pdf (1023.3KB, .pdf)
│   │   │   │   │   ├── 2024-98 - 05_07_2024.pdf (2.2MB, .pdf)
│   │   │   │   │   └── 2024-99 - 05_07_2024.pdf (792.7KB, .pdf)
│   │   │   │   ├── [EXCLUDED] 1 items: .DS_Store (excluded file)
│   │   │   ├── Verbating Items/
│   │   │   │   ├── 2024/
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - E-4.pdf (2.4MB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - E-5.pdf (735.6KB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - E-7.pdf (306.9KB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - E-8.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - F-10.pdf (925.3KB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - F-2.pdf (288.2KB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - F-5.pdf (360.6KB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - F-6.pdf (212.8KB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - Public Comment.pdf (762.7KB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - C- Public Comment.pdf (1.3MB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - E-3 and E-4.pdf (2.4MB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - E-5.pdf (290.7KB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - F-1.pdf (855.5KB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - F-5.pdf (3.7MB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - F-7.pdf (768.6KB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - H-1.pdf (1.5MB, .pdf)
│   │   │   │   │   ├── 02_13_2024 - Meeting Minutes - Public.pdf (363.5KB, .pdf)
│   │   │   │   │   ├── 02_13_2024 - Verbatim Transcripts - F-12.pdf (7.2MB, .pdf)
│   │   │   │   │   ├── 02_13_2024 - Verbatim Transcripts - H-1.pdf (570.8KB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - E-1.pdf (4.2MB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - E-2.pdf (281.3KB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - E-3.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - F-10.pdf (2.5MB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - F-12.pdf (448.2KB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - H-1.pdf (794.2KB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - Public Comment.pdf (439.4KB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - E-1.pdf (124.3KB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - E-2.pdf (487.9KB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - E-4 and E-11.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - F-1.pdf (416.0KB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - F-2.pdf (1.5MB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - F-4 and F-5 and F-11 and F-12.pdf (1.4MB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - H-1.pdf (121.5KB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - K - Discussion Items.pdf (285.0KB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - Public Comment.pdf (412.4KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - 2-1 AND 2-2.pdf (652.3KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - E-11.pdf (365.8KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - E-2.pdf (369.8KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - E-4.pdf (410.2KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - E-5 E-6 E-7 E-8 E-9 E-10.pdf (3.6MB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-1.pdf (751.9KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-10.pdf (1.3MB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-11.pdf (2.4MB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-12.pdf (2.8MB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-15.pdf (255.0KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-5.pdf (205.0KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-8.pdf (961.1KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-9.pdf (330.1KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - Public Comment.pdf (1.5MB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - 2-1.pdf (1.7MB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - E-1.pdf (4.6MB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - E-10.pdf (128.5KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - E-11.pdf (559.6KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - E-12.pdf (156.1KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - E-8.pdf (288.5KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - E-9.pdf (263.4KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - F-1.pdf (561.1KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - F-13.pdf (151.9KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - F-16.pdf (782.8KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - F-3.pdf (352.6KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - F-9.pdf (736.0KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - H-1.pdf (455.3KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - Public Comment.pdf (1.6MB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - D-3.pdf (539.3KB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - E-10.pdf (629.7KB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - E-3.pdf (350.4KB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - F-2.pdf (1.7MB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - F-3.pdf (2.4MB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - F-4.pdf (3.4MB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - F-5.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - F-7 and F-10.pdf (1.6MB, .pdf)
│   │   │   │   │   └── 06_11_2024 - Verbatim Transcripts - Public Comment.pdf (513.4KB, .pdf)
│   │   │   │   ├── [EXCLUDED] 1 items: .DS_Store (excluded file)
│   │   │   ├── [EXCLUDED] 1 items: .DS_Store (excluded file)
│   │   ├── [EXCLUDED] 1 items: .DS_Store (excluded file)
│   ├── graph_json/
│   │   ├── Agenda 01.9.2024_docling_extracted.json (22.8KB, .json)
│   │   ├── Agenda 01.9.2024_extracted.json (22.8KB, .json)
│   │   ├── Agenda 01.9.2024_full_text.txt (16.1KB, .txt)
│   │   ├── Agenda 01.9.2024_ontology.json (9.6KB, .json)
│   │   ├── debug/
│   │   │   ├── all_ordinance_files.txt (170.0B, .txt)
│   │   │   ├── document_search_info.txt (174.0B, .txt)
│   │   │   ├── linked_documents.json (1014.0B, .json)
│   │   │   ├── llm_input_2024-01.txt (3.7KB, .txt)
│   │   │   ├── llm_input_2024-02.txt (49.8KB, .txt)
│   │   │   ├── llm_input_2024-03.txt (26.1KB, .txt)
│   │   │   ├── llm_response_2024-01_cleaned.txt (1.5KB, .txt)
│   │   │   ├── llm_response_2024-01_raw.txt (1.5KB, .txt)
│   │   │   ├── llm_response_2024-02_cleaned.txt (1.2KB, .txt)
│   │   │   ├── llm_response_2024-02_raw.txt (1.3KB, .txt)
│   │   │   ├── llm_response_2024-03_cleaned.txt (1.2KB, .txt)
│   │   │   ├── llm_response_2024-03_raw.txt (1.2KB, .txt)
│   │   │   └── matched_documents.txt (123.0B, .txt)
│   │   └── pipeline_report_20250602_203056.json (5.8KB, .json)
│   ├── [EXCLUDED] 2 items: .DS_Store (excluded file), global (excluded dir)
├── config.py (1.7KB, .py)
├── debug/
├── graph_clear_database.py (963.0B, .py)
├── graph_visualizer.py (23.6KB, .py)
├── repository_directory_structure.txt (5.7KB, .txt)
├── requirements.txt (1018.0B, .txt)
├── run_city_clerk_pipeline.sh (440.0B, .sh)
├── run_graph_pipeline.sh (1.3KB, .sh)
├── run_graph_visualizer.sh (3.6KB, .sh)
├── scripts/
│   ├── check_pipeline_setup.py (3.1KB, .py)
│   ├── clear_database.py (1.6KB, .py)
│   ├── clear_database_sync.py (1.6KB, .py)
│   ├── clear_meeting_data.py (1.8KB, .py)
│   ├── debug_agenda_items.py (2.4KB, .py)
│   ├── debug_ordinance_2024_02.py (3.5KB, .py)
│   ├── debug_pipeline_output.py (1.5KB, .py)
│   ├── find_duplicates.py (5.2KB, .py)
│   ├── graph_pipeline.py (17.5KB, .py)
│   ├── graph_stages/
│   │   ├── __init__.py (525.0B, .py)
│   │   ├── agenda_graph_builder.py (39.0KB, .py)
│   │   ├── agenda_ontology_extractor.py (24.4KB, .py)
│   │   ├── agenda_pdf_extractor.py (13.4KB, .py)
│   │   ├── cosmos_db_client.py (10.2KB, .py)
│   │   ├── document_linker.py (13.7KB, .py)
│   │   ├── ontology_extractor.py (25.2KB, .py)
│   │   └── pdf_extractor.py (6.5KB, .py)
│   ├── pipeline_modular_optimized.py (12.7KB, .py)
│   ├── rag_local_web_app.py (18.4KB, .py)
│   ├── stages/
│   │   ├── __init__.py (378.0B, .py)
│   │   ├── acceleration_utils.py (3.8KB, .py)
│   │   ├── chunk_text.py (18.6KB, .py)
│   │   ├── db_upsert.py (8.9KB, .py)
│   │   ├── embed_vectors.py (27.0KB, .py)
│   │   ├── extract_clean.py (21.7KB, .py)
│   │   └── llm_enrich.py (5.9KB, .py)
│   ├── supabase_clear_database.py (6.0KB, .py)
│   ├── test_graph_pipeline.py (2.1KB, .py)
│   ├── test_vector_search.py (5.5KB, .py)
│   ├── topic_filter_and_title.py (5.6KB, .py)
│   ├── [EXCLUDED] 1 items: .DS_Store (excluded file)
├── [EXCLUDED] 6 items: .DS_Store (excluded file), .env (excluded file), .git (excluded dir)
    ... and 3 more excluded items


================================================================================


# File Index - Which Files Are in Which Parts
################################################################################

## Part 1 (11 files):
  - scripts/graph_stages/agenda_graph_builder.py
  - scripts/stages/chunk_text.py
  - scripts/graph_stages/document_linker.py
  - scripts/graph_stages/cosmos_db_client.py
  - scripts/supabase_clear_database.py
  - scripts/test_vector_search.py
  - scripts/debug_ordinance_2024_02.py
  - scripts/debug_agenda_items.py
  - scripts/clear_meeting_data.py
  - scripts/clear_database.py
  - requirements.txt

## Part 2 (11 files):
  - scripts/stages/embed_vectors.py
  - scripts/stages/extract_clean.py
  - scripts/graph_pipeline.py
  - scripts/graph_stages/agenda_pdf_extractor.py
  - scripts/stages/db_upsert.py
  - scripts/find_duplicates.py
  - scripts/topic_filter_and_title.py
  - scripts/test_graph_pipeline.py
  - config.py
  - city_clerk_documents/graph_json/debug/linked_documents.json
  - graph_clear_database.py

## Part 3 (12 files):
  - scripts/graph_stages/ontology_extractor.py
  - scripts/graph_stages/agenda_ontology_extractor.py
  - scripts/rag_local_web_app.py
  - scripts/pipeline_modular_optimized.py
  - scripts/graph_stages/pdf_extractor.py
  - scripts/stages/llm_enrich.py
  - scripts/stages/acceleration_utils.py
  - scripts/check_pipeline_setup.py
  - scripts/clear_database_sync.py
  - scripts/debug_pipeline_output.py
  - scripts/graph_stages/__init__.py
  - scripts/stages/__init__.py


================================================================================


################################################################################
# File: scripts/graph_stages/agenda_graph_builder.py
################################################################################

# File: scripts/graph_stages/agenda_graph_builder.py

"""
Enhanced Agenda Graph Builder - RICH VERSION
Builds comprehensive graph representation from LLM-extracted agenda ontology.
"""
import logging
from typing import Dict, List, Optional
from pathlib import Path
import hashlib
import json
import calendar
import re

from .cosmos_db_client import CosmosGraphClient

log = logging.getLogger('pipeline_debug.graph_builder')


class AgendaGraphBuilder:
    """Build comprehensive graph representation from rich agenda ontology."""
    
    def __init__(self, cosmos_client: CosmosGraphClient, upsert_mode: bool = True):
        self.cosmos = cosmos_client
        self.upsert_mode = upsert_mode
        self.entity_id_cache = {}  # Cache for entity IDs
        self.partition_value = 'demo'  # Partition value property
        
        # Track statistics
        self.stats = {
            'nodes_created': 0,
            'nodes_updated': 0,
            'edges_created': 0,
            'edges_skipped': 0
        }
    
    @staticmethod
    def normalize_item_code(code: str) -> str:
        """Normalize item codes to consistent format for matching with ordinances."""
        if not code:
            return code
        
        # Remove trailing dots: "E.-1." -> "E.-1"
        code = code.rstrip('.')
        
        # Remove dots between letter and dash: "E.-1" -> "E-1"
        code = re.sub(r'([A-Z])\.(-)', r'\1\2', code)
        
        # Also handle cases without dash: "E.1" -> "E-1"
        code = re.sub(r'([A-Z])\.(\d)', r'\1-\2', code)
        
        # Ensure we have a dash between letter and number
        code = re.sub(r'([A-Z])(\d)', r'\1-\2', code)
        
        return code
    
    @staticmethod
    def ensure_us_date_format(date_str: str) -> str:
        """Ensure date is in US format MM-DD-YYYY with dashes."""
        # Handle different input formats
        if '.' in date_str:
            # Format: 01.23.2024 -> 01-23-2024
            return date_str.replace('.', '-')
        elif '/' in date_str:
            # Format: 01/23/2024 -> 01-23-2024
            return date_str.replace('/', '-')
        elif re.match(r'^\d{4}-\d{2}-\d{2}$', date_str):
            # ISO format: 2024-01-23 -> 01-23-2024
            parts = date_str.split('-')
            return f"{parts[1]}-{parts[2]}-{parts[0]}"
        else:
            # Already in correct format or unknown
            return date_str

    async def build_graph(self, ontology_file: Path, linked_docs: Optional[Dict] = None, upsert: bool = True) -> Dict:
        """Build graph from ontology file - main entry point."""
        # Load ontology
        with open(ontology_file, 'r', encoding='utf-8') as f:
            ontology = json.load(f)
        
        return await self.build_graph_from_ontology(ontology, ontology_file, linked_docs)

    async def build_graph_from_ontology(self, ontology: Dict, source_path: Path, linked_docs: Optional[Dict] = None) -> Dict:
        """Build comprehensive graph representation from rich ontology."""
        log.info(f"🔨 Starting enhanced graph build for {source_path.name}")
        log.info(f"🔧 Upsert mode: {'ENABLED' if self.upsert_mode else 'DISABLED'}")
        
        # Reset statistics
        self.stats = {
            'nodes_created': 0,
            'nodes_updated': 0,
            'edges_created': 0,
            'edges_skipped': 0
        }
        
        try:
            graph_data = {
                'nodes': {},
                'edges': [],
                'statistics': {}
            }
            
            # CRITICAL: Ensure meeting date is in US format
            meeting_date_original = ontology['meeting_date']
            meeting_date_us = self.ensure_us_date_format(meeting_date_original)
            meeting_info = ontology['meeting_info']
            
            log.info(f"📅 Meeting date: {meeting_date_original} -> {meeting_date_us}")
            
            # 1. Create Meeting node as the root
            meeting_id = f"meeting-{meeting_date_us}"
            await self._create_meeting_node(meeting_date_us, meeting_info, source_path.name)
            log.info(f"✅ Created meeting node: {meeting_id}")
            
            # 1.5 Create Date node and link to meeting
            try:
                date_id = await self._create_date_node(meeting_date_original, meeting_id)
                graph_data['nodes'][date_id] = {
                    'type': 'Date',
                    'date': meeting_date_original
                }
            except Exception as e:
                log.error(f"Failed to create date node: {e}")
            
            graph_data['nodes'][meeting_id] = {
                'type': 'Meeting',
                'date': meeting_date_us,
                'info': meeting_info
            }
            
            # 2. Create nodes for officials present  
            await self._create_official_nodes(meeting_info, meeting_id)
            
            # 3. Process sections and agenda items
            section_count = 0
            item_count = 0
            
            sections = ontology.get('sections', [])
            log.info(f"📑 Processing {len(sections)} sections")
            
            for section_idx, section in enumerate(sections):
                try:
                    section_count += 1
                    section_id = f"section-{meeting_date_us}-{section_idx}"
                    
                    # Create Section node
                    await self._create_section_node(section_id, section, section_idx)
                    log.info(f"✅ Created section {section_idx}: {section.get('section_name', 'Unknown')}")
                    
                    graph_data['nodes'][section_id] = {
                        'type': 'Section',
                        'name': section['section_name'],
                        'order': section_idx
                    }
                    
                    # Link section to meeting
                    if await self.cosmos.create_edge_if_not_exists(
                        from_id=meeting_id,
                        to_id=section_id,
                        edge_type='HAS_SECTION',
                        properties={'order': section_idx}
                    ):
                        self.stats['edges_created'] += 1
                    else:
                        self.stats['edges_skipped'] += 1
                    
                    # Process items in section
                    previous_item_id = None
                    items = section.get('items', [])
                    
                    for item_idx, item in enumerate(items):
                        try:
                            if not item.get('item_code'):
                                log.warning(f"Skipping item without code in section {section['section_name']}")
                                continue
                                
                            item_count += 1
                            # Normalize the item code
                            normalized_code = self.normalize_item_code(item['item_code'])
                            # Use US date format for item ID
                            item_id = f"item-{meeting_date_us}-{normalized_code}"
                            
                            log.info(f"Creating item: {item_id} (from code: {item['item_code']})")
                            
                            # Create enhanced AgendaItem node
                            await self._create_enhanced_agenda_item_node(item_id, item, section)
                            
                            graph_data['nodes'][item_id] = {
                                'type': 'AgendaItem',
                                'code': normalized_code,
                                'original_code': item['item_code'],
                                'title': item.get('title', 'Unknown')
                            }
                            
                            # Link item to section
                            if await self.cosmos.create_edge_if_not_exists(
                                from_id=section_id,
                                to_id=item_id,
                                edge_type='CONTAINS_ITEM',
                                properties={'order': item_idx}
                            ):
                                self.stats['edges_created'] += 1
                            else:
                                self.stats['edges_skipped'] += 1
                            
                            # Create sequential relationships
                            if previous_item_id:
                                if await self.cosmos.create_edge_if_not_exists(
                                    from_id=previous_item_id,
                                    to_id=item_id,
                                    edge_type='FOLLOWS',
                                    properties={'sequence': item_idx}
                                ):
                                    self.stats['edges_created'] += 1
                                else:
                                    self.stats['edges_skipped'] += 1
                            
                            previous_item_id = item_id
                            
                            # Create rich relationships for this item
                            await self._create_item_relationships(item, item_id, meeting_date_us)
                            
                            # Create URL nodes and relationships
                            await self._create_url_relationships(item, item_id)
                                
                        except Exception as e:
                            log.error(f"Failed to process item {item.get('item_code', 'unknown')}: {e}")
                            
                except Exception as e:
                    log.error(f"Failed to process section {section.get('section_name', 'unknown')}: {e}")
            
            # 4. Create entity nodes from extracted entities
            entity_count = await self._create_entity_nodes(ontology.get('entities', []), meeting_id)
            
            # 5. Create relationships from ontology
            relationship_count = 0
            for rel in ontology.get('relationships', []):
                try:
                    await self._create_ontology_relationship(rel, meeting_date_us)
                    relationship_count += 1
                except Exception as e:
                    log.error(f"Failed to create relationship: {e}")
            
            # 6. Process linked documents if available
            if linked_docs:
                await self.process_linked_documents(linked_docs, meeting_id, meeting_date_us)
            
            # Update statistics
            graph_data['statistics'] = {
                'sections': section_count,
                'items': item_count, 
                'entities': entity_count,
                'relationships': relationship_count,
                'meeting_date': meeting_date_us
            }
            
            log.info(f"🎉 Enhanced graph build complete for {source_path.name}")
            log.info(f"   📊 Statistics:")
            log.info(f"      - Nodes created: {self.stats['nodes_created']}")
            log.info(f"      - Nodes updated: {self.stats['nodes_updated']}")
            log.info(f"      - Edges created: {self.stats['edges_created']}")
            log.info(f"      - Edges skipped: {self.stats['edges_skipped']}")
            log.info(f"   - Sections: {section_count}")
            log.info(f"   - Items: {item_count}")
            log.info(f"   - Entities: {entity_count}")
            log.info(f"   - Relationships: {relationship_count}")
            
            return graph_data
            
        except Exception as e:
            log.error(f"CRITICAL ERROR in build_graph_from_ontology: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    async def _create_meeting_node(self, meeting_date: str, meeting_info: Dict, source_file: str = None) -> str:
        """Create or update Meeting node with comprehensive metadata."""
        meeting_id = f"meeting-{meeting_date}"
        
        # Handle location - could be string or dict
        location = meeting_info.get('location', 'City Commission Chambers')
        if isinstance(location, dict):
            location_str = f"{location.get('name', 'City Commission Chambers')}"
            if location.get('address'):
                location_str += f" - {location['address']}"
        else:
            location_str = str(location) if location else "City Commission Chambers"
        
        properties = {
            'nodeType': 'Meeting',
            'date': meeting_date,
            'type': meeting_info.get('type', 'Regular Meeting'),
            'time': meeting_info.get('time', '5:30 PM'),
            'location': location_str
        }
        
        if source_file:
            properties['source_file'] = source_file
        
        # Use upsert instead of create
        if self.upsert_mode:
            created = await self.cosmos.upsert_vertex('Meeting', meeting_id, properties)
            if created:
                self.stats['nodes_created'] += 1
                log.info(f"✅ Created Meeting node: {meeting_id}")
            else:
                self.stats['nodes_updated'] += 1
                log.info(f"📝 Updated Meeting node: {meeting_id}")
        else:
            await self.cosmos.create_vertex('Meeting', meeting_id, properties)
            self.stats['nodes_created'] += 1
            log.info(f"✅ Created Meeting node: {meeting_id}")
        
        return meeting_id
    
    async def _create_date_node(self, date_str: str, meeting_id: str) -> str:
        """Create a Date node and link it to the meeting."""
        from datetime import datetime
        
        # Parse date from MM.DD.YYYY format
        parts = date_str.split('.')
        if len(parts) != 3:
            log.error(f"Invalid date format: {date_str}")
            return None
            
        month, day, year = int(parts[0]), int(parts[1]), int(parts[2])
        
        # Create consistent date ID in ISO format
        date_id = f"date-{year:04d}-{month:02d}-{day:02d}"
        
        # Check if date already exists
        if await self.cosmos.vertex_exists(date_id):
            log.info(f"Date {date_id} already exists")
            # Still create the relationship
            await self.cosmos.create_edge(
                from_id=meeting_id,
                to_id=date_id,
                edge_type='OCCURRED_ON',
                properties={'primary_date': True}
            )
            return date_id
        
        # Get day of week
        date_obj = datetime(year, month, day)
        day_of_week = date_obj.strftime('%A')
        
        # Create date node
        properties = {
            'nodeType': 'Date',
            'full_date': date_str,
            'year': year,
            'month': month,
            'day': day,
            'quarter': (month - 1) // 3 + 1,
            'month_name': calendar.month_name[month],
            'day_of_week': day_of_week,
            'iso_date': f'{year:04d}-{month:02d}-{day:02d}'
        }
        
        await self.cosmos.create_vertex('Date', date_id, properties)
        log.info(f"✅ Created Date node: {date_id}")
        
        # Create relationship: Meeting -> OCCURRED_ON -> Date
        await self.cosmos.create_edge(
            from_id=meeting_id,
            to_id=date_id,
            edge_type='OCCURRED_ON',
            properties={'primary_date': True}
        )
        
        return date_id
    
    async def _create_section_node(self, section_id: str, section: Dict, order: int) -> str:
        """Create or update Section node."""
        properties = {
            'nodeType': 'Section',
            'title': section.get('section_name', 'Unknown'),
            'type': section.get('section_type', 'OTHER'),
            'description': section.get('description', ''),
            'order': order
        }
        
        if self.upsert_mode:
            created = await self.cosmos.upsert_vertex('Section', section_id, properties)
            if created:
                self.stats['nodes_created'] += 1
            else:
                self.stats['nodes_updated'] += 1
        else:
            if await self.cosmos.vertex_exists(section_id):
                log.info(f"Section {section_id} already exists, skipping creation")
                return section_id
            await self.cosmos.create_vertex('Section', section_id, properties)
            self.stats['nodes_created'] += 1
        
        return section_id
    
    async def _create_enhanced_agenda_item_node(self, item_id: str, item: Dict, section: Dict) -> str:
        """Create or update AgendaItem node with rich metadata from LLM extraction."""
        # Store both original and normalized codes
        original_code = item.get('item_code', '')
        normalized_code = self.normalize_item_code(original_code)
        
        properties = {
            'nodeType': 'AgendaItem',
            'code': normalized_code,
            'original_code': original_code,
            'title': item.get('title', 'Unknown'),
            'type': item.get('item_type', 'Item'),
            'section': section.get('section_name', 'Unknown'),
            'section_type': section.get('section_type', 'other')
        }
        
        # Add enhanced details from LLM extraction
        if item.get('description'):
            properties['description'] = item['description'][:500]  # Limit length
        
        if item.get('document_reference'):
            properties['document_reference'] = item['document_reference']
        
        # Add sponsors as JSON array
        if item.get('sponsors'):
            properties['sponsors_json'] = json.dumps(item['sponsors'])
        
        # Add departments as JSON array  
        if item.get('departments'):
            properties['departments_json'] = json.dumps(item['departments'])
        
        # Add actions as JSON array
        if item.get('actions'):
            properties['actions_json'] = json.dumps(item['actions'])
        
        # Add stakeholders as JSON array
        if item.get('stakeholders'):
            properties['stakeholders_json'] = json.dumps(item['stakeholders'])
        
        # Add URLs as JSON array
        if item.get('urls'):
            properties['urls_json'] = json.dumps(item['urls'])
            properties['has_urls'] = True
        
        if self.upsert_mode:
            created = await self.cosmos.upsert_vertex('AgendaItem', item_id, properties)
            if created:
                self.stats['nodes_created'] += 1
            else:
                self.stats['nodes_updated'] += 1
        else:
            await self.cosmos.create_vertex('AgendaItem', item_id, properties)
            self.stats['nodes_created'] += 1
        
        return item_id
    
    async def _create_official_nodes(self, meeting_info: Dict, meeting_id: str):
        """Create nodes for city officials and link to meeting."""
        officials = meeting_info.get('officials', {})
        commissioners = meeting_info.get('commissioners', [])
        
        # Create official nodes
        for role, name in officials.items():
            if name and name != 'null':
                person_id = await self._ensure_person_node(name, role.replace('_', ' ').title())
                await self.cosmos.create_edge_if_not_exists(
                    from_id=person_id,
                    to_id=meeting_id,
                    edge_type='ATTENDED',
                    properties={'role': role.replace('_', ' ').title()}
                )
        
        # Create commissioner nodes
        for idx, commissioner in enumerate(commissioners):
            if commissioner and commissioner != 'null':
                person_id = await self._ensure_person_node(commissioner, 'Commissioner')
                await self.cosmos.create_edge_if_not_exists(
                    from_id=person_id,
                    to_id=meeting_id,
                    edge_type='ATTENDED',
                    properties={'role': 'Commissioner', 'seat': idx + 1}
                )
    
    async def _create_entity_nodes(self, entities: List[Dict], meeting_id: str) -> int:
        """Create nodes for all extracted entities."""
        entity_count = 0
        
        for entity in entities:
            try:
                entity_type = entity.get('type', 'unknown')
                entity_name = entity.get('name', '')
                entity_role = entity.get('role', '')
                entity_context = entity.get('context', '')
                
                if not entity_name:
                    continue
                
                if entity_type == 'person':
                    person_id = await self._ensure_person_node(entity_name, entity_role)
                    await self.cosmos.create_edge_if_not_exists(
                        from_id=person_id,
                        to_id=meeting_id,
                        edge_type='MENTIONED_IN',
                        properties={
                            'context': entity_context[:100],
                            'role': entity_role
                        }
                    )
                    entity_count += 1
                    
                elif entity_type == 'organization':
                    org_id = await self._ensure_organization_node(entity_name, entity_role)
                    await self.cosmos.create_edge_if_not_exists(
                        from_id=org_id,
                        to_id=meeting_id,
                        edge_type='MENTIONED_IN',
                        properties={
                            'context': entity_context[:100],
                            'org_type': entity_role
                        }
                    )
                    entity_count += 1
                    
                elif entity_type == 'department':
                    dept_id = await self._ensure_department_node(entity_name)
                    await self.cosmos.create_edge_if_not_exists(
                        from_id=dept_id,
                        to_id=meeting_id,
                        edge_type='INVOLVED_IN',
                        properties={'context': entity_context[:100]}
                    )
                    entity_count += 1
                    
                elif entity_type == 'location':
                    loc_id = await self._ensure_location_node(entity_name, entity_context)
                    await self.cosmos.create_edge_if_not_exists(
                        from_id=loc_id,
                        to_id=meeting_id,
                        edge_type='REFERENCED_IN',
                        properties={'context': entity_context[:100]}
                    )
                    entity_count += 1
                    
            except Exception as e:
                log.error(f"Failed to create entity node for {entity}: {e}")
        
        return entity_count
    
    async def _create_item_relationships(self, item: Dict, item_id: str, meeting_date: str):
        """Create rich relationships for agenda items."""
        
        # Sponsor relationships
        for sponsor in item.get('sponsors', []):
            try:
                person_id = await self._ensure_person_node(sponsor, 'Commissioner')
                await self.cosmos.create_edge_if_not_exists(
                    from_id=person_id,
                    to_id=item_id,
                    edge_type='SPONSORS',
                    properties={'role': 'sponsor'}
                )
                self.stats['edges_created'] += 1
            except Exception as e:
                log.error(f"Failed to create sponsor relationship: {e}")
        
        # Department relationships
        for dept in item.get('departments', []):
            try:
                dept_id = await self._ensure_department_node(dept)
                await self.cosmos.create_edge_if_not_exists(
                    from_id=dept_id,
                    to_id=item_id,
                    edge_type='RESPONSIBLE_FOR',
                    properties={'role': 'responsible_department'}
                )
                self.stats['edges_created'] += 1
            except Exception as e:
                log.error(f"Failed to create department relationship: {e}")
        
        # Stakeholder relationships  
        for stakeholder in item.get('stakeholders', []):
            try:
                org_id = await self._ensure_organization_node(stakeholder, 'Stakeholder')
                await self.cosmos.create_edge_if_not_exists(
                    from_id=org_id,
                    to_id=item_id,
                    edge_type='INVOLVED_IN',
                    properties={'role': 'stakeholder'}
                )
                self.stats['edges_created'] += 1
            except Exception as e:
                log.error(f"Failed to create stakeholder relationship: {e}")
        
        # Action relationships
        for action in item.get('actions', []):
            try:
                action_id = await self._ensure_action_node(action)
                await self.cosmos.create_edge_if_not_exists(
                    from_id=item_id,
                    to_id=action_id,
                    edge_type='REQUIRES_ACTION',
                    properties={'action_type': action}
                )
                self.stats['edges_created'] += 1
            except Exception as e:
                log.error(f"Failed to create action relationship: {e}")
    
    async def _create_url_relationships(self, item: Dict, item_id: str):
        """Create URL nodes and link to agenda items."""
        for url in item.get('urls', []):
            try:
                url_id = await self._ensure_url_node(url)
                await self.cosmos.create_edge_if_not_exists(
                    from_id=item_id,
                    to_id=url_id,
                    edge_type='HAS_URL',
                    properties={'url_type': 'document_link'}
                )
                self.stats['edges_created'] += 1
            except Exception as e:
                log.error(f"Failed to create URL relationship: {e}")
    
    async def _create_ontology_relationship(self, rel: Dict, meeting_date: str):
        """Create relationship from ontology data."""
        try:
            source = rel.get('source', '')
            target = rel.get('target', '')
            relationship = rel.get('relationship', '')
            source_type = rel.get('source_type', '')
            target_type = rel.get('target_type', '')
            
            # Determine source and target IDs based on type
            if source_type == 'person':
                source_id = await self._ensure_person_node(source, 'Participant')
            elif source_type == 'department':
                source_id = await self._ensure_department_node(source)
            elif source_type == 'organization':
                source_id = await self._ensure_organization_node(source, 'Organization')
            else:
                log.warning(f"Unknown source type: {source_type}")
                return
            
            if target_type == 'agenda_item':
                # Normalize target agenda item code
                normalized_target = self.normalize_item_code(target)
                target_id = f"item-{meeting_date}-{normalized_target}"
            else:
                log.warning(f"Unknown target type: {target_type}")
                return
            
            # Create the relationship
            await self.cosmos.create_edge_if_not_exists(
                from_id=source_id,
                to_id=target_id,
                edge_type=relationship.upper(),
                properties={
                    'source_type': source_type,
                    'target_type': target_type
                }
            )
            
        except Exception as e:
            log.error(f"Failed to create ontology relationship: {e}")
    
    async def _ensure_person_node(self, name: str, role: str) -> str:
        """Create or retrieve person node with upsert support."""
        clean_name = name.strip()
        # Clean the ID by removing invalid characters
        cleaned_id_part = clean_name.lower().replace(' ', '-').replace('.', '').replace("'", '').replace('"', '').replace('/', '-')
        person_id = f"person-{cleaned_id_part}"
        
        # Check cache first
        if person_id in self.entity_id_cache:
            return person_id
        
        # Check if exists in database
        if await self.cosmos.vertex_exists(person_id):
            self.entity_id_cache[person_id] = True
            return person_id
        
        # Create new person
        properties = {
            'nodeType': 'Person',
            'name': clean_name,
            'roles': role
        }
        
        if self.upsert_mode:
            created = await self.cosmos.upsert_vertex('Person', person_id, properties)
            if created:
                self.stats['nodes_created'] += 1
            else:
                self.stats['nodes_updated'] += 1
        else:
            if not await self.cosmos.vertex_exists(person_id):
                await self.cosmos.create_vertex('Person', person_id, properties)
                self.stats['nodes_created'] += 1
        
        self.entity_id_cache[person_id] = True
        return person_id
    
    async def _ensure_organization_node(self, name: str, org_type: str) -> str:
        """Create or retrieve organization node."""
        # Clean the ID
        cleaned_org_name = name.lower().replace(' ', '-').replace('.', '').replace("'", '').replace('"', '').replace('/', '-').replace(',', '')
        org_id = f"org-{cleaned_org_name}"
        
        if org_id in self.entity_id_cache:
            return org_id
        
        if await self.cosmos.vertex_exists(org_id):
            self.entity_id_cache[org_id] = True
            return org_id
        
        properties = {
            'nodeType': 'Organization',
            'name': name,
            'type': org_type
        }
        
        await self.cosmos.create_vertex('Organization', org_id, properties)
        self.entity_id_cache[org_id] = True
        self.stats['nodes_created'] += 1
        return org_id
    
    async def _ensure_department_node(self, name: str) -> str:
        """Create or retrieve department node."""
        cleaned_dept_name = name.lower().replace(' ', '-').replace('.', '').replace("'", '').replace('"', '').replace('/', '-')
        dept_id = f"dept-{cleaned_dept_name}"
        
        if dept_id in self.entity_id_cache:
            return dept_id
        
        if await self.cosmos.vertex_exists(dept_id):
            self.entity_id_cache[dept_id] = True
            return dept_id
        
        properties = {
            'nodeType': 'Department', 
            'name': name,
            'type': 'CityDepartment'
        }
        
        await self.cosmos.create_vertex('Department', dept_id, properties)
        self.entity_id_cache[dept_id] = True
        self.stats['nodes_created'] += 1
        return dept_id
    
    async def _ensure_location_node(self, name: str, context: str = '') -> str:
        """Create or retrieve location node."""
        cleaned_loc_name = name.lower().replace(' ', '-').replace('.', '').replace("'", '').replace('"', '').replace('/', '-').replace(',', '')
        loc_id = f"location-{cleaned_loc_name}"
        
        if loc_id in self.entity_id_cache:
            return loc_id
        
        if await self.cosmos.vertex_exists(loc_id):
            self.entity_id_cache[loc_id] = True
            return loc_id
        
        properties = {
            'nodeType': 'Location',
            'name': name,
            'context': context[:200] if context else '',
            'type': 'Location'
        }
        
        await self.cosmos.create_vertex('Location', loc_id, properties)
        self.entity_id_cache[loc_id] = True
        self.stats['nodes_created'] += 1
        return loc_id
    
    async def _ensure_action_node(self, action: str) -> str:
        """Create or retrieve action node."""
        cleaned_action = action.lower().replace(' ', '-').replace('.', '')
        action_id = f"action-{cleaned_action}"
        
        if action_id in self.entity_id_cache:
            return action_id
        
        if await self.cosmos.vertex_exists(action_id):
            self.entity_id_cache[action_id] = True
            return action_id
        
        properties = {
            'nodeType': 'Action',
            'name': action,
            'type': 'RequiredAction'
        }
        
        await self.cosmos.create_vertex('Action', action_id, properties)
        self.entity_id_cache[action_id] = True
        self.stats['nodes_created'] += 1
        return action_id
    
    async def _ensure_url_node(self, url: str) -> str:
        """Create or retrieve URL node."""
        url_hash = hashlib.md5(url.encode()).hexdigest()[:12]
        url_id = f"url-{url_hash}"
        
        if url_id in self.entity_id_cache:
            return url_id
        
        if await self.cosmos.vertex_exists(url_id):
            self.entity_id_cache[url_id] = True
            return url_id
        
        properties = {
            'nodeType': 'URL',
            'url': url,
            'domain': url.split('/')[2] if '://' in url else 'unknown',
            'type': 'Hyperlink'
        }
        
        await self.cosmos.create_vertex('URL', url_id, properties)
        self.entity_id_cache[url_id] = True
        self.stats['nodes_created'] += 1
        return url_id

    async def process_linked_documents(self, linked_docs: Dict, meeting_id: str, meeting_date: str):
        """Process and create nodes for linked documents."""
        log.info("📄 Processing linked documents...")
        
        created_count = 0
        missing_items = []
        
        for doc_type, docs in linked_docs.items():
            if not docs:
                continue
                
            log.info(f"\n   📂 Processing {len(docs)} {doc_type}")
            
            for doc in docs:
                if doc_type in ['ordinances', 'resolutions']:
                    log.info(f"\n   Processing {doc_type[:-1]} {doc.get('document_number', 'unknown')}")
                    log.info(f"      Item code: {doc.get('item_code', 'MISSING')}")
                    
                    # Create document node
                    doc_id = await self._create_document_node(doc, doc_type, meeting_date)
                    
                    if doc_id:
                        created_count += 1
                        log.info(f"      ✅ Created document node: {doc_id}")
                        
                        # Link to meeting
                        await self.cosmos.create_edge(
                            from_id=doc_id,
                            to_id=meeting_id,
                            edge_type='PRESENTED_AT',
                            properties={'date': meeting_date}
                        )
                        
                        # Try to link to agenda item if item_code exists
                        item_code = doc.get('item_code')
                        if item_code:
                            normalized_code = self.normalize_item_code(item_code)
                            item_id = f"item-{meeting_date}-{normalized_code}"
                            
                            # Check if agenda item exists
                            if await self.cosmos.vertex_exists(item_id):
                                await self.cosmos.create_edge(
                                    from_id=item_id,
                                    to_id=doc_id,
                                    edge_type='REFERENCES_DOCUMENT',
                                    properties={'document_type': doc_type[:-1]}
                                )
                                log.info(f"      🔗 Linked to agenda item: {item_id}")
                            else:
                                log.warning(f"      ❌ Agenda item not found: {item_id}")
                                missing_items.append({
                                    'document_number': doc.get('document_number'),
                                    'item_code': item_code,
                                    'normalized_code': normalized_code,
                                    'expected_item_id': item_id
                                })
                        else:
                            log.warning(f"      ⚠️  No item_code found for {doc.get('document_number')}")
        
        log.info(f"📄 Document processing complete: {created_count} documents created")
        if missing_items:
            log.warning(f"⚠️  {len(missing_items)} documents could not be linked to agenda items")
        
        return missing_items

    async def _create_document_node(self, doc_info: Dict, doc_type: str, meeting_date: str) -> str:
        """Create or update an Ordinance or Resolution node."""
        doc_number = doc_info.get('document_number', 'unknown')
        doc_id = f"{doc_type[:-1]}-{doc_number}"  # Remove 's' from type
        
        # Get full title without truncation
        title = doc_info.get('title', '')
        if not title and doc_info.get('parsed_data', {}).get('title'):
            title = doc_info['parsed_data']['title']
        
        if title is None:
            title = f"Untitled {doc_type.capitalize()} {doc_number}"
            log.warning(f"No title found for {doc_type} {doc_number}, using default")
        
        properties = {
            'nodeType': doc_type[:-1].capitalize(),  # 'Ordinance' or 'Resolution'
            'document_number': doc_number,
            'full_title': title,
            'title': title[:200] if len(title) > 200 else title,
            'document_type': doc_type[:-1],  # Remove 's'
            'meeting_date': meeting_date
        }
        
        # Add parsed metadata
        parsed_data = doc_info.get('parsed_data', {})
        
        if parsed_data.get('date_passed'):
            properties['date_passed'] = parsed_data['date_passed']
        
        if parsed_data.get('agenda_item'):
            properties['agenda_item'] = parsed_data['agenda_item']
        
        # Add vote details as JSON
        if parsed_data.get('vote_details'):
            properties['vote_details'] = json.dumps(parsed_data['vote_details'])
        
        # Add signatories
        if parsed_data.get('signatories', {}).get('mayor'):
            properties['mayor_signature'] = parsed_data['signatories']['mayor']
        
        if self.upsert_mode:
            created = await self.cosmos.upsert_vertex(doc_type[:-1].capitalize(), doc_id, properties)
            if created:
                self.stats['nodes_created'] += 1
                log.info(f"✅ Created document node: {doc_id}")
            else:
                self.stats['nodes_updated'] += 1
                log.info(f"📝 Updated document node: {doc_id}")
        else:
            await self.cosmos.create_vertex(doc_type[:-1].capitalize(), doc_id, properties)
            self.stats['nodes_created'] += 1
        
        # Create edges for sponsors
        if parsed_data.get('motion', {}).get('moved_by'):
            person_id = await self._ensure_person_node(
                parsed_data['motion']['moved_by'], 
                'Commissioner'
            )
            if await self.cosmos.create_edge_if_not_exists(person_id, doc_id, 'MOVED'):
                self.stats['edges_created'] += 1
            else:
                self.stats['edges_skipped'] += 1
        
        return doc_id


================================================================================


################################################################################
# File: scripts/stages/chunk_text.py
################################################################################

# File: scripts/stages/chunk_text.py

#!/usr/bin/env python3
"""
Stage 5 — Token-based chunking with tiktoken validation.
"""
from __future__ import annotations
import json, logging, math, pathlib, re
from typing import Dict, List, Sequence, Tuple, Any, Optional
import asyncio
from concurrent.futures import ProcessPoolExecutor
import multiprocessing as mp

# 🎯 TIKTOKEN VALIDATION - Add token validation layer
try:
    import tiktoken
    TIKTOKEN_AVAILABLE = True
except ImportError:
    TIKTOKEN_AVAILABLE = False
    logging.warning("tiktoken not available - token-based chunking requires tiktoken")

# Token limits for validation (consistent with embed_vectors.py)
MAX_CHUNK_TOKENS = 6000  # Maximum tokens per chunk for embedding
MIN_CHUNK_TOKENS = 100   # Minimum viable chunk size
EMBEDDING_MODEL = "text-embedding-ada-002"

# ─── TOKEN-BASED CHUNKING PARAMETERS ───────────────────────────
WINDOW_TOKENS = 3000   # Token-based window (was 768 words)
OVERLAP_TOKENS = 600   # Token-based overlap (20% of window)
log = logging.getLogger(__name__)

# ─── helpers to split into token windows ───────────────────────────
def concat_tokens_by_encoding(sections: Sequence[Dict[str, Any]]) -> Tuple[List[int], List[int]]:
    """Convert sections to encoded tokens with page mapping."""
    if not TIKTOKEN_AVAILABLE:
        raise RuntimeError("tiktoken is required for token-based chunking")
    
    encoder = tiktoken.encoding_for_model(EMBEDDING_MODEL)
    all_tokens = []
    token_to_page = []
    
    for s in sections:
        # Be defensive - build text if missing
        if 'text' not in s:
            if 'elements' in s:
                text = '\n'.join(
                    el.get('text', '') for el in s['elements'] 
                    if el.get('text', '') and not el.get('text', '').startswith('self_ref=')
                )
            else:
                text = ""
            log.warning(f"Section missing 'text' field, built from elements: {s.get('section', 'untitled')}")
        else:
            text = s.get("text", "")
        
        # Encode text to tokens
        tokens = encoder.encode(text)
        all_tokens.extend(tokens)
        
        # Handle page mapping
        if 'page_number' in s:
            page_num = s['page_number']
        elif 'page_start' in s and 'page_end' in s:
            page_start = s['page_start']
            page_end = s['page_end']
            if page_start == page_end:
                page_num = page_start
            else:
                # For multi-page sections, distribute tokens across pages
                pages_span = page_end - page_start + 1
                tokens_per_page = max(1, len(tokens) // pages_span)
                for i, token in enumerate(tokens):
                    page = min(page_end, page_start + (i // tokens_per_page))
                    token_to_page.append(page)
                continue  # Skip the extend below since we handled it above
        else:
            page_num = 1
            log.warning(f"Section has no page info: {s.get('section', 'untitled')}")
        
        # Map all tokens in this section to the page (for single page sections)
        if 'page_start' not in s or s['page_start'] == s.get('page_end', s['page_start']):
            token_to_page.extend([page_num] * len(tokens))
    
    return all_tokens, token_to_page

def sliding_chunks_by_tokens(
    encoded_tokens: List[int], 
    token_to_page: List[int], 
    *, 
    window_tokens: int, 
    overlap_tokens: int
) -> List[Dict[str, Any]]:
    """Create sliding chunks based on token count."""
    if not TIKTOKEN_AVAILABLE:
        raise RuntimeError("tiktoken is required for token-based chunking")
    
    encoder = tiktoken.encoding_for_model(EMBEDDING_MODEL)
    step = max(1, window_tokens - overlap_tokens)
    chunks = []
    i = 0
    
    while i < len(encoded_tokens):
        start = i
        end = min(len(encoded_tokens), i + window_tokens)
        
        # Extract token chunk
        chunk_tokens = encoded_tokens[start:end]
        
        # Decode back to text
        chunk_text = encoder.decode(chunk_tokens)
        
        # Determine page range for this chunk
        page_start = token_to_page[start] if start < len(token_to_page) else 1
        page_end = token_to_page[end - 1] if end - 1 < len(token_to_page) else page_start
        
        chunk = {
            "chunk_index": len(chunks),
            "token_start": start,
            "token_end": end - 1,
            "page_start": page_start,
            "page_end": page_end,
            "text": chunk_text,
            "token_count": len(chunk_tokens)  # Actual token count
        }
        
        chunks.append(chunk)
        
        # Break if we've reached the end
        if end == len(encoded_tokens):
            break
            
        i += step
    
    return chunks

# Legacy word-based functions (kept for fallback if tiktoken unavailable)
def concat_tokens(sections:Sequence[Dict[str,Any]])->Tuple[List[str],List[int]]:
    tokens,page_map=[],[]
    for s in sections:
        # Be defensive - build text if missing
        if 'text' not in s:
            if 'elements' in s:
                text = '\n'.join(
                    el.get('text', '') for el in s['elements'] 
                    if el.get('text', '') and not el.get('text', '').startswith('self_ref=')
                )
            else:
                text = ""
            log.warning(f"Section missing 'text' field, built from elements: {s.get('section', 'untitled')}")
        else:
            text = s.get("text", "")
            
        words=text.split()
        tokens.extend(words)
        
        # Handle both page_number (from page_secs) and page_start/page_end (from logical_secs)
        if 'page_number' in s:
            # Single page section
            page_map.extend([s['page_number']]*len(words))
        elif 'page_start' in s and 'page_end' in s:
            # Multi-page section - distribute words across pages
            page_start = s['page_start']
            page_end = s['page_end']
            if page_start == page_end:
                # All on same page
                page_map.extend([page_start]*len(words))
            else:
                # Distribute words evenly across pages
                pages_span = page_end - page_start + 1
                words_per_page = max(1, len(words) // pages_span)
                for i, word in enumerate(words):
                    page = min(page_end, page_start + (i // words_per_page))
                    page_map.append(page)
        else:
            # Fallback to page 1
            page_map.extend([1]*len(words))
            log.warning(f"Section has no page info: {s.get('section', 'untitled')}")
            
    return tokens,page_map

def sliding_chunks(tokens:List[str],page_map:List[int],*,window:int,overlap:int)->List[Dict[str,Any]]:
    step=max(1,window-overlap)
    out,i=[],0
    SENT_END_RE=re.compile(r"[.!?]$")
    while i<len(tokens):
        start,end=i,min(len(tokens),i+window)
        while end<len(tokens) and not SENT_END_RE.search(tokens[end-1]) and end-start<window+256:
            end+=1
        out.append({"chunk_index":len(out),"token_start":start,"token_end":end-1,
                    "page_start":page_map[start],"page_end":page_map[end-1],
                    "text":" ".join(tokens[start:end])})
        if end==len(tokens): break
        i+=step
    return out
# ───────────────────────────────────────────────────────────────────

# 🎯 TIKTOKEN VALIDATION FUNCTIONS
def count_tiktoken_accurate(text: str) -> int:
    """Count tokens accurately using tiktoken."""
    if TIKTOKEN_AVAILABLE:
        try:
            encoder = tiktoken.encoding_for_model(EMBEDDING_MODEL)
            return len(encoder.encode(text))
        except:
            pass
    
    # Conservative fallback estimation
    return int(len(text.split()) * 1.5) + 50

def smart_split_chunk(chunk: Dict[str, Any], max_tokens: int = MAX_CHUNK_TOKENS) -> List[Dict[str, Any]]:
    """Split an oversized chunk into smaller token-compliant chunks."""
    text = chunk["text"]
    tokens = count_tiktoken_accurate(text)
    
    if tokens <= max_tokens:
        return [chunk]
    
    # Calculate how many sub-chunks we need
    num_splits = math.ceil(tokens / max_tokens)
    target_words_per_split = len(text.split()) // num_splits
    
    log.info(f"🎯 Splitting oversized chunk: {tokens} tokens → {num_splits} chunks of ~{max_tokens} tokens each")
    
    words = text.split()
    sentences = re.split(r'[.!?]+', text)
    
    sub_chunks = []
    current_words = []
    current_tokens = 0
    
    # Split by sentences when possible, otherwise by words
    for sentence in sentences:
        sentence_words = sentence.strip().split()
        if not sentence_words:
            continue
            
        sentence_tokens = count_tiktoken_accurate(sentence)
        
        # If adding this sentence would exceed limit, finalize current chunk
        if current_tokens + sentence_tokens > max_tokens and current_words:
            # Create sub-chunk
            sub_text = " ".join(current_words)
            sub_chunk = {
                "chunk_index": chunk["chunk_index"],
                "sub_chunk_index": len(sub_chunks),
                "token_start": chunk["token_start"] + len(" ".join(words[:words.index(current_words[0])])),
                "token_end": chunk["token_start"] + len(" ".join(words[:words.index(current_words[-1])])) + len(current_words[-1]),
                "page_start": chunk["page_start"],
                "page_end": chunk["page_end"],
                "text": sub_text
            }
            sub_chunks.append(sub_chunk)
            current_words = []
            current_tokens = 0
        
        # Add sentence to current chunk
        current_words.extend(sentence_words)
        current_tokens += sentence_tokens
    
    # Add final sub-chunk if there's remaining content
    if current_words:
        sub_text = " ".join(current_words)
        if count_tiktoken_accurate(sub_text) >= MIN_CHUNK_TOKENS:
            sub_chunk = {
                "chunk_index": chunk["chunk_index"],
                "sub_chunk_index": len(sub_chunks),
                "token_start": chunk["token_start"],
                "token_end": chunk["token_end"],
                "page_start": chunk["page_start"],
                "page_end": chunk["page_end"],
                "text": sub_text
            }
            sub_chunks.append(sub_chunk)
    
    log.info(f"🎯 Split complete: {len(sub_chunks)} sub-chunks created")
    return sub_chunks

def validate_and_fix_chunks(chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Validate chunks against tiktoken limits and fix oversized ones."""
    if not TIKTOKEN_AVAILABLE:
        log.warning("🎯 tiktoken not available - skipping token validation")
        return chunks
    
    valid_chunks = []
    oversized_count = 0
    total_tokens_before = 0
    total_tokens_after = 0
    
    log.info(f"🎯 TIKTOKEN VALIDATION: Processing {len(chunks)} chunks")
    
    for chunk in chunks:
        tokens = count_tiktoken_accurate(chunk["text"])
        total_tokens_before += tokens
        
        if tokens <= MAX_CHUNK_TOKENS:
            # Chunk is fine, keep as-is
            valid_chunks.append(chunk)
            total_tokens_after += tokens
        else:
            # Split oversized chunk
            log.warning(f"🎯 Oversized chunk detected: {tokens} tokens (max: {MAX_CHUNK_TOKENS})")
            oversized_count += 1
            
            split_chunks = smart_split_chunk(chunk, MAX_CHUNK_TOKENS)
            valid_chunks.extend(split_chunks)
            
            # Count tokens in split chunks
            for split_chunk in split_chunks:
                total_tokens_after += count_tiktoken_accurate(split_chunk["text"])
    
    # Final validation check
    max_tokens_found = max((count_tiktoken_accurate(chunk["text"]) for chunk in valid_chunks), default=0)
    
    log.info(f"🎯 TIKTOKEN VALIDATION COMPLETE:")
    log.info(f"   📊 Original chunks: {len(chunks)}")
    log.info(f"   📊 Final chunks: {len(valid_chunks)}")
    log.info(f"   📊 Oversized chunks split: {oversized_count}")
    log.info(f"   📊 Largest chunk: {max_tokens_found} tokens (limit: {MAX_CHUNK_TOKENS})")
    log.info(f"   ✅ GUARANTEED: All chunks ≤ {MAX_CHUNK_TOKENS} tokens")
    
    if max_tokens_found > MAX_CHUNK_TOKENS:
        log.error(f"🚨 VALIDATION FAILED: Chunk still exceeds limit!")
        raise RuntimeError(f"Chunk validation failed: {max_tokens_found} > {MAX_CHUNK_TOKENS}")
    
    return valid_chunks

# Legacy constants for fallback
WINDOW  = 768
OVERLAP = int(WINDOW*0.20)           # 154-token (20 %) overlap

# Parallel chunking for multiple documents
async def chunk_batch_async(
    json_paths: List[pathlib.Path],
    max_workers: Optional[int] = None
) -> Dict[pathlib.Path, List[Dict[str, Any]]]:
    """Chunk multiple documents in parallel with token-based chunking."""
    from .acceleration_utils import hardware
    
    results = {}
    
    # 🎯 Show token-based chunking status
    if TIKTOKEN_AVAILABLE:
        log.info(f"🎯 TOKEN-BASED CHUNKING ENABLED: Using {WINDOW_TOKENS} token windows with {OVERLAP_TOKENS} token overlap")
    else:
        log.warning(f"⚠️  TIKTOKEN NOT AVAILABLE: Falling back to word-based chunking (install: pip install tiktoken)")
    
    # CPU-bound task - use process pool
    with hardware.get_process_pool(max_workers) as executor:
        future_to_path = {
            executor.submit(chunk, path): path 
            for path in json_paths
        }
        
        from tqdm import tqdm
        from concurrent.futures import as_completed
        
        for future in tqdm(as_completed(future_to_path), 
                          total=len(json_paths), 
                          desc="Chunking documents"):
            path = future_to_path[future]
            try:
                chunks = future.result()
                results[path] = chunks
            except Exception as exc:
                log.error(f"Failed to chunk {path.name}: {exc}")
                results[path] = []
    
    # 🎯 Summary of token-based chunking results
    total_chunks = sum(len(chunks) for chunks in results.values())
    total_tokens = sum(chunk.get('token_count', count_tiktoken_accurate(chunk['text'])) 
                      for chunks in results.values() 
                      for chunk in chunks)
    log.info(f"🎯 BATCH CHUNKING COMPLETE: {total_chunks} chunks created ({total_tokens} total tokens)")
    
    return results

# Optimized chunking with better memory management
def chunk_optimized(json_path: pathlib.Path) -> List[Dict[str, Any]]:
    """Token-based optimized chunking with validation."""
    root = json_path.with_name(json_path.stem + "_chunks.json")
    if root.exists():
        existing_chunks = json.loads(root.read_text())
        # Validate existing chunks if they don't have tiktoken validation yet
        if existing_chunks and "sub_chunk_index" not in str(existing_chunks):
            log.info(f"🎯 Re-validating existing chunks in {root.name}")
            validated_chunks = validate_and_fix_chunks(existing_chunks)
            if len(validated_chunks) != len(existing_chunks):
                # Chunks were modified, save the validated version
                with open(root, 'w') as f:
                    json.dump(validated_chunks, f, indent=2, ensure_ascii=False)
                log.info(f"🎯 Updated {root.name} with validated chunks")
            return validated_chunks
        return existing_chunks
    
    # Stream large JSON files
    with open(json_path, 'r') as f:
        data = json.load(f)
    
    if "sections" not in data:
        raise RuntimeError(f"{json_path} has no 'sections' key")
    
    # Process in smaller batches to reduce memory usage
    sections = data["sections"]
    batch_size = 100  # Process 100 sections at a time
    
    if TIKTOKEN_AVAILABLE:
        # 🎯 TOKEN-BASED CHUNKING - Primary approach
        log.info(f"🎯 Using token-based chunking with {WINDOW_TOKENS} token windows")
        
        all_tokens = []
        all_token_to_page = []
        
        for i in range(0, len(sections), batch_size):
            batch = sections[i:i + batch_size]
            tokens, page_map = concat_tokens_by_encoding(batch)
            all_tokens.extend(tokens)
            all_token_to_page.extend(page_map)
        
        chunks = sliding_chunks_by_tokens(
            all_tokens, 
            all_token_to_page, 
            window_tokens=WINDOW_TOKENS, 
            overlap_tokens=OVERLAP_TOKENS
        )
        
        # Token-based chunks are already guaranteed to be within limits
        log.info(f"🎯 Token-based chunking produced {len(chunks)} chunks")
        
        # Additional validation for safety
        max_tokens = max((chunk.get('token_count', count_tiktoken_accurate(chunk['text'])) for chunk in chunks), default=0)
        if max_tokens > MAX_CHUNK_TOKENS:
            log.warning(f"🎯 Unexpected: Token-based chunk exceeds limit ({max_tokens} > {MAX_CHUNK_TOKENS}), applying fallback validation")
            chunks = validate_and_fix_chunks(chunks)
    else:
        # 🎯 FALLBACK: Word-based chunking when tiktoken unavailable
        log.warning(f"🎯 Falling back to word-based chunking (tiktoken unavailable)")
        
        all_tokens = []
        all_page_map = []
        
        for i in range(0, len(sections), batch_size):
            batch = sections[i:i + batch_size]
            toks, pmap = concat_tokens(batch)
            all_tokens.extend(toks)
            all_page_map.extend(pmap)
        
        chunks = sliding_chunks(all_tokens, all_page_map, window=WINDOW, overlap=OVERLAP)
        
        # Apply validation for word-based chunks
        if chunks:
            log.info(f"🎯 Applying validation to {len(chunks)} word-based chunks")
            chunks = validate_and_fix_chunks(chunks)
    
    # Write chunks
    if chunks:
        with open(root, 'w') as f:
            json.dump(chunks, f, indent=2, ensure_ascii=False)
        log.info("✓ %s chunks → %s", len(chunks), root.name)
        
        return chunks
    else:
        log.warning("%s – no chunks produced; file skipped", json_path.name)
        return []

# Keep original interface
def chunk(json_path: pathlib.Path) -> List[Dict[str, Any]]:
    """Original interface maintained for compatibility."""
    return chunk_optimized(json_path)

if __name__ == "__main__":
    import argparse, logging
    logging.basicConfig(level=logging.INFO, format="%(message)s")
    p=argparse.ArgumentParser(); p.add_argument("json",type=pathlib.Path)
    chunk(p.parse_args().json)


================================================================================


################################################################################
# File: scripts/graph_stages/document_linker.py
################################################################################

# File: scripts/graph_stages/document_linker.py

"""
Document Linker
Links ordinance documents to their corresponding agenda items.
"""

import logging
import re
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import json
from datetime import datetime
import PyPDF2
from groq import Groq
import os
from dotenv import load_dotenv

load_dotenv()

log = logging.getLogger('document_linker')


class DocumentLinker:
    """Links ordinance and resolution documents to agenda items."""
    
    def __init__(self,
                 groq_api_key: Optional[str] = None,
                 model: str = "qwen-qwq-32b",
                 agenda_extraction_max_tokens: int = 100000):
        """Initialize the document linker."""
        self.api_key = groq_api_key or os.getenv("GROQ_API_KEY")
        if not self.api_key:
            raise ValueError("GROQ_API_KEY not found in environment")
        
        self.client = Groq(api_key=self.api_key)
        self.model = model
        self.agenda_extraction_max_tokens = agenda_extraction_max_tokens
    
    def _parse_qwen_response(self, response_text: str) -> str:
        """Parse qwen response to extract content outside thinking tags."""
        # Remove thinking tags and their content
        # Pattern to match <thinking>...</thinking> tags
        thinking_pattern = r'<thinking>.*?</thinking>'
        cleaned_text = re.sub(thinking_pattern, '', response_text, flags=re.DOTALL)
        
        # Also remove any remaining XML-like tags that qwen might use
        cleaned_text = re.sub(r'<[^>]+>', '', cleaned_text)
        
        return cleaned_text.strip()
    
    async def link_documents_for_meeting(self, 
                                       meeting_date: str,
                                       documents_dir: Path) -> Dict[str, List[Dict]]:
        """Find and link all documents for a specific meeting date."""
        log.info(f"🔗 Linking documents for meeting date: {meeting_date}")
        log.info(f"📁 Looking for ordinances in: {documents_dir}")
        
        # Create debug directory
        debug_dir = Path("city_clerk_documents/graph_json/debug")
        debug_dir.mkdir(exist_ok=True)
        
        # Convert date format: "01.09.2024" -> "01_09_2024"
        date_underscore = meeting_date.replace(".", "_")
        
        # Log what we're searching for
        with open(debug_dir / "document_search_info.txt", 'w') as f:
            f.write(f"Meeting Date: {meeting_date}\n")
            f.write(f"Date with underscores: {date_underscore}\n")
            f.write(f"Search directory: {documents_dir}\n")
            f.write(f"Search pattern: *{date_underscore}.pdf\n")
        
        # Find all matching ordinance/resolution files
        # Pattern: YYYY-## - MM_DD_YYYY.pdf
        pattern = f"*{date_underscore}.pdf"
        matching_files = list(documents_dir.glob(pattern))
        
        # Also try without spaces in case filenames vary
        pattern2 = f"*{date_underscore}*.pdf"
        additional_files = [f for f in documents_dir.glob(pattern2) if f not in matching_files]
        matching_files.extend(additional_files)
        
        # List all files in directory for debugging
        all_files = list(documents_dir.glob("*.pdf"))
        with open(debug_dir / "all_ordinance_files.txt", 'w') as f:
            f.write(f"Total files in {documents_dir}: {len(all_files)}\n")
            for file in sorted(all_files):
                f.write(f"  - {file.name}\n")
        
        log.info(f"📄 Found {len(matching_files)} documents for date {meeting_date}")
        
        if matching_files:
            log.info("📄 Documents found:")
            for f in matching_files[:5]:
                log.info(f"   - {f.name}")
            if len(matching_files) > 5:
                log.info(f"   ... and {len(matching_files) - 5} more")
        
        # Save matched files list
        with open(debug_dir / "matched_documents.txt", 'w') as f:
            f.write(f"Documents matching date {meeting_date}:\n")
            for file in matching_files:
                f.write(f"  - {file.name}\n")
        
        # Process each document
        linked_documents = {
            "ordinances": [],
            "resolutions": []
        }
        
        for doc_path in matching_files:
            # Extract document info
            doc_info = await self._process_document(doc_path, meeting_date)
            
            if doc_info:
                # Categorize by type
                if "ordinance" in doc_info.get("title", "").lower():
                    linked_documents["ordinances"].append(doc_info)
                else:
                    linked_documents["resolutions"].append(doc_info)
        
        # Save linked documents info
        with open(debug_dir / "linked_documents.json", 'w') as f:
            json.dump(linked_documents, f, indent=2)
        
        log.info(f"✅ Linked {len(linked_documents['ordinances'])} ordinances, {len(linked_documents['resolutions'])} resolutions")
        return linked_documents
    
    async def _process_document(self, doc_path: Path, meeting_date: str) -> Optional[Dict[str, Any]]:
        """Process a single document to extract agenda item reference."""
        try:
            # Extract document number from filename (e.g., "2024-04" from "2024-04 - 01_23_2024.pdf")
            doc_match = re.match(r'^(\d{4}-\d{2})', doc_path.name)
            if not doc_match:
                log.warning(f"Could not parse document number from {doc_path.name}")
                return None
            
            document_number = doc_match.group(1)
            
            # Extract text from PDF
            text = self._extract_pdf_text(doc_path)
            if not text:
                log.warning(f"No text extracted from {doc_path.name}")
                return None
            
            # Extract agenda item code using LLM
            item_code = await self._extract_agenda_item_code(text, document_number)
            
            # Extract additional metadata
            parsed_data = self._parse_document_metadata(text)
            
            doc_info = {
                "path": str(doc_path),
                "filename": doc_path.name,
                "document_number": document_number,
                "item_code": item_code,
                "document_type": "Ordinance" if "ordinance" in text[:500].lower() else "Resolution",
                "title": self._extract_title(text),
                "parsed_data": parsed_data
            }
            
            log.info(f"📄 Processed {doc_path.name}: Item {item_code or 'NOT_FOUND'}")
            return doc_info
            
        except Exception as e:
            log.error(f"Error processing {doc_path.name}: {e}")
            return None
    
    def _extract_pdf_text(self, pdf_path: Path) -> str:
        """Extract text from PDF file."""
        try:
            with open(pdf_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                text_parts = []
                
                # Extract text from all pages
                for page in reader.pages:
                    text = page.extract_text()
                    if text:
                        text_parts.append(text)
                
                return "\n".join(text_parts)
        except Exception as e:
            log.error(f"Failed to extract text from {pdf_path.name}: {e}")
            return ""
    
    async def _extract_agenda_item_code(self, text: str, document_number: str) -> Optional[str]:
        """Extract agenda item code from document text using LLM."""
        # Create debug directory if it doesn't exist
        debug_dir = Path("city_clerk_documents/graph_json/debug")
        debug_dir.mkdir(exist_ok=True)
        
        # Send the entire document to qwen-32b
        text_excerpt = text
        
        # Save the full text being sent to LLM for debugging
        with open(debug_dir / f"llm_input_{document_number}.txt", 'w', encoding='utf-8') as f:
            f.write(f"Document: {document_number}\n")
            f.write(f"Text length: {len(text)} characters\n")
            f.write(f"Using model: {self.model}\n")
            f.write(f"Max tokens: {self.agenda_extraction_max_tokens}\n")
            f.write("\n--- FULL DOCUMENT SENT TO LLM ---\n")
            f.write(text_excerpt)
        
        log.info(f"📄 Sending full document to LLM for {document_number}: {len(text)} characters")
        
        prompt = f"""You are analyzing a City of Coral Gables ordinance document (Document #{document_number}).

Your task is to find the AGENDA ITEM CODE referenced in this document.

IMPORTANT: The agenda item can appear ANYWHERE in the document - on page 3, at the end, or anywhere else. Search the ENTIRE document carefully.

The agenda item typically appears in formats like:
- (Agenda Item: E-1)
- Agenda Item: E-3)
- (Agenda Item E-1)
- Item H-3
- H.-3. (with periods and dots)
- E.-2. (with dots)
- E-2 (without dots)
- Item E-2

Full document text:
{text_excerpt}

Respond in this EXACT format:
AGENDA_ITEM: [code] or AGENDA_ITEM: NOT_FOUND

Important: Return the code as it appears (e.g., E-2, not E.-2.)

Examples:
- If you find "(Agenda Item: E-2)" → respond: AGENDA_ITEM: E-2
- If you find "Item E-2" → respond: AGENDA_ITEM: E-2
- If you find "H.-3." → respond: AGENDA_ITEM: H-3
- If no agenda item found → respond: AGENDA_ITEM: NOT_FOUND"""
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a precise data extractor. Find and extract only the agenda item code. Search the ENTIRE document thoroughly."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_tokens=self.agenda_extraction_max_tokens  # Use 100,000 for qwen
            )
            
            raw_response = response.choices[0].message.content.strip()
            
            # Save raw LLM response for debugging
            with open(debug_dir / f"llm_response_{document_number}_raw.txt", 'w', encoding='utf-8') as f:
                f.write(raw_response)
            
            # Parse qwen response to remove thinking tags
            result = self._parse_qwen_response(raw_response)
            
            # Save cleaned response for debugging
            with open(debug_dir / f"llm_response_{document_number}_cleaned.txt", 'w', encoding='utf-8') as f:
                f.write(result)
            
            # Log the response for debugging
            log.info(f"LLM response for {document_number} (first 200 chars): {result[:200]}")
            
            # Parse the response
            if "AGENDA_ITEM:" in result:
                code = result.split("AGENDA_ITEM:")[1].strip()
                if code != "NOT_FOUND":
                    # Normalize the code (remove dots, ensure format)
                    code = self._normalize_item_code(code)
                    log.info(f"✅ Found agenda item code for {document_number}: {code}")
                    return code
                else:
                    log.warning(f"❌ LLM could not find agenda item in {document_number}")
            else:
                log.error(f"❌ Invalid LLM response format for {document_number}: {result[:100]}")
            
            return None
            
        except Exception as e:
            log.error(f"Failed to extract agenda item for {document_number}: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _normalize_item_code(self, code: str) -> str:
        """Normalize item code to consistent format."""
        if not code:
            return code
        
        # Remove trailing dots and spaces
        code = code.rstrip('. ')
        
        # Remove dots between letter and dash: "E.-1" -> "E-1"
        code = re.sub(r'([A-Z])\.(-)', r'\1\2', code)
        
        # Handle cases without dash: "E.1" -> "E-1"
        code = re.sub(r'([A-Z])\.(\d)', r'\1-\2', code)
        
        # Remove any remaining dots
        code = code.replace('.', '')
        
        return code
    
    def _extract_title(self, text: str) -> str:
        """Extract document title from text."""
        # Look for "AN ORDINANCE" or "A RESOLUTION" pattern
        title_match = re.search(r'(AN?\s+(ORDINANCE|RESOLUTION)[^.]+\.)', text[:2000], re.IGNORECASE)
        if title_match:
            return title_match.group(1).strip()
        
        # Fallback to first substantive line
        lines = text.split('\n')
        for line in lines[:20]:
            if len(line) > 20 and not line.isdigit():
                return line.strip()[:200]
        
        return "Untitled Document"
    
    def _parse_document_metadata(self, text: str) -> Dict[str, Any]:
        """Parse additional metadata from document."""
        metadata = {}
        
        # Extract date passed
        date_match = re.search(r'day\s+of\s+(\w+),?\s+(\d{4})', text)
        if date_match:
            metadata["date_passed"] = date_match.group(0)
        
        # Extract vote information
        vote_match = re.search(r'PASSED\s+AND\s+ADOPTED.*?(\d+).*?(\d+)', text, re.IGNORECASE | re.DOTALL)
        if vote_match:
            metadata["vote_details"] = {
                "ayes": vote_match.group(1),
                "nays": vote_match.group(2) if len(vote_match.groups()) > 1 else "0"
            }
        
        # Extract motion information
        motion_match = re.search(r'motion\s+(?:was\s+)?made\s+by\s+([^,]+)', text, re.IGNORECASE)
        if motion_match:
            metadata["motion"] = {"moved_by": motion_match.group(1).strip()}
        
        # Extract mayor signature
        mayor_match = re.search(r'Mayor[:\s]+([^\n]+)', text[-1000:])
        if mayor_match:
            metadata["signatories"] = {"mayor": mayor_match.group(1).strip()}
        
        return metadata


================================================================================


################################################################################
# File: scripts/graph_stages/cosmos_db_client.py
################################################################################

# File: scripts/graph_stages/cosmos_db_client.py

"""
Azure Cosmos DB Gremlin client for city clerk graph database.
Provides async operations for graph manipulation.
"""

from __future__ import annotations
import asyncio
import logging
from typing import Any, Dict, List, Optional, Union
import os
from gremlin_python.driver import client, serializer
from gremlin_python.driver.driver_remote_connection import DriverRemoteConnection
from gremlin_python.structure.graph import Graph
from dotenv import load_dotenv
import json

load_dotenv()

log = logging.getLogger('cosmos_graph_client')


class CosmosGraphClient:
    """Async client for Azure Cosmos DB Gremlin API."""
    
    def __init__(self, 
                 endpoint: Optional[str] = None,
                 key: Optional[str] = None,
                 database: Optional[str] = None,
                 container: Optional[str] = None,
                 partition_value: str = "demo"):
        """Initialize Cosmos DB client."""
        self.endpoint = endpoint or os.getenv("COSMOS_ENDPOINT")
        self.key = key or os.getenv("COSMOS_KEY")
        self.database = database or os.getenv("COSMOS_DATABASE", "cgGraph")
        self.container = container or os.getenv("COSMOS_CONTAINER", "cityClerk")
        self.partition_value = partition_value
        
        if not all([self.endpoint, self.key, self.database, self.container]):
            raise ValueError("Missing required Cosmos DB configuration")
        
        self._client = None
        self._loop = None  # Don't get the loop in __init__
    
    async def connect(self) -> None:
        """Establish connection to Cosmos DB."""
        try:
            # Get the current running loop
            self._loop = asyncio.get_running_loop()
            
            self._client = client.Client(
                f"{self.endpoint}/gremlin",
                "g",
                username=f"/dbs/{self.database}/colls/{self.container}",
                password=self.key,
                message_serializer=serializer.GraphSONSerializersV2d0()
            )
            log.info(f"✅ Connected to Cosmos DB: {self.database}/{self.container}")
        except Exception as e:
            log.error(f"❌ Failed to connect to Cosmos DB: {e}")
            raise
    
    async def _execute_query(self, query: str, bindings: Optional[Dict] = None) -> List[Any]:
        """Execute a Gremlin query asynchronously."""
        if not self._client:
            await self.connect()
        
        try:
            # Get the current event loop
            loop = asyncio.get_running_loop()
            
            # Run synchronous operation in thread pool
            future = loop.run_in_executor(
                None,
                lambda: self._client.submit(query, bindings or {})
            )
            callback = await future
            
            # Collect results
            results = []
            for result in callback:
                results.extend(result)
            
            return results
        except Exception as e:
            log.error(f"Query execution failed: {query[:100]}... Error: {e}")
            raise
    
    async def clear_graph(self) -> None:
        """Clear all vertices and edges from the graph."""
        log.warning("🗑️  Clearing entire graph...")
        try:
            # Drop all vertices (edges are automatically removed)
            await self._execute_query("g.V().drop()")
            log.info("✅ Graph cleared successfully")
        except Exception as e:
            log.error(f"Failed to clear graph: {e}")
            raise
    
    async def create_vertex(self, 
                          label: str,
                          vertex_id: str,
                          properties: Dict[str, Any],
                          update_if_exists: bool = True) -> None:
        """Create a vertex with properties, optionally updating if exists."""
        
        # Check if vertex already exists
        if await self.vertex_exists(vertex_id):
            if update_if_exists:
                # Update existing vertex
                await self.update_vertex(vertex_id, properties)
                log.info(f"Updated existing vertex: {vertex_id}")
            else:
                log.info(f"Vertex already exists, skipping: {vertex_id}")
            return
        
        # Build property chain
        prop_chain = ""
        for key, value in properties.items():
            if value is not None:
                # Handle different value types
                if isinstance(value, bool):
                    prop_chain += f".property('{key}', {str(value).lower()})"
                elif isinstance(value, (int, float)):
                    prop_chain += f".property('{key}', {value})"
                elif isinstance(value, list):
                    # Convert list to JSON string
                    json_val = json.dumps(value).replace("'", "\\'")
                    prop_chain += f".property('{key}', '{json_val}')"
                else:
                    # Escape string values
                    escaped_val = str(value).replace("'", "\\'").replace('"', '\\"')
                    prop_chain += f".property('{key}', '{escaped_val}')"
        
        # Always add partition key
        prop_chain += f".property('partitionKey', '{self.partition_value}')"
        
        query = f"g.addV('{label}').property('id', '{vertex_id}'){prop_chain}"
        
        await self._execute_query(query)

    async def update_vertex(self, vertex_id: str, properties: Dict[str, Any]) -> None:
        """Update properties of an existing vertex."""
        # Build property update chain
        prop_chain = ""
        for key, value in properties.items():
            if value is not None:
                if isinstance(value, bool):
                    prop_chain += f".property('{key}', {str(value).lower()})"
                elif isinstance(value, (int, float)):
                    prop_chain += f".property('{key}', {value})"
                elif isinstance(value, list):
                    json_val = json.dumps(value).replace("'", "\\'")
                    prop_chain += f".property('{key}', '{json_val}')"
                else:
                    escaped_val = str(value).replace("'", "\\'").replace('"', '\\"')
                    prop_chain += f".property('{key}', '{escaped_val}')"
        
        query = f"g.V('{vertex_id}'){prop_chain}"
        
        try:
            await self._execute_query(query)
            log.info(f"Updated vertex {vertex_id}")
        except Exception as e:
            log.error(f"Failed to update vertex {vertex_id}: {e}")
            raise

    async def upsert_vertex(self, 
                           label: str,
                           vertex_id: str,
                           properties: Dict[str, Any]) -> bool:
        """Create or update a vertex. Returns True if created, False if updated."""
        # Check if vertex exists
        if await self.vertex_exists(vertex_id):
            await self.update_vertex(vertex_id, properties)
            return False  # Updated
        else:
            await self.create_vertex(label, vertex_id, properties)
            return True  # Created

    async def create_edge(self,
                         from_id: str,
                         to_id: str,
                         edge_type: str,
                         properties: Optional[Dict[str, Any]] = None) -> None:
        """Create an edge between two vertices."""
        # Build property chain for edge
        prop_chain = ""
        if properties:
            for key, value in properties.items():
                if value is not None:
                    if isinstance(value, bool):
                        prop_chain += f".property('{key}', {str(value).lower()})"
                    elif isinstance(value, (int, float)):
                        prop_chain += f".property('{key}', {value})"
                    else:
                        escaped_val = str(value).replace("'", "\\'")
                        prop_chain += f".property('{key}', '{escaped_val}')"
        
        query = f"g.V('{from_id}').addE('{edge_type}').to(g.V('{to_id}')){prop_chain}"
        
        try:
            await self._execute_query(query)
        except Exception as e:
            log.error(f"Failed to create edge {from_id} -> {to_id}: {e}")
            raise
    
    async def create_edge_if_not_exists(self,
                                       from_id: str,
                                       to_id: str,
                                       edge_type: str,
                                       properties: Optional[Dict[str, Any]] = None) -> bool:
        """Create an edge if it doesn't already exist. Returns True if created."""
        # Check if edge already exists
        check_query = f"g.V('{from_id}').outE('{edge_type}').where(inV().hasId('{to_id}')).count()"
        
        try:
            result = await self._execute_query(check_query)
            exists = result[0] > 0 if result else False
            
            if not exists:
                await self.create_edge(from_id, to_id, edge_type, properties)
                return True
            else:
                log.debug(f"Edge already exists: {from_id} -[{edge_type}]-> {to_id}")
                return False
        except Exception as e:
            log.error(f"Failed to check/create edge: {e}")
            raise
    
    async def vertex_exists(self, vertex_id: str) -> bool:
        """Check if a vertex exists."""
        result = await self._execute_query(f"g.V('{vertex_id}').count()")
        return result[0] > 0 if result else False
    
    async def get_vertex(self, vertex_id: str) -> Optional[Dict]:
        """Get a vertex by ID."""
        result = await self._execute_query(f"g.V('{vertex_id}').valueMap(true)")
        return result[0] if result else None
    
    async def close(self) -> None:
        """Close the connection properly."""
        if self._client:
            try:
                # Close the client synchronously since it's not async
                self._client.close()
                self._client = None
                log.info("Connection closed")
            except Exception as e:
                log.warning(f"Error during client close: {e}")
    
    async def __aenter__(self):
        await self.connect()
        return self
    
    async def __aexit__(self, *args):
        await self.close()


================================================================================


################################################################################
# File: scripts/supabase_clear_database.py
################################################################################

# File: scripts/supabase_clear_database.py

#!/usr/bin/env python3
"""
Database Clear Utility
======================

Safely clears Supabase database tables for the Misophonia Research system.
This script will delete all data from:
- research_documents table
- documents_chunks table

⚠️  WARNING: This operation is irreversible!
"""
from __future__ import annotations
import os
import sys
import logging
from typing import Optional

from dotenv import load_dotenv
from supabase import create_client

# Load environment variables
load_dotenv()

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")

if not SUPABASE_URL or not SUPABASE_KEY:
    sys.exit("❌ Missing SUPABASE_URL or SUPABASE_SERVICE_ROLE_KEY environment variables")

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s — %(levelname)s — %(message)s"
)
log = logging.getLogger(__name__)

def init_supabase():
    """Initialize Supabase client."""
    return create_client(SUPABASE_URL, SUPABASE_KEY)

def get_table_counts(sb) -> dict:
    """Get current row counts for all tables."""
    counts = {}
    
    try:
        # Count documents
        doc_res = sb.table("research_documents").select("id", count="exact").execute()
        counts["research_documents"] = doc_res.count or 0
        
        # Count chunks
        chunk_res = sb.table("documents_chunks").select("id", count="exact").execute()
        counts["documents_chunks"] = chunk_res.count or 0
        
        # Count chunks with embeddings
        embedded_res = sb.table("documents_chunks").select("id", count="exact").not_.is_("embedding", "null").execute()
        counts["chunks_with_embeddings"] = embedded_res.count or 0
        
    except Exception as e:
        log.error(f"Error getting table counts: {e}")
        return {}
    
    return counts

def confirm_deletion() -> bool:
    """Ask user for confirmation before deletion."""
    print("\n" + "="*60)
    print("⚠️  DATABASE CLEAR WARNING")
    print("="*60)
    print("This will permanently delete ALL data from:")
    print("  • research_documents table")
    print("  • documents_chunks table")
    print("  • All embeddings and metadata")
    print("\n❌ This operation CANNOT be undone!")
    print("="*60)
    
    response = input("\nType 'DELETE ALL DATA' to confirm (or anything else to cancel): ")
    return response.strip() == "DELETE ALL DATA"

def clear_table_batch(sb, table_name: str, batch_size: int = 1000) -> int:
    """Clear all rows from a specific table in batches to avoid timeouts."""
    log.info(f"Clearing table: {table_name} (batch size: {batch_size})")
    
    total_deleted = 0
    
    while True:
        try:
            # Get a batch of IDs to delete
            result = sb.table(table_name).select("id").limit(batch_size).execute()
            
            if not result.data or len(result.data) == 0:
                break
            
            ids_to_delete = [row["id"] for row in result.data]
            log.info(f"Deleting batch of {len(ids_to_delete)} rows from {table_name}")
            
            # Delete this batch
            delete_result = sb.table(table_name).delete().in_("id", ids_to_delete).execute()
            
            if hasattr(delete_result, 'error') and delete_result.error:
                log.error(f"Error deleting batch from {table_name}: {delete_result.error}")
                break
            
            batch_deleted = len(delete_result.data) if delete_result.data else 0
            total_deleted += batch_deleted
            log.info(f"✅ Deleted {batch_deleted} rows from {table_name} (total: {total_deleted})")
            
            # If we deleted fewer than the batch size, we're done
            if batch_deleted < batch_size:
                break
                
        except Exception as e:
            log.error(f"Exception deleting batch from {table_name}: {e}")
            break
    
    log.info(f"✅ Total deleted from {table_name}: {total_deleted}")
    return total_deleted

def clear_table(sb, table_name: str) -> int:
    """Clear all rows from a specific table."""
    return clear_table_batch(sb, table_name, batch_size=500)

def main():
    """Main function to clear the database."""
    print("🗑️  Misophonia Database Clear Utility")
    print("=" * 50)
    
    # Initialize Supabase
    sb = init_supabase()
    
    # Get current counts
    print("\n📊 Current database status:")
    counts = get_table_counts(sb)
    
    if not counts:
        print("❌ Could not retrieve database counts. Exiting.")
        return
    
    print(f"  • Documents: {counts['research_documents']:,}")
    print(f"  • Chunks: {counts['documents_chunks']:,}")
    print(f"  • Chunks with embeddings: {counts['chunks_with_embeddings']:,}")
    
    if counts['research_documents'] == 0 and counts['documents_chunks'] == 0:
        print("\n✅ Database is already empty!")
        return
    
    # Get confirmation
    if not confirm_deletion():
        print("\n✅ Operation cancelled. Database unchanged.")
        return
    
    print("\n🗑️  Starting database clear operation...")
    
    # Clear chunks first (has foreign key to documents)
    chunks_deleted = clear_table(sb, "documents_chunks")
    
    # Clear documents
    docs_deleted = clear_table(sb, "research_documents")
    
    # Verify deletion
    print("\n📊 Verifying deletion...")
    final_counts = get_table_counts(sb)
    
    if final_counts:
        print(f"  • Documents remaining: {final_counts['research_documents']:,}")
        print(f"  • Chunks remaining: {final_counts['documents_chunks']:,}")
        
        if final_counts['research_documents'] == 0 and final_counts['documents_chunks'] == 0:
            print("\n✅ Database successfully cleared!")
            print(f"  • Deleted {docs_deleted:,} documents")
            print(f"  • Deleted {chunks_deleted:,} chunks")
        else:
            print("\n⚠️  Some data may remain in the database.")
    else:
        print("❌ Could not verify deletion status.")

if __name__ == "__main__":
    main()


================================================================================


################################################################################
# File: scripts/test_vector_search.py
################################################################################

#!/usr/bin/env python3
################################################################################
################################################################################
"""
Vector‑search smoke‑test (Supabase edition)
==========================================

• This script tests vector search functionality using Supabase's PostgREST API.

• The script calls a SQL helper function that must exist on your database:
    public.search_research_chunks(query_text TEXT,
                                  match_count INT DEFAULT 10,
                                  similarity_threshold REAL DEFAULT 0.6)
  which should:
    1. embed the incoming `query_text`
    2. invoke your `match_documents` similarity function
    3. return the top‑`match_count` rows as
       (id UUID, text TEXT, metadata JSONB, similarity REAL)

  See README / earlier instructions for a ready‑made implementation.

Environment variables required
------------------------------
SUPABASE_URL                 – e.g. https://xxxx.supabase.co
SUPABASE_SERVICE_ROLE_KEY    – or an anon key if RLS permits the RPC
OPENAI_API_KEY               – only if your SQL helper embeds via an HTTP call
"""

from __future__ import annotations

import os
import sys
import time
from typing import Any, Dict, List

from dotenv import load_dotenv
from supabase import create_client
from openai import OpenAI

# ──────────────────────── configuration & sanity checks ───────────────────── #

load_dotenv()

# ---------- Supabase config ----------
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")   # or anon if RLS permits
sb = create_client(SUPABASE_URL, SUPABASE_KEY)

if not SUPABASE_URL or not SUPABASE_KEY:
    sys.exit(
        "❌  SUPABASE_URL / SUPABASE_SERVICE_ROLE_KEY env vars are missing.\n"
        "    export them and rerun."
    )

# Sample questions to probe the index
SAMPLE_QUERIES: List[str] = [
    "What are the symptoms of misophonia?",
    "How prevalent is misophonia in university students?",
    "What is the relationship between misophonia and hyperacusis?",
    "What treatments are effective for misophonia?",
    "How does misophonia affect quality of life?",
]

# Add this after other configuration
openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

def embed(text: str) -> List[float]:
    """Return OpenAI ada‑002 embedding (1536‑dim list of floats)."""
    resp = openai_client.embeddings.create(
        model="text-embedding-ada-002",
        input=text,
    )
    return resp.data[0].embedding

# ─────────────────────────── helper functions ─────────────────────────────── #


def perform_vector_search(query_vec, top_k=5, thresh=0.6):
    """
    Call the SQL RPC we just created.
    `query_vec` is a list[float] length 1536 coming from OpenAI.
    """
    try:
        resp = sb.rpc(
            "search_documents_chunks",
            {
                "query_embedding": query_vec,
                "match_count": top_k,
                "similarity_threshold": thresh,
            },
        ).execute()
        if getattr(resp, "error", None):
            raise RuntimeError(resp.error)
        return resp.data or []
    except Exception as e:
        print(f"   ⚠  RPC failed: {e}")
        return []


def print_results(rows: List[Dict[str, Any]]) -> None:
    """
    Nicely format the search results.
    """
    if not rows:
        print("   (no matches)\n")
        return

    for idx, row in enumerate(rows, 1):
        meta = row.get("metadata", {}) or {}
        title = meta.get("title", "Unknown title")
        year = meta.get("year", "????")
        author = meta.get("primary_author", "Unknown author")
        sim = row.get("similarity", 0.0)

        snippet = (row.get("text", "") or "").replace("\n", " ")[:280] + "…"

        print(f"\nResult {idx}  •  sim={sim:.3f}")
        print(f"  {title} — {author} ({year})")
        print(f"  {snippet}")


# ────────────────────────────────── main ──────────────────────────────────── #


def main() -> None:
    print("\n🔍  Supabase vector search smoke‑test\n" + "—" * 60)
    for i, q in enumerate(SAMPLE_QUERIES, 1):
        print(f'\nQuery {i + 1}/{len(SAMPLE_QUERIES)}: "{q}"')


        # 1. get the vector
        print("Generating embedding...")
        query_vec = embed(q)  # Python list[float]

        # 2. PostgREST / Postgres expects a *string* like: [0.1,0.2,…]
        vec_literal = "[" + ",".join(f"{x:.6f}" for x in query_vec) + "]"

        # 3. call the RPC
        print("Performing vector search...")
        resp = sb.rpc(
            "search_documents_chunks",
            {
                "query_embedding": vec_literal,  # <- NOT the raw text
                "match_count": 5,
                "similarity_threshold": 0.6,
            },
        ).execute()
        
        if getattr(resp, "error", None):
            print(f"   ⚠  RPC failed: {resp.error}\n")
            continue
            
        results = resp.data or []
        print_results(results)

        if i < len(SAMPLE_QUERIES):
            print("\nPausing 2 s before the next query …")
            time.sleep(2)

    print("\n✔  Done")


if __name__ == "__main__":
    main()


================================================================================


################################################################################
# File: scripts/debug_ordinance_2024_02.py
################################################################################

# File: scripts/debug_ordinance_2024_02.py

#!/usr/bin/env python3
"""
Debug script to analyze ordinance 2024-02 and find why agenda item extraction failed.
"""

import PyPDF2
from pathlib import Path
import re

def analyze_ordinance_2024_02():
    """Analyze the problematic ordinance document."""
    
    # Find the ordinance file
    ordinance_dir = Path("city_clerk_documents/global/City Comissions 2024/Ordinances/2024")
    ordinance_files = list(ordinance_dir.glob("2024-02*.pdf"))
    
    if not ordinance_files:
        print("❌ Could not find ordinance 2024-02")
        return
    
    ordinance_path = ordinance_files[0]
    print(f"📄 Analyzing: {ordinance_path.name}")
    
    # Extract text
    try:
        with open(ordinance_path, 'rb') as f:
            reader = PyPDF2.PdfReader(f)
            full_text = ""
            
            print(f"📊 Document has {len(reader.pages)} pages")
            
            # Extract all text
            for i, page in enumerate(reader.pages):
                page_text = page.extract_text()
                full_text += page_text
                
                # Check last few pages for agenda items
                if i >= len(reader.pages) - 3:
                    print(f"\n--- Page {i+1} (checking for agenda items) ---")
                    # Look for agenda item patterns
                    agenda_patterns = [
                        r'\(Agenda Item[:\s]*([A-Z][\.-]?\d+)\)',
                        r'Agenda Item[:\s]*([A-Z][\.-]?\d+)',
                        r'Item[:\s]*([A-Z][\.-]?\d+)',
                        r'([A-Z][\.-]?\d+[\.-]?)\s*\n',  # E.-2. at end of line
                    ]
                    
                    for pattern in agenda_patterns:
                        matches = re.findall(pattern, page_text, re.IGNORECASE)
                        if matches:
                            print(f"🎯 Found potential agenda items: {matches}")
            
            # Check the last 2000 characters
            print(f"\n--- Last 2000 characters of document ---")
            print(full_text[-2000:])
            
            # Search entire document for agenda patterns
            print(f"\n--- Searching entire document for agenda patterns ---")
            print(f"Total document length: {len(full_text)} characters")
            
            # More comprehensive search
            comprehensive_patterns = [
                r'\(Agenda\s+Item[:\s]*([A-Z][\.-]?\d+[\.-]?)\)',
                r'Agenda\s+Item[:\s]*([A-Z][\.-]?\d+[\.-]?)',
                r'Item[:\s]*([A-Z][\.-]?\d+[\.-]?)',
                r'([A-Z])\.-(\d+)\.',  # E.-2.
                r'([A-Z])-(\d+)',      # E-2
            ]
            
            for pattern in comprehensive_patterns:
                matches = re.findall(pattern, full_text, re.IGNORECASE | re.MULTILINE)
                if matches:
                    print(f"Pattern '{pattern}' found: {matches}")
            
            # Save full text for manual inspection
            debug_dir = Path("city_clerk_documents/graph_json/debug")
            debug_dir.mkdir(exist_ok=True)
            
            with open(debug_dir / "ordinance_2024_02_full_text.txt", 'w', encoding='utf-8') as f:
                f.write(full_text)
            
            print(f"\n✅ Full text saved to: {debug_dir / 'ordinance_2024_02_full_text.txt'}")
            
    except Exception as e:
        print(f"❌ Error analyzing document: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    analyze_ordinance_2024_02()


================================================================================


################################################################################
# File: scripts/debug_agenda_items.py
################################################################################

# File: scripts/debug_agenda_items.py

#!/usr/bin/env python3
"""
Debug script to check what agenda items were extracted from the agenda.
"""

import json
from pathlib import Path

def check_agenda_items():
    """Check what agenda items were extracted."""
    
    # Find the ontology file
    ontology_dir = Path("city_clerk_documents/graph_json")
    ontology_files = list(ontology_dir.glob("*01.9.2024*_ontology.json"))
    
    if not ontology_files:
        print("❌ No ontology file found for 01.09.2024")
        return
    
    ontology_file = ontology_files[0]
    print(f"📄 Checking ontology: {ontology_file.name}")
    
    # Load the ontology
    with open(ontology_file, 'r') as f:
        ontology = json.load(f)
    
    print(f"\n📅 Meeting date: {ontology.get('meeting_date')}")
    
    # Extract all agenda items
    all_items = []
    agenda_structure = ontology.get('agenda_structure', [])
    
    print(f"\n📑 Found {len(agenda_structure)} sections:")
    
    for section in agenda_structure:
        section_name = section.get('section_name', 'Unknown')
        items = section.get('items', [])
        print(f"\n  Section: {section_name}")
        print(f"  Items in section: {len(items)}")
        
        for item in items:
            item_code = item.get('item_code', 'NO_CODE')
            title = item.get('title', 'No title')[:80]
            all_items.append({
                'code': item_code,
                'title': title,
                'section': section_name
            })
            print(f"    - {item_code}: {title}...")
    
    print(f"\n📊 Total agenda items found: {len(all_items)}")
    
    # Check specifically for E items
    e_items = [item for item in all_items if item['code'].startswith('E')]
    print(f"\n🔍 E-section items found:")
    for item in sorted(e_items, key=lambda x: x['code']):
        print(f"  - {item['code']}: {item['title'][:60]}...")
    
    # Check if E-2 exists
    e2_exists = any(item['code'] in ['E-2', 'E.-2.', 'E.-2', 'E.2'] for item in all_items)
    print(f"\n❓ Does E-2 exist in agenda? {e2_exists}")
    
    # Save all items for inspection
    debug_dir = Path("city_clerk_documents/graph_json/debug")
    debug_dir.mkdir(exist_ok=True)
    
    with open(debug_dir / "all_agenda_items.json", 'w') as f:
        json.dump(all_items, f, indent=2)
    
    print(f"\n✅ All agenda items saved to: {debug_dir / 'all_agenda_items.json'}")

if __name__ == "__main__":
    check_agenda_items()


================================================================================


################################################################################
# File: scripts/clear_meeting_data.py
################################################################################

# File: scripts/clear_meeting_data.py

#!/usr/bin/env python3
"""
Clear data for specific meetings without affecting the entire graph.
"""
import asyncio
import argparse
from pathlib import Path
from graph_stages.cosmos_db_client import CosmosGraphClient

async def clear_meeting_data(meeting_date: str):
    """Clear all data related to a specific meeting."""
    # Convert date format
    meeting_date_dashes = meeting_date.replace('.', '-')
    meeting_id = f"meeting-{meeting_date_dashes}"
    
    async with CosmosGraphClient() as cosmos:
        # First, get all connected nodes
        query = f"""
        g.V('{meeting_id}')
          .union(
            __.identity(),
            __.out('HAS_SECTION'),
            __.out('HAS_SECTION').out('CONTAINS_ITEM'),
            __.out('HAS_SECTION').out('CONTAINS_ITEM').out('REFERENCES_DOCUMENT')
          )
          .dedup()
          .id()
        """
        
        try:
            node_ids = await cosmos._execute_query(query)
            
            if node_ids:
                print(f"Found {len(node_ids)} nodes to delete for meeting {meeting_date}")
                
                # Delete all nodes (edges are automatically removed)
                for node_id in node_ids:
                    delete_query = f"g.V('{node_id}').drop()"
                    await cosmos._execute_query(delete_query)
                
                print(f"✅ Cleared all data for meeting {meeting_date}")
            else:
                print(f"No data found for meeting {meeting_date}")
                
        except Exception as e:
            print(f"❌ Error clearing meeting data: {e}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("meeting_date", help="Meeting date (e.g., 01.09.2024)")
    args = parser.parse_args()
    
    asyncio.run(clear_meeting_data(args.meeting_date))


================================================================================


################################################################################
# File: scripts/clear_database.py
################################################################################

#!/usr/bin/env python3
"""
Clear Cosmos DB Graph Database
This script will clear all vertices and edges from the graph database.
"""

import asyncio
import sys
import os
from pathlib import Path

# Add scripts directory to path
script_dir = Path(__file__).parent
sys.path.append(str(script_dir))

from graph_stages.cosmos_db_client import CosmosGraphClient


async def clear_database():
    """Clear the entire graph database."""
    print('🗑️  Clearing Cosmos DB graph database...')
    print('⚠️  This will delete ALL vertices and edges!')
    
    client = None
    try:
        # Create and connect client
        client = CosmosGraphClient()
        await client.connect()
        
        print('🗑️  Clearing entire graph...')
        await client.clear_graph()
        
        print('✅ Graph database cleared successfully!')
        return True
        
    except Exception as e:
        print(f'❌ Error clearing database: {e}')
        return False
    
    finally:
        # Ensure proper cleanup
        if client:
            await client.close()


def main():
    """Main entry point with proper async handling."""
    # Create new event loop
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    
    try:
        # Run the async function
        success = loop.run_until_complete(clear_database())
        
        # Exit with appropriate code
        if not success:
            sys.exit(1)
            
    finally:
        # Clean up the loop
        loop.close()


if __name__ == "__main__":
    main()


================================================================================


################################################################################
# File: requirements.txt
################################################################################

################################################################################
################################################################################
# Python dependencies for Misophonia Research RAG System

# Core dependencies
openai>=1.82.0
groq>=0.4.1
gremlinpython>=3.7.3
aiofiles>=24.1.0
python-dotenv>=1.0.0

# Graph Visualization dependencies
dash>=2.14.0
plotly>=5.17.0
networkx>=3.2.0
dash-table>=5.0.0
dash-cytoscape>=0.3.0

# Database and API dependencies
supabase>=2.0.0

# Optional for async progress bars
tqdm

# Data processing
pandas>=2.0.0
numpy

# PDF processing
PyPDF2>=3.0.0
unstructured[pdf]==0.10.30
pytesseract>=0.3.10
pdf2image>=1.16.3
docling
pdfminer.six>=20221105

# PDF processing with hyperlink support
PyMuPDF>=1.23.0

# Utilities
colorama>=0.4.6

# For concurrent processing
concurrent-log-handler>=0.9.20

# Async dependencies for optimized pipeline
aiohttp>=3.8.0

# Token counting for OpenAI rate limiting
tiktoken>=0.5.0


================================================================================

