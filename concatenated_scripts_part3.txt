# Concatenated Project Code - Part 3 of 3
# Generated: 2025-06-10 15:21:54
# Root Directory: /Users/gianmariatroiani/Documents/knologi/graph_database
================================================================================

# File Index - Which Files Are in Which Parts
################################################################################

## Part 1 (10 files):
  - scripts/graph_stages/verbatim_transcript_linker.py
  - scripts/microsoft_framework/prompt_tuner.py
  - scripts/microsoft_framework/README.md
  - scripts/microsoft_framework/graphrag_initializer.py
  - extract_documents_for_graphrag.py
  - explore_graphrag_sources.py
  - scripts/microsoft_framework/graphrag_pipeline.py
  - scripts/microsoft_framework/incremental_processor.py
  - requirements.txt
  - scripts/graph_stages/__init__.py

## Part 2 (10 files):
  - scripts/microsoft_framework/run_graphrag_pipeline.py
  - scripts/graph_stages/cosmos_db_client.py
  - scripts/extract_all_to_markdown.py
  - scripts/extract_all_pdfs_direct.py
  - check_status.py
  - investigate_graph.py
  - settings.yaml
  - check_ordinances.py
  - scripts/microsoft_framework/__init__.py
  - scripts/microsoft_framework/run_graphrag_direct.py

## Part 3 (10 files):
  - scripts/microsoft_framework/query_router.py
  - scripts/graph_stages/document_linker.py
  - scripts/graph_stages/pdf_extractor.py
  - scripts/microsoft_framework/create_custom_prompts.py
  - scripts/microsoft_framework/cosmos_synchronizer.py
  - scripts/microsoft_framework/graphrag_output_processor.py
  - scripts/microsoft_framework/city_clerk_settings_template.yaml
  - config.py
  - scripts/json_to_markdown_converter.py
  - scripts/microsoft_framework/query_graphrag.py


================================================================================


################################################################################
# File: scripts/microsoft_framework/query_router.py
################################################################################

# File: scripts/microsoft_framework/query_router.py

from enum import Enum
from typing import Dict, Any, Optional, List, Tuple
import re
import logging

logger = logging.getLogger(__name__)

class QueryIntent(Enum):
    ENTITY_SPECIFIC = "entity_specific"  # Use Local
    HOLISTIC = "holistic"               # Use Global  
    EXPLORATORY = "exploratory"         # Use DRIFT
    TEMPORAL = "temporal"               # Use DRIFT

class QueryFocus(Enum):
    SPECIFIC_ENTITY = "specific_entity"          # User wants info about ONE specific item
    MULTIPLE_SPECIFIC = "multiple_specific"      # User wants info about MULTIPLE specific items
    COMPARISON = "comparison"                    # User wants to compare entities
    CONTEXTUAL = "contextual"                    # User wants relationships/context
    GENERAL = "general"                          # No specific entity mentioned

class SmartQueryRouter:
    """Automatically route queries to the optimal search method with intelligent intent detection."""
    
    def __init__(self):
        # Entity extraction patterns
        self.entity_patterns = {
            'agenda_item': [
                r'(?:agenda\s+)?(?:item|items)\s+([A-Z]-?\d+)',
                r'(?:item|items)\s+([A-Z]-?\d+)',
                r'([A-Z]-\d+)(?:\s+agenda)?',
                r'\b([A-Z]-\d+)\b'  # Just the code itself
            ],
            'ordinance': [
                r'ordinance(?:\s+(?:number|no\.?|#))?\s*(\d{4}-\d+|\d+)',
                r'(?:city\s+)?ordinance\s+(\d{4}-\d+|\d+)',
                r'\b(\d{4}-\d+)\b(?=.*ordinance)',
                r'ordinance\s+(\w+)'
            ],
            'resolution': [
                r'resolution(?:\s+(?:number|no\.?|#))?\s*(\d{4}-\d+|\d+)',
                r'(?:city\s+)?resolution\s+(\d{4}-\d+|\d+)',
                r'\b(\d{4}-\d+)\b(?=.*resolution)',
                r'resolution\s+(\w+)'
            ]
        }
        
        # Intent indicators
        self.specific_indicators = {
            'singular_determiners': ['the', 'this', 'that', 'a', 'an'],
            'identity_verbs': ['is', 'are', 'was', 'were', 'means', 'mean', 'refers to', 'refer to', 'concerns', 'concern', 'about'],
            'detail_nouns': ['details', 'information', 'content', 'text', 'provision', 'provisions', 'summary', 'summaries', 'description', 'descriptions'],
            'specific_question_words': ['what', 'which', 'show', 'tell', 'explain', 'describe', 'list'],
            'limiting_adverbs': ['only', 'just', 'specifically', 'exactly', 'precisely', 'individually', 'separately']
        }
        
        self.comparison_indicators = {
            'comparison_verbs': ['compare', 'contrast', 'differ', 'differentiate', 'distinguish'],
            'comparison_words': ['versus', 'vs', 'against', 'compared to', 'difference', 'differences', 'similarity', 'similarities'],
            'comparison_phrases': ['how do', 'what is the difference', 'what are the differences']
        }
        
        self.contextual_indicators = {
            'plural_forms': ['items', 'ordinances', 'resolutions', 'documents'],
            'relationship_words': ['related', 'connected', 'associated', 'linked', 'relationship', 
                                  'connections', 'references', 'mentions', 'together', 'context',
                                  'affects', 'impacts', 'influences', 'between', 'among'],
            'exploration_verbs': ['explore', 'analyze', 'understand', 'investigate'],
            'scope_expanders': ['all', 'other', 'various', 'multiple', 'several', 'any']
        }
        
        # Holistic patterns for global search
        self.holistic_patterns = [
            r"what are the (?:main|top|key) (themes|topics|issues)",
            r"summarize (?:the|all) (.*)",
            r"overall (.*)",
            r"trends in (.*)",
            r"patterns across (.*)"
        ]
        
        # Temporal patterns for drift search
        self.temporal_patterns = [
            r"how has (.*) (?:changed|evolved)",
            r"timeline of (.*)",
            r"history of (.*)",
            r"development of (.*) over time",
            r"evolution of (.*)",
            r"changes in (.*)"
        ]
    
    def determine_query_method(self, query: str) -> Dict[str, Any]:
        """Intelligently determine query intent and method."""
        query_lower = query.lower()
        
        # First check for holistic queries (global search)
        for pattern in self.holistic_patterns:
            if re.search(pattern, query_lower):
                return {
                    "method": "global",
                    "intent": QueryIntent.HOLISTIC,
                    "params": {
                        "community_level": self._determine_community_level(query),
                        "response_type": "multiple paragraphs"
                    }
                }
        
        # Check for temporal/exploratory queries (drift search)
        for pattern in self.temporal_patterns:
            if re.search(pattern, query_lower):
                return {
                    "method": "drift",
                    "intent": QueryIntent.TEMPORAL,
                    "params": {
                        "initial_community_level": 2,
                        "max_follow_ups": 5
                    }
                }
        
        # Extract ALL entity references
        all_entities = self._extract_all_entities(query)
        
        if not all_entities:
            # No specific entity found - use local search as default
            return {
                "method": "local",
                "intent": QueryIntent.EXPLORATORY,
                "params": {
                    "top_k_entities": 10,
                    "include_community_context": True
                }
            }
        
        # Handle single entity
        if len(all_entities) == 1:
            entity_info = all_entities[0]
            query_focus = self._determine_single_entity_focus(query_lower, entity_info)
            
            if query_focus == QueryFocus.SPECIFIC_ENTITY:
                return {
                    "method": "local",
                    "intent": QueryIntent.ENTITY_SPECIFIC,
                    "params": {
                        "entity_filter": {
                            "type": entity_info['type'].upper(),
                            "value": entity_info['value']
                        },
                        "top_k_entities": 1,
                        "include_community_context": False,
                        "strict_entity_focus": True,
                        "disable_community": True
                    }
                }
            else:  # CONTEXTUAL
                return {
                    "method": "local",
                    "intent": QueryIntent.ENTITY_SPECIFIC,
                    "params": {
                        "entity_filter": {
                            "type": entity_info['type'].upper(),
                            "value": entity_info['value']
                        },
                        "top_k_entities": 10,
                        "include_community_context": True,
                        "strict_entity_focus": False,
                        "disable_community": False
                    }
                }
        
        # Handle multiple entities
        else:
            query_focus = self._determine_multi_entity_focus(query_lower, all_entities)
            
            if query_focus == QueryFocus.MULTIPLE_SPECIFIC:
                # User wants specific info about each entity separately
                return {
                    "method": "local",
                    "intent": QueryIntent.ENTITY_SPECIFIC,
                    "params": {
                        "multiple_entities": all_entities,
                        "top_k_entities": 1,
                        "include_community_context": False,
                        "strict_entity_focus": True,
                        "disable_community": True,
                        "aggregate_results": True  # Combine results for each entity
                    }
                }
            elif query_focus == QueryFocus.COMPARISON:
                # User wants to compare entities
                return {
                    "method": "local",
                    "intent": QueryIntent.ENTITY_SPECIFIC,
                    "params": {
                        "multiple_entities": all_entities,
                        "top_k_entities": 5,
                        "include_community_context": True,  # Need context for comparison
                        "strict_entity_focus": False,
                        "disable_community": False,
                        "comparison_mode": True
                    }
                }
            else:  # CONTEXTUAL
                # User wants relationships between entities
                return {
                    "method": "local",
                    "intent": QueryIntent.ENTITY_SPECIFIC,
                    "params": {
                        "multiple_entities": all_entities,
                        "top_k_entities": 10,
                        "include_community_context": True,
                        "strict_entity_focus": False,
                        "disable_community": False,
                        "focus_on_relationships": True
                    }
                }
    
    def _extract_all_entities(self, query: str) -> List[Dict[str, str]]:
        """Extract ALL entity references from query."""
        query_lower = query.lower()
        entities = []
        found_positions = {}  # Track positions to avoid duplicates
        
        for entity_type, patterns in self.entity_patterns.items():
            for pattern in patterns:
                for match in re.finditer(pattern, query_lower, re.IGNORECASE):
                    value = match.group(1)
                    position = match.start()
                    
                    # Normalize value
                    if entity_type == 'agenda_item':
                        value = value.upper()
                        if not '-' in value and len(value) > 1:
                            value = f"{value[0]}-{value[1:]}"
                    
                    # Check if we already found an entity at this position
                    if position not in found_positions:
                        found_positions[position] = True
                        entities.append({
                            'type': entity_type,
                            'value': value,
                            'position': position
                        })
        
        # Sort by position and remove position info
        entities = sorted(entities, key=lambda x: x['position'])
        for entity in entities:
            del entity['position']
        
        # Remove duplicates while preserving order
        seen = set()
        unique_entities = []
        for entity in entities:
            key = f"{entity['type']}:{entity['value']}"
            if key not in seen:
                seen.add(key)
                unique_entities.append(entity)
        
        return unique_entities
    
    def _determine_single_entity_focus(self, query_lower: str, entity_info: Dict) -> QueryFocus:
        """Determine focus for single entity queries."""
        specific_score = 0
        contextual_score = 0
        
        tokens = query_lower.split()
        
        # Check for limiting words
        for word in self.specific_indicators['limiting_adverbs']:
            if word in tokens:
                specific_score += 3
        
        # Check for relationship words
        for word in self.contextual_indicators['relationship_words']:
            if word in tokens:
                contextual_score += 3
        
        # Simple "what is X" patterns
        if re.match(r'^(what|whats|what\'s)\s+(is|are)\s+', query_lower):
            specific_score += 2
        
        # Very short queries tend to be specific
        if len(tokens) <= 3:
            specific_score += 2
        
        # Check for detail-seeking patterns
        for noun in self.specific_indicators['detail_nouns']:
            if noun in query_lower:
                specific_score += 1
        
        logger.info(f"Single entity focus scores - Specific: {specific_score}, Contextual: {contextual_score}")
        
        return QueryFocus.SPECIFIC_ENTITY if specific_score > contextual_score else QueryFocus.CONTEXTUAL
    
    def _determine_multi_entity_focus(self, query_lower: str, entities: List[Dict]) -> QueryFocus:
        """Determine focus for multi-entity queries."""
        tokens = query_lower.split()
        
        # Check for comparison indicators
        comparison_score = 0
        for verb in self.comparison_indicators['comparison_verbs']:
            if verb in tokens:
                comparison_score += 3
        
        for word in self.comparison_indicators['comparison_words']:
            if word in query_lower:
                comparison_score += 2
        
        for phrase in self.comparison_indicators['comparison_phrases']:
            if phrase in query_lower:
                comparison_score += 2
        
        # Check for specific information indicators
        specific_score = 0
        
        # "What are E-1 and E-2?" suggests wanting specific info
        if re.match(r'^(what|whats|what\'s)\s+(are|is)\s+', query_lower):
            specific_score += 2
        
        # Check for "separately" or "individually"
        if any(word in tokens for word in ['separately', 'individually', 'each']):
            specific_score += 3
        
        # Check for detail nouns with plural entities
        for noun in self.specific_indicators['detail_nouns']:
            if noun in query_lower:
                specific_score += 1
        
        # Check for contextual/relationship indicators
        contextual_score = 0
        for word in self.contextual_indicators['relationship_words']:
            if word in tokens:
                contextual_score += 2
        
        # Check for "and" patterns that suggest relationships
        # e.g., "relationship between E-1 and E-2"
        if re.search(r'between.*and', query_lower):
            contextual_score += 3
        
        logger.info(f"Multi-entity focus scores - Comparison: {comparison_score}, Specific: {specific_score}, Contextual: {contextual_score}")
        
        # Determine focus based on highest score
        if comparison_score >= specific_score and comparison_score >= contextual_score:
            return QueryFocus.COMPARISON
        elif specific_score > contextual_score:
            return QueryFocus.MULTIPLE_SPECIFIC
        else:
            return QueryFocus.CONTEXTUAL
    
    def _determine_community_level(self, query: str) -> int:
        """Determine optimal community level based on query scope."""
        if any(word in query.lower() for word in ["entire", "all", "overall", "whole"]):
            return 0  # Highest level
        elif any(word in query.lower() for word in ["department", "district", "area"]):
            return 1  # Mid level
        else:
            return 2  # Lower level for more specific summaries


================================================================================


################################################################################
# File: scripts/graph_stages/document_linker.py
################################################################################

# File: scripts/graph_stages/document_linker.py

"""
Document Linker
Links ordinance documents to their corresponding agenda items.
"""

import logging
import re
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import json
from datetime import datetime
import PyPDF2
from openai import OpenAI
import os
from dotenv import load_dotenv

load_dotenv()

log = logging.getLogger('document_linker')


class DocumentLinker:
    """Links ordinance and resolution documents to agenda items."""
    
    def __init__(self,
                 openai_api_key: Optional[str] = None,
                 model: str = "gpt-4.1-mini-2025-04-14",
                 agenda_extraction_max_tokens: int = 32768):
        """Initialize the document linker."""
        self.api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY not found in environment")
        
        self.client = OpenAI(api_key=self.api_key)
        self.model = model
        self.agenda_extraction_max_tokens = agenda_extraction_max_tokens
    
    async def link_documents_for_meeting(self, 
                                       meeting_date: str,
                                       documents_dir: Path) -> Dict[str, List[Dict]]:
        """Find and link all documents for a specific meeting date."""
        log.info(f"ðŸ”— Linking documents for meeting date: {meeting_date}")
        log.info(f"ðŸ“ Looking for ordinances in: {documents_dir}")
        
        # Create debug directory
        debug_dir = Path("city_clerk_documents/graph_json/debug")
        debug_dir.mkdir(exist_ok=True)
        
        # Convert date format: "01.09.2024" -> "01_09_2024"
        date_underscore = meeting_date.replace(".", "_")
        
        # Log what we're searching for
        with open(debug_dir / "document_search_info.txt", 'w') as f:
            f.write(f"Meeting Date: {meeting_date}\n")
            f.write(f"Date with underscores: {date_underscore}\n")
            f.write(f"Search directory: {documents_dir}\n")
            f.write(f"Search pattern: *{date_underscore}.pdf\n")
        
        # Find all matching ordinance/resolution files
        # Pattern: YYYY-## - MM_DD_YYYY.pdf
        pattern = f"*{date_underscore}.pdf"
        matching_files = list(documents_dir.glob(pattern))
        
        # Also try without spaces in case filenames vary
        pattern2 = f"*{date_underscore}*.pdf"
        additional_files = [f for f in documents_dir.glob(pattern2) if f not in matching_files]
        matching_files.extend(additional_files)
        
        # List all files in directory for debugging
        all_files = list(documents_dir.glob("*.pdf"))
        with open(debug_dir / "all_ordinance_files.txt", 'w') as f:
            f.write(f"Total files in {documents_dir}: {len(all_files)}\n")
            for file in sorted(all_files):
                f.write(f"  - {file.name}\n")
        
        log.info(f"ðŸ“„ Found {len(matching_files)} documents for date {meeting_date}")
        
        if matching_files:
            log.info("ðŸ“„ Documents found:")
            for f in matching_files[:5]:
                log.info(f"   - {f.name}")
            if len(matching_files) > 5:
                log.info(f"   ... and {len(matching_files) - 5} more")
        
        # Save matched files list
        with open(debug_dir / "matched_documents.txt", 'w') as f:
            f.write(f"Documents matching date {meeting_date}:\n")
            for file in matching_files:
                f.write(f"  - {file.name}\n")
        
        # Process each document
        linked_documents = {
            "ordinances": [],
            "resolutions": []
        }
        
        for doc_path in matching_files:
            # Extract document info
            doc_info = await self._process_document(doc_path, meeting_date)
            
            if doc_info:
                # Categorize by type
                if "ordinance" in doc_info.get("title", "").lower():
                    linked_documents["ordinances"].append(doc_info)
                else:
                    linked_documents["resolutions"].append(doc_info)
        
        # Save linked documents info
        with open(debug_dir / "linked_documents.json", 'w') as f:
            json.dump(linked_documents, f, indent=2)
        
        log.info(f"âœ… Linked {len(linked_documents['ordinances'])} ordinances, {len(linked_documents['resolutions'])} resolutions")
        return linked_documents
    
    async def _process_document(self, doc_path: Path, meeting_date: str) -> Optional[Dict[str, Any]]:
        """Process a single document to extract agenda item reference."""
        try:
            # Extract document number from filename (e.g., "2024-04" from "2024-04 - 01_23_2024.pdf")
            doc_match = re.match(r'^(\d{4}-\d{2})', doc_path.name)
            if not doc_match:
                log.warning(f"Could not parse document number from {doc_path.name}")
                return None
            
            document_number = doc_match.group(1)
            
            # Extract text from PDF
            text = self._extract_pdf_text(doc_path)
            if not text:
                log.warning(f"No text extracted from {doc_path.name}")
                return None
            
            # Extract agenda item code using LLM
            item_code = await self._extract_agenda_item_code(text, document_number)
            
            # Extract additional metadata
            parsed_data = self._parse_document_metadata(text)
            
            doc_info = {
                "path": str(doc_path),
                "filename": doc_path.name,
                "document_number": document_number,
                "item_code": item_code,
                "document_type": "Ordinance" if "ordinance" in text[:500].lower() else "Resolution",
                "title": self._extract_title(text),
                "parsed_data": parsed_data
            }
            
            log.info(f"ðŸ“„ Processed {doc_path.name}: Item {item_code or 'NOT_FOUND'}")
            return doc_info
            
        except Exception as e:
            log.error(f"Error processing {doc_path.name}: {e}")
            return None
    
    def _extract_pdf_text(self, pdf_path: Path) -> str:
        """Extract text from PDF file."""
        try:
            with open(pdf_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                text_parts = []
                
                # Extract text from all pages
                for page in reader.pages:
                    text = page.extract_text()
                    if text:
                        text_parts.append(text)
                
                return "\n".join(text_parts)
        except Exception as e:
            log.error(f"Failed to extract text from {pdf_path.name}: {e}")
            return ""
    
    async def _extract_agenda_item_code(self, text: str, document_number: str) -> Optional[str]:
        """Extract agenda item code from document text using LLM."""
        # Create debug directory if it doesn't exist
        debug_dir = Path("city_clerk_documents/graph_json/debug")
        debug_dir.mkdir(exist_ok=True)
        
        # Send the entire document to qwen-32b
        text_excerpt = text
        
        # Save the full text being sent to LLM for debugging
        with open(debug_dir / f"llm_input_{document_number}.txt", 'w', encoding='utf-8') as f:
            f.write(f"Document: {document_number}\n")
            f.write(f"Text length: {len(text)} characters\n")
            f.write(f"Using model: {self.model}\n")
            f.write(f"Max tokens: {self.agenda_extraction_max_tokens}\n")
            f.write("\n--- FULL DOCUMENT SENT TO LLM ---\n")
            f.write(text_excerpt)
        
        log.info(f"ðŸ“„ Sending full document to LLM for {document_number}: {len(text)} characters")
        
        prompt = f"""You are analyzing a City of Coral Gables ordinance document (Document #{document_number}).

Your task is to find the AGENDA ITEM CODE referenced in this document.

IMPORTANT: The agenda item can appear ANYWHERE in the document - on page 3, at the end, or anywhere else. Search the ENTIRE document carefully.

The agenda item typically appears in formats like:
- (Agenda Item: E-1)
- Agenda Item: E-3)
- (Agenda Item E-1)
- Item H-3
- H.-3. (with periods and dots)
- E.-2. (with dots)
- E-2 (without dots)
- Item E-2

Full document text:
{text_excerpt}

Respond in this EXACT format:
AGENDA_ITEM: [code] or AGENDA_ITEM: NOT_FOUND

Important: Return the code as it appears (e.g., E-2, not E.-2.)

Examples:
- If you find "(Agenda Item: E-2)" â†’ respond: AGENDA_ITEM: E-2
- If you find "Item E-2" â†’ respond: AGENDA_ITEM: E-2
- If you find "H.-3." â†’ respond: AGENDA_ITEM: H-3
- If no agenda item found â†’ respond: AGENDA_ITEM: NOT_FOUND"""
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a precise data extractor. Find and extract only the agenda item code. Search the ENTIRE document thoroughly."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_tokens=self.agenda_extraction_max_tokens  # Use 100,000 for qwen
            )
            
            raw_response = response.choices[0].message.content.strip()
            
            # Save raw LLM response for debugging
            with open(debug_dir / f"llm_response_{document_number}_raw.txt", 'w', encoding='utf-8') as f:
                f.write(raw_response)
            
            # Parse response directly (no qwen parsing needed)
            result = raw_response
            
            # Log the response for debugging
            log.info(f"LLM response for {document_number} (first 200 chars): {result[:200]}")
            
            # Parse the response
            if "AGENDA_ITEM:" in result:
                code = result.split("AGENDA_ITEM:")[1].strip()
                if code != "NOT_FOUND":
                    # Normalize the code (remove dots, ensure format)
                    code = self._normalize_item_code(code)
                    log.info(f"âœ… Found agenda item code for {document_number}: {code}")
                    return code
                else:
                    log.warning(f"âŒ LLM could not find agenda item in {document_number}")
            else:
                log.error(f"âŒ Invalid LLM response format for {document_number}: {result[:100]}")
            
            return None
            
        except Exception as e:
            log.error(f"Failed to extract agenda item for {document_number}: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _normalize_item_code(self, code: str) -> str:
        """Normalize item code to consistent format."""
        if not code:
            return code
        
        # Remove trailing dots and spaces
        code = code.rstrip('. ')
        
        # Remove dots between letter and dash: "E.-1" -> "E-1"
        code = re.sub(r'([A-Z])\.(-)', r'\1\2', code)
        
        # Handle cases without dash: "E.1" -> "E-1"
        code = re.sub(r'([A-Z])\.(\d)', r'\1-\2', code)
        
        # Remove any remaining dots
        code = code.replace('.', '')
        
        return code
    
    def _extract_title(self, text: str) -> str:
        """Extract document title from text."""
        # Look for "AN ORDINANCE" or "A RESOLUTION" pattern
        title_match = re.search(r'(AN?\s+(ORDINANCE|RESOLUTION)[^.]+\.)', text[:2000], re.IGNORECASE)
        if title_match:
            return title_match.group(1).strip()
        
        # Fallback to first substantive line
        lines = text.split('\n')
        for line in lines[:20]:
            if len(line) > 20 and not line.isdigit():
                return line.strip()[:200]
        
        return "Untitled Document"
    
    def _parse_document_metadata(self, text: str) -> Dict[str, Any]:
        """Parse additional metadata from document."""
        metadata = {}
        
        # Extract date passed
        date_match = re.search(r'day\s+of\s+(\w+),?\s+(\d{4})', text)
        if date_match:
            metadata["date_passed"] = date_match.group(0)
        
        # Extract vote information
        vote_match = re.search(r'PASSED\s+AND\s+ADOPTED.*?(\d+).*?(\d+)', text, re.IGNORECASE | re.DOTALL)
        if vote_match:
            metadata["vote_details"] = {
                "ayes": vote_match.group(1),
                "nays": vote_match.group(2) if len(vote_match.groups()) > 1 else "0"
            }
        
        # Extract motion information
        motion_match = re.search(r'motion\s+(?:was\s+)?made\s+by\s+([^,]+)', text, re.IGNORECASE)
        if motion_match:
            metadata["motion"] = {"moved_by": motion_match.group(1).strip()}
        
        # Extract mayor signature
        mayor_match = re.search(r'Mayor[:\s]+([^\n]+)', text[-1000:])
        if mayor_match:
            metadata["signatories"] = {"mayor": mayor_match.group(1).strip()}
        
        return metadata


================================================================================


################################################################################
# File: scripts/graph_stages/pdf_extractor.py
################################################################################

# scripts/graph_stages/pdf_extractor.py

import os
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import logging
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.datamodel.base_models import InputFormat, DocumentStream
from docling.datamodel.pipeline_options import PdfPipelineOptions
import json
from io import BytesIO

# Set up logging
logging.basicConfig(level=logging.INFO)
log = logging.getLogger(__name__)

class PDFExtractor:
    """Extract text from PDFs using Docling for accurate OCR and text extraction."""
    
    def __init__(self, pdf_dir: Path, output_dir: Optional[Path] = None):
        """Initialize the PDF extractor with Docling."""
        self.pdf_dir = Path(pdf_dir)
        self.output_dir = output_dir or Path("city_clerk_documents/extracted_text")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Create debug directory
        self.debug_dir = self.output_dir / "debug"
        self.debug_dir.mkdir(exist_ok=True)
        
        # Initialize Docling converter with OCR enabled
        # Configure for better OCR and structure detection
        pipeline_options = PdfPipelineOptions()
        pipeline_options.do_ocr = True  # Enable OCR for better text extraction
        pipeline_options.do_table_structure = True  # Better table extraction
        
        self.converter = DocumentConverter(
            format_options={
                InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
            }
        )
    
    def extract_text_from_pdf(self, pdf_path: Path) -> Tuple[str, List[Dict[str, str]]]:
        """
        Extract text from PDF using Docling with OCR support.
        
        Returns:
            Tuple of (full_text, pages) where pages is a list of dicts with 'text' and 'page_num'
        """
        log.info(f"ðŸ“„ Extracting text from: {pdf_path.name}")
        
        # Convert PDF with Docling - just pass the path directly
        result = self.converter.convert(str(pdf_path))
        
        # Get the document
        doc = result.document
        
        # Get the markdown representation which preserves structure better
        full_markdown = doc.export_to_markdown()
        
        # Extract pages if available
        pages = []
        
        # Try to extract page-level content from the document structure
        if hasattr(doc, 'pages') and doc.pages:
            for page_num, page in enumerate(doc.pages, 1):
                # Get page text
                page_text = ""
                if hasattr(page, 'text'):
                    page_text = page.text
                elif hasattr(page, 'get_text'):
                    page_text = page.get_text()
                else:
                    # Try to extract from elements
                    page_elements = []
                    if hasattr(page, 'elements'):
                        for element in page.elements:
                            if hasattr(element, 'text'):
                                page_elements.append(element.text)
                    page_text = "\n".join(page_elements)
                
                if page_text:
                    pages.append({
                        'text': page_text,
                        'page_num': page_num
                    })
        
        # If we couldn't extract pages, create one page with all content
        if not pages and full_markdown:
            pages = [{
                'text': full_markdown,
                'page_num': 1
            }]
        
        # Use markdown as the full text (better structure preservation)
        full_text = full_markdown if full_markdown else ""
        
        # Save debug information
        debug_info = {
            'file': pdf_path.name,
            'total_pages': len(pages),
            'total_characters': len(full_text),
            'extraction_method': 'docling',
            'has_markdown': bool(full_markdown),
            'doc_attributes': list(dir(doc)) if doc else []
        }
        
        debug_file = self.debug_dir / f"{pdf_path.stem}_extraction_debug.json"
        with open(debug_file, 'w', encoding='utf-8') as f:
            json.dump(debug_info, f, indent=2)
        
        # Save the full extracted text for inspection
        text_file = self.debug_dir / f"{pdf_path.stem}_full_text.txt"
        with open(text_file, 'w', encoding='utf-8') as f:
            f.write(full_text)
        
        # Save markdown version separately for debugging
        if full_markdown:
            markdown_file = self.debug_dir / f"{pdf_path.stem}_markdown.md"
            with open(markdown_file, 'w', encoding='utf-8') as f:
                f.write(full_markdown)
        
        log.info(f"âœ… Extracted {len(pages)} pages, {len(full_text)} characters")
        
        return full_text, pages
    
    def extract_all_pdfs(self) -> Dict[str, Dict[str, any]]:
        """Extract text from all PDFs in the directory."""
        pdf_files = list(self.pdf_dir.glob("*.pdf"))
        log.info(f"ðŸ“š Found {len(pdf_files)} PDF files to process")
        
        extracted_data = {}
        
        for pdf_path in pdf_files:
            try:
                full_text, pages = self.extract_text_from_pdf(pdf_path)
                
                extracted_data[pdf_path.stem] = {
                    'full_text': full_text,
                    'pages': pages,
                    'metadata': {
                        'filename': pdf_path.name,
                        'num_pages': len(pages),
                        'total_chars': len(full_text),
                        'extraction_method': 'docling'
                    }
                }
                
                # Save extracted text
                output_file = self.output_dir / f"{pdf_path.stem}_extracted.json"
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(extracted_data[pdf_path.stem], f, indent=2, ensure_ascii=False)
                
                log.info(f"âœ… Saved extracted text to: {output_file}")
                
            except Exception as e:
                log.error(f"âŒ Failed to process {pdf_path.name}: {e}")
                import traceback
                traceback.print_exc()
                # No fallback - if Docling fails, we fail
                raise
        
        return extracted_data

# Add this to requirements.txt:
# unstructured[pdf]==0.10.30
# pytesseract>=0.3.10
# pdf2image>=1.16.3
# poppler-utils (system dependency - install with: brew install poppler)


================================================================================


################################################################################
# File: scripts/microsoft_framework/create_custom_prompts.py
################################################################################

# File: scripts/microsoft_framework/create_custom_prompts.py

#!/usr/bin/env python3
"""Create custom prompts for city clerk entity extraction."""

from pathlib import Path

def create_entity_extraction_prompt():
    """Create custom entity extraction prompt with STRICT isolation rules."""
    
    prompt_dir = Path("graphrag_data/prompts")
    prompt_dir.mkdir(parents=True, exist_ok=True)
    
    prompt = """
You are an AI assistant specialized in analyzing government documents for the City of Coral Gables. Your task is to extract entities and their relationships with EXTREME PRECISION and STRICT ISOLATION.

**CRITICAL ISOLATION RULES:**

1. **ABSOLUTE ENTITY SEPARATION**: Each entity is completely independent. Information about one entity (e.g., E-1) must NEVER be mixed with information about another entity (e.g., E-4).

2. **CONTEXT BOUNDARIES**: When you see isolation markers like "=== ISOLATED ENTITY: AGENDA ITEM E-1 ===" treat everything within those markers as belonging ONLY to that entity.

3. **NO CROSS-CONTAMINATION**: Even if entities appear similar or sequential (E-1, E-2, E-3, E-4), they are COMPLETELY SEPARATE entities with no shared information.

4. **STRICT SCOPE**: When extracting information about an entity, use ONLY the text within its specific section or document. Do not infer or assume relationships unless explicitly stated.

**ENTITY EXTRACTION RULES:**

For each entity, extract ONLY information that is explicitly mentioned in its isolated context:

1. **AGENDA_ITEM**: 
   - Extract ONLY the specific item code mentioned (e.g., "E-1")
   - Use ONLY the description from within that item's section
   - Do NOT reference other agenda items in the description

2. **ORDINANCE/RESOLUTION**:
   - Extract the document number exactly as stated
   - If linked to an agenda item, create a relationship ONLY if explicitly stated

3. **RELATIONSHIPS**:
   - Create relationships ONLY when explicitly stated in the text
   - Example: "Agenda Item E-1 relates to Ordinance 2024-01" â†’ Create relationship
   - Do NOT create relationships based on proximity or sequence

**OUTPUT FORMAT:**

For each entity: ("entity"<|><id><|><type><|><description>)
- id: The exact identifier (E-1, 2024-01, etc.)
- type: AGENDA_ITEM, ORDINANCE, RESOLUTION, PERSON, etc.
- description: Information from ONLY this entity's isolated context

For relationships: ("relationship"<|><source><|><target><|><description><|><weight>)
- Only create when explicitly stated
- description must quote the text that establishes the relationship

**EXAMPLES:**

CORRECT:
Text: "=== ISOLATED ENTITY: AGENDA ITEM E-1 === 
Agenda Item E-1: Zoning amendment for 123 Main St.
=== END ==="
Output: ("entity"<|>E-1<|>AGENDA_ITEM<|>Zoning amendment for 123 Main St.)

INCORRECT (mixing entities):
Text about E-1...
Output: ("entity"<|>E-1<|>AGENDA_ITEM<|>Zoning amendment, similar to E-4's proposal) âŒ

Remember: COMPLETE ISOLATION. Each entity stands alone.
"""
    
    with open(prompt_dir / "entity_extraction.txt", 'w') as f:
        f.write(prompt)
    
    print(f"âœ… Created strict isolation entity extraction prompt")

def create_entity_specific_prompt():
    """Create a custom prompt for strict entity-specific queries."""
    
    prompt_dir = Path("graphrag_data/prompts")
    prompt_dir.mkdir(parents=True, exist_ok=True)
    
    prompt = """
You are answering a question about a SPECIFIC entity in the City of Coral Gables government documents.

CRITICAL INSTRUCTIONS:
1. Focus EXCLUSIVELY on the SPECIFIC entity mentioned in the question.
2. DO NOT include information about other similar entities, even if they're related.
3. If asked about Agenda Item E-1, ONLY discuss E-1, not E-2, E-3, E-4, etc.
4. If asked about Ordinance 2024-01, ONLY discuss that specific ordinance, not other ordinances.
5. If asked about Resolution 2024-123, ONLY discuss that specific resolution, not other resolutions.

The user has specifically requested information about ONE entity. Keep your response focused ONLY on that entity.

If you're uncertain about details of the specific entity, state this clearly rather than including information about other entities.

Question: {input_query}
"""
    
    with open(prompt_dir / "entity_specific_query.txt", 'w') as f:
        f.write(prompt)
    
    print(f"âœ… Created custom entity-specific query prompt")

if __name__ == "__main__":
    create_entity_extraction_prompt()


================================================================================


################################################################################
# File: scripts/microsoft_framework/cosmos_synchronizer.py
################################################################################

# File: scripts/microsoft_framework/cosmos_synchronizer.py

from pathlib import Path
import pandas as pd
import json
from typing import Dict, List, Any
import asyncio
from scripts.graph_stages.cosmos_db_client import CosmosGraphClient

class GraphRAGCosmosSync:
    """Synchronize GraphRAG output with Cosmos DB."""
    
    def __init__(self, graphrag_output_dir: Path):
        self.output_dir = Path(graphrag_output_dir)
        self.cosmos_client = CosmosGraphClient()
        
    async def sync_to_cosmos(self):
        """Sync GraphRAG data to Cosmos DB."""
        await self.cosmos_client.connect()
        
        try:
            # Load GraphRAG artifacts
            entities_df = pd.read_parquet(self.output_dir / "entities.parquet")
            relationships_df = pd.read_parquet(self.output_dir / "relationships.parquet")
            communities_df = pd.read_parquet(self.output_dir / "communities.parquet")
            
            # Sync entities
            print(f"ðŸ“¤ Syncing {len(entities_df)} entities to Cosmos DB...")
            for _, entity in entities_df.iterrows():
                await self._sync_entity(entity)
            
            # Sync relationships
            print(f"ðŸ”— Syncing {len(relationships_df)} relationships...")
            for _, rel in relationships_df.iterrows():
                await self._sync_relationship(rel)
            
            # Sync communities as properties
            print(f"ðŸ˜ï¸ Syncing {len(communities_df)} communities...")
            for _, community in communities_df.iterrows():
                await self._sync_community(community)
                
        finally:
            await self.cosmos_client.close()
    
    async def _sync_entity(self, entity: pd.Series):
        """Sync a GraphRAG entity to Cosmos DB."""
        # Map GraphRAG entity to Cosmos vertex
        vertex_id = f"graphrag_entity_{entity['id']}"
        
        properties = {
            'name': entity['name'],
            'type': entity['type'],
            'description': entity['description'],
            'graphrag_id': entity['id'],
            'community_ids': json.dumps(entity.get('community_ids', [])),
            'has_graphrag': True
        }
        
        # Map to appropriate label based on type
        label_map = {
            'person': 'Person',
            'organization': 'Organization',
            'location': 'Location',
            'document': 'Document',
            'meeting': 'Meeting',
            'agenda_item': 'AgendaItem',
            'project': 'Project'
        }
        
        label = label_map.get(entity['type'].lower(), 'Entity')
        
        await self.cosmos_client.upsert_vertex(
            label=label,
            vertex_id=vertex_id,
            properties=properties
        )
    
    async def _sync_relationship(self, rel: pd.Series):
        """Sync a GraphRAG relationship to Cosmos DB."""
        from_id = f"graphrag_entity_{rel['source']}"
        to_id = f"graphrag_entity_{rel['target']}"
        
        properties = {
            'description': rel['description'],
            'weight': rel.get('weight', 1.0),
            'graphrag_rel_id': rel['id']
        }
        
        await self.cosmos_client.create_edge_if_not_exists(
            from_id=from_id,
            to_id=to_id,
            edge_type=rel['type'].upper(),
            properties=properties
        )
    
    async def _sync_community(self, community: pd.Series):
        """Sync a GraphRAG community as metadata to relevant entities."""
        # Communities can be stored as properties on entities
        # or as separate vertices depending on your schema preference
        pass


================================================================================


################################################################################
# File: scripts/microsoft_framework/graphrag_output_processor.py
################################################################################

# File: scripts/microsoft_framework/graphrag_output_processor.py

from pathlib import Path
import pandas as pd
import json
from typing import Dict, List, Any

class GraphRAGOutputProcessor:
    """Process and load GraphRAG output artifacts."""
    
    def __init__(self, output_dir: Path):
        self.output_dir = Path(output_dir)
        
    def load_graphrag_artifacts(self) -> Dict[str, Any]:
        """Load all GraphRAG output artifacts."""
        artifacts = {}
        
        # Load entities
        entities_path = self.output_dir / "entities.parquet"
        if entities_path.exists():
            artifacts['entities'] = pd.read_parquet(entities_path)
        
        # Load relationships
        relationships_path = self.output_dir / "relationships.parquet"
        if relationships_path.exists():
            artifacts['relationships'] = pd.read_parquet(relationships_path)
        
        # Load communities
        communities_path = self.output_dir / "communities.parquet"
        if communities_path.exists():
            artifacts['communities'] = pd.read_parquet(communities_path)
        
        # Load community reports
        reports_path = self.output_dir / "community_reports.parquet"
        if reports_path.exists():
            artifacts['community_reports'] = pd.read_parquet(reports_path)
        
        return artifacts
    
    def get_entity_summary(self) -> Dict[str, int]:
        """Get summary statistics of extracted entities."""
        entities_path = self.output_dir / "entities.parquet"
        if not entities_path.exists():
            return {}
        
        entities_df = pd.read_parquet(entities_path)
        
        summary = {
            'total_entities': len(entities_df),
            'entity_types': entities_df['type'].value_counts().to_dict()
        }
        
        return summary
    
    def get_relationship_summary(self) -> Dict[str, Any]:
        """Get a summary of extracted relationships."""
        relationships_path = self.output_dir / "relationships.parquet"
        if not relationships_path.exists():
            return {}
        
        relationships_df = pd.read_parquet(relationships_path)
        if relationships_df is None or relationships_df.empty:
            return {}
        
        # In newer GraphRAG versions, relationship type is part of the description.
        # We'll approximate by grabbing the first word of the description.
        def get_rel_type(description):
            if isinstance(description, str) and description:
                return description.split()[0]
            return "UNKNOWN"

        summary = {
            'total_relationships': len(relationships_df),
            'relationship_types': relationships_df['description'].apply(get_rel_type).value_counts().to_dict()
        }
        return summary
    
    def get_community_summary(self) -> Dict[str, Any]:
        """Get summary statistics of extracted communities."""
        communities_path = self.output_dir / "communities.parquet"
        if communities_path.exists():
            communities_df = pd.read_parquet(communities_path)
            
            summary = {
                'total_communities': len(communities_df),
                'community_types': communities_df['type'].value_counts().to_dict()
            }
            return summary
        else:
            return {}


================================================================================


################################################################################
# File: scripts/microsoft_framework/city_clerk_settings_template.yaml
################################################################################

# File: scripts/microsoft_framework/city_clerk_settings_template.yaml

llm:
  api_type: "openai"
  model: "gpt-4.1-mini-2025-04-14"
  api_key: "${OPENAI_API_KEY}"
  max_tokens: 32768
  temperature: 0
  
chunks:
  size: 1200
  overlap: 200
  group_by_columns: ["document_type", "meeting_date", "item_code"]
  
entity_extraction:
  prompt: "prompts/city_clerk_entity_extraction.txt"
  entity_types: ["person", "organization", "location", "document", 
                 "meeting", "money", "project", "agenda_item",
                 "ordinance", "resolution", "contract"]
  max_gleanings: 2
  
claim_extraction:
  enabled: true
  prompt: "prompts/city_clerk_claims.txt"
  description: "Extract voting records, motions, and decisions"
  
community_reports:
  prompt: "prompts/city_clerk_community_report.txt"
  max_length: 2000
  max_input_length: 32768
  
embeddings:
  model: "text-embedding-3-small"
  batch_size: 16
  batch_max_tokens: 2048
  
cluster_graph:
  max_cluster_size: 10
  
storage:
  type: "file"
  base_dir: "./output/artifacts"

# Query configuration section
query:
  # Global search settings
  global_search:
    community_level: 2  # Which hierarchical level to use
    max_tokens: 32768
    temperature: 0.0
    top_p: 1.0
    n: 1
    use_dynamic_community_selection: true
    relevance_score_threshold: 0.7
    rate_relevancy_model: "gpt-4.1-mini-2025-04-14"  # Same model for consistency
    
  # Local search settings  
  local_search:
    text_unit_prop: 0.5  # Proportion of context window for text units
    community_prop: 0.1  # Proportion for community summaries
    conversation_history_max_turns: 5
    top_k_entities: 10  # Number of related entities to retrieve
    top_k_relationships: 10
    max_tokens: 32768
    temperature: 0.0
    
  # DRIFT search settings
  drift_search:
    initial_community_level: 2
    max_iterations: 5
    follow_up_expansion: 3
    relevance_threshold: 0.7
    max_tokens: 32768
    temperature: 0.0
    primer_queries: 3  # Initial community queries
    follow_up_depth: 5  # Max recursion depth
    similarity_threshold: 0.8
    termination_strategy: "convergence"  # or "max_depth"
    include_global_context: true


================================================================================


################################################################################
# File: config.py
################################################################################

# File: config.py

#!/usr/bin/env python3
"""
Configuration file for graph database visualization
Manages database credentials and connection settings
"""

import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Azure Cosmos DB Gremlin Configuration
COSMOS_ENDPOINT = os.getenv("COSMOS_ENDPOINT", "wss://aida-graph-db.gremlin.cosmos.azure.com:443")
COSMOS_KEY = os.getenv("COSMOS_KEY", "")  # This will be set from .env file
DATABASE = os.getenv("COSMOS_DATABASE", "cgGraph")
CONTAINER = os.getenv("COSMOS_CONTAINER", "cityClerk")
PARTITION_KEY = os.getenv("COSMOS_PARTITION_KEY", "partitionKey")
PARTITION_VALUE = os.getenv("COSMOS_PARTITION_VALUE", "demo")

def validate_config():
    """Validate that all required configuration is available"""
    required_vars = {
        "COSMOS_KEY": COSMOS_KEY,
        "COSMOS_ENDPOINT": COSMOS_ENDPOINT,
        "DATABASE": DATABASE,
        "CONTAINER": CONTAINER
    }
    
    missing_vars = [var for var, value in required_vars.items() if not value]
    
    if missing_vars:
        print("âŒ Missing required configuration:")
        for var in missing_vars:
            print(f"   - {var}")
        print("\nðŸ”§ Please create a .env file with your credentials:")
        print("   COSMOS_KEY=your_actual_cosmos_key_here")
        print("   COSMOS_ENDPOINT=wss://aida-graph-db.gremlin.cosmos.azure.com:443")
        print("   COSMOS_DATABASE=cgGraph")
        print("   COSMOS_CONTAINER=cityClerk")
        return False
    
    return True

if __name__ == "__main__":
    print("ðŸ”§ Configuration Check:")
    if validate_config():
        print("âœ… All configuration variables are set!")
    else:
        print("âŒ Configuration incomplete!")


================================================================================


################################################################################
# File: scripts/json_to_markdown_converter.py
################################################################################

# File: scripts/json_to_markdown_converter.py

#!/usr/bin/env python3
"""Convert existing JSON extractions to markdown."""

import json
from pathlib import Path
import sys
sys.path.append('.')

from scripts.graph_stages.agenda_pdf_extractor import AgendaPDFExtractor

def convert_jsons_to_markdown():
    """Convert all existing JSON files to markdown."""
    
    json_dir = Path("city_clerk_documents/extracted_text")
    
    # Process agenda JSONs
    print("Converting agenda JSONs to markdown...")
    extractor = AgendaPDFExtractor()
    
    for json_file in json_dir.glob("Agenda*_extracted.json"):
        print(f"Processing: {json_file.name}")
        
        with open(json_file, 'r') as f:
            agenda_data = json.load(f)
        
        # Call the markdown save method
        extractor._save_agenda_as_markdown(agenda_data, json_file)
    
    print("âœ… Conversion complete!")
    
    # Check results
    markdown_dir = Path("city_clerk_documents/extracted_markdown")
    md_files = list(markdown_dir.glob("*.md"))
    print(f"\nMarkdown files: {len(md_files)}")
    print("- Agendas:", len([f for f in md_files if f.name.startswith('agenda_')]))
    print("- Ordinances:", len([f for f in md_files if f.name.startswith('ordinance_')]))
    print("- Resolutions:", len([f for f in md_files if f.name.startswith('resolution_')]))
    print("- Verbatims:", len([f for f in md_files if f.name.startswith('verbatim_')]))

if __name__ == "__main__":
    convert_jsons_to_markdown()


================================================================================


################################################################################
# File: scripts/microsoft_framework/query_graphrag.py
################################################################################

# File: scripts/microsoft_framework/query_graphrag.py

#!/usr/bin/env python3
import subprocess
import sys
from pathlib import Path

# Use venv Python
venv_python = Path("venv/bin/python3")
if not venv_python.exists():
    print("Error: venv not found!")
    sys.exit(1)

# Get query from command line
query = " ".join(sys.argv[1:]) if len(sys.argv) > 1 else "Who is Mayor Lago?"

# Run query
cmd = [
    str(venv_python),
    "-m", "graphrag", "query",
    "--root", "graphrag_data",
    "--method", "local",
    "--query", query
]

result = subprocess.run(cmd, capture_output=True, text=True)
print(result.stdout)


================================================================================

