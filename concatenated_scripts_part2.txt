# Concatenated Project Code - Part 2 of 3
# Generated: 2025-05-30 00:13:50
# Root Directory: /Users/gianmariatroiani/Documents/knologi̊/graph_database
================================================================================

# File Index - Which Files Are in Which Parts
################################################################################

## Part 1 (7 files):
  - city_clerk_graph.html
  - scripts/pipeline_modular_optimized.py
  - debug_graph.py
  - scripts/stages/llm_enrich.py
  - scripts/graph_stages/entity_deduplicator.py
  - config.py
  - scripts/stages/__init__.py

## Part 2 (9 files):
  - scripts/stages/embed_vectors.py
  - scripts/agenda_structure_pipeline.py
  - scripts/rag_local_web_app.py
  - scripts/graph_stages/agenda_ontology_extractor.py
  - scripts/clear_database.py
  - relationOPENAI.py
  - scripts/stages/acceleration_utils.py
  - test_query.py
  - requirements.txt

## Part 3 (9 files):
  - scripts/stages/extract_clean.py
  - scripts/graph_stages/agenda_graph_builder.py
  - scripts/stages/chunk_text.py
  - scripts/graph_stages/agenda_pdf_extractor.py
  - scripts/graph_stages/cosmos_db_client.py
  - scripts/stages/db_upsert.py
  - scripts/graph_stages/agenda_parser.py
  - clear_gremlin.py
  - scripts/graph_stages/__init__.py


================================================================================


################################################################################
# File: scripts/stages/embed_vectors.py
################################################################################

# File: scripts/stages/embed_vectors.py

#!/usr/bin/env python3
"""
Stage 7 — Optimized embedding with rate limiting, deduplication, and conservative batching.
"""
from __future__ import annotations
import argparse, json, logging, os, sys, time
from datetime import datetime
from typing import Any, Dict, List, Optional, Set
import asyncio
import aiohttp
from dotenv import load_dotenv
from openai import OpenAI
from supabase import create_client
from tqdm import tqdm
from tqdm.asyncio import tqdm as async_tqdm
import hashlib

# Try to import tiktoken for accurate token counting
try:
    import tiktoken
    TIKTOKEN_AVAILABLE = True
except ImportError:
    TIKTOKEN_AVAILABLE = False
    logging.warning("tiktoken not available - using conservative token estimation")

load_dotenv()
SUPABASE_URL  = os.getenv("SUPABASE_URL")
SUPABASE_KEY  = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
OPENAI_API_KEY= os.getenv("OPENAI_API_KEY")
EMBEDDING_MODEL="text-embedding-ada-002"
MODEL_TOKEN_LIMIT=8192
TOKEN_GUARD=200

# 🎯 DYNAMIC BATCHING - Token limits with safety margins
MAX_BATCH_TOKENS = 7692  # Conservative limit (8192 - 500 safety margin)
MAX_CHUNK_TOKENS = 6000  # Individual chunk limit
MIN_BATCH_TOKENS = 100   # Minimum viable batch size

MAX_TOTAL_TOKENS=MAX_BATCH_TOKENS  # Use dynamic batch limit instead

# Conservative defaults for rate limiting
DEFAULT_BATCH_ROWS=200  # Reduced from 5000
DEFAULT_COMMIT_ROWS=10  # Reduced from 100
DEFAULT_MAX_CONCURRENT=3  # Reduced from 50
MAX_RETRIES=5
RETRY_DELAY=2
RATE_LIMIT_DELAY=0.3  # 300ms between API calls
MAX_CALLS_PER_MINUTE=150  # Conservative limit

openai_client=OpenAI(api_key=OPENAI_API_KEY)
logging.basicConfig(level=logging.INFO,
    format="%(asctime)s — %(levelname)s — %(message)s")
log=logging.getLogger(__name__)

# Global variables for rate limiting
call_timestamps = []
total_tokens_used = 0

class AsyncEmbedder:
    """Async embedding client with strict rate limiting and connection pooling."""
    
    def __init__(self, api_key: str, max_concurrent: int = DEFAULT_MAX_CONCURRENT):
        self.api_key = api_key
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.session = None
        self.call_count = 0
        self.start_time = time.time()
        
        # Initialize tiktoken encoder if available
        if TIKTOKEN_AVAILABLE:
            self.encoder = tiktoken.encoding_for_model(EMBEDDING_MODEL)
        else:
            self.encoder = None
    
    async def __aenter__(self):
        timeout = aiohttp.ClientTimeout(total=300)
        connector = aiohttp.TCPConnector(limit=20, limit_per_host=10)  # Reduced limits
        self.session = aiohttp.ClientSession(
            headers={"Authorization": f"Bearer {self.api_key}"},
            timeout=timeout,
            connector=connector
        )
        return self
    
    async def __aexit__(self, *args):
        await self.session.close()
    
    def count_tokens(self, text: str) -> int:
        """Count tokens accurately using tiktoken if available."""
        if self.encoder:
            return len(self.encoder.encode(text))
        else:
            # Conservative fallback
            return int(len(text.split()) * 0.75) + 50
    
    async def rate_limit_check(self):
        """Enforce rate limiting."""
        current_time = time.time()
        
        # Remove timestamps older than 1 minute
        call_timestamps[:] = [ts for ts in call_timestamps if current_time - ts < 60]
        
        # Check if we're approaching the rate limit
        if len(call_timestamps) >= MAX_CALLS_PER_MINUTE - 5:
            sleep_time = 60 - (current_time - call_timestamps[0])
            if sleep_time > 0:
                log.info(f"Rate limit protection: sleeping {sleep_time:.1f}s")
                await asyncio.sleep(sleep_time)
        
        # Always enforce minimum delay between calls
        await asyncio.sleep(RATE_LIMIT_DELAY)
        
        # Record this call
        call_timestamps.append(current_time)
    
    async def embed_batch_async(self, texts: List[str]) -> List[List[float]]:
        """Embed a batch of texts asynchronously with rate limiting and token validation."""
        global total_tokens_used
        
        async with self.semaphore:
            await self.rate_limit_check()
            
            # 🛡️ FINAL TOKEN VALIDATION - Last safety check before API call
            batch_tokens = sum(self.count_tokens(text) for text in texts)
            
            if batch_tokens > MAX_BATCH_TOKENS:
                error_msg = f"🚨 CRITICAL: Batch tokens {batch_tokens} exceed limit {MAX_BATCH_TOKENS}"
                log.error(error_msg)
                raise RuntimeError(error_msg)
            
            total_tokens_used += batch_tokens
            
            log.info(f"🎯 API call: {len(texts)} texts (~{batch_tokens} tokens) - WITHIN LIMITS ✅")
            log.info(f"📊 Total tokens used so far: ~{total_tokens_used}")
            
            for attempt in range(MAX_RETRIES):
                try:
                    async with self.session.post(
                        "https://api.openai.com/v1/embeddings",
                        json={
                            "model": EMBEDDING_MODEL,
                            "input": texts
                        }
                    ) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            self.call_count += 1
                            log.info(f"✅ API call successful: {len(texts)} embeddings generated")
                            return [item["embedding"] for item in data["data"]]
                        elif resp.status == 400:  # Bad request - likely token limit
                            error = await resp.text()
                            log.error(f"🚨 API error 400 (likely token limit): {error}")
                            log.error(f"🚨 Batch details: {len(texts)} texts, {batch_tokens} tokens")
                            raise RuntimeError(f"Token limit API error: {error}")
                        elif resp.status == 429:  # Rate limit error
                            error = await resp.text()
                            log.warning(f"Rate limit hit: {error}")
                            # Exponential backoff for rate limits
                            sleep_time = (2 ** attempt) * 2
                            log.info(f"Exponential backoff: sleeping {sleep_time}s")
                            await asyncio.sleep(sleep_time)
                        else:
                            error = await resp.text()
                            log.warning(f"API error {resp.status}: {error}")
                            await asyncio.sleep(RETRY_DELAY)
                except Exception as e:
                    log.warning(f"Attempt {attempt + 1} failed: {e}")
                    await asyncio.sleep(RETRY_DELAY * (attempt + 1))
            
            raise RuntimeError("Failed to embed batch after retries")

def deduplicate_chunks(chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Remove duplicate text chunks based on content hash."""
    seen_hashes: Set[str] = set()
    unique_chunks = []
    duplicates_removed = 0
    
    for chunk in chunks:
        text = chunk.get("text", "").strip()
        if not text:
            continue
            
        # Create hash of the text content
        text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()
        
        if text_hash not in seen_hashes:
            seen_hashes.add(text_hash)
            unique_chunks.append(chunk)
        else:
            duplicates_removed += 1
    
    if duplicates_removed > 0:
        log.info(f"Deduplication: removed {duplicates_removed} duplicate chunks")
    
    return unique_chunks

async def process_chunks_async(
    sb,
    chunks: List[Dict[str, Any]],
    embedder: AsyncEmbedder,
    commit_size: int = DEFAULT_COMMIT_ROWS
) -> int:
    """Process chunks with async embedding and batch updates."""
    # Deduplicate chunks first
    unique_chunks = deduplicate_chunks(chunks)
    log.info(f"📊 CHUNK PROCESSING PROGRESS:")
    log.info(f"   📄 Original chunks: {len(chunks)}")
    log.info(f"   📄 Unique chunks: {len(unique_chunks)}")
    if len(chunks) != len(unique_chunks):
        log.info(f"   🔄 Duplicates removed: {len(chunks) - len(unique_chunks)}")
    
    # Create token-safe slices
    slices = safe_slices(unique_chunks, commit_size)
    total_embedded = 0
    total_slices = len(slices)
    
    if total_slices == 0:
        log.warning(f"📊 NO SLICES TO PROCESS - All chunks were filtered out")
        return 0
    
    log.info(f"📊 EMBEDDING SLICES PROGRESS:")
    log.info(f"   🎯 Total slices to process: {total_slices}")
    log.info(f"   📄 Total chunks to embed: {sum(len(slice_data) for slice_data in slices)}")
    
    # Process each slice
    for i, slice_data in enumerate(slices):
        slice_num = i + 1
        slice_progress = (slice_num / total_slices * 100)
        
        log.info(f"📊 SLICE {slice_num}/{total_slices} ({slice_progress:.1f}%):")
        log.info(f"   📄 Processing {len(slice_data)} chunks in this slice")
        
        texts = [row["text"] for row in slice_data]
        
        try:
            # Get embeddings asynchronously
            log.info(f"   🔄 Calling OpenAI API for {len(texts)} embeddings...")
            embeddings = await embedder.embed_batch_async(texts)
            log.info(f"   ✅ Received {len(embeddings)} embeddings from API")
            
            # Update database in batch
            log.info(f"   💾 Updating database for {len(slice_data)} chunks...")
            update_tasks = []
            for row, emb in zip(slice_data, embeddings):
                # ✅ GUARANTEED SKIP LOGIC - Layer 3: Final update verification
                def update_with_verification(r, e):
                    # Final check before updating
                    final_check = sb.table("documents_chunks").select("embedding")\
                        .eq("id", r["id"]).execute()
                    
                    if final_check.data and final_check.data[0].get("embedding"):
                        log.info(f"🛡️ Chunk {r['id']} already has embedding - SKIPPING update")
                        return {"skipped": True}
                    
                    # Safe to update
                    return sb.table("documents_chunks")\
                        .update({"embedding": e})\
                        .eq("id", r["id"])\
                        .execute()
                
                # Use thread pool for database updates
                loop = asyncio.get_event_loop()
                task = loop.run_in_executor(None, update_with_verification, row, emb)
                update_tasks.append(task)
            
            # Wait for all updates
            log.info(f"   ⏳ Waiting for {len(update_tasks)} database updates...")
            results = await asyncio.gather(*update_tasks, return_exceptions=True)
            successful = sum(1 for r in results if not isinstance(r, Exception) and not (isinstance(r, dict) and r.get("skipped")))
            skipped = sum(1 for r in results if isinstance(r, dict) and r.get("skipped"))
            failed = sum(1 for r in results if isinstance(r, Exception))
            total_embedded += successful
            
            # 📊 SLICE COMPLETION PROGRESS
            log.info(f"📊 SLICE {slice_num} COMPLETE:")
            log.info(f"   ✅ Successful updates: {successful}")
            if skipped > 0:
                log.info(f"   ⏭️  Skipped (already embedded): {skipped}")
            if failed > 0:
                log.info(f"   ❌ Failed updates: {failed}")
            log.info(f"   📈 Total embedded so far: {total_embedded}")
            
        except Exception as e:
            log.error(f"❌ SLICE {slice_num} FAILED: {e}")
            log.error(f"   📄 Chunks in failed slice: {len(slice_data)}")
    
    # Final summary
    log.info(f"📊 CHUNK PROCESSING COMPLETE:")
    log.info(f"   ✅ Total chunks embedded: {total_embedded}")
    log.info(f"   📊 Slices processed: {total_slices}")
    log.info(f"   📈 Success rate: {(total_embedded/len(unique_chunks)*100):.1f}%" if unique_chunks else "0%")
    
    return total_embedded

async def main_async(batch_size: int = None, commit_size: int = None, max_concurrent: int = None):
    """Async main function for embedding."""
    global EMBEDDING_MODEL, MODEL_TOKEN_LIMIT, MAX_TOTAL_TOKENS, total_tokens_used
    
    # Use provided parameters or conservative defaults
    batch_size = batch_size or DEFAULT_BATCH_ROWS
    commit_size = commit_size or DEFAULT_COMMIT_ROWS
    max_concurrent = max_concurrent or DEFAULT_MAX_CONCURRENT
    
    log.info(f"Starting with conservative settings:")
    log.info(f"  Batch size: {batch_size}")
    log.info(f"  Commit size: {commit_size}")
    log.info(f"  Max concurrent: {max_concurrent}")
    log.info(f"  Rate limit delay: {RATE_LIMIT_DELAY}s")
    
    # 🎯 Dynamic batching status
    log.info(f"🎯 DYNAMIC BATCHING ENABLED:")
    log.info(f"   Max batch tokens: {MAX_BATCH_TOKENS} (with safety margin)")
    log.info(f"   Max chunk tokens: {MAX_CHUNK_TOKENS}")
    log.info(f"   Min batch tokens: {MIN_BATCH_TOKENS}")
    log.info(f"   ✅ GUARANTEED: No token limit API errors")
    
    # ✅ Check tiktoken availability for accurate token counting
    if not TIKTOKEN_AVAILABLE:
        log.warning("⚠️  tiktoken not available - using conservative token estimation")
        log.warning("⚠️  For accurate token counting, install: pip install tiktoken")
    else:
        log.info("✅ tiktoken available - using accurate token counting")
    
    if not OPENAI_API_KEY:
        log.error("OPENAI_API_KEY missing")
        sys.exit(1)
    
    sb = init_supabase()
    existing_count = count_processed_chunks(sb)
    log.info(f"Rows already embedded: {existing_count}")
    
    # ✅ GUARANTEED SKIP LOGIC - Status check
    total_chunks_res = sb.table("documents_chunks").select("id", count="exact")\
        .eq("chunking_strategy", "token_window").execute()
    total_chunks = total_chunks_res.count or 0
    
    pending_chunks = total_chunks - existing_count
    
    log.info("📊 Current status:")
    log.info(f"   Chunks already embedded: {existing_count}")
    log.info(f"   Chunks needing embedding: {pending_chunks}")
    log.info(f"   Total chunks: {total_chunks}")
    
    if pending_chunks == 0:
        log.info("✅ All chunks already have embeddings - nothing to do!")
        return
    
    async with AsyncEmbedder(OPENAI_API_KEY, max_concurrent) as embedder:
        loop, total = 0, 0
        
        # 📊 PROGRESS TRACKING - Initialize counters
        total_chunks_to_process = pending_chunks
        chunks_processed = 0
        chunks_skipped = 0
        
        log.info(f"📊 EMBEDDING PROGRESS TRACKING INITIALIZED:")
        log.info(f"   🎯 Total chunks to embed: {total_chunks_to_process}")
        log.info(f"   🎯 Starting embedding process...")
        
        while True:
            loop += 1
            rows = fetch_unprocessed_chunks(sb, limit=batch_size, offset=0)
            
            if not rows:
                log.info("✨ Done — no more rows.")
                break
            
            # 📊 PROGRESS: Show current status before processing
            remaining_chunks = total_chunks_to_process - chunks_processed
            progress_percent = (chunks_processed / total_chunks_to_process * 100) if total_chunks_to_process > 0 else 0
            
            log.info(f"📊 EMBEDDING PROGRESS - Loop {loop}:")
            log.info(f"   📄 Fetched: {len(rows)} chunks")
            log.info(f"   ✅ Processed: {chunks_processed}/{total_chunks_to_process} ({progress_percent:.1f}%)")
            log.info(f"   ⏳ Remaining: {remaining_chunks} chunks")
            if chunks_skipped > 0:
                log.info(f"   ⚠️  Skipped: {chunks_skipped} oversized chunks")
            
            # Process chunks asynchronously
            embedded = await process_chunks_async(sb, rows, embedder, commit_size)
            total += embedded
            chunks_processed += embedded
            
            # Update skipped count (this will be calculated in process_chunks_async)
            current_chunk_count = count_processed_chunks(sb)
            actual_embedded_this_loop = current_chunk_count - (existing_count + chunks_processed - embedded)
            
            # 📊 PROGRESS: Show results after processing
            final_progress_percent = (chunks_processed / total_chunks_to_process * 100) if total_chunks_to_process > 0 else 0
            log.info(f"📊 LOOP {loop} COMPLETE:")
            log.info(f"   ✅ This loop: {embedded} chunks embedded")
            log.info(f"   📈 Total progress: {chunks_processed}/{total_chunks_to_process} ({final_progress_percent:.1f}%)")
            log.info(f"   🔄 API calls made: {embedder.call_count}")
            log.info(f"   📊 Total tokens used: ~{total_tokens_used}")
            
            # Check if we're making progress or stuck
            if embedded == 0 and len(rows) > 0:
                chunks_skipped += len(rows)
                log.warning(f"⚠️  WARNING: No chunks embedded in this loop - all {len(rows)} chunks were skipped")
                log.warning(f"⚠️  Total skipped so far: {chunks_skipped} chunks")
                
                # Prevent infinite loop by limiting consecutive failed attempts
                if loop > 5 and embedded == 0:
                    log.error(f"🚨 STUCK: {loop} consecutive loops with no progress - stopping to prevent infinite loop")
                    break
    
    # Save report
    ts = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    report = {
        "timestamp": ts,
        "batch_size": batch_size,
        "commit": commit_size,
        "max_concurrent": max_concurrent,
        "total_embedded": total,
        "total_with_embeddings": count_processed_chunks(sb),
        "total_tokens_used": total_tokens_used,
        "api_calls_made": embedder.call_count if 'embedder' in locals() else 0
    }
    
    # Create reports directory if it doesn't exist
    reports_dir = "reports/embedding"
    os.makedirs(reports_dir, exist_ok=True)
    
    fname = f"{reports_dir}/supabase_embedding_report_{ts}.json"
    with open(fname, "w") as fp:
        json.dump(report, fp, indent=2)
    
    log.info(f"Report saved to {fname}")

async def main_async_cli():
    """CLI version with argument parsing."""
    global EMBEDDING_MODEL, MODEL_TOKEN_LIMIT, MAX_TOTAL_TOKENS
    
    ap = argparse.ArgumentParser()
    ap.add_argument("--batch-size", type=int, default=DEFAULT_BATCH_ROWS)
    ap.add_argument("--commit", type=int, default=DEFAULT_COMMIT_ROWS)
    ap.add_argument("--skip", type=int, default=0)
    ap.add_argument("--model", default=EMBEDDING_MODEL)
    ap.add_argument("--model-limit", type=int, default=MODEL_TOKEN_LIMIT)
    ap.add_argument("--max-concurrent", type=int, default=DEFAULT_MAX_CONCURRENT,
                   help="Maximum concurrent API calls (default: 3 for rate limiting)")
    args = ap.parse_args()
    
    EMBEDDING_MODEL = args.model
    MODEL_TOKEN_LIMIT = args.model_limit
    MAX_TOTAL_TOKENS = MODEL_TOKEN_LIMIT - TOKEN_GUARD
    
    await main_async(args.batch_size, args.commit, args.max_concurrent)

# Keep original interface for compatibility
def main() -> None:
    """Original synchronous interface."""
    asyncio.run(main_async_cli())

# Keep all existing helper functions unchanged...
def init_supabase():
    if not SUPABASE_URL or not SUPABASE_KEY:
        log.error("SUPABASE creds missing"); sys.exit(1)
    return create_client(SUPABASE_URL,SUPABASE_KEY)

def count_processed_chunks(sb)->int:
    res=sb.table("documents_chunks").select("id",count="exact").not_.is_("embedding","null").execute()
    return res.count or 0

def fetch_unprocessed_chunks(sb,*,limit:int,offset:int=0)->List[Dict[str,Any]]:
    first,last=offset,offset+limit-1
    res=sb.table("documents_chunks").select("id,text,token_start,token_end")\
        .eq("chunking_strategy","token_window").is_("embedding","null")\
        .range(first,last).execute()
    
    chunks = res.data or []
    
    if chunks:
        # ✅ GUARANTEED SKIP LOGIC - Layer 2: Double-check verification
        chunk_ids = [chunk["id"] for chunk in chunks]
        verification = sb.table("documents_chunks").select("id")\
            .in_("id", chunk_ids).not_.is_("embedding", "null").execute()
        
        already_embedded_ids = {row["id"] for row in verification.data or []}
        
        if already_embedded_ids:
            log.warning(f"🛡️ Found {len(already_embedded_ids)} chunks that already have embeddings - SKIPPING them")
            chunks = [chunk for chunk in chunks if chunk["id"] not in already_embedded_ids]
        
        log.info(f"✅ GUARANTEED: Fetched {len(chunks)} chunks WITHOUT embeddings")
        
        if not chunks:
            log.info("✅ All chunks already have embeddings - nothing to do!")
    
    return chunks

def generate_embedding_batch(texts:List[str])->List[List[float]]:
    attempt=0
    while attempt<MAX_RETRIES:
        try:
            resp=openai_client.embeddings.create(model=EMBEDDING_MODEL,input=texts)
            return [d.embedding for d in resp.data]
        except Exception as exc:
            attempt+=1; log.warning("Embedding batch %s/%s failed: %s",attempt,MAX_RETRIES,exc)
            time.sleep(RETRY_DELAY)
    raise RuntimeError("OpenAI embedding batch failed after retries")

def chunk_tokens(row:Dict[str,Any])->int:
    try:
        t=int(row["token_end"])-int(row["token_start"])+1
        if 0<t<=16384: return t
    except: pass
    
    # Use tiktoken if available for more accurate counting
    text = row.get("text", "")
    if TIKTOKEN_AVAILABLE:
        try:
            encoder = tiktoken.encoding_for_model(EMBEDDING_MODEL)
            return len(encoder.encode(text))
        except:
            pass
    
    # Conservative fallback
    approx=int(len(text.split())*0.75)+50  # More conservative estimate
    return min(max(1,approx),MODEL_TOKEN_LIMIT)

def safe_slices(rows:List[Dict[str,Any]],max_rows:int)->List[List[Dict[str,Any]]]:
    """🎯 DYNAMIC BATCH SIZING - Guarantees no token limit errors."""
    return dynamic_batch_slices(rows)

def dynamic_batch_slices(rows: List[Dict[str, Any]]) -> List[List[Dict[str, Any]]]:
    """
    🎯 DYNAMIC BATCH SIZING - Creates variable-sized batches that GUARANTEE no API token errors.
    
    Protection Layers:
    1. Individual chunk validation (max 6,000 tokens)
    2. Dynamic batch sizing (max 7,692 tokens total)
    3. Conservative safety margins
    4. Multiple validation checks
    """
    batches = []
    current_batch = []
    current_tokens = 0
    skipped_chunks = 0
    
    # Initialize token encoder for accurate counting
    encoder = None
    if TIKTOKEN_AVAILABLE:
        try:
            encoder = tiktoken.encoding_for_model(EMBEDDING_MODEL)
        except:
            pass
    
    def count_tokens_accurate(text: str) -> int:
        """Count tokens with maximum accuracy."""
        if encoder:
            return len(encoder.encode(text))
        else:
            # Conservative fallback estimation
            return int(len(text.split()) * 0.75) + 50
    
    log.info(f"🎯 Starting dynamic batch creation with {len(rows)} chunks")
    log.info(f"🎯 Limits: {MAX_CHUNK_TOKENS} tokens/chunk, {MAX_BATCH_TOKENS} tokens/batch")
    
    for i, row in enumerate(rows):
        # Clean text and validate
        text = (row.get("text") or "").replace("\x00", "").strip()
        if not text:
            log.warning(f"🎯 Skipping empty chunk {row.get('id', 'unknown')}")
            continue
        
        # ️ PROTECTION LAYER 1: Individual chunk validation
        chunk_tokens = count_tokens_accurate(text)
        
        if chunk_tokens > MAX_CHUNK_TOKENS:
            log.warning(f"🎯 Skipping oversized chunk {row.get('id', 'unknown')}: {chunk_tokens} tokens (max: {MAX_CHUNK_TOKENS})")
            skipped_chunks += 1
            continue
        
        # 🛡️ PROTECTION LAYER 2: Dynamic batch sizing
        would_exceed = current_tokens + chunk_tokens > MAX_BATCH_TOKENS
        
        if would_exceed and current_batch:
            # Finalize current batch
            log.info(f"🎯 Created batch with {len(current_batch)} chunks (~{current_tokens} tokens)")
            batches.append(current_batch)
            current_batch = []
            current_tokens = 0
        
        # Add chunk to current batch
        current_batch.append({"id": row["id"], "text": text})
        current_tokens += chunk_tokens
        
        # 🛡️ PROTECTION LAYER 3: Safety validation
        if current_tokens > MAX_BATCH_TOKENS:
            log.error(f"🚨 CRITICAL: Batch exceeded limit! {current_tokens} > {MAX_BATCH_TOKENS}")
            # Emergency fallback - remove last chunk and finalize batch
            if len(current_batch) > 1:
                current_batch.pop()
                current_tokens -= chunk_tokens
                log.info(f"🎯 Emergency: Created batch with {len(current_batch)} chunks (~{current_tokens} tokens)")
                batches.append(current_batch)
                current_batch = [{"id": row["id"], "text": text}]
                current_tokens = chunk_tokens
            else:
                log.error(f"🚨 Single chunk too large: {chunk_tokens} tokens")
                current_batch = []
                current_tokens = 0
                skipped_chunks += 1
    
    # Add final batch if not empty
    if current_batch and current_tokens >= MIN_BATCH_TOKENS:
        log.info(f"🎯 Created final batch with {len(current_batch)} chunks (~{current_tokens} tokens)")
        batches.append(current_batch)
    elif current_batch:
        log.warning(f"🎯 Skipping tiny final batch: {current_tokens} tokens < {MIN_BATCH_TOKENS} minimum")
    
    # 🛡️ PROTECTION LAYER 4: Final validation
    total_chunks = sum(len(batch) for batch in batches)
    max_batch_tokens = max((sum(count_tokens_accurate(chunk["text"]) for chunk in batch) for batch in batches), default=0)
    
    log.info(f"🎯 DYNAMIC BATCHING COMPLETE:")
    log.info(f"   📊 {len(batches)} batches created")
    log.info(f"   📊 {total_chunks} chunks processed")
    log.info(f"   📊 {skipped_chunks} chunks skipped")
    log.info(f"   📊 Largest batch: {max_batch_tokens} tokens (limit: {MAX_BATCH_TOKENS})")
    log.info(f"   ✅ GUARANTEED: No batch exceeds {MAX_BATCH_TOKENS} tokens")
    
    if max_batch_tokens > MAX_BATCH_TOKENS:
        log.error(f"🚨 CRITICAL ERROR: Batch validation failed!")
        raise RuntimeError(f"Batch token validation failed: {max_batch_tokens} > {MAX_BATCH_TOKENS}")
    
    return batches

def embed_slice(sb,slice_rows:List[Dict[str,Any]])->int:
    embeds=generate_embedding_batch([r["text"] for r in slice_rows])
    ok=0
    for row,emb in zip(slice_rows,embeds):
        attempt=0
        while attempt<MAX_RETRIES:
            res=sb.table("documents_chunks").update({"embedding":emb}).eq("id",row["id"]).execute()
            if getattr(res,"error",None):
                attempt+=1; log.warning("Update %s failed (%s/%s): %s",row["id"],attempt,MAX_RETRIES,res.error)
                time.sleep(RETRY_DELAY)
            else: ok+=1; break
    return ok

if __name__=="__main__": 
    asyncio.run(main_async_cli())


================================================================================


################################################################################
# File: scripts/agenda_structure_pipeline.py
################################################################################

# File: scripts/agenda_structure_pipeline.py

#!/usr/bin/env python3
"""
City Clerk Agenda Graph Pipeline - Main Orchestrator
====================================================
Orchestrates the extraction of meaningful entities and relationships from agenda documents.
Now uses dedicated graph pipeline PDF extractor.
"""
import json
import logging
import asyncio
from pathlib import Path
from typing import Dict, List, Optional
from collections import Counter

from dotenv import load_dotenv
from openai import AzureOpenAI
import os

# ============================================================================
# PIPELINE STAGE CONTROLS - Set to False to skip specific stages
# ============================================================================
RUN_PDF_EXTRACT = True      # Stage 1: Extract PDF content with hierarchy
RUN_ONTOLOGY    = True      # Stage 2: Extract ontology using LLM
RUN_BUILD_GRAPH = True      # Stage 3: Build graph from ontology
RUN_CLEAR_GRAPH = False     # Clear existing graph data before processing
RUN_CONN_TEST   = True      # Run connection test before processing
INTERACTIVE     = True      # Ask for user input (set False for automation)

# ============================================================================

# Import graph stages (no longer using RAG pipeline stages)
from graph_stages.agenda_pdf_extractor import AgendaPDFExtractor
from graph_stages.cosmos_db_client import CosmosGraphClient
from graph_stages.agenda_ontology_extractor import CityClerkOntologyExtractor
from graph_stages.agenda_graph_builder import AgendaGraphBuilder

load_dotenv()

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
log = logging.getLogger("agenda_pipeline")

# Azure OpenAI Configuration
AZURE_OPENAI_API_KEY = os.getenv("AZURE_OPENAI_API_KEY")
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_OPENAI_API_VERSION = os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-15-preview")
DEPLOYMENT_NAME = os.getenv("AZURE_OPENAI_DEPLOYMENT", "gpt-4o")

# Initialize Azure OpenAI client
aoai = AzureOpenAI(
    api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    api_version=AZURE_OPENAI_API_VERSION
)
aoai.deployment_name = DEPLOYMENT_NAME  # Add deployment name as attribute


class AgendaPipelineOrchestrator:
    """Main orchestrator for agenda document processing pipeline."""
    
    def __init__(self, cosmos_client: CosmosGraphClient):
        self.cosmos_client = cosmos_client
        self.pdf_extractor = AgendaPDFExtractor()
        self.ontology_extractor = CityClerkOntologyExtractor(aoai)
        self.graph_builder = AgendaGraphBuilder(cosmos_client)
        self.stats = Counter()
        
    async def process_agenda(self, agenda_path: Path) -> Dict:
        """Process a single agenda document through all stages."""
        log.info(f"Processing agenda: {agenda_path.name}")
        
        # Track which stages ran
        stages_run = []
        
        try:
            extracted_data = None
            ontology = None
            graph_data = None
            
            # Stage 1: Extract PDF content with hierarchy preservation
            if RUN_PDF_EXTRACT:
                log.info(f"Stage 1: Extracting PDF content for {agenda_path.name}")
                extracted_data = self.pdf_extractor.extract_agenda(agenda_path)
                
                # Log extraction statistics
                stats = self.pdf_extractor.get_extraction_stats(extracted_data)
                log.info(f"Extraction stats: {stats}")
                stages_run.append("PDF_EXTRACT")
            else:
                log.info("Stage 1: SKIPPED (RUN_PDF_EXTRACT=False)")
                # Try to load existing extraction if available
                json_path = self.pdf_extractor.output_dir / f"{agenda_path.stem}_extracted.json"
                if json_path.exists():
                    log.info(f"Loading existing extraction from: {json_path}")
                    extracted_data = json.loads(json_path.read_text())
                else:
                    log.warning(f"No existing extraction found for {agenda_path.name}")
                    return {'error': 'No extraction available', 'stages_run': stages_run}
            
            # Convert to format expected by ontology extractor
            agenda_data = self._convert_to_agenda_format(extracted_data)
            
            # Stage 2: Extract ontology using LLM
            if RUN_ONTOLOGY:
                log.info(f"Stage 2: Extracting ontology for {agenda_path.name}")
                ontology = await self.ontology_extractor.extract_agenda_ontology(
                    agenda_data, 
                    agenda_path.name
                )
                
                # Save ontology for debugging/reuse
                ontology_path = self.pdf_extractor.output_dir / f"{agenda_path.stem}_ontology.json"
                ontology_path.write_text(json.dumps(ontology, indent=2))
                log.info(f"Saved ontology to: {ontology_path}")
                stages_run.append("ONTOLOGY")
            else:
                log.info("Stage 2: SKIPPED (RUN_ONTOLOGY=False)")
                # Try to load existing ontology
                ontology_path = self.pdf_extractor.output_dir / f"{agenda_path.stem}_ontology.json"
                if ontology_path.exists():
                    log.info(f"Loading existing ontology from: {ontology_path}")
                    ontology = json.loads(ontology_path.read_text())
                else:
                    log.warning(f"No existing ontology found for {agenda_path.name}")
                    return {'error': 'No ontology available', 'stages_run': stages_run}
            
            # Stage 3: Build graph from ontology
            if RUN_BUILD_GRAPH:
                log.info(f"Stage 3: Building graph for {agenda_path.name}")
                graph_data = await self.graph_builder.build_graph_from_ontology(
                    ontology, 
                    agenda_path
                )
                stages_run.append("BUILD_GRAPH")
                
                # Update statistics
                self._update_stats(graph_data)
            else:
                log.info("Stage 3: SKIPPED (RUN_BUILD_GRAPH=False)")
                # Create minimal graph data for statistics
                graph_data = {
                    'statistics': {
                        'sections': len(ontology.get('agenda_structure', [])),
                        'items': len(ontology.get('item_codes', [])),
                        'entities': len(ontology.get('entities', {})),
                        'relationships': len(ontology.get('relationships', []))
                    }
                }
            
            # Add stages run to result
            if graph_data:
                graph_data['stages_run'] = stages_run
            
            return graph_data or {'stages_run': stages_run}
            
        except Exception as e:
            log.error(f"Failed to process {agenda_path.name}: {e}")
            self.stats['failed'] += 1
            raise
    
    def _convert_to_agenda_format(self, extracted_data: Dict) -> Dict:
        """Convert extracted data to format expected by ontology extractor."""
        # Build sections from the extracted hierarchy
        sections = []
        
        # Add title as first section
        if extracted_data.get('title'):
            sections.append({
                'section': 'Title',
                'text': extracted_data['title'],
                'page_number': 1
            })
        
        # Add preamble if exists
        if extracted_data.get('preamble'):
            preamble_text = '\n'.join([
                item['text'] for item in extracted_data['preamble']
            ])
            sections.append({
                'section': 'Preamble',
                'text': preamble_text,
                'page_number': 1
            })
        
        # Convert hierarchical sections
        for section in extracted_data.get('sections', []):
            section_text = section.get('title', '') + '\n\n'
            
            # Add section content
            for content in section.get('content', []):
                section_text += content.get('text', '') + '\n'
            
            # Add subsections
            for subsection in section.get('subsections', []):
                section_text += f"\n{subsection.get('title', '')}\n"
                for content in subsection.get('content', []):
                    section_text += content.get('text', '') + '\n'
            
            sections.append({
                'section': section.get('title', 'Untitled'),
                'text': section_text.strip(),
                'page_start': section.get('page_start', 1),
                'elements': section.get('content', [])
            })
        
        # Include agenda items as a special section
        if extracted_data.get('agenda_items'):
            items_text = "EXTRACTED AGENDA ITEMS:\n\n"
            for item in extracted_data['agenda_items']:
                items_text += f"{item['code']}: {item.get('title', item.get('context', '')[:100])}\n"
            
            sections.append({
                'section': 'Agenda Items Summary',
                'text': items_text
            })
        
        return {
            'sections': sections,
            'metadata': extracted_data.get('metadata', {}),
            'agenda_items': extracted_data.get('agenda_items', [])
        }
    
    async def process_batch(self, agenda_files: List[Path], batch_size: int = 3):
        """Process multiple agenda files in batches."""
        total_files = len(agenda_files)
        
        # Log pipeline configuration
        log.info(f"\n{'='*60}")
        log.info("PIPELINE CONFIGURATION:")
        log.info(f"  PDF Extract: {'ENABLED' if RUN_PDF_EXTRACT else 'DISABLED'}")
        log.info(f"  Ontology:    {'ENABLED' if RUN_ONTOLOGY else 'DISABLED'}")
        log.info(f"  Build Graph: {'ENABLED' if RUN_BUILD_GRAPH else 'DISABLED'}")
        log.info(f"{'='*60}\n")
        
        for i in range(0, total_files, batch_size):
            batch = agenda_files[i:i + batch_size]
            batch_num = i // batch_size + 1
            total_batches = (total_files + batch_size - 1) // batch_size
            
            log.info(f"\n{'='*60}")
            log.info(f"Processing batch {batch_num}/{total_batches} ({len(batch)} files)")
            log.info(f"Overall progress: {i}/{total_files} files completed ({i/total_files*100:.1f}%)")
            log.info(f"{'='*60}")
            
            # Process each file in the batch
            for agenda_file in batch:
                try:
                    result = await self.process_agenda(agenda_file)
                    log.info(f"Completed {agenda_file.name} - Stages run: {result.get('stages_run', [])}")
                except Exception as e:
                    log.error(f"Failed to process {agenda_file.name}: {e}")
                    import traceback
                    traceback.print_exc()
            
            # Log batch statistics
            self._log_batch_stats(batch_num)
    
    def _update_stats(self, graph_data: Dict):
        """Update pipeline statistics."""
        stats = graph_data.get('statistics', {})
        self.stats['meetings'] += 1
        self.stats['sections'] += stats.get('sections', 0)
        self.stats['items'] += stats.get('items', 0)
        self.stats['relationships'] += stats.get('relationships', 0)
        
        # Update entity counts
        for entity_type, count in stats.get('entities', {}).items():
            self.stats[f'entity_{entity_type}'] = self.stats.get(f'entity_{entity_type}', 0) + count
    
    def _log_batch_stats(self, batch_num: int):
        """Log statistics for the current batch."""
        log.info(f"\nBatch {batch_num} Statistics:")
        log.info(f"  - Meetings processed: {self.stats['meetings']}")
        log.info(f"  - Total sections: {self.stats['sections']}")
        log.info(f"  - Total items: {self.stats['items']}")
        log.info(f"  - Total relationships: {self.stats['relationships']}")
        
        # Log entity counts
        entity_stats = {k.replace('entity_', ''): v for k, v in self.stats.items() if k.startswith('entity_')}
        if entity_stats:
            log.info(f"  - Entities: {entity_stats}")
    
    def get_final_stats(self) -> Dict:
        """Get final pipeline statistics."""
        return dict(self.stats)


async def find_agenda_files() -> List[Path]:
    """Find all agenda PDF files in the project."""
    # Start from the project root (parent of scripts directory)
    project_root = Path(__file__).parent.parent
    
    log.info(f"Searching for agenda files from: {project_root}")
    
    # List of possible locations
    possible_paths = [
        project_root / "city_clerk_documents" / "global" / "City Commissions 2024" / "Agendas",
        project_root / "city_clerk_documents" / "Agendas",
        project_root / "Agendas",
    ]
    
    # Also search recursively
    log.info("Searching recursively for Agendas directories...")
    agenda_dirs = list(project_root.rglob("**/Agendas"))
    possible_paths.extend(agenda_dirs)
    
    agenda_files = []
    searched_paths = []
    
    for path in possible_paths:
        searched_paths.append(str(path))
        if path.exists() and path.is_dir():
            # Look for files matching "Agenda *.pdf" pattern
            found_files = sorted(path.glob("Agenda *.pdf"))
            if found_files:
                log.info(f"✅ Found {len(found_files)} agenda files in: {path}")
                agenda_files = found_files
                break
            else:
                # Also try without space
                found_files = sorted(path.glob("Agenda*.pdf"))
                if found_files:
                    log.info(f"✅ Found {len(found_files)} agenda files in: {path}")
                    agenda_files = found_files
                    break
    
    if not agenda_files:
        log.error("❌ No agenda files found! Searched in:")
        for path in searched_paths[:10]:  # Show first 10 paths
            log.error(f"   - {path}")
        
        # Try to find any PDF files to help debug
        all_pdfs = list(project_root.rglob("*.pdf"))
        if all_pdfs:
            log.info(f"\nFound {len(all_pdfs)} total PDF files. Showing some examples:")
            # Show PDFs that might be agendas
            agenda_like = [p for p in all_pdfs if 'agenda' in p.name.lower()]
            if agenda_like:
                log.info(f"Found {len(agenda_like)} PDFs with 'agenda' in name:")
                for pdf in agenda_like[:5]:
                    log.info(f"   - {pdf.relative_to(project_root)}")
            else:
                log.info("First few PDFs found:")
                for pdf in all_pdfs[:5]:
                    log.info(f"   - {pdf.relative_to(project_root)}")
    
    return agenda_files


async def test_cosmos_connection(cosmos_client: CosmosGraphClient):
    """Test basic Cosmos DB operations."""
    log.info("🧪 Testing Cosmos DB connection...")
    
    try:
        # Test 1: Count vertices
        count_query = "g.V().count()"
        result = await cosmos_client._execute_query(count_query)
        log.info(f"✅ Vertex count: {result[0] if result else 0}")
        
        # Test 2: Create a test node
        test_id = "test-node-12345"
        create_query = f"g.addV('TestNode').property('id','{test_id}').property('partitionKey','demo')"
        result = await cosmos_client._execute_query(create_query)
        log.info(f"✅ Created test node")
        
        # Test 3: Query the test node
        query = f"g.V('{test_id}')"
        result = await cosmos_client._execute_query(query)
        log.info(f"✅ Found test node: {len(result)} nodes")
        
        # Clean up
        await cosmos_client._execute_query(f"g.V('{test_id}').drop()")
        
        log.info("✅ All tests passed!")
        
    except Exception as e:
        log.error(f"❌ Test failed: {e}")
        import traceback
        traceback.print_exc()


async def main():
    """Main entry point for the agenda pipeline."""
    # Initialize Cosmos DB client
    cosmos_client = CosmosGraphClient(
        endpoint=os.getenv("COSMOS_ENDPOINT"),
        username=f"/dbs/{os.getenv('COSMOS_DATABASE')}/colls/{os.getenv('COSMOS_CONTAINER')}",
        password=os.getenv("COSMOS_KEY"),
        partition_key="partitionKey",
        partition_value="demo"
    )
    
    await cosmos_client.connect()
    
    try:
        # Run connection test if enabled
        if RUN_CONN_TEST:
            await test_cosmos_connection(cosmos_client)
        else:
            log.info("Connection test SKIPPED (RUN_CONN_TEST=False)")
        
        # Clear existing data if enabled
        if RUN_CLEAR_GRAPH:
            log.info("Clearing existing graph data...")
            await cosmos_client.clear_graph()
        else:
            log.info("Clear graph SKIPPED (RUN_CLEAR_GRAPH=False)")
            
        # Interactive mode check
        if INTERACTIVE and not RUN_CLEAR_GRAPH:
            clear_existing = input("\nClear existing graph data? (y/N): ").lower() == 'y'
            if clear_existing:
                log.info("Clearing existing graph data...")
                await cosmos_client.clear_graph()
        
        # Find agenda files
        agenda_files = await find_agenda_files()
        if not agenda_files:
            return
        
        log.info(f"Found {len(agenda_files)} agenda files to process")
        
        # Determine how many files to process
        num_to_process = 3  # Default
        if INTERACTIVE:
            user_input = input(f"\nHow many files to process? (1-{len(agenda_files)}, default=3): ")
            try:
                num_to_process = int(user_input)
            except:
                pass
        
        num_to_process = min(max(1, num_to_process), len(agenda_files))
        log.info(f"Will process {num_to_process} files")
        
        # Create pipeline orchestrator
        pipeline = AgendaPipelineOrchestrator(cosmos_client)
        
        # Process files in batches
        await pipeline.process_batch(
            agenda_files[:num_to_process],
            batch_size=1  # Process one at a time for better error tracking
        )
        
        # Log final statistics
        final_stats = pipeline.get_final_stats()
        log.info(f"\n{'='*60}")
        log.info("PIPELINE COMPLETE - FINAL STATISTICS")
        log.info(f"{'='*60}")
        log.info("Stages run configuration:")
        log.info(f"  PDF Extract: {'ENABLED' if RUN_PDF_EXTRACT else 'DISABLED'}")
        log.info(f"  Ontology:    {'ENABLED' if RUN_ONTOLOGY else 'DISABLED'}")
        log.info(f"  Build Graph: {'ENABLED' if RUN_BUILD_GRAPH else 'DISABLED'}")
        log.info("")
        log.info("Results:")
        for key, value in sorted(final_stats.items()):
            log.info(f"  {key}: {value}")
        log.info(f"{'='*60}")
        
    finally:
        await cosmos_client.close()


if __name__ == "__main__":
    # Display current configuration at startup
    print("\n" + "="*60)
    print("AGENDA GRAPH PIPELINE - CONFIGURATION")
    print("="*60)
    print(f"PDF Extract:    {'ENABLED' if RUN_PDF_EXTRACT else 'DISABLED'}")
    print(f"Ontology (LLM): {'ENABLED' if RUN_ONTOLOGY else 'DISABLED'}")
    print(f"Build Graph:    {'ENABLED' if RUN_BUILD_GRAPH else 'DISABLED'}")
    print(f"Clear Graph:    {'ENABLED' if RUN_CLEAR_GRAPH else 'DISABLED'}")
    print(f"Connection Test:{'ENABLED' if RUN_CONN_TEST else 'DISABLED'}")
    print(f"Interactive:    {'ENABLED' if INTERACTIVE else 'DISABLED'}")
    print("="*60 + "\n")
    
    asyncio.run(main())


================================================================================


################################################################################
# File: scripts/rag_local_web_app.py
################################################################################

# File: scripts/rag_local_web_app.py

#!/usr/bin/env python3
################################################################################
################################################################################
"""
Mini Flask app that answers misophonia questions with Retrieval‑Augmented
Generation (gpt-4.1-mini-2025-04-14 + Supabase pgvector).

### Patch 2  (2025‑05‑06)
• **Embeddings** now created with **text‑embedding‑ada‑002** (1536‑D).  
• Similarity is re‑computed client‑side with a **plain cosine function** so the
  ranking no longer depends on pgvector's built‑in distance or any RPC
  threshold quirks.

The rest of the grounded‑answer logic (added in Patch 1) is unchanged.
"""
from __future__ import annotations

import logging
import math
import os
import re
from pathlib import Path        # (unused but left in to mirror original)
from typing import Dict, List
import json

from dotenv import load_dotenv
from flask import Flask, jsonify, request, make_response
from openai import OpenAI
from supabase import create_client
from flask_compress import Compress
from flask_cors import CORS

# ────────────────────────────── configuration ───────────────────────────── #

load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
SUPABASE_URL   = os.getenv("SUPABASE_URL")
SUPABASE_KEY   = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
PORT           = int(os.getenv("PORT", 8080))

if not (OPENAI_API_KEY and SUPABASE_URL and SUPABASE_KEY):
    raise SystemExit(
        "❌  Required env vars: OPENAI_API_KEY, SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY"
    )

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s — %(levelname)s — %(message)s",
)
log = logging.getLogger("rag_app")

sb = create_client(SUPABASE_URL, SUPABASE_KEY)
oa = OpenAI(api_key=OPENAI_API_KEY)

app = Flask(__name__)
CORS(app)
app.config['COMPRESS_ALGORITHM'] = 'gzip'
Compress(app)

# ────────────────────────────── helper functions ────────────────────────── #


def embed(text: str) -> List[float]:
    """
    Return OpenAI embedding vector for *text* using text‑embedding‑ada‑002.

    ada‑002 has 1536 dimensions and is inexpensive yet solid for similarity.
    """
    resp = oa.embeddings.create(
        model="text-embedding-ada-002",
        input=text[:8192],  # safety slice
    )
    return resp.data[0].embedding


def cosine_similarity(a: List[float], b: List[float]) -> float:
    """Plain cosine similarity between two equal‑length vectors."""
    dot = sum(x * y for x, y in zip(a, b))
    na = math.sqrt(sum(x * x for x in a))
    nb = math.sqrt(sum(y * y for y in b))
    return dot / (na * nb + 1e-9)


# Add in-memory embedding cache
_qcache = {}
def embed_cached(text):
    if text in _qcache: return _qcache[text]
    vec = embed(text)
    _qcache[text] = vec
    return vec


# Add regex patterns for bibliography detection
_DOI_RE   = re.compile(r'\b10\.\d{4,9}/[-._;()/:A-Z0-9]+\b', re.I)
_YEAR_RE  = re.compile(r'\b(19|20)\d{2}\b')

def looks_like_refs(text: str) -> bool:
    """
    Return True if this chunk is likely just a bibliography list:
      • more than 12 DOIs, or
      • more than 15 year mentions.
    """
    doi_count  = len(_DOI_RE.findall(text))
    year_count = len(_YEAR_RE.findall(text))
    return doi_count > 12 or year_count > 15


def semantic_search(
    query: str,
    *,
    limit: int = 8,
    threshold: float = 0.0,
) -> List[Dict]:
    """
    Retrieve candidate chunks via the pgvector RPC, then re-rank with an
    **explicit cosine similarity** so the final score is always in
    **[-100 … +100] percent**.

    Why the extra work?
    -------------------
    •  The SQL function returns a raw inner-product that can be > 1.  
       (embeddings are *not* unit-length.)  
    •  By pulling the real 1 536-D vectors and re-computing a cosine we get a
       true, bounded similarity that front-end code can safely show.

    The -100 … +100 range is produced by:  
        pct = clamp(cosine × 100, -100, 100)
    """
    # 1. Embed the query once and keep it cached
    q_vec = embed_cached(query)

    # 2. Fast ANN search in Postgres (over-fetch 4× so we can re-rank)
    rows = (
        sb.rpc(
            "match_documents_chunks",
            {
                "query_embedding": q_vec,
                "match_threshold": threshold,
                "match_count": limit * 4,
            },
        )
        .execute()
        .data
    ) or []

    # 3. Filter out bibliography-only chunks
    rows = [r for r in rows if not looks_like_refs(r["text"])]

    if not rows:
        return []

    # 4. Fetch document metadata (title, authors …) in one round-trip
    doc_ids = {r["document_id"] for r in rows}
    meta = {
        d["id"]: d
        for d in (
            sb.table("city_clerk_documents")
              .select("id,document_type,title,date,year,month,day,mayor,vice_mayor,commissioners,city_attorney,city_manager,city_clerk,public_works_director,agenda,keywords,source_pdf")
              .in_("id", list(doc_ids))
              .execute()
              .data
            or []
        )
    }

    # 5. Pull embeddings and page info once and compute **plain cosine** (no scaling)
    chunk_ids = [r["id"] for r in rows]

    emb_rows = (
        sb.table("documents_chunks")
          .select("id, embedding, page_start, page_end")
          .in_("id", chunk_ids)
          .execute()
          .data
    ) or []

    emb_map: Dict[str, List[float]] = {}
    page_map: Dict[str, Dict] = {}
    for e in emb_rows:
        raw = e["embedding"]
        if isinstance(raw, list):                    # list[Decimal]
            emb_map[e["id"]] = [float(x) for x in raw]
        elif isinstance(raw, str) and raw.startswith('['):   # TEXT  "[…]"
            emb_map[e["id"]] = [float(x) for x in raw.strip('[]').split(',')]
        
        # Store page info
        page_map[e["id"]] = {
            "page_start": e.get("page_start", 1),
            "page_end": e.get("page_end", 1)
        }

    for r in rows:
        vec = emb_map.get(r["id"])
        if vec:                                     # we now have the real vector
            cos = cosine_similarity(q_vec, vec)
            r["similarity"] = round(cos * 100, 1)   # –100…+100 % (or 0…100 %)
        else:                                       # fallback if something failed
            dist = float(r.get("similarity", 1.0))  # 0…2 cosine-distance
            r["similarity"] = round((1.0 - dist) * 100, 1)

        r["doc"] = meta.get(r["document_id"], {})
        
        # Add page info to the row
        page_info = page_map.get(r["id"], {"page_start": 1, "page_end": 1})
        r["page_start"] = page_info["page_start"]
        r["page_end"] = page_info["page_end"]

    # 6. Keep the top *limit* rows after proper re-ranking
    ranked = sorted(rows, key=lambda x: x["similarity"], reverse=True)[:limit]
    return ranked


# ──────────────────────── NEW RAG‑PROMPT HELPERS ───────────────────────── #

MAX_PROMPT_CHARS: int = 24_000  # ~6 k tokens @ 4 chars/token heuristic


def trim_chunks(chunks: List[Dict]) -> List[Dict]:
    """
    Fail‑safe guard: ensure concatenated chunk texts remain under the
    MAX_PROMPT_CHARS budget.  Keeps highest‑similarity chunks first.
    """
    sorted_chunks = sorted(chunks, key=lambda c: c.get("similarity", 0), reverse=True)
    output: List[Dict] = []
    total_chars = 0
    for c in sorted_chunks:
        chunk_len = len(c["text"])
        if total_chars + chunk_len > MAX_PROMPT_CHARS:
            break
        output.append(c)
        total_chars += chunk_len
    return output


def build_prompt(question: str, chunks: List[Dict]) -> str:
    """
    Build a structured prompt that asks GPT to:
      • answer in Markdown with short intro + numbered list of key points
      • cite inline like [1], [2] …
      • finish with a Bibliography that includes the document title and type
    """
    snippet_lines, biblio_lines = [], []
    for i, c in enumerate(chunks, 1):
        page_start = c.get('page_start', 1)
        page_end = c.get('page_end', 1)
        snippet_lines.append(
            f"[{i}] \"{c['text'].strip()}\" "
            f"(pp. {page_start}-{page_end})"
        )

        d = c["doc"]
        title = d.get("title", "Untitled Document")
        doc_type = d.get("document_type", "Document")
        date = d.get("date", "Unknown date")
        year = d.get("year", "n.d.")
        pages = f"pp. {page_start}-{page_end}"
        source_pdf = d.get("source_pdf", "")

        # City clerk document bibliography format
        biblio_lines.append(
            f"[{i}] *{title}* · {doc_type} · {date} · {pages}"
        )

    prompt_parts = [
        "You are City Clerk Assistant, a knowledgeable AI that helps with questions about city government documents, including resolutions, ordinances, proclamations, contracts, meeting minutes, and agendas.",
        "You draw on evidence from official city documents and municipal records.",
        "Your responses are clear, professional, and grounded in the provided context.",
        "====",
        "QUESTION:",
        question,
        "====",
        "CONTEXT:",
        *snippet_lines,
        "====",
        "INSTRUCTIONS:",
        "• Write your answer in **Markdown**.",
        "• Begin with a concise summary (2–3 sentences).",
        "• Then elaborate on key points using well-structured paragraphs.",
        "• Provide relevant insights about city governance, policies, or procedures.",
        "• If helpful, use lists, subheadings, or clear explanations to enhance understanding.",
        "• Use a professional and informative tone.",
        "• Cite sources inline like [1], [2] etc.",
        "• After the answer, include a 'BIBLIOGRAPHY:' section that lists each source exactly as provided below.",
        "• If none of the context answers the question, reply: \"I'm sorry, I don't have sufficient information to answer that.\"",
        "====",
        "BEGIN OUTPUT",
        "ANSWER:",
        "",  # where the model writes the main response
        "BIBLIOGRAPHY:",
        *biblio_lines,
    ]

    return '\n'.join(prompt_parts)


def extract_citations(answer: str) -> List[str]:
    """
    Parse numeric citations (e.g., "[1]", "[2]") from the answer text.
    Returns unique citation numbers in ascending order.
    """
    citations = re.findall(r"\[(\d+)\]", answer)
    return sorted(set(citations), key=int)


# ──────────────────────────────── routes ────────────────────────────────── #

@app.route("/")
def home():
    """Simple homepage for the City Clerk RAG application."""
    html = """
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>City Clerk RAG Assistant</title>
        <style>
            body { 
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
                max-width: 800px; 
                margin: 0 auto; 
                padding: 2rem;
                line-height: 1.6;
                color: #333;
            }
            .header { 
                text-align: center; 
                margin-bottom: 2rem;
                padding-bottom: 1rem;
                border-bottom: 2px solid #e0e0e0;
            }
            .search-container {
                background: #f8f9fa;
                padding: 2rem;
                border-radius: 8px;
                margin: 2rem 0;
            }
            .search-box {
                width: 100%;
                padding: 1rem;
                border: 2px solid #ddd;
                border-radius: 4px;
                font-size: 16px;
                margin-bottom: 1rem;
            }
            .search-btn {
                background: #007bff;
                color: white;
                padding: 1rem 2rem;
                border: none;
                border-radius: 4px;
                cursor: pointer;
                font-size: 16px;
            }
            .search-btn:hover { background: #0056b3; }
            .results { margin-top: 2rem; }
            .answer { 
                background: white; 
                padding: 1.5rem; 
                border-radius: 8px; 
                border-left: 4px solid #007bff;
                margin: 1rem 0;
            }
            .sources { 
                background: #f8f9fa; 
                padding: 1rem; 
                border-radius: 4px; 
                margin-top: 1rem;
                font-size: 0.9em;
            }
            .loading { color: #666; font-style: italic; }
            .error { color: #dc3545; background: #f8d7da; padding: 1rem; border-radius: 4px; }
        </style>
    </head>
    <body>
        <div class="header">
            <h1>🏛️ City Clerk RAG Assistant</h1>
            <p>Ask questions about city government documents, resolutions, ordinances, and meeting minutes</p>
        </div>
        
        <div class="search-container">
            <input type="text" id="queryInput" class="search-box" 
                   placeholder="Ask a question about city documents..." 
                   onkeypress="if(event.key==='Enter') search()">
            <button onclick="search()" class="search-btn">Search</button>
        </div>
        
        <div id="results" class="results"></div>
        
        <script>
            async function search() {
                const query = document.getElementById('queryInput').value.trim();
                if (!query) return;
                
                const resultsDiv = document.getElementById('results');
                resultsDiv.innerHTML = '<div class="loading">Searching...</div>';
                
                try {
                    const response = await fetch('/search', {
                        method: 'POST',
                        headers: { 'Content-Type': 'application/json' },
                        body: JSON.stringify({ query: query })
                    });
                    
                    const data = await response.json();
                    
                    if (data.error) {
                        resultsDiv.innerHTML = `<div class="error">Error: ${data.error}</div>`;
                        return;
                    }
                    
                    let html = `<div class="answer">${data.answer.replace(/\\n/g, '<br>')}</div>`;
                    
                    if (data.results && data.results.length > 0) {
                        html += '<div class="sources"><strong>Sources:</strong><ul>';
                        data.results.forEach((result, i) => {
                            const doc = result.doc || {};
                            const title = doc.title || 'Untitled Document';
                            const similarity = Math.round(result.similarity || 0);
                            html += `<li>${title} (${similarity}% match)</li>`;
                        });
                        html += '</ul></div>';
                    }
                    
                    resultsDiv.innerHTML = html;
                } catch (error) {
                    resultsDiv.innerHTML = `<div class="error">Error: ${error.message}</div>`;
                }
            }
        </script>
    </body>
    </html>
    """
    return html

@app.post("/search")
def search():
    payload = request.get_json(force=True, silent=True) or {}
    question = (payload.get("query") or "").strip()
    if not question:
        return jsonify({"error": "Missing 'query'"}), 400

    try:
        # Retrieve semantic matches (client‑side cosine re‑ranked)
        raw_matches = semantic_search(question, limit=int(payload.get("limit", 8)))

        if not raw_matches:
            return jsonify(
                {
                    "answer": "I'm sorry, I don't have sufficient information to answer that.",
                    "citations": [],
                    "results": [],
                }
            )

        # ──────────────────── TRIM CHUNKS TO BUDGET ──────────────────── #
        chunks = trim_chunks(raw_matches)

        # ──────────────────── BUILD PROMPT & CALL LLM ─────────────────── #
        prompt = build_prompt(question, chunks)

        completion = oa.chat.completions.create(
            model="gpt-4.1-mini-2025-04-14",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0,
        )
        answer_text: str = completion.choices[0].message.content.strip()

        # ──────────────────── EXTRACT CITATIONS ──────────────────────── #
        citations = extract_citations(answer_text)

        # Remove embedding vectors before sending back to the browser
        for m in raw_matches:
            m.pop("embedding", None)

        # ──────────────────── RETURN JSON ─────────────────────────────── #
        response = jsonify(
            {
                "answer": answer_text,
                "citations": citations,
                "results": raw_matches,
            }
        )
        response.headers['Connection'] = 'keep-alive'
        return response
    except Exception as exc:  # noqa: BLE001
        log.exception("search failed")
        return jsonify({"error": str(exc)}), 500


@app.get("/stats")
def stats():
    """Tiny ops endpoint—count total chunks."""
    resp = sb.table("documents_chunks").select("id", count="exact").execute()
    return jsonify({"total_chunks": resp.count})


# ──────────────────────────────── main ─────────────────────────────────── #

if __name__ == "__main__":
    log.info("Starting Flask on 0.0.0.0:%s …", PORT)
    app.run(host="0.0.0.0", port=PORT, debug=True)


================================================================================


################################################################################
# File: scripts/graph_stages/agenda_ontology_extractor.py
################################################################################

# File: scripts/graph_stages/agenda_ontology_extractor.py

"""
Agenda Ontology Extractor
========================
Extracts city administration ontology from agenda documents using LLM.
"""
import json
import logging
import re
from typing import Dict, List, Any

log = logging.getLogger(__name__)


class CityClerkOntologyExtractor:
    """Extract city administration ontology from agenda documents."""
    
    STANDARD_SECTIONS = [
        "Call to Order",
        "Invocation", 
        "Pledge of Allegiance",
        "Presentations and Protocol Documents",
        "Approval of Minutes",
        "Public Comments",
        "Consent Agenda",
        "Public Hearings",
        "Ordinances on Second Reading",
        "Resolutions",
        "City Commission Items",
        "Board/Committee Items",
        "City Manager Items",
        "City Attorney Items",
        "City Clerk Items",
        "Discussion Items",
        "Adjournment"
    ]
    
    def __init__(self, llm_client):
        self.llm = llm_client
        
    async def extract_agenda_ontology(self, agenda_data: Dict, filename: str) -> Dict:
        """Extract complete ontology from agenda document."""
        
        # Extract meeting date from filename
        meeting_date = self._extract_meeting_date(filename)
        
        # Prepare document text
        full_text = self._prepare_document_text(agenda_data)
        
        ontology = {
            'meeting_date': meeting_date,
            'filename': filename,
            'entities': {},
            'relationships': []
        }
        
        # 1. Extract meeting information
        meeting_info = await self._extract_meeting_info(full_text[:4000])
        ontology['meeting_info'] = meeting_info
        
        # 2. Extract hierarchical agenda structure with ALL items
        agenda_structure = await self._extract_complete_agenda_structure(full_text)
        ontology['agenda_structure'] = agenda_structure
        
        # 3. Extract all entities (people, organizations, locations, etc.)
        entities = await self._extract_entities(full_text)
        ontology['entities'] = entities
        
        # 4. Extract item codes and their metadata
        item_codes = await self._extract_item_codes_and_metadata(full_text)
        ontology['item_codes'] = item_codes
        
        # 5. Extract cross-references and relationships
        relationships = await self._extract_relationships(agenda_structure, item_codes)
        ontology['relationships'] = relationships
        
        return ontology
    
    def _extract_meeting_date(self, filename: str) -> str:
        """Extract date from filename 'Agenda M.DD.YYYY.pdf'"""
        date_match = re.search(r'Agenda\s+(\d{1,2})\.(\d{2})\.(\d{4})', filename)
        if date_match:
            month, day, year = date_match.groups()
            return f"{int(month):02d}.{day}.{year}"
        return "unknown"
    
    def _prepare_document_text(self, agenda_data: Dict) -> str:
        """Prepare clean document text."""
        sections_text = []
        for section in agenda_data.get('sections', []):
            text = section.get('text', '').strip()
            if text and not text.startswith('self_ref='):
                sections_text.append(text)
        return "\n\n".join(sections_text)
    
    def _extract_json_from_response(self, response_content: str) -> Any:
        """Extract JSON from LLM response, handling various formats."""
        if not response_content:
            log.error("Empty response from LLM")
            return None
            
        # Try direct JSON parsing first
        try:
            return json.loads(response_content)
        except json.JSONDecodeError:
            pass
        
        # Try to find JSON in markdown code blocks
        json_match = re.search(r'```(?:json)?\s*(\{.*?\}|\[.*?\])\s*```', response_content, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group(1))
            except json.JSONDecodeError:
                pass
        
        # Try to find raw JSON
        json_match = re.search(r'(\{.*\}|\[.*\])', response_content, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group(1))
            except json.JSONDecodeError:
                pass
        
        log.error(f"Could not parse JSON from response: {response_content[:200]}...")
        return None
    
    async def _extract_meeting_info(self, text: str) -> Dict:
        """Extract detailed meeting information."""
        prompt = f"""Analyze this city commission meeting agenda and extract meeting details.

Text:
{text}

Return a JSON object with these fields:
{{
    "meeting_type": "Regular Meeting or Special Meeting or Workshop",
    "meeting_time": "time if mentioned",
    "location": {{
        "name": "venue name",
        "address": "full address"
    }},
    "officials_present": {{
        "mayor": "name or null",
        "vice_mayor": "name or null",
        "commissioners": ["names"] or [],
        "city_attorney": "name or null",
        "city_manager": "name or null",
        "city_clerk": "name or null",
        "other_officials": []
    }},
    "key_topics": ["main topics"],
    "special_presentations": []
}}

Return ONLY the JSON object, no additional text."""

        try:
            response = self.llm.chat.completions.create(
                model=self.llm.deployment_name if hasattr(self.llm, 'deployment_name') else "gpt-4o",
                temperature=0.0,
                messages=[
                    {"role": "system", "content": "You are a municipal document analyzer. Return only valid JSON."},
                    {"role": "user", "content": prompt}
                ]
            )
            
            content = response.choices[0].message.content
            result = self._extract_json_from_response(content)
            return result or {"meeting_type": "unknown"}
            
        except Exception as e:
            log.error(f"Failed to extract meeting info: {e}")
            return {"meeting_type": "unknown"}
    
    async def _extract_complete_agenda_structure(self, text: str) -> List[Dict]:
        """Extract the complete hierarchical structure of the agenda."""
        # Process in smaller chunks to avoid token limits
        chunks = self._chunk_text(text, 8000)
        all_sections = []
        
        for i, chunk in enumerate(chunks):
            prompt = f"""Extract the agenda structure from this text.

Text:
{chunk}

Find ALL sections and items. Look for patterns like:
- Section headers (e.g., "CONSENT AGENDA", "PUBLIC HEARINGS")
- Item codes (E-1, F-12, G-2, etc.)
- Item titles and descriptions

Return a JSON array like this:
[
    {{
        "section_name": "Consent Agenda",
        "section_type": "CONSENT",
        "order": 1,
        "items": [
            {{
                "item_code": "E-1",
                "title": "Resolution approving...",
                "item_type": "Resolution",
                "document_reference": "2024-66",
                "sponsor": "Commissioner Name",
                "department": "Department Name",
                "summary": "Brief summary"
            }}
        ]
    }}
]

Return ONLY the JSON array."""

            try:
                response = self.llm.chat.completions.create(
                    model=self.llm.deployment_name if hasattr(self.llm, 'deployment_name') else "gpt-4o",
                    temperature=0.0,
                    max_tokens=4000,
                    messages=[
                        {"role": "system", "content": "Extract agenda structure. Return only valid JSON."},
                        {"role": "user", "content": prompt}
                    ]
                )
                
                content = response.choices[0].message.content
                sections = self._extract_json_from_response(content)
                if sections and isinstance(sections, list):
                    all_sections.extend(sections)
                    log.info(f"Extracted {len(sections)} sections from chunk {i+1}")
                    
            except Exception as e:
                log.error(f"Failed to parse chunk {i+1}: {e}")
        
        return self._merge_and_clean_sections(all_sections)
    
    async def _extract_entities(self, text: str) -> Dict[str, List[Dict]]:
        """Extract all named entities from the agenda."""
        chunks = self._chunk_text(text, 6000)
        all_entities = {
            'people': [],
            'organizations': [],
            'locations': [],
            'monetary_amounts': [],
            'dates': [],
            'legal_references': []
        }
        
        for chunk in chunks[:3]:  # Limit to first 3 chunks
            prompt = f"""Extract entities from this agenda text.

Text:
{chunk[:4000]}

Return JSON with:
{{
    "people": [{{"name": "John Smith", "role": "Mayor", "context": "presiding"}}],
    "organizations": [{{"name": "City Commission", "type": "government"}}],
    "locations": [{{"name": "City Hall", "address": "405 Biltmore Way"}}],
    "monetary_amounts": [{{"amount": "$100,000", "purpose": "budget"}}],
    "dates": [{{"date": "01/09/2024", "event": "meeting date"}}],
    "legal_references": [{{"type": "Resolution", "number": "2024-01"}}]
}}

Return ONLY the JSON object."""

            try:
                response = self.llm.chat.completions.create(
                    model=self.llm.deployment_name if hasattr(self.llm, 'deployment_name') else "gpt-4o",
                    temperature=0.0,
                    messages=[
                        {"role": "system", "content": "Extract entities. Return only valid JSON."},
                        {"role": "user", "content": prompt}
                    ]
                )
                
                content = response.choices[0].message.content
                entities = self._extract_json_from_response(content)
                if entities and isinstance(entities, dict):
                    # Merge with existing entities
                    for category, items in entities.items():
                        if category in all_entities and isinstance(items, list):
                            all_entities[category].extend(items)
                            
            except Exception as e:
                log.error(f"Failed to extract entities: {e}")
        
        # Deduplicate entities
        for category in all_entities:
            all_entities[category] = self._deduplicate_entities(all_entities[category])
        
        return all_entities
    
    async def _extract_item_codes_and_metadata(self, text: str) -> Dict[str, Dict]:
        """Extract all item codes and their associated metadata."""
        # Use regex to find item codes first
        item_codes = {}
        
        # Common patterns for item codes
        patterns = [
            r'\b([A-Z])-(\d+)\b',  # E-1, F-12
            r'\b([A-Z])(\d+)\b',   # E1, F12
            r'\b(\d+)-(\d+)\b',    # 2-1, 2-2
        ]
        
        for pattern in patterns:
            matches = re.finditer(pattern, text)
            for match in matches:
                if pattern == r'\b(\d+)-(\d+)\b':
                    code = match.group(0)
                else:
                    code = f"{match.group(1)}-{match.group(2)}"
                
                if code not in item_codes:
                    # Extract context around the code
                    start = max(0, match.start() - 200)
                    end = min(len(text), match.end() + 500)
                    context = text[start:end]
                    
                    # Extract title from context
                    title_match = re.search(rf'{re.escape(code)}[:\s]+([^\n]+)', context)
                    title = title_match.group(1).strip() if title_match else "Unknown"
                    
                    item_codes[code] = {
                        "full_title": title,
                        "type": self._determine_item_type(context),
                        "context": context
                    }
        
        log.info(f"Found {len(item_codes)} item codes via regex")
        return item_codes
    
    def _determine_item_type(self, context: str) -> str:
        """Determine item type from context."""
        context_lower = context.lower()
        if 'resolution' in context_lower:
            return 'Resolution'
        elif 'ordinance' in context_lower:
            return 'Ordinance'
        elif 'contract' in context_lower:
            return 'Contract'
        elif 'proclamation' in context_lower:
            return 'Proclamation'
        elif 'report' in context_lower:
            return 'Report'
        else:
            return 'Item'
    
    async def _extract_relationships(self, agenda_structure: List[Dict], item_codes: Dict) -> List[Dict]:
        """Extract relationships between items."""
        relationships = []
        
        # Create sequential relationships
        all_items = []
        for section in agenda_structure:
            for item in section.get('items', []):
                all_items.append({
                    'code': item['item_code'],
                    'section': section['section_name']
                })
        
        # Sequential relationships within sections
        for i in range(len(all_items) - 1):
            if all_items[i]['section'] == all_items[i+1]['section']:
                relationships.append({
                    'from_code': all_items[i]['code'],
                    'to_code': all_items[i+1]['code'],
                    'relationship_type': 'FOLLOWS',
                    'description': 'Sequential items in same section',
                    'strength': 'strong'
                })
        
        return relationships
    
    def _chunk_text(self, text: str, chunk_size: int) -> List[str]:
        """Split text into overlapping chunks."""
        chunks = []
        overlap = 200
        
        for i in range(0, len(text), chunk_size - overlap):
            chunk = text[i:i + chunk_size]
            chunks.append(chunk)
        
        return chunks
    
    def _merge_and_clean_sections(self, sections: List[Dict]) -> List[Dict]:
        """Merge duplicate sections and ensure all items have codes."""
        merged = {}
        
        for section in sections:
            key = section.get('section_name', 'Unknown')
            
            if key not in merged:
                merged[key] = section
                merged[key]['items'] = section.get('items', [])
            else:
                # Merge items, avoiding duplicates
                existing_codes = {item.get('item_code', '') for item in merged[key].get('items', [])}
                
                for item in section.get('items', []):
                    if item.get('item_code') and item['item_code'] not in existing_codes:
                        merged[key]['items'].append(item)
        
        # Sort by order
        result = list(merged.values())
        result.sort(key=lambda x: x.get('order', 999))
        
        return result
    
    def _deduplicate_entities(self, entities: List[Dict]) -> List[Dict]:
        """Remove duplicate entities."""
        seen = set()
        unique = []
        
        for entity in entities:
            # Create a key based on the entity's main identifier
            if 'name' in entity:
                key = entity['name'].lower().strip()
            elif 'amount' in entity:
                key = entity['amount']
            elif 'date' in entity:
                key = entity['date']
            else:
                continue
            
            if key not in seen:
                seen.add(key)
                unique.append(entity)
        
        return unique


================================================================================


################################################################################
# File: scripts/clear_database.py
################################################################################

# File: scripts/clear_database.py

#!/usr/bin/env python3
"""
Database Clear Utility
======================

Safely clears Supabase database tables for the Misophonia Research system.
This script will delete all data from:
- research_documents table
- documents_chunks table

⚠️  WARNING: This operation is irreversible!
"""
from __future__ import annotations
import os
import sys
import logging
from typing import Optional
from gremlin_python.driver import client, serializer
import asyncio
import argparse

from dotenv import load_dotenv
from supabase import create_client

# Load environment variables
load_dotenv()

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")

# Cosmos DB configuration
COSMOS_ENDPOINT = os.getenv("COSMOS_ENDPOINT", "wss://aida-graph-db.gremlin.cosmos.azure.com:443")
COSMOS_KEY = os.getenv("COSMOS_KEY")
DATABASE = os.getenv("COSMOS_DATABASE", "cgGraph")
CONTAINER = os.getenv("COSMOS_CONTAINER", "cityClerk")

if not SUPABASE_URL or not SUPABASE_KEY:
    sys.exit("❌ Missing SUPABASE_URL or SUPABASE_SERVICE_ROLE_KEY environment variables")

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s — %(levelname)s — %(message)s"
)
log = logging.getLogger(__name__)

def init_supabase():
    """Initialize Supabase client."""
    return create_client(SUPABASE_URL, SUPABASE_KEY)

def get_table_counts(sb) -> dict:
    """Get current row counts for all tables."""
    counts = {}
    
    try:
        # Count documents
        doc_res = sb.table("research_documents").select("id", count="exact").execute()
        counts["research_documents"] = doc_res.count or 0
        
        # Count chunks
        chunk_res = sb.table("documents_chunks").select("id", count="exact").execute()
        counts["documents_chunks"] = chunk_res.count or 0
        
        # Count chunks with embeddings
        embedded_res = sb.table("documents_chunks").select("id", count="exact").not_.is_("embedding", "null").execute()
        counts["chunks_with_embeddings"] = embedded_res.count or 0
        
    except Exception as e:
        log.error(f"Error getting table counts: {e}")
        return {}
    
    return counts

def confirm_deletion() -> bool:
    """Ask user for confirmation before deletion."""
    print("\n" + "="*60)
    print("⚠️  DATABASE CLEAR WARNING")
    print("="*60)
    print("This will permanently delete ALL data from:")
    print("  • research_documents table")
    print("  • documents_chunks table")
    print("  • All embeddings and metadata")
    print("\n❌ This operation CANNOT be undone!")
    print("="*60)
    
    response = input("\nType 'DELETE ALL DATA' to confirm (or anything else to cancel): ")
    return response.strip() == "DELETE ALL DATA"

def clear_table_batch(sb, table_name: str, batch_size: int = 1000) -> int:
    """Clear all rows from a specific table in batches to avoid timeouts."""
    log.info(f"Clearing table: {table_name} (batch size: {batch_size})")
    
    total_deleted = 0
    
    while True:
        try:
            # Get a batch of IDs to delete
            result = sb.table(table_name).select("id").limit(batch_size).execute()
            
            if not result.data or len(result.data) == 0:
                break
            
            ids_to_delete = [row["id"] for row in result.data]
            log.info(f"Deleting batch of {len(ids_to_delete)} rows from {table_name}")
            
            # Delete this batch
            delete_result = sb.table(table_name).delete().in_("id", ids_to_delete).execute()
            
            if hasattr(delete_result, 'error') and delete_result.error:
                log.error(f"Error deleting batch from {table_name}: {delete_result.error}")
                break
            
            batch_deleted = len(delete_result.data) if delete_result.data else 0
            total_deleted += batch_deleted
            log.info(f"✅ Deleted {batch_deleted} rows from {table_name} (total: {total_deleted})")
            
            # If we deleted fewer than the batch size, we're done
            if batch_deleted < batch_size:
                break
                
        except Exception as e:
            log.error(f"Exception deleting batch from {table_name}: {e}")
            break
    
    log.info(f"✅ Total deleted from {table_name}: {total_deleted}")
    return total_deleted

def clear_table(sb, table_name: str) -> int:
    """Clear all rows from a specific table."""
    return clear_table_batch(sb, table_name, batch_size=500)

def init_cosmos():
    """Initialize Cosmos DB Gremlin client."""
    if not COSMOS_KEY:
        log.warning("Cosmos DB credentials not found - skipping Cosmos operations")
        return None
    
    try:
        gremlin_client = client.Client(
            f"{COSMOS_ENDPOINT}/gremlin",
            "g",
            username=f"/dbs/{DATABASE}/colls/{CONTAINER}",
            password=COSMOS_KEY,
            message_serializer=serializer.GraphSONSerializersV2d0()
        )
        return gremlin_client
    except Exception as e:
        log.error(f"Failed to connect to Cosmos DB: {e}")
        return None

def get_cosmos_counts(gremlin_client) -> dict:
    """Get current counts for Cosmos DB graph."""
    if not gremlin_client:
        return {}
    
    counts = {}
    try:
        # Count all vertices
        result = gremlin_client.submit("g.V().count()").all()
        counts["total_vertices"] = result[0] if result else 0
        
        # Count by label
        for label in ["Document", "Person", "Meeting", "Chunk"]:
            result = gremlin_client.submit(f"g.V().hasLabel('{label}').count()").all()
            counts[f"{label.lower()}_nodes"] = result[0] if result else 0
        
        # Count edges
        result = gremlin_client.submit("g.E().count()").all()
        counts["total_edges"] = result[0] if result else 0
        
    except Exception as e:
        log.error(f"Error getting Cosmos DB counts: {e}")
        return {}
    
    return counts

def clear_cosmos_graph(gremlin_client) -> tuple[int, int]:
    """Clear all nodes and edges from Cosmos DB graph."""
    if not gremlin_client:
        return 0, 0
    
    log.info("Clearing Cosmos DB graph...")
    
    try:
        # Get initial counts
        edge_result = gremlin_client.submit("g.E().count()").all()
        edge_count = edge_result[0] if edge_result else 0
        
        vertex_result = gremlin_client.submit("g.V().count()").all()
        vertex_count = vertex_result[0] if vertex_result else 0
        
        # Drop all edges first (required before dropping vertices)
        log.info(f"Dropping {edge_count} edges...")
        gremlin_client.submit("g.E().drop()").all()
        
        # Then drop all vertices
        log.info(f"Dropping {vertex_count} vertices...")
        gremlin_client.submit("g.V().drop()").all()
        
        log.info("✅ Cosmos DB graph cleared")
        return vertex_count, edge_count
        
    except Exception as e:
        log.error(f"Error clearing Cosmos DB graph: {e}")
        return 0, 0

async def main():
    """Main function to clear databases."""
    parser = argparse.ArgumentParser(description="Clear City Clerk databases")
    parser.add_argument("--supabase", action="store_true", help="Clear Supabase tables")
    parser.add_argument("--cosmos", action="store_true", help="Clear Cosmos DB graph")
    parser.add_argument("--all", action="store_true", help="Clear all databases")
    
    args = parser.parse_args()
    
    # If no specific database selected, default to all
    if not args.supabase and not args.cosmos and not args.all:
        args.all = True
    
    print("🗑️  City Clerk Database Clear Utility")
    print("=" * 50)
    
    # Initialize connections
    sb = None
    gremlin_client = None
    
    if args.supabase or args.all:
        sb = init_supabase()
    
    if args.cosmos or args.all:
        gremlin_client = init_cosmos()
    
    # Get current counts
    print("\n📊 Current database status:")
    
    supabase_counts = {}
    cosmos_counts = {}
    
    if sb:
        supabase_counts = get_table_counts(sb)
        if supabase_counts:
            print("  Supabase:")
            print(f"    • Documents: {supabase_counts['research_documents']:,}")
            print(f"    • Chunks: {supabase_counts['documents_chunks']:,}")
            print(f"    • Chunks with embeddings: {supabase_counts['chunks_with_embeddings']:,}")
    
    if gremlin_client:
        cosmos_counts = get_cosmos_counts(gremlin_client)
        if cosmos_counts:
            print("  Cosmos DB Graph:")
            print(f"    • Total vertices: {cosmos_counts['total_vertices']:,}")
            print(f"    • Total edges: {cosmos_counts['total_edges']:,}")
            print(f"    • Documents: {cosmos_counts.get('document_nodes', 0):,}")
            print(f"    • Persons: {cosmos_counts.get('person_nodes', 0):,}")
            print(f"    • Meetings: {cosmos_counts.get('meeting_nodes', 0):,}")
    
    # Check if any data exists
    has_data = False
    if sb and supabase_counts:
        has_data = has_data or (supabase_counts['research_documents'] > 0 or supabase_counts['documents_chunks'] > 0)
    if gremlin_client and cosmos_counts:
        has_data = has_data or (cosmos_counts['total_vertices'] > 0)
    
    if not has_data:
        print("\n✅ Databases are already empty!")
        return
    
    # Get confirmation
    if not confirm_deletion():
        print("\n✅ Operation cancelled. Databases unchanged.")
        return
    
    print("\n🗑️  Starting database clear operation...")
    
    # Clear Supabase if requested
    if sb and (args.supabase or args.all):
        print("\n📋 Clearing Supabase tables...")
        chunks_deleted = clear_table(sb, "documents_chunks")
        docs_deleted = clear_table(sb, "research_documents")
        print(f"✅ Supabase cleared: {docs_deleted:,} documents, {chunks_deleted:,} chunks")
    
    # Clear Cosmos DB if requested
    if gremlin_client and (args.cosmos or args.all):
        print("\n🌐 Clearing Cosmos DB graph...")
        vertices_deleted, edges_deleted = clear_cosmos_graph(gremlin_client)
        print(f"✅ Cosmos DB cleared: {vertices_deleted:,} vertices, {edges_deleted:,} edges")
    
    print("\n✅ Database clear operation completed!")

if __name__ == "__main__":
    asyncio.run(main())


================================================================================


################################################################################
# File: relationOPENAI.py
################################################################################

# File: relationOPENAI.py

import os, json, time, re, hashlib, traceback
from azure.storage.blob import BlobServiceClient
from gremlin_python.driver import client, serializer
from openai import AzureOpenAI

# ─────────────  TEST-MODE  ────────────────────────────────────
TEST_MODE, MAX_VERTICES = False, 5           # pon False cuando validado
vertex_count, early_exit = 0, False

# ─────────────  CONFIG GENERAL  ───────────────────────────────
BLOB_CONNECTION_STRING = (
    "DefaultEndpointsProtocol=https;"
    "AccountName=rasagptstorageaccount;"
    "AccountKey=[KEY_HERE];"
    "EndpointSuffix=core.windows.net"
)
COSMOS_ENDPOINT = "wss://aida-graph-db.gremlin.cosmos.azure.com:443"
COSMOS_KEY      = "[KEY_HERE]"
DATABASE, CONTAINER = "cgGraph", "cityClerk" 
PARTITION_KEY, PARTITION_VALUE = "partitionKey", "demo"

# ─────────────  NUEVO CLIENTE AZURE OPENAI  ───────────────────
aoai = AzureOpenAI(
    api_key        = [KEY HERE],
    azure_endpoint = "https://aida-gpt4o.openai.azure.com",
    api_version    = "2024-02-15-preview"
)

DEPLOYMENT_NAME = "gpt-4o"          # nombre EXACTO en Deployments

─────────────  RESTO DE CONFIG  ──────────────────────────────
ENTITY_CONTAINERS = [
    "ks-entities-person","ks-entities-organization","ks-entities-location",
    "ks-entities-address","ks-entities-phone","ks-entities-email",
    "ks-entities-url","ks-entities-event","ks-entities-product",
    "ks-entities-persontype","ks-entities-ipaddress",
    "ks-entities-quantity","ks-entities-skill"
]
CONTAINER_TO_LABEL = {
    "ks-entities-person":"Person","ks-entities-organization":"Organization","ks-entities-location":"Location",
    "ks-entities-address":"Address","ks-entities-phone":"PhoneNumber","ks-entities-email":"Email",
    "ks-entities-url":"URL","ks-entities-event":"Event","ks-entities-product":"Product",
    "ks-entities-persontype":"PersonType","ks-entities-ipaddress":"IPAddress",
    "ks-entities-quantity":"Quantity","ks-entities-skill":"Skill"
}
CHUNKS_CONTAINER = "ks-chunks-debug"

# ─────────────  HELPERS  ──────────────────────────────────────
ILLEGAL_ID_CHARS = re.compile(r'[\/\\?#]')
def clean_id(s: str)  -> str: return ILLEGAL_ID_CHARS.sub('_', s)
def clean_txt(s: str) -> str: return s.replace("'", "\\'").replace('"', "")
def limit_reached():
    global early_exit
    if TEST_MODE and vertex_count >= MAX_VERTICES:
        early_exit = True
        return True
    return False

# ─────────────  CONEXIONES  ───────────────────────────────────
print("🔗  Connecting …")
blob = BlobServiceClient.from_connection_string(BLOB_CONNECTION_STRING)
gremlin = client.Client(
    f"{COSMOS_ENDPOINT}/gremlin","g",
    username=f"/dbs/{DATABASE}/colls/{CONTAINER}",
    password=COSMOS_KEY,
    message_serializer=serializer.GraphSONSerializersV2d0())

# ─────────────  CARGA DE CHUNKS  ──────────────────────────────
chunk_text, chunk_entities = {}, {}
print("📥  Loading chunks …")
for b in blob.get_container_client(CHUNKS_CONTAINER).list_blobs():
    if not b.name.endswith(".json"): continue
    doc = json.loads(blob.get_blob_client(CHUNKS_CONTAINER, b.name).download_blob().readall())
    raw_id = doc.get("chunkId") or doc.get("metadata_storage_path") or b.name
    cid    = clean_id(raw_id)
    chunk_text[cid] = doc.get("content") or doc.get("text") or ""
    chunk_entities[cid] = []

print("📥  Loading entities …")
for cont in ENTITY_CONTAINERS:
    label = CONTAINER_TO_LABEL[cont]
    cc    = blob.get_container_client(cont)
    for b in cc.list_blobs():
        if not b.name.endswith(".json"): continue
        data = json.loads(cc.get_blob_client(b).download_blob().readall())
        if isinstance(data, dict): data = [data]
        for e in data:
            raw_cid = e.get("chunkId") or e.get("metadata_storage_path")
            if raw_cid and raw_cid.startswith(f"{CHUNKS_CONTAINER}/"):
                raw_cid = raw_cid[len(CHUNKS_CONTAINER)+1:]
            cid = clean_id(raw_cid)
            if cid not in chunk_entities:
                print(f"⚠️  Unmatched entity → {raw_cid}")
                continue
            name = e.get("text") or e.get("name")
            vid  = f"{label}:{hashlib.sha1(name.encode()).hexdigest()}"
            chunk_entities[cid].append({"id":vid,"label":label,"name":name})

print(f"➡️  Prepared {len(chunk_entities)} chunks.")

PROMPT = """You are a knowledge-graph extractor.
Return only factual triples (pure JSON):
[{{"source":"<id>","relation":"<label>","target":"<id>"}}]

TEXT:
\"\"\"{chunk}\"\"\"

ENTITIES (pairs [id, name]):
{ents}
"""

# ─────────────  MAIN LOOP  ────────────────────────────────────
for cid, ents in chunk_entities.items():
    if early_exit: break
    if not ents:   continue

    print(f"\n🚩  Chunk {cid[:60]}  ({len(ents)} entities)")
    text      = chunk_text[cid][:3000]
    ents_json = [[e["id"], e["name"]] for e in ents]

    # Llamada LLM
    # ── LLM call + robust JSON parse ───────────────────────────
    try:
        rsp = aoai.chat.completions.create(
            model       = DEPLOYMENT_NAME,     # "gpt-4o"
            temperature = 0.0,
            max_tokens  = 256,
            messages = [
                {"role":"system","content":
                "You are a knowledge-graph extractor. "
                "Return only factual triples in valid JSON."},
                {"role":"user","content":
                PROMPT.format(chunk=text, ents=json.dumps(ents_json))}
            ]
        )

        raw = (rsp.choices[0].message.content or "").strip()
        print("🧠  RAW reply:", raw[:120].replace("\n"," ") + ("…" if len(raw) > 120 else ""))

        if not raw:
            print("⚠️  Empty response (content filter?).")
            triples = []

        else:
            try:
                triples = json.loads(raw)
                print(f"🧠  Parsed {len(triples)} triples")
            except json.JSONDecodeError as je:
                print("⚠️  JSONDecodeError:", je)
                print("⚠️  Full reply kept for manual inspection:")
                print(raw)
                triples = []

    except Exception as ex:
        print("❌  LLM call failed:", ex)
        triples = []


    ts = int(time.time()*1000)

    # Chunk vertex
    if not limit_reached():
        gremlin.submit(
            f"g.V('{cid}').fold().coalesce(unfold(),"
            f"addV('Chunk').property(id,'{cid}')"
            f".property('{PARTITION_KEY}','{PARTITION_VALUE}'))").all()

    # Entity vertices & MENTIONS
    for e in ents:
        if limit_reached(): break
        try:
            gremlin.submit(
                f"g.V('{e['id']}').fold().coalesce(unfold(),"
                f"addV('{e['label']}').property(id,'{e['id']}')"
                f".property('name','{clean_txt(e['name'])}')"
                f".property('{PARTITION_KEY}','{PARTITION_VALUE}'))").all()
            gremlin.submit(
                f"g.V('{cid}').coalesce("
                f"outE('MENTIONS').where(inV().hasId('{e['id']}')),"
                f"addE('MENTIONS').to(g.V('{e['id']}')).property('ts',{ts}))").all()
            vertex_count += 1
            print(f"   ✔︎ {e['id']}")
        except Exception:
            print("⚠️  Vert/Edge error\n", traceback.format_exc())

    # Semantic edges
    if not limit_reached():
        for t in triples:
            s,r,d = t.get("source"), t.get("relation"), t.get("target")
            if not (s and r and d): continue
            try:
                gremlin.submit(
                    f"g.V('{s}').coalesce("
                    f"outE('{r}').where(inV().hasId('{d}')),"
                    f"addE('{r}').to(g.V('{d}')))").all()
                print(f"   ⇢ {s} -[{r}]-> {d}")
            except Exception:
                print("⚠️  Edge error\n", traceback.format_exc())

    if early_exit:
        print(f"🛑  Reached {MAX_VERTICES} vertices (TEST).")
        break

    time.sleep(0.05)

print("\n🏁  Finished.")
gremlin.close()


================================================================================


################################################################################
# File: scripts/stages/acceleration_utils.py
################################################################################

# File: scripts/stages/acceleration_utils.py

"""
Hardware acceleration utilities for the pipeline.
Provides Apple Silicon optimization when available, with CPU fallback.
"""
import os
import platform
import multiprocessing as mp
from typing import Optional, Callable, Any, List
import logging
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import asyncio
from functools import partial

log = logging.getLogger(__name__)

class HardwareAccelerator:
    """Detects and manages hardware acceleration capabilities."""
    
    def __init__(self):
        self.is_apple_silicon = self._detect_apple_silicon()
        self.cpu_count = mp.cpu_count()
        self.optimal_workers = self._calculate_optimal_workers()
        
        # Set environment for better performance on macOS
        if self.is_apple_silicon:
            os.environ['OPENBLAS_NUM_THREADS'] = '1'
            os.environ['MKL_NUM_THREADS'] = '1'
            os.environ['OMP_NUM_THREADS'] = '1'
        
        log.info(f"Hardware: {'Apple Silicon' if self.is_apple_silicon else platform.processor()}")
        log.info(f"CPU cores: {self.cpu_count}, Optimal workers: {self.optimal_workers}")
    
    def _detect_apple_silicon(self) -> bool:
        """Detect if running on Apple Silicon."""
        if platform.system() != 'Darwin':
            return False
        try:
            import subprocess
            result = subprocess.run(['sysctl', '-n', 'hw.optional.arm64'], 
                                  capture_output=True, text=True)
            return result.stdout.strip() == '1'
        except:
            return False
    
    def _calculate_optimal_workers(self) -> int:
        """Calculate optimal number of workers based on hardware."""
        if self.is_apple_silicon:
            # Apple Silicon has efficiency and performance cores
            # Use 75% of cores to leave room for system
            return max(1, int(self.cpu_count * 0.75))
        else:
            # Traditional CPU - use all but one core
            return max(1, self.cpu_count - 1)
    
    def get_process_pool(self, max_workers: Optional[int] = None) -> ProcessPoolExecutor:
        """Get optimized process pool executor."""
        workers = max_workers or self.optimal_workers
        return ProcessPoolExecutor(
            max_workers=workers,
            mp_context=mp.get_context('spawn')  # Required for macOS
        )
    
    def get_thread_pool(self, max_workers: Optional[int] = None) -> ThreadPoolExecutor:
        """Get optimized thread pool executor for I/O operations."""
        workers = max_workers or min(32, self.cpu_count * 4)
        return ThreadPoolExecutor(max_workers=workers)

# Global instance
hardware = HardwareAccelerator()

async def run_cpu_bound_concurrent(func: Callable, items: List[Any], 
                                 max_workers: Optional[int] = None,
                                 desc: str = "Processing") -> List[Any]:
    """Run CPU-bound function concurrently on multiple items."""
    loop = asyncio.get_event_loop()
    with hardware.get_process_pool(max_workers) as executor:
        futures = [loop.run_in_executor(executor, func, item) for item in items]
        
        from tqdm.asyncio import tqdm_asyncio
        results = await tqdm_asyncio.gather(*futures, desc=desc)
        return results

async def run_io_bound_concurrent(func: Callable, items: List[Any],
                                max_workers: Optional[int] = None,
                                desc: str = "Processing") -> List[Any]:
    """Run I/O-bound function concurrently on multiple items."""
    loop = asyncio.get_event_loop()
    with hardware.get_thread_pool(max_workers) as executor:
        futures = [loop.run_in_executor(executor, func, item) for item in items]
        
        from tqdm.asyncio import tqdm_asyncio
        results = await tqdm_asyncio.gather(*futures, desc=desc)
        return results


================================================================================


################################################################################
# File: test_query.py
################################################################################

# File: test_query.py

#!/usr/bin/env python3
"""Test query to see exact data format from Cosmos DB."""
import os
import json
from dotenv import load_dotenv
from gremlin_python.driver import client, serializer

load_dotenv()

COSMOS_ENDPOINT = os.getenv("COSMOS_ENDPOINT")
COSMOS_KEY = os.getenv("COSMOS_KEY")
DATABASE = os.getenv("COSMOS_DATABASE", "cgGraph")
CONTAINER = os.getenv("COSMOS_CONTAINER", "cityClerk")

print("Testing Cosmos DB queries...")

gremlin_client = client.Client(
    f"{COSMOS_ENDPOINT}/gremlin",
    "g",
    username=f"/dbs/{DATABASE}/colls/{CONTAINER}",
    password=COSMOS_KEY,
    message_serializer=serializer.GraphSONSerializersV2d0()
)

try:
    # Test 1: Get a sample vertex with valueMap(true)
    print("\n1. Sample vertex with valueMap(true):")
    print("-" * 50)
    result = gremlin_client.submit("g.V().limit(1).valueMap(true)").all().result()
    if result:
        print(json.dumps(result[0], indent=2, default=str))
    
    # Test 2: Get a Meeting vertex
    print("\n2. Sample Meeting vertex:")
    print("-" * 50)
    result = gremlin_client.submit("g.V().hasLabel('Meeting').limit(1).valueMap(true)").all().result()
    if result:
        print(json.dumps(result[0], indent=2, default=str))
    
    # Test 3: Check edge format
    print("\n3. Sample edge with project:")
    print("-" * 50)
    result = gremlin_client.submit("""g.E().limit(1).project('id','source','target','label')
                                      .by(id())
                                      .by(outV().id())
                                      .by(inV().id())
                                      .by(label())""").all().result()
    if result:
        print(json.dumps(result[0], indent=2, default=str))
    
finally:
    gremlin_client.close()


================================================================================


################################################################################
# File: requirements.txt
################################################################################

################################################################################
################################################################################
# Python dependencies for Misophonia Research RAG System

# Core dependencies
openai>=1.82.0
gremlinpython>=3.7.3
aiofiles>=24.1.0
python-dotenv>=1.0.0

# Graph Visualization dependencies
dash>=2.14.0
plotly>=5.17.0
networkx>=3.2.0
dash-table>=5.0.0
dash-cytoscape>=0.3.0

# Database and API dependencies
supabase>=2.0.0

# Optional for async progress bars
tqdm

# Data processing
pandas>=2.0.0
numpy

# PDF processing
PyPDF2>=3.0.0
unstructured>=0.10.0
pdfminer.six>=20221105

# Utilities
colorama>=0.4.6

# For concurrent processing
concurrent-log-handler>=0.9.20

# Async dependencies for optimized pipeline
aiohttp>=3.8.0

# Token counting for OpenAI rate limiting
tiktoken>=0.5.0


================================================================================

