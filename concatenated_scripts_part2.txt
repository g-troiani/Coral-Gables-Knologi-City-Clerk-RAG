# Concatenated Project Code - Part 2 of 3
# Generated: 2025-06-10 15:21:54
# Root Directory: /Users/gianmariatroiani/Documents/knologi/graph_database
================================================================================

# File Index - Which Files Are in Which Parts
################################################################################

## Part 1 (10 files):
  - scripts/graph_stages/verbatim_transcript_linker.py
  - scripts/microsoft_framework/prompt_tuner.py
  - scripts/microsoft_framework/README.md
  - scripts/microsoft_framework/graphrag_initializer.py
  - extract_documents_for_graphrag.py
  - explore_graphrag_sources.py
  - scripts/microsoft_framework/graphrag_pipeline.py
  - scripts/microsoft_framework/incremental_processor.py
  - requirements.txt
  - scripts/graph_stages/__init__.py

## Part 2 (10 files):
  - scripts/microsoft_framework/run_graphrag_pipeline.py
  - scripts/graph_stages/cosmos_db_client.py
  - scripts/extract_all_to_markdown.py
  - scripts/extract_all_pdfs_direct.py
  - check_status.py
  - investigate_graph.py
  - settings.yaml
  - check_ordinances.py
  - scripts/microsoft_framework/__init__.py
  - scripts/microsoft_framework/run_graphrag_direct.py

## Part 3 (10 files):
  - scripts/microsoft_framework/query_router.py
  - scripts/graph_stages/document_linker.py
  - scripts/graph_stages/pdf_extractor.py
  - scripts/microsoft_framework/create_custom_prompts.py
  - scripts/microsoft_framework/cosmos_synchronizer.py
  - scripts/microsoft_framework/graphrag_output_processor.py
  - scripts/microsoft_framework/city_clerk_settings_template.yaml
  - config.py
  - scripts/json_to_markdown_converter.py
  - scripts/microsoft_framework/query_graphrag.py


================================================================================


################################################################################
# File: scripts/microsoft_framework/run_graphrag_pipeline.py
################################################################################

# File: scripts/microsoft_framework/run_graphrag_pipeline.py

#!/usr/bin/env python3
"""
Runner script for the City Clerk GraphRAG Pipeline.

This script demonstrates how to run the complete GraphRAG pipeline and view results.
"""

import asyncio
import os
import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

# Detect virtual environment
def get_venv_python():
    """Get the correct Python executable from virtual environment."""
    # Check if we're in a venv
    if sys.prefix != sys.base_prefix:
        return sys.executable
    
    # Try common venv locations
    venv_paths = [
        'venv/bin/python3',
        'venv/bin/python',
        '.venv/bin/python3',
        '.venv/bin/python',
        'city_clerk_rag/bin/python3',
        'city_clerk_rag/bin/python'
    ]
    
    for venv_path in venv_paths:
        full_path = os.path.join(os.getcwd(), venv_path)
        if os.path.exists(full_path):
            return full_path
    
    # Fallback
    return sys.executable

# Use this in all subprocess calls
PYTHON_EXE = get_venv_python()
print(f"üêç Using Python: {PYTHON_EXE}")

from scripts.microsoft_framework import (
    CityClerkGraphRAGPipeline,
    CityClerkQueryEngine,
    SmartQueryRouter,
    GraphRAGCosmosSync,
    handle_user_query,
    GraphRAGInitializer,
    CityClerkDocumentAdapter,
    CityClerkPromptTuner,
    GraphRAGOutputProcessor
)

# ============================================================================
# PIPELINE CONTROL FLAGS - Set these to control which modules run
# ============================================================================

# Core Pipeline Steps
RUN_INITIALIZATION = True      # Initialize GraphRAG environment and settings
RUN_DOCUMENT_PREP = True       # Convert extracted JSONs to GraphRAG CSV format
RUN_PROMPT_TUNING = True       # Auto-tune prompts for city clerk domain
RUN_GRAPHRAG_INDEX = True      # Run the actual GraphRAG indexing process

# Post-Processing Steps  
DISPLAY_RESULTS = True         # Show summary of extracted entities/relationships
TEST_QUERIES = True            # Run example queries to test the system
SYNC_TO_COSMOS = False         # Sync GraphRAG results to Cosmos DB

# Advanced Options
FORCE_REINDEX = False          # Force re-indexing even if output exists
VERBOSE_MODE = True            # Show detailed progress information
SKIP_CONFIRMATION = False      # Skip confirmation prompts

# ============================================================================

async def main():
    """Main pipeline execution with modular control."""
    
    # Check environment variables
    if not os.getenv("OPENAI_API_KEY"):
        print("‚ùå Error: OPENAI_API_KEY environment variable not set")
        print("Please set it with: export OPENAI_API_KEY='your-api-key'")
        return
    
    print("üöÄ City Clerk GraphRAG Pipeline")
    print("=" * 50)
    print("üìã Module Configuration:")
    print(f"   Initialize Environment: {'‚úÖ' if RUN_INITIALIZATION else '‚è≠Ô∏è'}")
    print(f"   Prepare Documents:      {'‚úÖ' if RUN_DOCUMENT_PREP else '‚è≠Ô∏è'}")
    print(f"   Tune Prompts:          {'‚úÖ' if RUN_PROMPT_TUNING else '‚è≠Ô∏è'}")
    print(f"   Run GraphRAG Index:    {'‚úÖ' if RUN_GRAPHRAG_INDEX else '‚è≠Ô∏è'}")
    print(f"   Display Results:       {'‚úÖ' if DISPLAY_RESULTS else '‚è≠Ô∏è'}")
    print(f"   Test Queries:          {'‚úÖ' if TEST_QUERIES else '‚è≠Ô∏è'}")
    print(f"   Sync to Cosmos:        {'‚úÖ' if SYNC_TO_COSMOS else '‚è≠Ô∏è'}")
    print("=" * 50)
    
    if not SKIP_CONFIRMATION:
        confirm = input("\nProceed with this configuration? (y/N): ")
        if confirm.lower() not in ['y', 'yes']:
            print("‚ùå Pipeline cancelled")
            return
    
    try:
        graphrag_root = project_root / "graphrag_data"
        
        # Step 1: Initialize GraphRAG Environment
        if RUN_INITIALIZATION:
            print("\nüìã Step 1: Initializing GraphRAG Environment")
            print("-" * 30)
            
            initializer = GraphRAGInitializer(project_root)
            initializer.setup_environment()
            print("‚úÖ GraphRAG environment initialized")
        else:
            print("\n‚è≠Ô∏è  Skipping GraphRAG initialization")
            if not graphrag_root.exists():
                print("‚ùå GraphRAG root doesn't exist! Enable RUN_INITIALIZATION")
                return
        
        # Step 2: Prepare Documents
        if RUN_DOCUMENT_PREP:
            print("\nüìã Step 2: Preparing Documents for GraphRAG")
            print("-" * 30)
            
            adapter = CityClerkDocumentAdapter(
                project_root / "city_clerk_documents/extracted_text"
            )
            
            # Use JSON files directly for better structure preservation
            df = adapter.prepare_documents_for_graphrag(graphrag_root)
            print(f"‚úÖ Prepared {len(df)} isolated documents for GraphRAG")
            print("   Each agenda item is now a completely separate entity")
        else:
            print("\n‚è≠Ô∏è  Skipping document preparation")
            csv_path = graphrag_root / "city_clerk_documents.csv"
            if not csv_path.exists():
                print("‚ùå No prepared documents found! Enable RUN_DOCUMENT_PREP")
                return
        
        # Step 3: Prompt Tuning
        if RUN_PROMPT_TUNING:
            print("\nüìã Step 3: Tuning Prompts for City Clerk Domain")
            print("-" * 30)
            
            tuner = CityClerkPromptTuner(graphrag_root)
            prompts_dir = graphrag_root / "prompts"

            # If we are forcing a re-index or skipping confirmation, we should always regenerate prompts
            # to ensure the latest versions from the scripts are used.
            if FORCE_REINDEX or SKIP_CONFIRMATION:
                print("üìù Forcing prompt regeneration to apply new rules...")
                if prompts_dir.exists():
                    import shutil
                    shutil.rmtree(prompts_dir)
                tuner.create_manual_prompts()
                print("‚úÖ Prompts regenerated successfully.")
            
            # Original interactive logic for manual runs
            else:
                if prompts_dir.exists() and list(prompts_dir.glob("*.txt")):
                    print("üìÅ Existing prompts found")
                    reuse = input("Use existing prompts? (Y/n): ")
                    if reuse.lower() != 'n':
                        print("üîÑ Re-creating manual prompts...")
                        tuner.create_manual_prompts()
                        print("‚úÖ Prompts created manually")
                    else:
                        print("‚úÖ Using existing prompts")
                else:
                    print("üìù Creating prompts manually...")
                    tuner.create_manual_prompts()
                    print("‚úÖ Prompts created")
        else:
            print("\n‚è≠Ô∏è  Skipping prompt tuning")
        
        # Step 4: Run GraphRAG Indexing
        if RUN_GRAPHRAG_INDEX:
            print("\nüìã Step 4: Running GraphRAG Indexing")
            print("-" * 30)
            
            # Check if output already exists
            output_dir = graphrag_root / "output"
            if output_dir.exists() and list(output_dir.glob("*.parquet")):
                print("üìÅ Existing GraphRAG output found")
                if not FORCE_REINDEX:
                    reindex = input("Re-run indexing? This may take time (y/N): ")
                    if reindex.lower() != 'y':
                        print("‚úÖ Using existing index")
                    else:
                        print("üèóÔ∏è Re-indexing documents...")
                        await run_graphrag_indexing(graphrag_root, VERBOSE_MODE)
                else:
                    print("üèóÔ∏è Force re-indexing documents...")
                    await run_graphrag_indexing(graphrag_root, VERBOSE_MODE)
            else:
                print("üèóÔ∏è Running GraphRAG indexing (this may take several minutes)...")
                await run_graphrag_indexing(graphrag_root, VERBOSE_MODE)
        else:
            print("\n‚è≠Ô∏è  Skipping GraphRAG indexing")
        
        # Step 5: Display Results Summary
        if DISPLAY_RESULTS:
            print("\nüìä Step 5: Results Summary")
            await display_results_summary(project_root)
        else:
            print("\n‚è≠Ô∏è  Skipping results display")
        
        # Step 6: Test Queries
        if TEST_QUERIES:
            print("\nüîç Step 6: Testing Query System")
            await test_queries(project_root)
        else:
            print("\n‚è≠Ô∏è  Skipping query testing")
        
        # Step 7: Sync to Cosmos DB
        if SYNC_TO_COSMOS:
            print("\nüåê Step 7: Syncing to Cosmos DB")
            await sync_to_cosmos(project_root, skip_prompt=SKIP_CONFIRMATION)
        else:
            print("\n‚è≠Ô∏è  Skipping Cosmos DB sync")
        
        print("\n‚úÖ Pipeline completed successfully!")
        print("\nüìö Next Steps:")
        print("   - Run queries: python scripts/microsoft_framework/test_queries.py")
        print("   - View results: Check graphrag_data/output/")
        print("   - Sync to Cosmos: Set SYNC_TO_COSMOS = True and re-run")
        
    except Exception as e:
        print(f"\n‚ùå Error running pipeline: {e}")
        if VERBOSE_MODE:
            import traceback
            traceback.print_exc()

async def run_graphrag_indexing(graphrag_root: Path, verbose: bool = True):
    """Run GraphRAG indexing with optimized settings."""
    import subprocess
    
    # Set environment variables for better performance
    env = os.environ.copy()
    env['GRAPHRAG_CONCURRENCY'] = '5'      # Enable concurrent processing
    
    cmd = [
        PYTHON_EXE,  # Use the detected venv Python instead of sys.executable
        "-m", "graphrag", "index",
        "--root", str(graphrag_root),
        "--emit", "parquet",  # Use parquet for better performance
    ]
    
    if verbose:
        cmd.append("--verbose")
    
    # Run with optimized environment
    process = subprocess.Popen(
        cmd, 
        stdout=subprocess.PIPE, 
        stderr=subprocess.STDOUT, 
        text=True,
        env=env
    )
    
    # Stream output
    for line in iter(process.stdout.readline, ''):
        if line:
            print(f"   {line.strip()}")
    
    process.wait()
    
    if process.returncode == 0:
        print("‚úÖ GraphRAG indexing completed successfully")
    else:
        raise Exception(f"GraphRAG indexing failed with code {process.returncode}")

async def display_results_summary(project_root: Path):
    """Display summary of GraphRAG results."""
    print("-" * 30)
    
    from scripts.microsoft_framework import GraphRAGOutputProcessor
    
    output_dir = project_root / "graphrag_data/output"
    processor = GraphRAGOutputProcessor(output_dir)
    
    # Get summaries
    entity_summary = processor.get_entity_summary()
    relationship_summary = processor.get_relationship_summary()
    
    if entity_summary:
        print(f"üè∑Ô∏è Entities extracted: {entity_summary.get('total_entities', 0)}")
        print("üìã Entity types:")
        for entity_type, count in entity_summary.get('entity_types', {}).items():
            print(f"   - {entity_type}: {count}")
    
    if relationship_summary:
        print(f"\nüîó Relationships extracted: {relationship_summary.get('total_relationships', 0)}")
        print("üìã Relationship types:")
        for rel_type, count in relationship_summary.get('relationship_types', {}).items():
            print(f"   - {rel_type}: {count}")
    
    # Show file locations
    print(f"\nüìÅ Output files location: {output_dir}")
    output_files = [
        "entities.parquet",
        "relationships.parquet", 
        "communities.parquet",
        "community_reports.parquet"
    ]
    
    for filename in output_files:
        file_path = output_dir / filename
        if file_path.exists():
            size = file_path.stat().st_size / 1024  # KB
            print(f"   ‚úÖ {filename} ({size:.1f} KB)")
        else:
            print(f"   ‚ùå {filename} (not found)")

async def test_queries(project_root: Path):
    """Test the query system with example queries."""
    print("-" * 30)
    
    # Example queries for city clerk documents
    test_queries = [
        "Who is Commissioner Smith?",  # Should use Local search
        "What are the main themes in city development?",  # Should use Global search
        "How has the waterfront project evolved?",  # Should use DRIFT search
        "Tell me about ordinance 2024-01",  # Should use Local search
        "What are the overall budget trends?",  # Should use Global search
    ]
    
    query_engine = CityClerkQueryEngine(project_root / "graphrag_data")
    router = SmartQueryRouter()
    
    for query in test_queries:
        print(f"\n‚ùì Query: '{query}'")
        
        # Show routing decision
        route_info = router.determine_query_method(query)
        print(f"üéØ Router selected: {route_info['method']} ({route_info['intent'].value})")
        
        try:
            # Execute query
            result = await query_engine.query(query)
            print(f"üìù Answer preview: {result['answer'][:200]}...")
            print(f"üîß Parameters used: {result['parameters']}")
        except Exception as e:
            print(f"‚ùå Query failed: {e}")

async def sync_to_cosmos(project_root: Path, skip_prompt: bool = False):
    """Optionally sync results to Cosmos DB."""
    print("-" * 30)
    
    if not skip_prompt:
        user_input = input("Do you want to sync GraphRAG results to Cosmos DB? (y/N): ")
        if user_input.lower() not in ['y', 'yes']:
            print("‚è≠Ô∏è Skipping Cosmos DB sync")
            return
    
    try:
        output_dir = project_root / "graphrag_data/output"
        sync = GraphRAGCosmosSync(output_dir)
        await sync.sync_to_cosmos()
        print("‚úÖ Successfully synced to Cosmos DB")
    except Exception as e:
        print(f"‚ùå Cosmos DB sync failed: {e}")

def show_usage():
    """Show usage instructions."""
    print("""
üöÄ City Clerk GraphRAG Pipeline Runner

CONTROL FLAGS:
   Edit the boolean flags at the top of this file to control which modules run:
   
   RUN_INITIALIZATION - Initialize GraphRAG environment
   RUN_DOCUMENT_PREP - Convert documents to CSV format
   RUN_PROMPT_TUNING - Auto-tune prompts
   RUN_GRAPHRAG_INDEX - Run indexing process
   DISPLAY_RESULTS - Show summary statistics
   TEST_QUERIES - Test example queries
   SYNC_TO_COSMOS - Sync to Cosmos DB
   
USAGE:
   python3 scripts/microsoft_framework/run_graphrag_pipeline.py [options]
   
OPTIONS:
   -h, --help     Show this help message
   --force        Force re-indexing (sets FORCE_REINDEX=True)
   --quiet        Minimal output (sets VERBOSE_MODE=False)
   --yes          Skip confirmations (sets SKIP_CONFIRMATION=True)
   --cosmos       Enable Cosmos sync (sets SYNC_TO_COSMOS=True)

EXAMPLES:
   # Run with default settings
   python3 scripts/microsoft_framework/run_graphrag_pipeline.py
   
   # Force complete re-index
   python3 scripts/microsoft_framework/run_graphrag_pipeline.py --force --yes
   
   # Just test queries (edit flags to disable other steps)
   python3 scripts/microsoft_framework/run_graphrag_pipeline.py
    """)

if __name__ == "__main__":
    # Parse command line arguments
    if len(sys.argv) > 1:
        for arg in sys.argv[1:]:
            if arg in ['-h', '--help', 'help']:
                show_usage()
                sys.exit(0)
            elif arg == '--force':
                FORCE_REINDEX = True
            elif arg == '--quiet':
                VERBOSE_MODE = False
            elif arg == '--yes':
                SKIP_CONFIRMATION = True
            elif arg == '--cosmos':
                SYNC_TO_COSMOS = True
    
    # Run the pipeline
    asyncio.run(main())


================================================================================


################################################################################
# File: scripts/graph_stages/cosmos_db_client.py
################################################################################

# File: scripts/graph_stages/cosmos_db_client.py

"""
Azure Cosmos DB Gremlin client for city clerk graph database.
Provides async operations for graph manipulation.
"""

from __future__ import annotations
import asyncio
import logging
from typing import Any, Dict, List, Optional, Union
import os
from gremlin_python.driver import client, serializer
from gremlin_python.driver.driver_remote_connection import DriverRemoteConnection
from gremlin_python.structure.graph import Graph
from dotenv import load_dotenv
import json

load_dotenv()

log = logging.getLogger('cosmos_graph_client')


class CosmosGraphClient:
    """Async client for Azure Cosmos DB Gremlin API."""
    
    def __init__(self, 
                 endpoint: Optional[str] = None,
                 key: Optional[str] = None,
                 database: Optional[str] = None,
                 container: Optional[str] = None,
                 partition_value: str = "demo"):
        """Initialize Cosmos DB client."""
        self.endpoint = endpoint or os.getenv("COSMOS_ENDPOINT")
        self.key = key or os.getenv("COSMOS_KEY")
        self.database = database or os.getenv("COSMOS_DATABASE", "cgGraph")
        self.container = container or os.getenv("COSMOS_CONTAINER", "cityClerk")
        self.partition_value = partition_value
        
        if not all([self.endpoint, self.key, self.database, self.container]):
            raise ValueError("Missing required Cosmos DB configuration")
        
        self._client = None
        self._loop = None  # Don't get the loop in __init__
    
    async def connect(self) -> None:
        """Establish connection to Cosmos DB."""
        try:
            # Get the current running loop
            self._loop = asyncio.get_running_loop()
            
            self._client = client.Client(
                f"{self.endpoint}/gremlin",
                "g",
                username=f"/dbs/{self.database}/colls/{self.container}",
                password=self.key,
                message_serializer=serializer.GraphSONSerializersV2d0()
            )
            log.info(f"‚úÖ Connected to Cosmos DB: {self.database}/{self.container}")
        except Exception as e:
            log.error(f"‚ùå Failed to connect to Cosmos DB: {e}")
            raise
    
    async def _execute_query(self, query: str, bindings: Optional[Dict] = None) -> List[Any]:
        """Execute a Gremlin query asynchronously."""
        if not self._client:
            await self.connect()
        
        try:
            # Get the current event loop
            loop = asyncio.get_running_loop()
            
            # Run synchronous operation in thread pool
            future = loop.run_in_executor(
                None,
                lambda: self._client.submit(query, bindings or {})
            )
            callback = await future
            
            # Collect results
            results = []
            for result in callback:
                results.extend(result)
            
            return results
        except Exception as e:
            log.error(f"Query execution failed: {query[:100]}... Error: {e}")
            raise
    
    async def clear_graph(self) -> None:
        """Clear all vertices and edges from the graph."""
        log.warning("üóëÔ∏è  Clearing entire graph...")
        try:
            # Drop all vertices (edges are automatically removed)
            await self._execute_query("g.V().drop()")
            log.info("‚úÖ Graph cleared successfully")
        except Exception as e:
            log.error(f"Failed to clear graph: {e}")
            raise
    
    async def create_vertex(self, 
                          label: str,
                          vertex_id: str,
                          properties: Dict[str, Any],
                          update_if_exists: bool = True) -> None:
        """Create a vertex with properties, optionally updating if exists."""
        
        # Check if vertex already exists
        if await self.vertex_exists(vertex_id):
            if update_if_exists:
                # Update existing vertex
                await self.update_vertex(vertex_id, properties)
                log.info(f"Updated existing vertex: {vertex_id}")
            else:
                log.info(f"Vertex already exists, skipping: {vertex_id}")
            return
        
        # Build property chain
        prop_chain = ""
        for key, value in properties.items():
            if value is not None:
                # Handle different value types
                if isinstance(value, bool):
                    prop_chain += f".property('{key}', {str(value).lower()})"
                elif isinstance(value, (int, float)):
                    prop_chain += f".property('{key}', {value})"
                elif isinstance(value, list):
                    # Convert list to JSON string
                    json_val = json.dumps(value).replace("'", "\\'")
                    prop_chain += f".property('{key}', '{json_val}')"
                else:
                    # Escape string values
                    escaped_val = str(value).replace("'", "\\'").replace('"', '\\"')
                    prop_chain += f".property('{key}', '{escaped_val}')"
        
        # Always add partition key
        prop_chain += f".property('partitionKey', '{self.partition_value}')"
        
        query = f"g.addV('{label}').property('id', '{vertex_id}'){prop_chain}"
        
        await self._execute_query(query)

    async def update_vertex(self, vertex_id: str, properties: Dict[str, Any]) -> None:
        """Update properties of an existing vertex."""
        # Build property update chain
        prop_chain = ""
        for key, value in properties.items():
            if value is not None:
                if isinstance(value, bool):
                    prop_chain += f".property('{key}', {str(value).lower()})"
                elif isinstance(value, (int, float)):
                    prop_chain += f".property('{key}', {value})"
                elif isinstance(value, list):
                    json_val = json.dumps(value).replace("'", "\\'")
                    prop_chain += f".property('{key}', '{json_val}')"
                else:
                    escaped_val = str(value).replace("'", "\\'").replace('"', '\\"')
                    prop_chain += f".property('{key}', '{escaped_val}')"
        
        query = f"g.V('{vertex_id}'){prop_chain}"
        
        try:
            await self._execute_query(query)
            log.info(f"Updated vertex {vertex_id}")
        except Exception as e:
            log.error(f"Failed to update vertex {vertex_id}: {e}")
            raise

    async def upsert_vertex(self, 
                           label: str,
                           vertex_id: str,
                           properties: Dict[str, Any]) -> bool:
        """Create or update a vertex. Returns True if created, False if updated."""
        # Check if vertex exists
        if await self.vertex_exists(vertex_id):
            await self.update_vertex(vertex_id, properties)
            return False  # Updated
        else:
            await self.create_vertex(label, vertex_id, properties)
            return True  # Created

    async def create_edge(self,
                         from_id: str,
                         to_id: str,
                         edge_type: str,
                         properties: Optional[Dict[str, Any]] = None) -> None:
        """Create an edge between two vertices."""
        # Build property chain for edge
        prop_chain = ""
        if properties:
            for key, value in properties.items():
                if value is not None:
                    if isinstance(value, bool):
                        prop_chain += f".property('{key}', {str(value).lower()})"
                    elif isinstance(value, (int, float)):
                        prop_chain += f".property('{key}', {value})"
                    else:
                        escaped_val = str(value).replace("'", "\\'")
                        prop_chain += f".property('{key}', '{escaped_val}')"
        
        query = f"g.V('{from_id}').addE('{edge_type}').to(g.V('{to_id}')){prop_chain}"
        
        try:
            await self._execute_query(query)
        except Exception as e:
            log.error(f"Failed to create edge {from_id} -> {to_id}: {e}")
            raise
    
    async def create_edge_if_not_exists(self,
                                       from_id: str,
                                       to_id: str,
                                       edge_type: str,
                                       properties: Optional[Dict[str, Any]] = None) -> bool:
        """Create an edge if it doesn't already exist. Returns True if created."""
        # Check if edge already exists
        check_query = f"g.V('{from_id}').outE('{edge_type}').where(inV().hasId('{to_id}')).count()"
        
        try:
            result = await self._execute_query(check_query)
            exists = result[0] > 0 if result else False
            
            if not exists:
                await self.create_edge(from_id, to_id, edge_type, properties)
                return True
            else:
                log.debug(f"Edge already exists: {from_id} -[{edge_type}]-> {to_id}")
                return False
        except Exception as e:
            log.error(f"Failed to check/create edge: {e}")
            raise
    
    async def vertex_exists(self, vertex_id: str) -> bool:
        """Check if a vertex exists."""
        result = await self._execute_query(f"g.V('{vertex_id}').count()")
        return result[0] > 0 if result else False
    
    async def get_vertex(self, vertex_id: str) -> Optional[Dict]:
        """Get a vertex by ID."""
        result = await self._execute_query(f"g.V('{vertex_id}').valueMap(true)")
        return result[0] if result else None
    
    async def close(self) -> None:
        """Close the connection properly."""
        if self._client:
            try:
                # Close the client synchronously since it's not async
                self._client.close()
                self._client = None
                log.info("Connection closed")
            except Exception as e:
                log.warning(f"Error during client close: {e}")
    
    async def __aenter__(self):
        await self.connect()
        return self
    
    async def __aexit__(self, *args):
        await self.close()


================================================================================


################################################################################
# File: scripts/extract_all_to_markdown.py
################################################################################

# File: scripts/extract_all_to_markdown.py

#!/usr/bin/env python3
"""
Extract all PDFs to markdown format for GraphRAG processing.
"""

import asyncio
from pathlib import Path
import logging
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import os

from graph_stages.pdf_extractor import PDFExtractor
from graph_stages.agenda_pdf_extractor import AgendaPDFExtractor
from graph_stages.enhanced_document_linker import EnhancedDocumentLinker
from graph_stages.verbatim_transcript_linker import VerbatimTranscriptLinker

logging.basicConfig(level=logging.INFO)
log = logging.getLogger(__name__)

async def extract_all_documents():
    """Extract all city clerk documents with parallel processing."""
    
    base_dir = Path("city_clerk_documents/global/City Comissions 2024")
    markdown_dir = Path("city_clerk_documents/extracted_markdown")
    markdown_dir.mkdir(exist_ok=True)
    
    if not base_dir.exists():
        log.error(f"‚ùå Base directory not found: {base_dir}")
        log.error(f"   Current working directory: {Path.cwd()}")
        return
    
    log.info(f"üìÅ Base directory found: {base_dir}")
    
    # Set max workers based on CPU cores (but limit to avoid overwhelming system)
    max_workers = min(os.cpu_count() or 4, 8)
    
    stats = {
        'agendas': 0,
        'ordinances': 0,
        'resolutions': 0,
        'transcripts': 0,
        'errors': 0
    }
    
    # Process Agendas in parallel
    log.info(f"üìã Extracting Agendas with {max_workers} workers...")
    agenda_dir = base_dir / "Agendas"
    if agenda_dir.exists():
        log.info(f"   Found agenda directory: {agenda_dir}")
        extractor = AgendaPDFExtractor()
        agenda_pdfs = list(agenda_dir.glob("*.pdf"))
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_pdf = {
                executor.submit(process_agenda_pdf, extractor, pdf): pdf 
                for pdf in agenda_pdfs
            }
            
            for future in as_completed(future_to_pdf):
                pdf = future_to_pdf[future]
                try:
                    success = future.result()
                    if success:
                        stats['agendas'] += 1
                except Exception as e:
                    log.error(f"Failed to extract {pdf}: {e}")
                    stats['errors'] += 1
    else:
        log.warning(f"‚ö†Ô∏è  Agenda directory not found: {agenda_dir}")
    
    # Process Ordinances and Resolutions in parallel
    log.info("üìú Extracting Ordinances and Resolutions in parallel...")
    ord_dir = base_dir / "Ordinances"
    res_dir = base_dir / "Resolutions"
    
    if ord_dir.exists() and res_dir.exists():
        linker = EnhancedDocumentLinker()
        
        # Combine ordinances and resolutions for parallel processing
        all_docs = []
        for pdf in ord_dir.rglob("*.pdf"):
            all_docs.append(('ordinance', pdf))
        for pdf in res_dir.rglob("*.pdf"):
            all_docs.append(('resolution', pdf))
        
        # Process in parallel with asyncio
        async def process_documents_batch(docs_batch):
            tasks = []
            for doc_type, pdf_path in docs_batch:
                meeting_date = extract_meeting_date_from_filename(pdf_path.name)
                if meeting_date:
                    task = process_document_async(linker, pdf_path, meeting_date, doc_type)
                    tasks.append(task)
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            return results
        
        # Process in batches to avoid overwhelming the system
        batch_size = max_workers * 2
        for i in range(0, len(all_docs), batch_size):
            batch = all_docs[i:i + batch_size]
            results = await process_documents_batch(batch)
            
            for result, (doc_type, _) in zip(results, batch):
                if isinstance(result, Exception):
                    stats['errors'] += 1
                    log.error(f"Error: {result}")
                elif result:
                    stats['ordinances' if doc_type == 'ordinance' else 'resolutions'] += 1
    else:
        log.warning(f"‚ö†Ô∏è  Ordinances or Resolutions directory not found: {ord_dir}, {res_dir}")
    
    log.info("üé§ Extracting Verbatim Transcripts...")
    verbatim_dirs = [
        base_dir / "Verbatim Items",
        base_dir / "Verbating Items"
    ]
    
    verbatim_dir = None
    for vdir in verbatim_dirs:
        if vdir.exists():
            verbatim_dir = vdir
            break
    
    if verbatim_dir:
        log.info(f"   Found verbatim directory: {verbatim_dir}")
        transcript_linker = VerbatimTranscriptLinker()
        
        all_verb_pdfs = list(verbatim_dir.rglob("*.pdf"))
        log.info(f"   Found {len(all_verb_pdfs)} verbatim PDFs")
        
        for pdf_path in all_verb_pdfs:
            log.info(f"   Processing: {pdf_path.name}")
            try:
                meeting_date = extract_meeting_date_from_verbatim(pdf_path.name)
                if meeting_date:
                    transcript_info = await transcript_linker._process_transcript(pdf_path, meeting_date)
                    if transcript_info:
                        transcript_linker._save_extracted_text(pdf_path, transcript_info)
                        stats['transcripts'] += 1
                else:
                    log.warning(f"   Could not extract date from: {pdf_path.name}")
            except Exception as e:
                log.error(f"   Failed to process {pdf_path.name}: {e}")
                stats['errors'] += 1
    else:
        log.warning(f"‚ö†Ô∏è  Verbatim directory not found. Tried: {verbatim_dirs}")
    
    log.info("\nüìä Extraction Summary:")
    log.info(f"   Agendas: {stats['agendas']}")
    log.info(f"   Ordinances: {stats['ordinances']}")
    log.info(f"   Resolutions: {stats['resolutions']}")
    log.info(f"   Transcripts: {stats['transcripts']}")
    log.info(f"   Errors: {stats['errors']}")
    log.info(f"   Total: {sum(stats.values()) - stats['errors']}")
    
    log.info(f"\n‚úÖ All documents extracted to:")
    log.info(f"   JSON: city_clerk_documents/extracted_text/")
    log.info(f"   Markdown: {markdown_dir}")

def process_agenda_pdf(extractor, pdf):
    """Process single agenda PDF (for thread pool)."""
    try:
        log.info(f"   Processing: {pdf.name}")
        agenda_data = extractor.extract_agenda(pdf)
        output_path = Path("city_clerk_documents/extracted_text") / f"{pdf.stem}_extracted.json"
        extractor.save_extracted_agenda(agenda_data, output_path)
        return True
    except Exception as e:
        log.error(f"Failed to extract {pdf}: {e}")
        return False

async def process_document_async(linker, pdf_path, meeting_date, doc_type):
    """Process document asynchronously."""
    try:
        doc_info = await linker._process_document(pdf_path, meeting_date, doc_type)
        if doc_info:
            linker._save_extracted_text(pdf_path, doc_info, doc_type)
            return True
        return False
    except Exception as e:
        raise e

def extract_meeting_date_from_filename(filename: str) -> str:
    """Extract meeting date from ordinance/resolution filename."""
    import re
    
    match = re.search(r'(\d{2})_(\d{2})_(\d{4})', filename)
    if match:
        month, day, year = match.groups()
        return f"{month}.{day}.{year}"
    
    return None

def extract_meeting_date_from_verbatim(filename: str) -> str:
    """Extract meeting date from verbatim transcript filename."""
    import re
    
    match = re.match(r'(\d{2})_(\d{2})_(\d{4})', filename)
    if match:
        month, day, year = match.groups()
        return f"{month}.{day}.{year}"
    
    return None

if __name__ == "__main__":
    asyncio.run(extract_all_documents())


================================================================================


################################################################################
# File: scripts/extract_all_pdfs_direct.py
################################################################################

# File: scripts/extract_all_pdfs_direct.py

#!/usr/bin/env python3
"""
Direct extraction of all PDFs without relying on specific directory structure.
"""

import asyncio
from pathlib import Path
import logging
import re

from graph_stages.pdf_extractor import PDFExtractor
from graph_stages.agenda_pdf_extractor import AgendaPDFExtractor
from graph_stages.enhanced_document_linker import EnhancedDocumentLinker
from graph_stages.verbatim_transcript_linker import VerbatimTranscriptLinker

logging.basicConfig(level=logging.INFO)
log = logging.getLogger(__name__)

async def extract_all_pdfs():
    """Extract all PDFs found in city clerk documents."""
    
    base_dir = Path("city_clerk_documents/global")
    
    # Find ALL PDFs recursively
    all_pdfs = list(base_dir.rglob("*.pdf"))
    log.info(f"Found {len(all_pdfs)} total PDFs")
    
    # Categorize PDFs by type
    agendas = []
    ordinances = []
    resolutions = []
    verbatims = []
    unknown = []
    
    for pdf in all_pdfs:
        path_str = str(pdf).lower()
        filename = pdf.name.lower()
        
        if 'agenda' in filename and filename.startswith('agenda'):
            agendas.append(pdf)
        elif 'ordinance' in path_str:
            ordinances.append(pdf)
        elif 'resolution' in path_str:
            resolutions.append(pdf)
        elif 'verbat' in path_str or 'transcript' in filename:
            verbatims.append(pdf)
        else:
            unknown.append(pdf)
    
    log.info(f"\nCategorized PDFs:")
    log.info(f"  üìã Agendas: {len(agendas)}")
    log.info(f"  üìú Ordinances: {len(ordinances)}")
    log.info(f"  üìú Resolutions: {len(resolutions)}")
    log.info(f"  üé§ Verbatims: {len(verbatims)}")
    log.info(f"  ‚ùì Unknown: {len(unknown)}")
    
    # Process each type
    stats = {'success': 0, 'errors': 0}
    
    # 1. Process Agendas
    if agendas:
        log.info("\nüìã Processing Agendas...")
        extractor = AgendaPDFExtractor()
        for pdf in agendas:
            try:
                log.info(f"  Processing: {pdf.name}")
                agenda_data = extractor.extract_agenda(pdf)
                output_path = Path("city_clerk_documents/extracted_text") / f"{pdf.stem}_extracted.json"
                output_path.parent.mkdir(exist_ok=True)
                extractor.save_extracted_agenda(agenda_data, output_path)
                stats['success'] += 1
            except Exception as e:
                log.error(f"  Failed: {e}")
                stats['errors'] += 1
    
    # 2. Process Ordinances
    if ordinances:
        log.info("\nüìú Processing Ordinances...")
        linker = EnhancedDocumentLinker()
        for pdf in ordinances:
            try:
                log.info(f"  Processing: {pdf.name}")
                meeting_date = extract_date_from_path(pdf)
                if meeting_date:
                    doc_info = await linker._process_document(pdf, meeting_date, "ordinance")
                    if doc_info:
                        linker._save_extracted_text(pdf, doc_info, "ordinance")
                        stats['success'] += 1
                else:
                    log.warning(f"  No date found for: {pdf.name}")
            except Exception as e:
                log.error(f"  Failed: {e}")
                stats['errors'] += 1
    
    # 3. Process Resolutions
    if resolutions:
        log.info("\nüìú Processing Resolutions...")
        linker = EnhancedDocumentLinker()
        for pdf in resolutions:
            try:
                log.info(f"  Processing: {pdf.name}")
                meeting_date = extract_date_from_path(pdf)
                if meeting_date:
                    doc_info = await linker._process_document(pdf, meeting_date, "resolution")
                    if doc_info:
                        linker._save_extracted_text(pdf, doc_info, "resolution")
                        stats['success'] += 1
                else:
                    log.warning(f"  No date found for: {pdf.name}")
            except Exception as e:
                log.error(f"  Failed: {e}")
                stats['errors'] += 1
    
    # 4. Process Verbatims
    if verbatims:
        log.info("\nüé§ Processing Verbatim Transcripts...")
        transcript_linker = VerbatimTranscriptLinker()
        for pdf in verbatims:
            try:
                log.info(f"  Processing: {pdf.name}")
                meeting_date = extract_date_from_path(pdf)
                if meeting_date:
                    transcript_info = await transcript_linker._process_transcript(pdf, meeting_date)
                    if transcript_info:
                        transcript_linker._save_extracted_text(pdf, transcript_info)
                        stats['success'] += 1
                else:
                    log.warning(f"  No date found for: {pdf.name}")
            except Exception as e:
                log.error(f"  Failed: {e}")
                stats['errors'] += 1
    
    log.info(f"\n‚úÖ Extraction complete:")
    log.info(f"   Success: {stats['success']}")
    log.info(f"   Errors: {stats['errors']}")

def extract_date_from_path(pdf_path: Path) -> str:
    """Extract date from filename or path."""
    # Try different date patterns
    patterns = [
        r'(\d{2})[._](\d{2})[._](\d{4})',  # MM_DD_YYYY or MM.DD.YYYY
        r'(\d{4})-\d+\s*-\s*(\d{2})_(\d{2})_(\d{4})',  # Ordinance pattern
    ]
    
    for pattern in patterns:
        match = re.search(pattern, pdf_path.name)
        if match:
            groups = match.groups()
            if len(groups) == 3:
                month, day, year = groups
            else:  # ordinance pattern
                year, month, day, year2 = groups
            return f"{month}.{day}.{year}"
    
    # Try parent directory for year
    if '2024' in str(pdf_path):
        # Default to a date if we know it's 2024
        return "01.01.2024"
    
    return None

if __name__ == "__main__":
    asyncio.run(extract_all_pdfs())


================================================================================


################################################################################
# File: check_status.py
################################################################################

# File: check_status.py

#!/usr/bin/env python3
"""
Quick status checker for GraphRAG process
"""
import os
import time
from pathlib import Path
from datetime import datetime

def check_status():
    base_dir = Path("/Users/gianmariatroiani/Documents/knologi/graph_database/graphrag_data")
    logs_dir = base_dir / "logs"
    output_dir = base_dir / "output"
    
    print(f"üîç GraphRAG Status Check - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    # Check if process is running
    import subprocess
    try:
        result = subprocess.run(['pgrep', '-f', 'monitor_graphrag.py'], 
                              capture_output=True, text=True)
        if result.stdout.strip():
            print("‚úÖ GraphRAG monitor process is RUNNING")
            print(f"   Process ID: {result.stdout.strip()}")
        else:
            print("‚ùå GraphRAG monitor process is NOT running")
    except:
        print("‚ùì Cannot determine process status")
    
    # Check latest monitor log
    if logs_dir.exists():
        monitor_logs = list(logs_dir.glob("graphrag_monitor_*.log"))
        if monitor_logs:
            latest_log = max(monitor_logs, key=lambda x: x.stat().st_mtime)
            print(f"üìã Latest monitor log: {latest_log.name}")
            
            # Show last few lines
            try:
                with open(latest_log, 'r') as f:
                    lines = f.readlines()
                    if lines:
                        print("üìñ Last 3 log entries:")
                        for line in lines[-3:]:
                            print(f"   {line.strip()}")
            except:
                pass
    
    # Check output files
    print(f"\nüìÅ Output Files Status:")
    expected_files = [
        "entities.parquet", "relationships.parquet", "communities.parquet",
        "community_reports.parquet", "text_units.parquet"
    ]
    
    if output_dir.exists():
        for file_name in expected_files:
            file_path = output_dir / file_name
            if file_path.exists():
                size = file_path.stat().st_size
                mod_time = datetime.fromtimestamp(file_path.stat().st_mtime)
                print(f"   ‚úÖ {file_name}: {size:,} bytes (modified: {mod_time.strftime('%H:%M:%S')})")
            else:
                print(f"   ‚è≥ {file_name}: Not created yet")
    else:
        print("   ‚ùå Output directory doesn't exist yet")
    
    # Check GraphRAG engine log
    engine_log = logs_dir / "indexing-engine.log"
    if engine_log.exists():
        mod_time = datetime.fromtimestamp(engine_log.stat().st_mtime)
        print(f"\nüìä GraphRAG engine log last updated: {mod_time.strftime('%H:%M:%S')}")
        
        # Check for errors in recent lines
        try:
            with open(engine_log, 'r') as f:
                lines = f.readlines()
                recent_lines = lines[-10:] if len(lines) > 10 else lines
                errors = [line for line in recent_lines if 'ERROR' in line.upper() or 'FAILED' in line.upper()]
                if errors:
                    print("‚ö†Ô∏è  Recent errors found:")
                    for error in errors[-2:]:  # Show last 2 errors
                        print(f"   {error.strip()}")
                else:
                    print("‚úÖ No recent errors detected")
        except:
            pass
    
    print("\n" + "=" * 60)
    print("üí° To check status again, run: python3 check_status.py")
    print("üõë To stop the process: pkill -f monitor_graphrag.py")

if __name__ == "__main__":
    check_status()


================================================================================


################################################################################
# File: investigate_graph.py
################################################################################

# File: investigate_graph.py

#!/usr/bin/env python3
"""
Investigate the GraphRAG knowledge graph to debug data integrity issues.
This script reads the entities and relationships to find out what the graph knows about a specific item.
"""

import pandas as pd
from pathlib import Path

def investigate_entity(entity_name: str):
    """
    Investigate a specific entity in the GraphRAG output to find its connections.
    """
    print(f"üîç Investigating entity: '{entity_name}'")
    print("=" * 60)
    
    # Define paths to GraphRAG output
    output_dir = Path("graphrag_data/output")
    entities_path = output_dir / "entities.parquet"
    relationships_path = output_dir / "relationships.parquet"
    
    # Check if files exist
    if not entities_path.exists() or not relationships_path.exists():
        print("‚ùå GraphRAG output files (entities.parquet, relationships.parquet) not found.")
        return
    
    # Load the data
    try:
        entities_df = pd.read_parquet(entities_path)
        relationships_df = pd.read_parquet(relationships_path)
        print(f"‚úÖ Loaded {len(entities_df)} entities and {len(relationships_df)} relationships.")
    except Exception as e:
        print(f"‚ùå Error loading parquet files: {e}")
        return
    
    # Find the entity
    target_entity = entities_df[entities_df['title'].str.upper() == entity_name.upper()]
    
    if target_entity.empty:
        print(f"Entity '{entity_name}' not found in the knowledge graph.")
        return
    
    print(f"\n--- Entity Details for '{entity_name}' ---")
    print(target_entity.to_string())
    
    entity_id = target_entity.index[0]
    
    # Find all relationships involving this entity
    related_as_source = relationships_df[relationships_df['source'] == entity_id]
    related_as_target = relationships_df[relationships_df['target'] == entity_id]
    
    all_relations = pd.concat([related_as_source, related_as_target])
    
    if all_relations.empty:
        print(f"\n--- No relationships found for '{entity_name}' ---")
    else:
        print(f"\n--- Found {len(all_relations)} relationships for '{entity_name}' ---")
        
        # Get the names of the connected entities
        connected_entity_ids = set(all_relations['source']).union(set(all_relations['target']))
        connected_entity_ids.discard(entity_id) # Remove the entity itself
        
        connected_entities = entities_df[entities_df.index.isin(connected_entity_ids)]
        
        print("This entity is connected to:")
        for _, row in connected_entities.iterrows():
            print(f"  - {row['title']} (Type: {row['type']})")
        
        print("\nFull Relationship Details:")
        print(all_relations.to_string())

if __name__ == "__main__":
    # Investigate both E-1 and E-4 to see the difference
    investigate_entity("E-1")
    print("\n\n" + "="*80 + "\n\n")
    investigate_entity("E-4")


================================================================================


################################################################################
# File: settings.yaml
################################################################################

# File: settings.yaml

### GraphRAG Configuration for City Clerk Documents

### LLM settings ###
models:
  default_chat_model:
    type: openai_chat
    auth_type: api_key
    api_key: ${OPENAI_API_KEY}
    model: gpt-4.1-mini-2025-04-14
    encoding_model: cl100k_base
    max_tokens: 16384
    temperature: 0

  default_embedding_model:
    type: openai_embedding
    auth_type: api_key
    api_key: ${OPENAI_API_KEY}
    model: text-embedding-3-small
    batch_size: 16
    batch_max_tokens: 2048

### Input settings ###
input:
  type: file
  file_type: csv
  base_dir: "."
  file_pattern: "city_clerk_documents.csv"

### Chunking settings ###
chunks:
  size: 800
  overlap: 300

### Output/storage settings ###
storage:
  type: file
  base_dir: "output"

### Community detection settings ###
cluster_graph:
  max_cluster_size: 10

### Entity extraction ###
entity_extraction:
  entity_types:
    - person
    - organization 
    - location
    - document
    - meeting
    - money
    - project
    - agenda_item: "pattern: [A-Z]-?\\d+"
    - ordinance
    - resolution
    - contract
    - document_number: "pattern: \\d{4}-\\d+"
  max_gleanings: 3

claim_extraction:
  description: Extract voting records, motions, and decisions
  enabled: true
  prompt: prompts/city_clerk_claims.txt

community_reports:
  max_input_length: 16384
  max_length: 2000
  prompt: prompts/city_clerk_community_report.txt

# Legacy LLM config for backward compatibility
llm:
  api_key: ${OPENAI_API_KEY}
  api_type: openai
  max_tokens: 16384
  model: gpt-4.1-mini-2025-04-14
  temperature: 0

# Legacy embeddings config for backward compatibility  
embeddings:
  api_key: ${OPENAI_API_KEY}
  batch_max_tokens: 2048
  batch_size: 16
  model: text-embedding-3-small

query:
  drift_search:
    follow_up_depth: 5
    follow_up_expansion: 3
    include_global_context: true
    initial_community_level: 2
    max_iterations: 5
    max_tokens: 16384
    primer_queries: 3
    relevance_threshold: 0.7
    similarity_threshold: 0.8
    temperature: 0.0
    termination_strategy: convergence
  global_search:
    community_level: 2
    max_tokens: 16384
    n: 1
    rate_relevancy_model: gpt-4.1-mini-2025-04-14
    relevance_score_threshold: 0.7
    temperature: 0.0
    top_p: 1.0
    use_dynamic_community_selection: true
  local_search:
    community_prop: 0.1
    conversation_history_max_turns: 5
    max_tokens: 16384
    temperature: 0.0
    text_unit_prop: 0.5
    top_k_entities: 10
    top_k_relationships: 10


================================================================================


################################################################################
# File: check_ordinances.py
################################################################################

# File: check_ordinances.py

import pandas as pd

# Check what ordinances were extracted
entities = pd.read_parquet('graphrag_data/output/entities.parquet')

# First check the columns
print("Columns in entities:", entities.columns.tolist())
print("Sample row:")
print(entities.head(1))

# Look for ordinances
ordinances = entities[entities['type'] == 'ORDINANCE']
print(f"\nOrdinances extracted: {len(ordinances)}")
print(ordinances[['title', 'description']].head(10))

# Also check for 2024-01 in any entity
ord_2024_01 = entities[entities['title'].str.contains('2024-01', case=False, na=False) | 
                       entities['description'].str.contains('2024-01', case=False, na=False)]
print(f"\n2024-01 mentions: {len(ord_2024_01)}")
if len(ord_2024_01) > 0:
    print(ord_2024_01[['title', 'type', 'description']])

# Check if "ORDINANCE 3576" was extracted (that's the Cocoplum ordinance)
ord_3576 = entities[entities['title'].str.contains('3576', na=False)]
print(f"\nOrdinance 3576 found: {len(ord_3576)}")
if len(ord_3576) > 0:
    print(ord_3576[['title', 'type', 'description']].iloc[0])
    print(f"\nFull description of first 3576 ordinance:")
    print(ord_3576.iloc[0]['description'])

# Check if Ordinance 3576 is the E-1 Cocoplum ordinance
if len(ord_3576) > 0:
    desc = ord_3576.iloc[0]['description']
    if 'Cocoplum' in desc or 'E-1' in desc:
        print("\n‚úÖ Ordinance 3576 IS the Cocoplum/E-1 ordinance!")
        print(f"Description: {desc}")
    else:
        print("\n‚ùå Ordinance 3576 does not appear to be the Cocoplum/E-1 ordinance")
        print(f"Description: {desc}")


================================================================================


################################################################################
# File: scripts/microsoft_framework/__init__.py
################################################################################

# File: scripts/microsoft_framework/__init__.py

"""
Microsoft GraphRAG integration for City Clerk document processing.

This package provides components for integrating Microsoft GraphRAG with
the existing city clerk document processing pipeline.
"""

from .graphrag_initializer import GraphRAGInitializer
from .document_adapter import CityClerkDocumentAdapter
from .prompt_tuner import CityClerkPromptTuner
from .graphrag_pipeline import CityClerkGraphRAGPipeline
from .cosmos_synchronizer import GraphRAGCosmosSync
from .query_engine import CityClerkGraphRAGQuery, QueryType, CityClerkQueryEngine, handle_user_query
from .query_router import SmartQueryRouter, QueryIntent, QueryFocus
from .incremental_processor import IncrementalGraphRAGProcessor
from .graphrag_output_processor import GraphRAGOutputProcessor

__all__ = [
    'GraphRAGInitializer',
    'CityClerkDocumentAdapter',
    'CityClerkPromptTuner',
    'CityClerkGraphRAGPipeline',
    'GraphRAGCosmosSync',
    'CityClerkGraphRAGQuery',
    'CityClerkQueryEngine',
    'QueryType',
    'SmartQueryRouter',
    'QueryIntent',
    'QueryFocus',
    'handle_user_query',
    'IncrementalGraphRAGProcessor',
    'GraphRAGOutputProcessor'
]


================================================================================


################################################################################
# File: scripts/microsoft_framework/run_graphrag_direct.py
################################################################################

# File: scripts/microsoft_framework/run_graphrag_direct.py

#!/usr/bin/env python3
"""Run GraphRAG commands directly without subprocess."""

import sys
import os

def run_graphrag_index(root_dir, verbose=True):
    """Run GraphRAG indexing directly."""
    # Set up arguments
    sys.argv = [
        'graphrag', 'index',
        '--root', str(root_dir)
    ]
    if verbose:
        sys.argv.append('--verbose')
    
    # Import and run GraphRAG
    try:
        from graphrag.cli import app
        app()
    except ImportError:
        print("‚ùå GraphRAG not found in current environment")
        print(f"Python: {sys.executable}")
        print(f"Path: {sys.path}")
        raise

if __name__ == "__main__":
    if len(sys.argv) > 1:
        root = sys.argv[1]
    else:
        root = "graphrag_data"
    
    run_graphrag_index(root)


================================================================================

