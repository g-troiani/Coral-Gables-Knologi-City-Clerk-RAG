# Concatenated Project Code - Part 1 of 3
# Generated: 2025-06-10 15:21:54
# Root Directory: /Users/gianmariatroiani/Documents/knologi/graph_database
================================================================================

# Directory Structure
################################################################################
├── .gitignore (939.0B, no ext)
├── check_ordinances.py (1.5KB, .py)
├── check_status.py (3.5KB, .py)
├── city_clerk_documents/
│   ├── global copy/
│   │   ├── City Comissions 2024/
│   │   │   ├── Agendas/
│   │   │   │   ├── Agenda 01.23.2024.pdf (155.1KB, .pdf)
│   │   │   │   ├── Agenda 01.9.2024.pdf (151.1KB, .pdf)
│   │   │   │   ├── Agenda 02.13.2024.pdf (155.0KB, .pdf)
│   │   │   │   ├── Agenda 02.27.2024.pdf (152.9KB, .pdf)
│   │   │   │   ├── Agenda 03.12.2024.pdf (166.6KB, .pdf)
│   │   │   │   ├── Agenda 05.07.2024.pdf (167.6KB, .pdf)
│   │   │   │   ├── Agenda 05.21.2024.pdf (172.2KB, .pdf)
│   │   │   │   └── Agenda 06.11.2024.pdf (160.2KB, .pdf)
│   │   │   ├── ExportedFolderContents.zip (62.4MB, .zip)
│   │   │   ├── Ordinances/
│   │   │   │   ├── 2024/
│   │   │   │   │   ├── 2024-01 - 01_09_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-02 - 01_09_2024.pdf (15.1MB, .pdf)
│   │   │   │   │   ├── 2024-03 - 01_09_2024.pdf (11.1MB, .pdf)
│   │   │   │   │   ├── 2024-04 - 01_23_2024.pdf (2.3MB, .pdf)
│   │   │   │   │   ├── 2024-05 - 02_13_2024.pdf (2.0MB, .pdf)
│   │   │   │   │   ├── 2024-06 - 02_13_2024.pdf (1.9MB, .pdf)
│   │   │   │   │   ├── 2024-07 - 02_13_2024.pdf (6.5MB, .pdf)
│   │   │   │   │   ├── 2024-08 - 02_27_2024.pdf (2.5MB, .pdf)
│   │   │   │   │   ├── 2024-09 - 02_27_2024.pdf (1.6MB, .pdf)
│   │   │   │   │   ├── 2024-10 - 03_12_2024.pdf (3.7MB, .pdf)
│   │   │   │   │   ├── 2024-11 - 03_12_2024 - (As Amended).pdf (5.6MB, .pdf)
│   │   │   │   │   ├── 2024-11 - 03_12_2024.pdf (5.5MB, .pdf)
│   │   │   │   │   ├── 2024-12 - 03_12_2024.pdf (1.8MB, .pdf)
│   │   │   │   │   ├── 2024-13 - 03_12_2024.pdf (3.2MB, .pdf)
│   │   │   │   │   ├── 2024-14 - 05_07_2024.pdf (2.8MB, .pdf)
│   │   │   │   │   ├── 2024-15 - 05_07_2024.pdf (3.9MB, .pdf)
│   │   │   │   │   ├── 2024-16 - 05_07_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-17 - 05_07_2024.pdf (4.3MB, .pdf)
│   │   │   │   │   ├── 2024-18 - 05_21_2024.pdf (2.4MB, .pdf)
│   │   │   │   │   ├── 2024-19 - 05_21_2024.pdf (1.7MB, .pdf)
│   │   │   │   │   ├── 2024-20 - 05_21_2024.pdf (1.6MB, .pdf)
│   │   │   │   │   ├── 2024-21 - 06_11_2024.pdf (1.7MB, .pdf)
│   │   │   │   │   ├── 2024-22 - 06_11_2024.pdf (2.0MB, .pdf)
│   │   │   │   │   ├── 2024-23 - 06_11_2024.pdf (1.9MB, .pdf)
│   │   │   │   │   ├── 2024-24 - 06_11_2024.pdf (2.1MB, .pdf)
│   │   │   │   │   └── 2024-25 - 06_11_2024.pdf (1.3MB, .pdf)
│   │   │   │   ├── [EXCLUDED] 1 items: .DS_Store (excluded file)
│   │   │   ├── Resolutions/
│   │   │   │   ├── 2024/
│   │   │   │   │   ├── 2024-01 - 01_09_2024.pdf (448.9KB, .pdf)
│   │   │   │   │   ├── 2024-02 - 01_09_2024.pdf (451.9KB, .pdf)
│   │   │   │   │   ├── 2024-03 - 01_09_2024.pdf (867.3KB, .pdf)
│   │   │   │   │   ├── 2024-04 - 01_09_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-05 - 01_09_2024.pdf (936.9KB, .pdf)
│   │   │   │   │   ├── 2024-06 - 01_09_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-07 - 01_09_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-08 - 01_23_2024.pdf (457.9KB, .pdf)
│   │   │   │   │   ├── 2024-09 - 01_23_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-10 - 01_23_2024.pdf (454.8KB, .pdf)
│   │   │   │   │   ├── 2024-10 - 03_12_2024.pdf (3.7MB, .pdf)
│   │   │   │   │   ├── 2024-100 - 05_21_2024.pdf (3.0MB, .pdf)
│   │   │   │   │   ├── 2024-101 - 05_21_2024.pdf (947.4KB, .pdf)
│   │   │   │   │   ├── 2024-102 - 05_21_2024.pdf (466.0KB, .pdf)
│   │   │   │   │   ├── 2024-103 - 05_21_2024.pdf (991.8KB, .pdf)
│   │   │   │   │   ├── 2024-104 - 05_21_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-105 - 05_21_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-106 - 05_21_2024.pdf (2.4MB, .pdf)
│   │   │   │   │   ├── 2024-107 - 05_21_2024.pdf (6.0MB, .pdf)
│   │   │   │   │   ├── 2024-108 - 05_21_2024.pdf (2.1MB, .pdf)
│   │   │   │   │   ├── 2024-109 - 05_21_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-11 - 01_23_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-110 - 05_21_2024.pdf (921.5KB, .pdf)
│   │   │   │   │   ├── 2024-111 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-112 - 05_21_2024.pdf (6.2MB, .pdf)
│   │   │   │   │   ├── 2024-113 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-114 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-115 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-116 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-117 - 05_21_2024.pdf (6.4MB, .pdf)
│   │   │   │   │   ├── 2024-118 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-119 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-12 - 01_23_2024.pdf (588.3KB, .pdf)
│   │   │   │   │   ├── 2024-120 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-121 - 05_21_2024.pdf (6.4MB, .pdf)
│   │   │   │   │   ├── 2024-122 - 05_21_2024.pdf (6.8MB, .pdf)
│   │   │   │   │   ├── 2024-123 - 05_21_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-124 - 05_21_2024.pdf (11.2MB, .pdf)
│   │   │   │   │   ├── 2024-125 - 05_21_2024.pdf (776.0KB, .pdf)
│   │   │   │   │   ├── 2024-126 - 05_21_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-127 - 05_21_2024.pdf (8.1MB, .pdf)
│   │   │   │   │   ├── 2024-129 - 06_11_2024.pdf (1.3MB, .pdf)
│   │   │   │   │   ├── 2024-13 - 01_23_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-130 - 06_11_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-131 - 06_11_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-132 - 06_11_2024.pdf (458.0KB, .pdf)
│   │   │   │   │   ├── 2024-133 - 06_11_2024 -As Amended.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-134 - 06_11_2024.pdf (884.8KB, .pdf)
│   │   │   │   │   ├── 2024-135 - 06_11_2024.pdf (1022.1KB, .pdf)
│   │   │   │   │   ├── 2024-136 - 06_11_2024.pdf (537.4KB, .pdf)
│   │   │   │   │   ├── 2024-137 - 06_11_2024.pdf (1.7MB, .pdf)
│   │   │   │   │   ├── 2024-138 - 06_11_2024.pdf (1.7MB, .pdf)
│   │   │   │   │   ├── 2024-139 - 06_11_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-14 - 01_23_2024.pdf (996.6KB, .pdf)
│   │   │   │   │   ├── 2024-140 - 06_11_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-141 - 06_11_2024.pdf (13.2MB, .pdf)
│   │   │   │   │   ├── 2024-142 - 06_11_2024.pdf (790.5KB, .pdf)
│   │   │   │   │   ├── 2024-143 -06_11_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-15 - 01_23_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-16 - 01_23_2024.pdf (1.8MB, .pdf)
│   │   │   │   │   ├── 2024-17 - 01_23_2024.pdf (488.0KB, .pdf)
│   │   │   │   │   ├── 2024-18 - 01_23_2024.pdf (849.1KB, .pdf)
│   │   │   │   │   ├── 2024-19 - 02_19_2024.pdf (1019.5KB, .pdf)
│   │   │   │   │   ├── 2024-20 - 02_13_2024.pdf (824.5KB, .pdf)
│   │   │   │   │   ├── 2024-21 - 02_13_2024.pdf (502.6KB, .pdf)
│   │   │   │   │   ├── 2024-22 - 02_13_2024.pdf (450.6KB, .pdf)
│   │   │   │   │   ├── 2024-23 - 02_13_2024.pdf (447.7KB, .pdf)
│   │   │   │   │   ├── 2024-24 - 02_13_2024.pdf (464.1KB, .pdf)
│   │   │   │   │   ├── 2024-25 - 02_13_2024.pdf (458.3KB, .pdf)
│   │   │   │   │   ├── 2024-26 - 02_13_2024.pdf (465.7KB, .pdf)
│   │   │   │   │   ├── 2024-27 - 02_13_2024.pdf (754.2KB, .pdf)
│   │   │   │   │   ├── 2024-28 - 02_13_2024.pdf (761.7KB, .pdf)
│   │   │   │   │   ├── 2024-29 - 02_13_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-30 - 02_13_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-31 - 02_13_2024.pdf (936.6KB, .pdf)
│   │   │   │   │   ├── 2024-32 - 02_13_2024.pdf (1.9MB, .pdf)
│   │   │   │   │   ├── 2024-33 - 02_13_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-34 - 02_13_2024.pdf (1.9MB, .pdf)
│   │   │   │   │   ├── 2024-35 - 02_27_2024.pdf (480.7KB, .pdf)
│   │   │   │   │   ├── 2024-36 - 02_27_2024.pdf (1.3MB, .pdf)
│   │   │   │   │   ├── 2024-37 - 02_27_2024.pdf (940.3KB, .pdf)
│   │   │   │   │   ├── 2024-38 - 02_27_2024.pdf (829.6KB, .pdf)
│   │   │   │   │   ├── 2024-39 - 02_27_2024.pdf (816.5KB, .pdf)
│   │   │   │   │   ├── 2024-41 - 02_27_2024.pdf (833.1KB, .pdf)
│   │   │   │   │   ├── 2024-42 - 02_27_2024.pdf (473.4KB, .pdf)
│   │   │   │   │   ├── 2024-43 - 02_27_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-44 - 02_27_2024.pdf (1.8MB, .pdf)
│   │   │   │   │   ├── 2024-45 - 03_12_2024.pdf (1002.1KB, .pdf)
│   │   │   │   │   ├── 2024-46 - 03_12_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-47 - 03_12_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-48 - 03_12_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-49 - 03_12_2024.pdf (899.6KB, .pdf)
│   │   │   │   │   ├── 2024-50 - 03_12_2024.pdf (462.1KB, .pdf)
│   │   │   │   │   ├── 2024-51 - 03_12_2024.pdf (538.1KB, .pdf)
│   │   │   │   │   ├── 2024-52 - 03_12_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-53 - 03_12_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-54 - 03_12_2024.pdf (2.0MB, .pdf)
│   │   │   │   │   ├── 2024-55 - 03_12_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-56 - 03_12_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-57 - 03_12_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-58 - 03_12_2024.pdf (775.8KB, .pdf)
│   │   │   │   │   ├── 2024-60 - 03_12_2024.pdf (2.3MB, .pdf)
│   │   │   │   │   ├── 2024-61 - 04_16_2024.pdf (6.4MB, .pdf)
│   │   │   │   │   ├── 2024-62 - 04_16_2024.pdf (2.7MB, .pdf)
│   │   │   │   │   ├── 2024-63 -  04_16_2024.pdf (832.3KB, .pdf)
│   │   │   │   │   ├── 2024-64 - 04_16_2024.pdf (452.6KB, .pdf)
│   │   │   │   │   ├── 2024-65 - 04_16_2024.pdf (894.5KB, .pdf)
│   │   │   │   │   ├── 2024-66 - 04_16_2024.pdf (446.6KB, .pdf)
│   │   │   │   │   ├── 2024-67 - 04_16_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-68 - 04_16_2024.pdf (5.6MB, .pdf)
│   │   │   │   │   ├── 2024-69- -04_16_2024.pdf (443.1KB, .pdf)
│   │   │   │   │   ├── 2024-70 - 04_16_2024.pdf (878.9KB, .pdf)
│   │   │   │   │   ├── 2024-71 - 04_16_2024.pdf (951.8KB, .pdf)
│   │   │   │   │   ├── 2024-72 - 04_16_2024.pdf (821.7KB, .pdf)
│   │   │   │   │   ├── 2024-73 - 04_16_2024.pdf (810.8KB, .pdf)
│   │   │   │   │   ├── 2024-74 - 04_16_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-75 - 04_16_2024.pdf (1.6MB, .pdf)
│   │   │   │   │   ├── 2024-76 - 04_16_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-77 - 04_16_2024.pdf (1013.9KB, .pdf)
│   │   │   │   │   ├── 2024-78 - 04_16_2024.pdf (1007.5KB, .pdf)
│   │   │   │   │   ├── 2024-79 - 04_16_2024.pdf (997.7KB, .pdf)
│   │   │   │   │   ├── 2024-80 - 04_16_2024.pdf (525.7KB, .pdf)
│   │   │   │   │   ├── 2024-81 - 04_16_2024.pdf (923.2KB, .pdf)
│   │   │   │   │   ├── 2024-82 - 04_16_2024.pdf (473.6KB, .pdf)
│   │   │   │   │   ├── 2024-83 - 04_16_2024.pdf (915.3KB, .pdf)
│   │   │   │   │   ├── 2024-84 - 05_07_2024.pdf (992.7KB, .pdf)
│   │   │   │   │   ├── 2024-85 - 05_07_2024.pdf (2.0MB, .pdf)
│   │   │   │   │   ├── 2024-86 - 05_07_2024.pdf (503.1KB, .pdf)
│   │   │   │   │   ├── 2024-87 - 05_07_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-88 - 05_07_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-89 - 05_07_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-90 - 05_07_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-91 - 05_07_2024.pdf (1.8MB, .pdf)
│   │   │   │   │   ├── 2024-92 - 05_07_2024.pdf (452.4KB, .pdf)
│   │   │   │   │   ├── 2024-93 - 05_07_2024.pdf (1014.3KB, .pdf)
│   │   │   │   │   ├── 2024-94 - 05_05_2024.pdf (1.3MB, .pdf)
│   │   │   │   │   ├── 2024-95 - 05_07_2024.pdf (974.5KB, .pdf)
│   │   │   │   │   ├── 2024-96 - 05_07_2024.pdf (1.5MB, .pdf)
│   │   │   │   │   ├── 2024-97 - 05_07_2024.pdf (1023.3KB, .pdf)
│   │   │   │   │   ├── 2024-98 - 05_07_2024.pdf (2.2MB, .pdf)
│   │   │   │   │   └── 2024-99 - 05_07_2024.pdf (792.7KB, .pdf)
│   │   │   │   ├── [EXCLUDED] 1 items: .DS_Store (excluded file)
│   │   │   ├── Verbating Items/
│   │   │   │   ├── 2024/
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - E-4.pdf (2.4MB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - E-5.pdf (735.6KB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - E-7.pdf (306.9KB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - E-8.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - F-10.pdf (925.3KB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - F-2.pdf (288.2KB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - F-5.pdf (360.6KB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - F-6.pdf (212.8KB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - Public Comment.pdf (762.7KB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - C- Public Comment.pdf (1.3MB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - E-3 and E-4.pdf (2.4MB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - E-5.pdf (290.7KB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - F-1.pdf (855.5KB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - F-5.pdf (3.7MB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - F-7.pdf (768.6KB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - H-1.pdf (1.5MB, .pdf)
│   │   │   │   │   ├── 02_13_2024 - Meeting Minutes - Public.pdf (363.5KB, .pdf)
│   │   │   │   │   ├── 02_13_2024 - Verbatim Transcripts - F-12.pdf (7.2MB, .pdf)
│   │   │   │   │   ├── 02_13_2024 - Verbatim Transcripts - H-1.pdf (570.8KB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - E-1.pdf (4.2MB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - E-2.pdf (281.3KB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - E-3.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - F-10.pdf (2.5MB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - F-12.pdf (448.2KB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - H-1.pdf (794.2KB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - Public Comment.pdf (439.4KB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - E-1.pdf (124.3KB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - E-2.pdf (487.9KB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - E-4 and E-11.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - F-1.pdf (416.0KB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - F-2.pdf (1.5MB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - F-4 and F-5 and F-11 and F-12.pdf (1.4MB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - H-1.pdf (121.5KB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - K - Discussion Items.pdf (285.0KB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - Public Comment.pdf (412.4KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - 2-1 AND 2-2.pdf (652.3KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - E-11.pdf (365.8KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - E-2.pdf (369.8KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - E-4.pdf (410.2KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - E-5 E-6 E-7 E-8 E-9 E-10.pdf (3.6MB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-1.pdf (751.9KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-10.pdf (1.3MB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-11.pdf (2.4MB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-12.pdf (2.8MB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-15.pdf (255.0KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-5.pdf (205.0KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-8.pdf (961.1KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-9.pdf (330.1KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - Public Comment.pdf (1.5MB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - 2-1.pdf (1.7MB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - E-1.pdf (4.6MB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - E-10.pdf (128.5KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - E-11.pdf (559.6KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - E-12.pdf (156.1KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - E-8.pdf (288.5KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - E-9.pdf (263.4KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - F-1.pdf (561.1KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - F-13.pdf (151.9KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - F-16.pdf (782.8KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - F-3.pdf (352.6KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - F-9.pdf (736.0KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - H-1.pdf (455.3KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - Public Comment.pdf (1.6MB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - D-3.pdf (539.3KB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - E-10.pdf (629.7KB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - E-3.pdf (350.4KB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - F-2.pdf (1.7MB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - F-3.pdf (2.4MB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - F-4.pdf (3.4MB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - F-5.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - F-7 and F-10.pdf (1.6MB, .pdf)
│   │   │   │   │   └── 06_11_2024 - Verbatim Transcripts - Public Comment.pdf (513.4KB, .pdf)
│   │   │   │   ├── [EXCLUDED] 1 items: .DS_Store (excluded file)
│   │   │   ├── [EXCLUDED] 1 items: .DS_Store (excluded file)
│   │   ├── [EXCLUDED] 1 items: .DS_Store (excluded file)
│   ├── graph_json/
│   │   └── debug/
│   │       ├── llm_response_ordinance_2024-01_cleaned.txt (16.0B, .txt)
│   │       ├── llm_response_ordinance_2024-01_raw.txt (16.0B, .txt)
│   │       ├── llm_response_ordinance_2024-02_cleaned.txt (16.0B, .txt)
│   │       ├── llm_response_ordinance_2024-02_raw.txt (16.0B, .txt)
│   │       ├── llm_response_ordinance_2024-03_cleaned.txt (16.0B, .txt)
│   │       ├── llm_response_ordinance_2024-03_raw.txt (16.0B, .txt)
│   │       ├── llm_response_resolution_2024-01_cleaned.txt (16.0B, .txt)
│   │       ├── llm_response_resolution_2024-01_raw.txt (16.0B, .txt)
│   │       ├── llm_response_resolution_2024-02_cleaned.txt (16.0B, .txt)
│   │       ├── llm_response_resolution_2024-02_raw.txt (16.0B, .txt)
│   │       ├── llm_response_resolution_2024-03_cleaned.txt (17.0B, .txt)
│   │       ├── llm_response_resolution_2024-03_raw.txt (17.0B, .txt)
│   │       ├── llm_response_resolution_2024-04_cleaned.txt (16.0B, .txt)
│   │       ├── llm_response_resolution_2024-04_raw.txt (16.0B, .txt)
│   │       ├── llm_response_resolution_2024-05_cleaned.txt (17.0B, .txt)
│   │       ├── llm_response_resolution_2024-05_raw.txt (17.0B, .txt)
│   │       ├── llm_response_resolution_2024-06_cleaned.txt (16.0B, .txt)
│   │       ├── llm_response_resolution_2024-06_raw.txt (16.0B, .txt)
│   │       ├── llm_response_resolution_2024-07_cleaned.txt (16.0B, .txt)
│   │       ├── llm_response_resolution_2024-07_raw.txt (16.0B, .txt)
│   │       └── verbatim/
│   ├── [EXCLUDED] 4 items: .DS_Store (excluded file), extracted_markdown (excluded dir), extracted_text (excluded dir)
│       ... and 1 more excluded items
├── config.py (1.7KB, .py)
├── debug_enhanced_query.py (1.8KB, .py)
├── explore_graphrag_sources.py (3.0KB, .py)
├── extract_documents_for_graphrag.py (3.5KB, .py)
├── graph_clear_database.py (963.0B, .py)
├── graph_visualizer.py (25.1KB, .py)
├── graphrag_query_ui.py (21.4KB, .py)
├── investigate_graph.py (2.8KB, .py)
├── ontology_model.txt (6.6KB, .txt)
├── ontology_modelv2.txt (10.2KB, .txt)
├── pipeline_summary.py (7.9KB, .py)
├── prompts/
│   ├── city_clerk_claims.txt (530.0B, .txt)
│   └── city_clerk_community_report.txt (804.0B, .txt)
├── repository_directory_structure.txt (5.7KB, .txt)
├── requirements.txt (1.1KB, .txt)
├── run_city_clerk_pipeline.sh (440.0B, .sh)
├── run_graph_pipeline.sh (1.3KB, .sh)
├── run_graph_visualizer.sh (3.6KB, .sh)
├── run_graphrag.sh (5.6KB, .sh)
├── run_query_ui.sh (761.0B, .sh)
├── scripts/
│   ├── extract_all_pdfs_direct.py (5.8KB, .py)
│   ├── extract_all_to_markdown.py (7.7KB, .py)
│   ├── graph_pipeline.py (21.9KB, .py)
│   ├── graph_stages/
│   │   ├── __init__.py (647.0B, .py)
│   │   ├── agenda_graph_builder.py (63.2KB, .py)
│   │   ├── agenda_ontology_extractor.py (25.7KB, .py)
│   │   ├── agenda_pdf_extractor.py (30.9KB, .py)
│   │   ├── cosmos_db_client.py (10.2KB, .py)
│   │   ├── document_linker.py (12.9KB, .py)
│   │   ├── enhanced_document_linker.py (26.6KB, .py)
│   │   ├── ontology_extractor.py (41.9KB, .py)
│   │   ├── pdf_extractor.py (6.5KB, .py)
│   │   ├── verbatim_transcript_linker.py (19.2KB, .py)
│   │   ├── [EXCLUDED] 1 items: __pycache__ (excluded dir)
│   ├── json_to_markdown_converter.py (1.4KB, .py)
│   ├── microsoft_framework/
│   │   ├── README.md (6.9KB, .md)
│   │   ├── __init__.py (1.1KB, .py)
│   │   ├── city_clerk_settings_template.yaml (2.1KB, .yaml)
│   │   ├── cosmos_synchronizer.py (3.5KB, .py)
│   │   ├── create_custom_prompts.py (4.3KB, .py)
│   │   ├── document_adapter.py (24.3KB, .py)
│   │   ├── graphrag_initializer.py (5.1KB, .py)
│   │   ├── graphrag_output_processor.py (3.2KB, .py)
│   │   ├── graphrag_pipeline.py (2.0KB, .py)
│   │   ├── incremental_processor.py (1.9KB, .py)
│   │   ├── prompt_tuner.py (7.9KB, .py)
│   │   ├── query_engine.py (29.4KB, .py)
│   │   ├── query_graphrag.py (561.0B, .py)
│   │   ├── query_router.py (14.9KB, .py)
│   │   ├── run_graphrag.sh (5.6KB, .sh)
│   │   ├── run_graphrag_direct.py (783.0B, .py)
│   │   ├── run_graphrag_pipeline.py (15.7KB, .py)
│   │   ├── test_enhanced_query.py (4.6KB, .py)
│   │   ├── test_queries.py (5.7KB, .py)
│   │   ├── test_routing.py (4.7KB, .py)
│   │   ├── [EXCLUDED] 1 items: __pycache__ (excluded dir)
│   ├── [EXCLUDED] 4 items: pipeline_modular_optimized.py (excluded file), rag_local_web_app.py (excluded file), stages (excluded dir)
│       ... and 1 more excluded items
├── settings.yaml (2.4KB, .yaml)
├── test_enhanced_pipeline.py (8.6KB, .py)
├── test_query_functionality.py (8.1KB, .py)
├── test_query_system.py (8.6KB, .py)
├── [EXCLUDED] 10 items: .DS_Store (excluded file), .env (excluded file), .git (excluded dir)
    ... and 7 more excluded items


================================================================================


# File Index - Which Files Are in Which Parts
################################################################################

## Part 1 (10 files):
  - scripts/graph_stages/verbatim_transcript_linker.py
  - scripts/microsoft_framework/prompt_tuner.py
  - scripts/microsoft_framework/README.md
  - scripts/microsoft_framework/graphrag_initializer.py
  - extract_documents_for_graphrag.py
  - explore_graphrag_sources.py
  - scripts/microsoft_framework/graphrag_pipeline.py
  - scripts/microsoft_framework/incremental_processor.py
  - requirements.txt
  - scripts/graph_stages/__init__.py

## Part 2 (10 files):
  - scripts/microsoft_framework/run_graphrag_pipeline.py
  - scripts/graph_stages/cosmos_db_client.py
  - scripts/extract_all_to_markdown.py
  - scripts/extract_all_pdfs_direct.py
  - check_status.py
  - investigate_graph.py
  - settings.yaml
  - check_ordinances.py
  - scripts/microsoft_framework/__init__.py
  - scripts/microsoft_framework/run_graphrag_direct.py

## Part 3 (10 files):
  - scripts/microsoft_framework/query_router.py
  - scripts/graph_stages/document_linker.py
  - scripts/graph_stages/pdf_extractor.py
  - scripts/microsoft_framework/create_custom_prompts.py
  - scripts/microsoft_framework/cosmos_synchronizer.py
  - scripts/microsoft_framework/graphrag_output_processor.py
  - scripts/microsoft_framework/city_clerk_settings_template.yaml
  - config.py
  - scripts/json_to_markdown_converter.py
  - scripts/microsoft_framework/query_graphrag.py


================================================================================


################################################################################
# File: scripts/graph_stages/verbatim_transcript_linker.py
################################################################################

# File: scripts/graph_stages/verbatim_transcript_linker.py

"""
Verbatim Transcript Linker
Links verbatim transcript documents to their corresponding agenda items.
Now with full OCR support for all pages.
"""

import logging
import re
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Set
import json
from datetime import datetime
import PyPDF2
import os

# Import the PDF extractor for OCR support
from .pdf_extractor import PDFExtractor

log = logging.getLogger('verbatim_transcript_linker')


class VerbatimTranscriptLinker:
    """Links verbatim transcript documents to agenda items in the graph."""
    
    def __init__(self):
        """Initialize the verbatim transcript linker."""
        # Pattern to extract date and item info from filename
        self.filename_pattern = re.compile(
            r'(\d{2})_(\d{2})_(\d{4})\s*-\s*Verbatim Transcripts\s*-\s*(.+)\.pdf',
            re.IGNORECASE
        )
        
        # Debug directory for logging - ensure parent exists
        self.debug_dir = Path("city_clerk_documents/graph_json/debug/verbatim")
        self.debug_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize PDF extractor for OCR
        self.pdf_extractor = PDFExtractor(
            pdf_dir=Path("."),  # We'll use it file by file
            output_dir=Path("city_clerk_documents/extracted_text")
        )
    
    async def link_transcripts_for_meeting(self, 
                                         meeting_date: str,
                                         verbatim_dir: Path) -> Dict[str, List[Dict]]:
        """Find and link all verbatim transcripts for a specific meeting date."""
        log.info(f"🎤 Linking verbatim transcripts for meeting date: {meeting_date}")
        log.info(f"📁 Verbatim directory: {verbatim_dir}")
        
        # Debug logging for troubleshooting
        log.info(f"🔍 Looking for verbatim transcripts in: {verbatim_dir}")
        log.info(f"🔍 Directory exists: {verbatim_dir.exists()}")
        if verbatim_dir.exists():
            all_files = list(verbatim_dir.glob("*.pdf"))
            log.info(f"🔍 Total PDF files in directory: {len(all_files)}")
            if all_files:
                log.info(f"🔍 Sample files: {[f.name for f in all_files[:3]]}")
        
        # Convert meeting date format: "01.09.2024" -> "01_09_2024"
        date_underscore = meeting_date.replace(".", "_")
        
        # Initialize results
        linked_transcripts = {
            "item_transcripts": [],      # Transcripts for specific agenda items
            "public_comments": [],       # Public comment transcripts
            "section_transcripts": []    # Transcripts for entire sections
        }
        
        if not verbatim_dir.exists():
            log.warning(f"⚠️  Verbatim directory not found: {verbatim_dir}")
            return linked_transcripts
        
        # Find all transcript files for this date
        # Try multiple patterns to ensure we catch all files
        patterns = [
            f"{date_underscore}*Verbatim*.pdf",
            f"{date_underscore} - Verbatim*.pdf",
            f"*{date_underscore}*Verbatim*.pdf"
        ]

        transcript_files = []
        for pattern in patterns:
            files = list(verbatim_dir.glob(pattern))
            log.info(f"🔍 Pattern '{pattern}' found {len(files)} files")
            transcript_files.extend(files)

        # Remove duplicates
        transcript_files = list(set(transcript_files))
        
        log.info(f"📄 Found {len(transcript_files)} transcript files")
        
        # Process each transcript file
        for transcript_path in transcript_files:
            try:
                transcript_info = await self._process_transcript(transcript_path, meeting_date)
                if transcript_info:
                    # Categorize based on transcript type
                    if transcript_info['transcript_type'] == 'public_comment':
                        linked_transcripts['public_comments'].append(transcript_info)
                    elif transcript_info['transcript_type'] == 'section':
                        linked_transcripts['section_transcripts'].append(transcript_info)
                    else:
                        linked_transcripts['item_transcripts'].append(transcript_info)
                    
                    # Save extracted text for GraphRAG
                    self._save_extracted_text(transcript_path, transcript_info)
                        
            except Exception as e:
                log.error(f"Error processing transcript {transcript_path.name}: {e}")
        
        # Save linked transcripts info for debugging
        self._save_linking_report(meeting_date, linked_transcripts)
        
        # Log summary
        total_linked = (len(linked_transcripts['item_transcripts']) + 
                       len(linked_transcripts['public_comments']) + 
                       len(linked_transcripts['section_transcripts']))
        
        log.info(f"✅ Verbatim transcript linking complete:")
        log.info(f"   🎤 Item transcripts: {len(linked_transcripts['item_transcripts'])}")
        log.info(f"   🎤 Public comments: {len(linked_transcripts['public_comments'])}")
        log.info(f"   🎤 Section transcripts: {len(linked_transcripts['section_transcripts'])}")
        log.info(f"   📄 Total linked: {total_linked}")
        
        return linked_transcripts
    
    async def _process_transcript(self, transcript_path: Path, meeting_date: str) -> Optional[Dict[str, Any]]:
        """Process a single transcript file with full OCR."""
        try:
            # Parse filename
            match = self.filename_pattern.match(transcript_path.name)
            if not match:
                log.warning(f"Could not parse transcript filename: {transcript_path.name}")
                return None
            
            month, day, year = match.groups()[:3]
            item_info = match.group(4).strip()
            
            # Parse item codes from the item info
            parsed_items = self._parse_item_codes(item_info)
            
            # Extract text from ALL pages using Docling OCR
            log.info(f"🔍 Running OCR on full transcript: {transcript_path.name}...")
            full_text, pages = self.pdf_extractor.extract_text_from_pdf(transcript_path)
            
            if not full_text:
                log.warning(f"No text extracted from {transcript_path.name}")
                return None
            
            log.info(f"✅ OCR extracted {len(full_text)} characters from {len(pages)} pages")
            
            # Determine transcript type and normalize item codes
            transcript_type = self._determine_transcript_type(item_info, parsed_items)
            
            transcript_info = {
                "path": str(transcript_path),
                "filename": transcript_path.name,
                "meeting_date": meeting_date,
                "item_info_raw": item_info,
                "item_codes": parsed_items['item_codes'],
                "section_codes": parsed_items['section_codes'],
                "transcript_type": transcript_type,
                "page_count": len(pages),
                "full_text": full_text,  # Store complete transcript text
                "pages": pages,          # Store page-level data
                "extraction_method": "docling_ocr"
            }
            
            log.info(f"📄 Processed transcript: {transcript_path.name}")
            log.info(f"   Items: {parsed_items['item_codes']}")
            log.info(f"   Type: {transcript_type}")
            
            return transcript_info
            
        except Exception as e:
            log.error(f"Error processing transcript {transcript_path.name}: {e}")
            return None
    
    def _parse_item_codes(self, item_info: str) -> Dict[str, List[str]]:
        """Parse item codes from the filename item info section."""
        result = {
            'item_codes': [],
            'section_codes': []
        }
        
        # Check for public comment first
        if re.search(r'public\s+comment', item_info, re.IGNORECASE):
            result['section_codes'].append('PUBLIC_COMMENT')
            return result
        
        # Special case: Meeting Minutes or other general labels
        if re.search(r'meeting\s+minutes', item_info, re.IGNORECASE):
            result['item_codes'].append('MEETING_MINUTES')
            return result
        
        # Special case: Full meeting transcript
        if re.search(r'public|full\s+meeting', item_info, re.IGNORECASE) and not re.search(r'comment', item_info, re.IGNORECASE):
            result['item_codes'].append('FULL_MEETING')
            return result
        
        # Special case: Discussion Items (K section)
        if re.match(r'^K\s*$', item_info.strip()):
            result['section_codes'].append('K')
            return result
        
        # Clean the item info
        item_info = item_info.strip()
        
        # Handle multiple items with "and" or "AND"
        # Examples: "F-7 and F-10", "2-1 AND 2-2"
        if ' and ' in item_info.lower():
            parts = re.split(r'\s+and\s+', item_info, flags=re.IGNORECASE)
            for part in parts:
                codes = self._extract_single_item_codes(part.strip())
                result['item_codes'].extend(codes)
        
        # Handle space-separated items
        # Examples: "E-5 E-6 E-7 E-8 E-9 E-10"
        elif re.match(r'^([A-Z]-?\d+\s*)+$', item_info):
            # Split by spaces and extract each item
            items = item_info.split()
            for item in items:
                if re.match(r'^[A-Z]-?\d+$', item):
                    normalized = self._normalize_item_code(item)
                    if normalized:
                        result['item_codes'].append(normalized)
        
        # Handle comma-separated items
        elif ',' in item_info:
            parts = item_info.split(',')
            for part in parts:
                codes = self._extract_single_item_codes(part.strip())
                result['item_codes'].extend(codes)
        
        # Single item or other format
        else:
            codes = self._extract_single_item_codes(item_info)
            result['item_codes'].extend(codes)
        
        # Remove duplicates while preserving order
        result['item_codes'] = list(dict.fromkeys(result['item_codes']))
        result['section_codes'] = list(dict.fromkeys(result['section_codes']))
        
        return result
    
    def _extract_single_item_codes(self, text: str) -> List[str]:
        """Extract item codes from a single text segment."""
        codes = []
        
        # Pattern for item codes: letter-number, letter.number, or just number-number
        # Handles: E-1, E1, E.-1., E.1, 2-1, etc.
        patterns = [
            r'([A-Z])\.?\-?(\d+)\.?',  # Letter-based items
            r'(\d+)\-(\d+)'             # Number-only items like 2-1
        ]
        
        for pattern in patterns:
            for match in re.finditer(pattern, text):
                if pattern.startswith('(\\d'):  # Number-only pattern
                    # For number-only, just use as is
                    codes.append(f"{match.group(1)}-{match.group(2)}")
                else:
                    # For letter-number format
                    letter = match.group(1)
                    number = match.group(2)
                    codes.append(f"{letter}-{number}")
        
        return codes
    
    def _normalize_item_code(self, code: str) -> str:
        """Normalize item code to consistent format (e.g., E-1, 2-1)."""
        # Remove dots and ensure dash format
        code = code.strip('. ')
        
        # Pattern: letter followed by optional punctuation and number
        letter_match = re.match(r'^([A-Z])\.?\-?(\d+)\.?$', code)
        if letter_match:
            letter = letter_match.group(1)
            number = letter_match.group(2)
            return f"{letter}-{number}"
        
        # Pattern: number-number format
        number_match = re.match(r'^(\d+)\-(\d+)$', code)
        if number_match:
            return code  # Already in correct format
        
        return code
    
    def _determine_transcript_type(self, item_info: str, parsed_items: Dict) -> str:
        """Determine the type of transcript based on parsed information."""
        if 'PUBLIC_COMMENT' in parsed_items['item_codes']:
            return 'public_comment'
        elif parsed_items['section_codes']:
            return 'section'
        elif len(parsed_items['item_codes']) > 3:
            return 'multi_item'
        elif len(parsed_items['item_codes']) == 1:
            return 'single_item'
        else:
            return 'item_group'
    
    def _save_linking_report(self, meeting_date: str, linked_transcripts: Dict):
        """Save detailed report of transcript linking."""
        report = {
            "meeting_date": meeting_date,
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total_transcripts": sum(len(v) for v in linked_transcripts.values()),
                "item_transcripts": len(linked_transcripts["item_transcripts"]),
                "public_comments": len(linked_transcripts["public_comments"]),
                "section_transcripts": len(linked_transcripts["section_transcripts"])
            },
            "transcripts": linked_transcripts
        }
        
        report_filename = f"verbatim_linking_report_{meeting_date.replace('.', '_')}.json"
        report_path = self.debug_dir / report_filename
        
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        log.info(f"📊 Verbatim linking report saved to: {report_path}")
    
    def _validate_meeting_date(self, meeting_date: str) -> bool:
        """Validate meeting date format MM.DD.YYYY"""
        return bool(re.match(r'^\d{2}\.\d{2}\.\d{4}$', meeting_date))

    def _save_extracted_text(self, transcript_path: Path, transcript_info: Dict[str, Any]):
        """Save extracted transcript text to JSON for GraphRAG processing."""
        output_dir = Path("city_clerk_documents/extracted_text")
        output_dir.mkdir(exist_ok=True)
        
        # Create filename based on transcript info
        meeting_date = transcript_info['meeting_date'].replace('.', '_')
        item_info_clean = re.sub(r'[^a-zA-Z0-9-]', '_', transcript_info['item_info_raw'])
        output_filename = f"verbatim_{meeting_date}_{item_info_clean}_extracted.json"
        output_path = output_dir / output_filename
        
        # Prepare data for saving
        save_data = {
            "document_type": "verbatim_transcript",
            "meeting_date": transcript_info['meeting_date'],
            "item_codes": transcript_info['item_codes'],
            "section_codes": transcript_info['section_codes'],
            "transcript_type": transcript_info['transcript_type'],
            "full_text": transcript_info['full_text'],
            "pages": transcript_info['pages'],
            "metadata": {
                "filename": transcript_info['filename'],
                "item_info_raw": transcript_info['item_info_raw'],
                "page_count": transcript_info['page_count'],
                "extraction_method": "docling_ocr",
                "extracted_at": datetime.now().isoformat()
            }
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, indent=2, ensure_ascii=False)
        
        log.info(f"💾 Saved transcript text to: {output_path}")
        
        # NEW: Also save as markdown
        self._save_transcript_as_markdown(transcript_path, transcript_info, output_dir)

    def _save_transcript_as_markdown(self, transcript_path: Path, transcript_info: Dict[str, Any], output_dir: Path):
        """Save transcript as markdown with metadata header."""
        markdown_dir = output_dir.parent / "extracted_markdown"
        markdown_dir.mkdir(exist_ok=True)
        
        # Build header
        items_str = ', '.join(transcript_info['item_codes']) if transcript_info['item_codes'] else 'N/A'
        
        header = f"""---
DOCUMENT METADATA AND CONTEXT
=============================

**DOCUMENT IDENTIFICATION:**
- Full Path: Verbatim Items/{transcript_info['meeting_date'].split('.')[2]}/{transcript_path.name}
- Document Type: VERBATIM_TRANSCRIPT
- Filename: {transcript_path.name}

**PARSED INFORMATION:**
- Meeting Date: {transcript_info['meeting_date']}
- Agenda Items Discussed: {items_str}
- Transcript Type: {transcript_info['transcript_type']}
- Page Count: {transcript_info['page_count']}

**SEARCHABLE IDENTIFIERS:**
- MEETING_DATE: {transcript_info['meeting_date']}
- DOCUMENT_TYPE: VERBATIM_TRANSCRIPT
{self._format_item_identifiers(transcript_info['item_codes'])}

**NATURAL LANGUAGE DESCRIPTION:**
This is the verbatim transcript from the {transcript_info['meeting_date']} City Commission meeting covering the discussion of {self._describe_items(transcript_info)}.

**QUERY HELPERS:**
{self._build_transcript_query_helpers(transcript_info)}

---

{self._build_item_questions(transcript_info['item_codes'])}

# VERBATIM TRANSCRIPT CONTENT
"""
        
        # Combine with text
        full_content = header + "\n\n" + transcript_info.get('full_text', '')
        
        # Save file
        meeting_date = transcript_info['meeting_date'].replace('.', '_')
        item_info_clean = re.sub(r'[^a-zA-Z0-9-]', '_', transcript_info['item_info_raw'])
        md_filename = f"verbatim_{meeting_date}_{item_info_clean}.md"
        md_path = markdown_dir / md_filename
        
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(full_content)
        
        log.info(f"📝 Saved transcript markdown to: {md_path}")

    def _format_item_identifiers(self, item_codes: List[str]) -> str:
        """Format agenda items as searchable identifiers."""
        lines = []
        for item in item_codes:
            lines.append(f"- AGENDA_ITEM: {item}")
        return '\n'.join(lines)

    def _describe_items(self, transcript_info: Dict) -> str:
        """Create natural language description of items."""
        if transcript_info['item_codes']:
            if len(transcript_info['item_codes']) == 1:
                return f"agenda item {transcript_info['item_codes'][0]}"
            else:
                return f"agenda items {', '.join(transcript_info['item_codes'])}"
        elif 'PUBLIC_COMMENT' in transcript_info.get('section_codes', []):
            return "public comments section"
        else:
            return "the meeting proceedings"

    def _build_transcript_query_helpers(self, transcript_info: Dict) -> str:
        """Build query helpers for transcripts."""
        helpers = []
        for item in transcript_info.get('item_codes', []):
            helpers.append(f"- To find discussion about {item}, search for 'Item {item}' or '{item} discussion'")
        helpers.append(f"- To find all discussions from this meeting, search for '{transcript_info['meeting_date']}'")
        helpers.append("- This transcript contains the exact words spoken during the meeting")
        return '\n'.join(helpers)

    def _build_item_questions(self, item_codes: List[str]) -> str:
        """Build Q&A style entries for items."""
        questions = []
        for item in item_codes:
            questions.append(f"## What was discussed about Item {item}?")
            questions.append(f"The discussion of Item {item} is transcribed in this document.\n")
        return '\n'.join(questions)


================================================================================


################################################################################
# File: scripts/microsoft_framework/prompt_tuner.py
################################################################################

# File: scripts/microsoft_framework/prompt_tuner.py

import subprocess
import sys
import os
from pathlib import Path
import shutil

class CityClerkPromptTuner:
    """Auto-tune GraphRAG prompts for city clerk documents."""
    
    def __init__(self, graphrag_root: Path):
        self.graphrag_root = Path(graphrag_root)
        self.prompts_dir = self.graphrag_root / "prompts"
        
    def run_auto_tuning(self):
        """Run GraphRAG auto-tuning for city clerk domain."""
        
        # First, ensure prompts directory is clean
        if self.prompts_dir.exists():
            import shutil
            shutil.rmtree(self.prompts_dir)
        self.prompts_dir.mkdir(parents=True, exist_ok=True)
        
        # Create domain-specific examples file
        examples_file = self.graphrag_root / "domain_examples.txt"
        with open(examples_file, 'w') as f:
            f.write("""
Examples of entities in city clerk documents:

AGENDA_ITEM: E-1, F-10, H-3 (format: letter-number identifying agenda items)
ORDINANCE: 2024-01, 2024-15 (format: year-number for city ordinances)  
RESOLUTION: 2024-123, 2024-45 (format: year-number for city resolutions)
MEETING: January 9, 2024 City Commission Meeting
PERSON: Commissioner Smith, Mayor Johnson
ORGANIZATION: City of Coral Gables, Parks Department
MONEY: $1.5 million, $250,000
PROJECT: Waterfront Development, Parks Renovation

Example text:
"Agenda Item E-1 relates to Ordinance 2024-01 regarding the Cocoplum security district."
"Commissioner Smith moved to approve Resolution 2024-15 for $1.5 million funding."
""")
        
        # Get the correct Python executable
        python_exe = self.get_venv_python()
        print(f"🐍 Using Python: {python_exe}")
        
        # Run tuning with correct arguments
        cmd = [
            python_exe,
            "-m", "graphrag", "prompt-tune",
            "--root", str(self.graphrag_root),
            "--config", str(self.graphrag_root / "settings.yaml"),
            "--domain", "city government meetings, ordinances, resolutions, agenda items like E-1 and F-10",
            "--selection-method", "random",
            "--limit", "50",
            "--language", "English",
            "--max-tokens", "2000",
            "--chunk-size", "1200",
            "--output", str(self.prompts_dir)
        ]
        
        # Note: --examples flag might not exist in this version
        # Remove it if it causes issues
        
        subprocess.run(cmd, check=True)
        
    def get_venv_python(self):
        """Get the correct Python executable."""
        # Check if we're in a venv
        if sys.prefix != sys.base_prefix:
            return sys.executable
        
        # Try common venv locations
        venv_paths = [
            'venv/bin/python3',
            'venv/bin/python',
            '.venv/bin/python3',
            '.venv/bin/python',
            'city_clerk_rag/bin/python3',
            'city_clerk_rag/bin/python'
        ]
        
        for venv_path in venv_paths:
            full_path = os.path.join(os.getcwd(), venv_path)
            if os.path.exists(full_path):
                return full_path
        
        # Fallback
        return sys.executable
        
    def create_manual_prompts(self):
        """Create prompts manually without auto-tuning."""
        self.prompts_dir.mkdir(parents=True, exist_ok=True)
        
        # Entity extraction prompt - GraphRAG expects specific format
        entity_prompt = """
-Goal-
Given a text document from the City of Coral Gables, identify all entities and their relationships with high precision, following strict context rules.

**CRITICAL RULES FOR CONTEXT AND IDENTITY:**
1.  **Strict Association**: When you extract an entity, its description and relationships MUST come from the immediate surrounding text ONLY.
2.  **Detect Aliases and Identity**: If the text states or strongly implies that two different identifiers (e.g., "Agenda Item E-1" and "Ordinance 2024-01") refer to the SAME underlying legislative action, you MUST create a relationship between them.
    *   **Action**: Create both entities (e.g., `AGENDA_ITEM:E-1` and `ORDINANCE:2024-01`).
    *   **Relationship**: Link them with a relationship like `("relationship"<|>E-1<|>2024-01<|>is the same legislative action<|>10)`.
    *   **Description**: The descriptions for both entities should be consistent and reflect their shared identity.

-Steps-
1. Identify all entities. For each identified entity, extract the following information:
- title: Name of the entity, capitalized
- type: One of the following types: [{entity_types}]
- description: Comprehensive description of the entity's attributes and activities, **based ONLY on the immediate context and aliasing rules**.
Format each entity as ("entity"<|><title><|><type><|><description>)

2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly and directly related* to each other in the immediate text.
For each pair of related entities, extract the following information:
- source: name of the source entity, as identified in step 1
- target: name of the target entity, as identified in step 1
- description: explanation as to why you think the source entity and the target entity are related to each other, **citing direct evidence from the text**.
- weight: a numeric score indicating strength of the relationship between the source entity and target entity
Format each relationship as ("relationship"<|><source><|><target><|><description><|><weight>)

3. Return output in English as a single list of all the entities and relationships identified in steps 1 and 2. Use **{record_delimiter}** as the list delimiter.

4. When finished, output {completion_delimiter}

-Examples-
Entity types: AGENDA_ITEM, ORDINANCE, PERSON, ORGANIZATION
Text: "Agenda Item E-1, an ordinance to amend zoning, was passed. This is Ordinance 2024-01."
Output:
("entity"<|>E-1<|>AGENDA_ITEM<|>Agenda item E-1, which is the same as Ordinance 2024-01 and amends zoning.)
{record_delimiter}
("entity"<|>2024-01<|>ORDINANCE<|>Ordinance 2024-01, also known as agenda item E-1, which amends zoning.)
{record_delimiter}
("relationship"<|>E-1<|>2024-01<|>is the same as<|>10)
{completion_delimiter}

-Real Data-
Entity types: {entity_types}
Text: {input_text}
Output:
"""
        
        with open(self.prompts_dir / "entity_extraction.txt", 'w') as f:
            f.write(entity_prompt)
        
        # Community report prompt
        community_prompt = """
You are analyzing a community of related entities from city government documents.
Provide a comprehensive summary of the community, focusing on:
1. Key entities and their roles
2. Main relationships and interactions
3. Important decisions or actions
4. Overall significance to city governance

Community data:
{input_text}

Summary:
"""
        
        with open(self.prompts_dir / "community_report.txt", 'w') as f:
            f.write(community_prompt)
        
        print("✅ Created manual prompts with GraphRAG format")
        
    def customize_prompts(self):
        """Further customize prompts for city clerk specifics."""
        # Load and modify entity extraction prompt
        entity_prompt_path = self.prompts_dir / "entity_extraction.txt"
        
        if entity_prompt_path.exists():
            with open(entity_prompt_path, 'r') as f:
                prompt = f.read()
            
            # Add city clerk specific examples
            custom_additions = """
### City Clerk Specific Instructions:
- Pay special attention to agenda item codes (e.g., E-1, F-10, H-3)
- Extract voting records (who voted yes/no on what)
- Identify ordinance and resolution numbers (e.g., 2024-01, Resolution 2024-123)
- Extract budget amounts and financial figures
- Identify project names and development proposals
- Note public comment speakers and their concerns
"""
            
            # Insert custom additions
            prompt = prompt.replace("-Real Data-", custom_additions + "\n-Real Data-")
            
            with open(entity_prompt_path, 'w') as f:
                f.write(prompt)


================================================================================


################################################################################
# File: scripts/microsoft_framework/README.md
################################################################################

<!-- 
File: scripts/microsoft_framework/README.md
 -->

# City Clerk GraphRAG System

Microsoft GraphRAG integration for city clerk document processing with advanced entity extraction, community detection, and intelligent query routing.

## 🚀 Quick Start

### 1. Set up your environment:
```bash
export OPENAI_API_KEY='your-api-key-here'
```

### 2. Run the complete pipeline:
```bash
./run_graphrag.sh run
```

### 3. Test queries interactively:
```bash
./run_graphrag.sh query
```

## 📋 Prerequisites

1. **Environment Variables**:
   ```bash
   export OPENAI_API_KEY='your-openai-api-key'
   ```

2. **Extracted Documents**: 
   - City clerk documents should be extracted as JSON files in `city_clerk_documents/extracted_text/`
   - Run your existing document extraction pipeline first

3. **Dependencies**:
   - Python 3.8+
   - GraphRAG library
   - All dependencies in `requirements.txt`

## 🛠️ Installation & Setup

### Option 1: Using the Shell Script (Recommended)
```bash
# Setup environment and dependencies
./run_graphrag.sh setup

# Run the complete pipeline
./run_graphrag.sh run
```

### Option 2: Manual Setup
```bash
# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Run pipeline
python3 scripts/microsoft_framework/run_graphrag_pipeline.py
```

## 🔍 Query System

The system supports three types of queries with automatic routing:

### 1. 🎯 Local Search (Entity-Specific)
Best for specific entities and their immediate relationships:
```
"Who is Commissioner Smith?"
"What is ordinance 2024-01?"
"Tell me about agenda item E-1"
```

### 2. 🌐 Global Search (Holistic)
Best for broad themes and dataset-wide analysis:
```
"What are the main themes in city development?"
"Summarize all budget discussions"
"Overall trends in housing policy"
```

### 3. 🔄 DRIFT Search (Temporal/Complex)
Best for temporal changes and complex exploratory queries:
```
"How has the waterfront project evolved?"
"Timeline of budget decisions"
"Development of housing policy over time"
```

## 📊 Pipeline Steps

The GraphRAG pipeline includes:

1. **🔧 Environment Setup** - Initialize GraphRAG with city clerk configuration
2. **📄 Document Adaptation** - Convert extracted JSON documents to GraphRAG format
3. **🎯 Prompt Tuning** - Auto-tune prompts for city government domain
4. **🏗️ GraphRAG Indexing** - Extract entities, relationships, and communities
5. **📊 Results Processing** - Load and summarize GraphRAG outputs
6. **🔍 Query Testing** - Test with example queries
7. **🌐 Cosmos DB Sync** - Optionally sync to existing Cosmos DB

## 📁 Output Structure

After running the pipeline, you'll find:

```
graphrag_data/
├── settings.yaml           # GraphRAG configuration
├── city_clerk_documents.csv # Input documents in GraphRAG format
├── prompts/                # Auto-tuned prompts
│   ├── entity_extraction.txt
│   └── community_report.txt
└── output/                 # GraphRAG results
    ├── entities.parquet    # Extracted entities
    ├── relationships.parquet # Entity relationships
    ├── communities.parquet # Community clusters
    └── community_reports.parquet # Community summaries
```

## 🎮 Usage Examples

### Run Complete Pipeline
```bash
./run_graphrag.sh run
```

### Interactive Query Session
```bash
./run_graphrag.sh query
```

### View Results Summary
```bash
./run_graphrag.sh results
```

### Clean Up Data
```bash
./run_graphrag.sh clean
```

### Example Queries to Try

**Entity-specific (Local Search):**
- "Who is Mayor Johnson?"
- "What is resolution 2024-15?"
- "Tell me about the parks department"

**Holistic (Global Search):**
- "What are the main development themes?"
- "Summarize all transportation discussions"
- "Overall budget allocation patterns"

**Temporal (DRIFT Search):**
- "How has zoning policy evolved?"
- "Timeline of infrastructure projects"
- "Development of affordable housing initiatives"

## ⚙️ Configuration

### Model Settings
The system is configured to use:
- **Model**: `gpt-4.1-mini-2025-04-14`
- **Max Tokens**: `32768`
- **Temperature**: `0` (deterministic)

### Entity Types
Configured for city government entities:
- `person`, `organization`, `location`
- `document`, `meeting`, `agenda_item`
- `money`, `project`, `ordinance`, `resolution`, `contract`

### Query Configuration
- **Global Search**: Community-level analysis with dynamic selection
- **Local Search**: Top-K entity retrieval with community context
- **DRIFT Search**: Iterative exploration with follow-up expansion

## 🔧 Advanced Usage

### Python API
```python
from scripts.microsoft_framework import CityClerkQueryEngine

# Initialize query engine
engine = CityClerkQueryEngine("./graphrag_data")

# Auto-routed query
result = await engine.query("What are the main budget themes?")

# Specific method
result = await engine.query(
    "Who is Commissioner Smith?", 
    method="local",
    top_k_entities=15
)
```

### Custom Query Routing
```python
from scripts.microsoft_framework import SmartQueryRouter

router = SmartQueryRouter()
route_info = router.determine_query_method("Your query here")
print(f"Recommended method: {route_info['method']}")
```

### Cosmos DB Integration
```python
from scripts.microsoft_framework import GraphRAGCosmosSync

sync = GraphRAGCosmosSync("./graphrag_data/output")
await sync.sync_to_cosmos()
```

## 📈 Performance Tips

1. **Incremental Processing**: Use `IncrementalGraphRAGProcessor` for new documents
2. **Community Levels**: Adjust community levels for different query scopes
3. **Query Optimization**: Use specific entity names and agenda codes when known
4. **Batch Processing**: Process documents in batches for large datasets

## 🐛 Troubleshooting

### Common Issues

**GraphRAG not found:**
```bash
pip install graphrag
```

**No documents found:**
- Ensure documents are in `city_clerk_documents/extracted_text/`
- Run document extraction pipeline first

**API Key issues:**
```bash
export OPENAI_API_KEY='your-key-here'
```

**Memory issues:**
- Reduce `max_tokens` in settings
- Process documents in smaller batches

### Debug Mode
```bash
# Run with verbose output
python3 scripts/microsoft_framework/run_graphrag_pipeline.py --verbose

# Check GraphRAG logs
tail -f graphrag_data/logs/*.log
```

## 🤝 Integration with Existing System

This GraphRAG system integrates seamlessly with your existing infrastructure:

- **Reuses**: Docling PDF extraction, URL preservation, Cosmos DB client
- **Extends**: Adds advanced entity extraction and community detection
- **Maintains**: Existing graph schema and document processing pipeline
- **Enhances**: Query capabilities with intelligent routing

## 📚 More Information

- [Microsoft GraphRAG Documentation](https://microsoft.github.io/graphrag/)
- [Query Configuration Guide](./city_clerk_settings_template.yaml)
- [Entity Types and Prompts](./prompt_tuner.py)

---

For issues or questions, check the troubleshooting section or review the pipeline logs in `graphrag_data/logs/`.


================================================================================


################################################################################
# File: scripts/microsoft_framework/graphrag_initializer.py
################################################################################

# File: scripts/microsoft_framework/graphrag_initializer.py

import os
from pathlib import Path
import yaml
import subprocess
import sys
from typing import Dict, Any

class GraphRAGInitializer:
    """Initialize and configure Microsoft GraphRAG for city clerk documents."""
    
    def __init__(self, project_root: Path):
        self.project_root = Path(project_root)
        self.graphrag_root = self.project_root / "graphrag_data"
        
    def setup_environment(self):
        """Setup GraphRAG environment and configuration."""
        # Get the correct Python executable
        if hasattr(sys, 'real_prefix') or (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix):
            # We're in a virtualenv
            python_exe = sys.executable
        else:
            # Try to find venv Python
            venv_python = os.path.join(os.path.dirname(sys.executable), '..', 'venv', 'bin', 'python3')
            if os.path.exists(venv_python):
                python_exe = venv_python
            else:
                python_exe = sys.executable
        
        print(f"🐍 Using Python: {python_exe}")
        
        # Create directory structure
        self.graphrag_root.mkdir(exist_ok=True)
        
        # Run GraphRAG init
        subprocess.run([
            python_exe,  # Use the correct Python
            "-m", "graphrag", "init", 
            "--root", str(self.graphrag_root),
            "--force"
        ])
        
        # Configure settings
        self._configure_settings()
        self._configure_prompts()
        
    def _configure_settings(self):
        """Configure with enhanced extraction."""
        settings = {
            "encoding_model": "cl100k_base",
            "skip_workflows": [],
            "models": {
                "default_chat_model": {
                    "api_key": "${OPENAI_API_KEY}",
                    "type": "openai_chat",
                    "model": "gpt-4.1-mini-2025-04-14",
                    "encoding_model": "cl100k_base",
                    "max_tokens": 32768,
                    "temperature": 0,
                    "api_type": "openai"
                },
                "default_embedding_model": {
                    "api_key": "${OPENAI_API_KEY}",
                    "type": "openai_embedding",
                    "model": "text-embedding-3-small",
                    "encoding_model": "cl100k_base",
                    "batch_size": 16,
                    "batch_max_tokens": 2048
                }
            },
            "input": {
                "type": "file",
                "file_type": "csv",
                "base_dir": ".",
                "source_column": "text",
                "text_column": "text",
                "title_column": "title"
            },
            "chunks": {
                "group_by_columns": [
                    "document_type",
                    "meeting_date",
                    "item_code"
                ],
                "overlap": 200,
                "size": 1200
            },
            "extract_graph": {
                "model_id": "default_chat_model",
                "prompt": "prompts/entity_extraction.txt",
                "entity_types": [
                    "agenda_item",
                    "ordinance", 
                    "resolution",
                    "document_number",
                    "cross_reference",
                    "person",
                    "organization",
                    "meeting",
                    "money",
                    "project"
                ],
                "max_gleanings": 3,
                "pattern_examples": {
                    "agenda_item": ["E-1", "F-10", "Item E-1", "(Agenda Item: E-1)"],
                    "ordinance": ["2024-01", "Ordinance 3576", "Ordinance No. 3576"],
                    "document_number": ["2024-01", "3576", "Resolution 2024-123"]
                }
            },
            "entity_extraction": {
                "entity_types": [
                    "person",
                    "organization",
                    "location",
                    "document",
                    "meeting",
                    "money",
                    "project",
                    "agenda_item",
                    "ordinance",
                    "resolution",
                    "contract"
                ],
                "max_gleanings": 2
            },
            "community_reports": {
                "max_input_length": 32768,
                "max_length": 2000
            },
            "claim_extraction": {
                "description": "Extract voting records, motions, and decisions",
                "enabled": True
            },
            "cluster_graph": {
                "max_cluster_size": 10
            },
            "storage": {
                "base_dir": "./output/artifacts",
                "type": "file"
            }
        }
        
        settings_path = self.graphrag_root / "settings.yaml"
        with open(settings_path, 'w') as f:
            yaml.dump(settings, f, sort_keys=False)
    
    def _configure_prompts(self):
        """Setup prompt configuration placeholders."""
        # This method will be called after auto-tuning
        pass


================================================================================


################################################################################
# File: extract_documents_for_graphrag.py
################################################################################

# File: extract_documents_for_graphrag.py

#!/usr/bin/env python3
"""
Extract documents using enhanced PDF extractor for GraphRAG processing.
This script uses the intelligent metadata header functionality.
"""

import sys
from pathlib import Path
import asyncio

# Add current directory to path
sys.path.append('.')

from scripts.graph_stages.pdf_extractor import PDFExtractor

def extract_documents_for_graphrag():
    """Extract a few sample documents for GraphRAG testing."""
    
    print("🚀 Starting document extraction for GraphRAG testing")
    print("="*60)
    
    # Define source and output directories
    pdf_dir = Path("city_clerk_documents/global copy/City Comissions 2024/Agendas")
    output_dir = Path("city_clerk_documents/extracted_text")
    markdown_dir = Path("city_clerk_documents/extracted_markdown")
    
    # Create output directories
    output_dir.mkdir(parents=True, exist_ok=True)
    markdown_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"📁 Source directory: {pdf_dir}")
    print(f"📁 JSON output directory: {output_dir}")
    print(f"📁 Markdown output directory: {markdown_dir}")
    
    # Find agenda PDFs (limit to a few for testing)
    agenda_files = sorted(pdf_dir.glob("Agenda*.pdf"))[:3]  # Just first 3 for testing
    
    if not agenda_files:
        print("❌ No agenda PDFs found!")
        return False
    
    print(f"📄 Found {len(agenda_files)} agenda files to process:")
    for pdf in agenda_files:
        print(f"   - {pdf.name}")
    
    # Initialize extractor
    extractor = PDFExtractor(pdf_dir, output_dir)
    
    # Process each PDF
    for pdf_path in agenda_files:
        try:
            print(f"\n📄 Processing: {pdf_path.name}")
            
            # Extract with intelligent metadata headers
            markdown_path, enhanced_content = extractor.extract_and_save_with_metadata(
                pdf_path, markdown_dir
            )
            print(f"✅ Enhanced markdown saved to: {markdown_path}")
            
            # Also extract regular JSON for compatibility
            full_text, pages = extractor.extract_text_from_pdf(pdf_path)
            extracted_data = {
                'full_text': full_text,
                'pages': pages,
                'document_type': extractor._determine_doc_type(pdf_path.name),
                'metadata': {
                    'filename': pdf_path.name,
                    'num_pages': len(pages),
                    'total_chars': len(full_text),
                    'extraction_method': 'docling_enhanced'
                }
            }
            
            # Save JSON
            import json
            json_path = output_dir / f"{pdf_path.stem}_extracted.json"
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(extracted_data, f, indent=2, ensure_ascii=False)
            print(f"✅ JSON saved to: {json_path}")
            
        except Exception as e:
            print(f"❌ Error processing {pdf_path.name}: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    print(f"\n✅ Document extraction complete!")
    print(f"📊 Extracted {len(agenda_files)} documents")
    print(f"📁 Files available in:")
    print(f"   - JSON: {output_dir}")
    print(f"   - Enhanced Markdown: {markdown_dir}")
    
    return True

if __name__ == "__main__":
    success = extract_documents_for_graphrag()
    if success:
        print("\n🎯 Ready for GraphRAG pipeline!")
    else:
        print("\n❌ Extraction failed")
        sys.exit(1)


================================================================================


################################################################################
# File: explore_graphrag_sources.py
################################################################################

# File: explore_graphrag_sources.py

#!/usr/bin/env python3
"""
Explore GraphRAG data sources and trace entity lineage.
"""

import pandas as pd
from pathlib import Path
import sys

def explore_sources():
    """Explore the GraphRAG knowledge base to understand data sources."""
    
    graphrag_root = Path("graphrag_data")
    
    print("🔍 GraphRAG Data Source Explorer")
    print("=" * 50)
    
    # Load all the parquet files
    print("\n📊 Loading GraphRAG outputs...")
    
    entities_df = pd.read_parquet(graphrag_root / "output/entities.parquet")
    relationships_df = pd.read_parquet(graphrag_root / "output/relationships.parquet")
    docs_df = pd.read_csv(graphrag_root / "city_clerk_documents.csv")
    
    print(f"✅ Loaded {len(entities_df)} entities")
    print(f"✅ Loaded {len(relationships_df)} relationships")
    print(f"✅ Loaded {len(docs_df)} source documents")
    
    # Show entity type breakdown
    print("\n📈 Entity Types:")
    print(entities_df['type'].value_counts())
    
    # Interactive exploration
    while True:
        print("\n" + "="*50)
        print("Enter an entity to explore (e.g., E-1, 2024-01) or 'quit':")
        query = input("> ").strip()
        
        if query.lower() == 'quit':
            break
        
        # Find matching entities
        matches = entities_df[
            entities_df['title'].str.contains(query, case=False, na=False) |
            entities_df['description'].str.contains(query, case=False, na=False)
        ]
        
        if matches.empty:
            print(f"❌ No entities found matching '{query}'")
            continue
        
        print(f"\n📍 Found {len(matches)} matching entities:")
        
        for idx, entity in matches.iterrows():
            print(f"\n🏷️ Entity ID: {idx}")
            print(f"   Title: {entity['title']}")
            print(f"   Type: {entity['type']}")
            print(f"   Description: {entity['description'][:200]}...")
            
            # Find relationships
            related = relationships_df[
                (relationships_df['source'] == idx) | 
                (relationships_df['target'] == idx)
            ]
            
            if not related.empty:
                print(f"   🔗 Relationships: {len(related)}")
                for _, rel in related.head(3).iterrows():
                    print(f"      - {rel['description'][:100]}...")
            
            # Try to trace back to source document
            print("\n   📄 Possible Source Documents:")
            for _, doc in docs_df.iterrows():
                if query.upper() in str(doc['item_code']).upper() or \
                   query in str(doc['document_number']) or \
                   query.lower() in str(doc['title']).lower():
                    print(f"      - {doc['source_file']}")
                    print(f"        Type: {doc['document_type']}")
                    print(f"        Meeting: {doc['meeting_date']}")
                    break

if __name__ == "__main__":
    explore_sources()


================================================================================


################################################################################
# File: scripts/microsoft_framework/graphrag_pipeline.py
################################################################################

# File: scripts/microsoft_framework/graphrag_pipeline.py

import asyncio
import subprocess
from pathlib import Path
import pandas as pd
import json
from typing import Dict, List, Any

# Import other components
from .graphrag_initializer import GraphRAGInitializer
from .document_adapter import CityClerkDocumentAdapter
from .prompt_tuner import CityClerkPromptTuner
from .graphrag_output_processor import GraphRAGOutputProcessor

class CityClerkGraphRAGPipeline:
    """Main pipeline for processing city clerk documents with GraphRAG."""
    
    def __init__(self, project_root: Path):
        self.project_root = Path(project_root)
        self.graphrag_root = self.project_root / "graphrag_data"
        self.output_dir = self.graphrag_root / "output"
        
    async def run_full_pipeline(self, force_reindex: bool = False):
        """Run the complete GraphRAG indexing pipeline."""
        
        # Step 1: Initialize GraphRAG
        print("🔧 Initializing GraphRAG...")
        initializer = GraphRAGInitializer(self.project_root)
        initializer.setup_environment()
        
        # Step 2: Prepare documents
        print("📄 Preparing documents...")
        adapter = CityClerkDocumentAdapter(
            self.project_root / "city_clerk_documents/extracted_text"
        )
        df = adapter.prepare_documents_for_graphrag(self.graphrag_root)
        
        # Step 3: Run prompt tuning
        print("🎯 Tuning prompts for city clerk domain...")
        tuner = CityClerkPromptTuner(self.graphrag_root)
        tuner.run_auto_tuning()
        tuner.customize_prompts()
        
        # Step 4: Run GraphRAG indexing
        print("🏗️ Running GraphRAG indexing...")
        subprocess.run([
            "graphrag", "index",
            "--root", str(self.graphrag_root),
            "--verbose"
        ])
        
        # Step 5: Process outputs
        print("📊 Processing GraphRAG outputs...")
        processor = GraphRAGOutputProcessor(self.output_dir)
        graph_data = processor.load_graphrag_artifacts()
        
        return graph_data


================================================================================


################################################################################
# File: scripts/microsoft_framework/incremental_processor.py
################################################################################

# File: scripts/microsoft_framework/incremental_processor.py

from pathlib import Path
import json
from typing import List, Set
import asyncio

class IncrementalGraphRAGProcessor:
    """Handle incremental updates to GraphRAG index."""
    
    def __init__(self, graphrag_root: Path):
        self.graphrag_root = Path(graphrag_root)
        self.processed_files = self._load_processed_files()
        
    async def process_new_documents(self, new_docs_dir: Path):
        """Process only new documents added since last run."""
        
        # Find new documents
        new_files = []
        for doc_path in new_docs_dir.glob("*.pdf"):
            if doc_path.name not in self.processed_files:
                new_files.append(doc_path)
        
        if not new_files:
            print("✅ No new documents to process")
            return
        
        print(f"📄 Processing {len(new_files)} new documents...")
        
        # Extract text using existing Docling pipeline
        from scripts.graph_stages.pdf_extractor import PDFExtractor
        extractor = PDFExtractor(new_docs_dir)
        
        # Process and add to GraphRAG
        # ... implementation details ...
        
        # Update processed files list
        self._update_processed_files(new_files)
    
    def _load_processed_files(self) -> Set[str]:
        """Load list of previously processed files."""
        processed_files_path = self.graphrag_root / "processed_files.json"
        
        if processed_files_path.exists():
            with open(processed_files_path, 'r') as f:
                return set(json.load(f))
        
        return set()
    
    def _update_processed_files(self, new_files: List[Path]):
        """Update the list of processed files."""
        for file_path in new_files:
            self.processed_files.add(file_path.name)
        
        processed_files_path = self.graphrag_root / "processed_files.json"
        with open(processed_files_path, 'w') as f:
            json.dump(list(self.processed_files), f)


================================================================================


################################################################################
# File: requirements.txt
################################################################################

################################################################################
################################################################################
# Python dependencies for Misophonia Research RAG System

# Core dependencies
openai>=1.82.0
groq>=0.4.1
gremlinpython>=3.7.3
aiofiles>=24.1.0
python-dotenv>=1.0.0

# Graph Visualization dependencies
dash>=2.14.0
plotly>=5.17.0
networkx>=3.2.0
dash-table>=5.0.0
dash-cytoscape>=0.3.0
dash-bootstrap-components>=1.0.0

# Database and API dependencies
supabase>=2.0.0

# GraphRAG dependencies
graphrag==2.3.0
pyyaml>=6.0.0
pyarrow>=14.0.0
scipy>=1.11.0

# Optional for async progress bars
tqdm

# Data processing
pandas>=2.0.0
numpy

# PDF processing
PyPDF2>=3.0.0
unstructured[pdf]==0.10.30
pytesseract>=0.3.10
pdf2image>=1.16.3
docling
pdfminer.six>=20221105

# PDF processing with hyperlink support
PyMuPDF>=1.23.0

# Utilities
colorama>=0.4.6

# For concurrent processing
concurrent-log-handler>=0.9.20

# Async dependencies for optimized pipeline
aiohttp>=3.8.0

# Token counting for OpenAI rate limiting
tiktoken>=0.5.0


================================================================================


################################################################################
# File: scripts/graph_stages/__init__.py
################################################################################

# File: scripts/graph_stages/__init__.py

"""
Graph Pipeline Stages
=====================
Components for building city clerk document knowledge graph.
"""

from .cosmos_db_client import CosmosGraphClient
from .agenda_pdf_extractor import AgendaPDFExtractor
from .agenda_ontology_extractor import CityClerkOntologyExtractor
from .enhanced_document_linker import EnhancedDocumentLinker
from .agenda_graph_builder import AgendaGraphBuilder
from .verbatim_transcript_linker import VerbatimTranscriptLinker

__all__ = [
    'CosmosGraphClient',
    'AgendaPDFExtractor',
    'CityClerkOntologyExtractor',
    'EnhancedDocumentLinker',
    'AgendaGraphBuilder',
    'VerbatimTranscriptLinker'
]


================================================================================

