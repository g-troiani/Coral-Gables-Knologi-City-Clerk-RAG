# Concatenated Project Code - Part 1 of 3
# Generated: 2025-06-02 13:02:29
# Root Directory: /Users/gianmariatroiani/Documents/knologi̊/graph_database
================================================================================

# Directory Structure
################################################################################
├── .gitignore (939.0B, no ext)
├── city_clerk_documents/
│   ├── global copy/
│   │   ├── City Comissions 2024/
│   │   │   ├── Agendas/
│   │   │   │   ├── Agenda 01.23.2024.pdf (155.1KB, .pdf)
│   │   │   │   ├── Agenda 01.9.2024.pdf (151.1KB, .pdf)
│   │   │   │   ├── Agenda 02.13.2024.pdf (155.0KB, .pdf)
│   │   │   │   ├── Agenda 02.27.2024.pdf (152.9KB, .pdf)
│   │   │   │   ├── Agenda 03.12.2024.pdf (166.6KB, .pdf)
│   │   │   │   ├── Agenda 05.07.2024.pdf (167.6KB, .pdf)
│   │   │   │   ├── Agenda 05.21.2024.pdf (172.2KB, .pdf)
│   │   │   │   └── Agenda 06.11.2024.pdf (160.2KB, .pdf)
│   │   │   ├── ExportedFolderContents.zip (62.4MB, .zip)
│   │   │   ├── Ordinances/
│   │   │   │   ├── 2024/
│   │   │   │   │   ├── 2024-01 - 01_09_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-02 - 01_09_2024.pdf (15.1MB, .pdf)
│   │   │   │   │   ├── 2024-03 - 01_09_2024.pdf (11.1MB, .pdf)
│   │   │   │   │   ├── 2024-04 - 01_23_2024.pdf (2.3MB, .pdf)
│   │   │   │   │   ├── 2024-05 - 02_13_2024.pdf (2.0MB, .pdf)
│   │   │   │   │   ├── 2024-06 - 02_13_2024.pdf (1.9MB, .pdf)
│   │   │   │   │   ├── 2024-07 - 02_13_2024.pdf (6.5MB, .pdf)
│   │   │   │   │   ├── 2024-08 - 02_27_2024.pdf (2.5MB, .pdf)
│   │   │   │   │   ├── 2024-09 - 02_27_2024.pdf (1.6MB, .pdf)
│   │   │   │   │   ├── 2024-10 - 03_12_2024.pdf (3.7MB, .pdf)
│   │   │   │   │   ├── 2024-11 - 03_12_2024 - (As Amended).pdf (5.6MB, .pdf)
│   │   │   │   │   ├── 2024-11 - 03_12_2024.pdf (5.5MB, .pdf)
│   │   │   │   │   ├── 2024-12 - 03_12_2024.pdf (1.8MB, .pdf)
│   │   │   │   │   ├── 2024-13 - 03_12_2024.pdf (3.2MB, .pdf)
│   │   │   │   │   ├── 2024-14 - 05_07_2024.pdf (2.8MB, .pdf)
│   │   │   │   │   ├── 2024-15 - 05_07_2024.pdf (3.9MB, .pdf)
│   │   │   │   │   ├── 2024-16 - 05_07_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-17 - 05_07_2024.pdf (4.3MB, .pdf)
│   │   │   │   │   ├── 2024-18 - 05_21_2024.pdf (2.4MB, .pdf)
│   │   │   │   │   ├── 2024-19 - 05_21_2024.pdf (1.7MB, .pdf)
│   │   │   │   │   ├── 2024-20 - 05_21_2024.pdf (1.6MB, .pdf)
│   │   │   │   │   ├── 2024-21 - 06_11_2024.pdf (1.7MB, .pdf)
│   │   │   │   │   ├── 2024-22 - 06_11_2024.pdf (2.0MB, .pdf)
│   │   │   │   │   ├── 2024-23 - 06_11_2024.pdf (1.9MB, .pdf)
│   │   │   │   │   ├── 2024-24 - 06_11_2024.pdf (2.1MB, .pdf)
│   │   │   │   │   └── 2024-25 - 06_11_2024.pdf (1.3MB, .pdf)
│   │   │   │   ├── [EXCLUDED] 1 items: .DS_Store (excluded file)
│   │   │   ├── Resolutions/
│   │   │   │   ├── 2024/
│   │   │   │   │   ├── 2024-01 - 01_09_2024.pdf (448.9KB, .pdf)
│   │   │   │   │   ├── 2024-02 - 01_09_2024.pdf (451.9KB, .pdf)
│   │   │   │   │   ├── 2024-03 - 01_09_2024.pdf (867.3KB, .pdf)
│   │   │   │   │   ├── 2024-04 - 01_09_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-05 - 01_09_2024.pdf (936.9KB, .pdf)
│   │   │   │   │   ├── 2024-06 - 01_09_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-07 - 01_09_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-08 - 01_23_2024.pdf (457.9KB, .pdf)
│   │   │   │   │   ├── 2024-09 - 01_23_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-10 - 01_23_2024.pdf (454.8KB, .pdf)
│   │   │   │   │   ├── 2024-10 - 03_12_2024.pdf (3.7MB, .pdf)
│   │   │   │   │   ├── 2024-100 - 05_21_2024.pdf (3.0MB, .pdf)
│   │   │   │   │   ├── 2024-101 - 05_21_2024.pdf (947.4KB, .pdf)
│   │   │   │   │   ├── 2024-102 - 05_21_2024.pdf (466.0KB, .pdf)
│   │   │   │   │   ├── 2024-103 - 05_21_2024.pdf (991.8KB, .pdf)
│   │   │   │   │   ├── 2024-104 - 05_21_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-105 - 05_21_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-106 - 05_21_2024.pdf (2.4MB, .pdf)
│   │   │   │   │   ├── 2024-107 - 05_21_2024.pdf (6.0MB, .pdf)
│   │   │   │   │   ├── 2024-108 - 05_21_2024.pdf (2.1MB, .pdf)
│   │   │   │   │   ├── 2024-109 - 05_21_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-11 - 01_23_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-110 - 05_21_2024.pdf (921.5KB, .pdf)
│   │   │   │   │   ├── 2024-111 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-112 - 05_21_2024.pdf (6.2MB, .pdf)
│   │   │   │   │   ├── 2024-113 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-114 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-115 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-116 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-117 - 05_21_2024.pdf (6.4MB, .pdf)
│   │   │   │   │   ├── 2024-118 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-119 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-12 - 01_23_2024.pdf (588.3KB, .pdf)
│   │   │   │   │   ├── 2024-120 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-121 - 05_21_2024.pdf (6.4MB, .pdf)
│   │   │   │   │   ├── 2024-122 - 05_21_2024.pdf (6.8MB, .pdf)
│   │   │   │   │   ├── 2024-123 - 05_21_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-124 - 05_21_2024.pdf (11.2MB, .pdf)
│   │   │   │   │   ├── 2024-125 - 05_21_2024.pdf (776.0KB, .pdf)
│   │   │   │   │   ├── 2024-126 - 05_21_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-127 - 05_21_2024.pdf (8.1MB, .pdf)
│   │   │   │   │   ├── 2024-129 - 06_11_2024.pdf (1.3MB, .pdf)
│   │   │   │   │   ├── 2024-13 - 01_23_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-130 - 06_11_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-131 - 06_11_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-132 - 06_11_2024.pdf (458.0KB, .pdf)
│   │   │   │   │   ├── 2024-133 - 06_11_2024 -As Amended.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-134 - 06_11_2024.pdf (884.8KB, .pdf)
│   │   │   │   │   ├── 2024-135 - 06_11_2024.pdf (1022.1KB, .pdf)
│   │   │   │   │   ├── 2024-136 - 06_11_2024.pdf (537.4KB, .pdf)
│   │   │   │   │   ├── 2024-137 - 06_11_2024.pdf (1.7MB, .pdf)
│   │   │   │   │   ├── 2024-138 - 06_11_2024.pdf (1.7MB, .pdf)
│   │   │   │   │   ├── 2024-139 - 06_11_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-14 - 01_23_2024.pdf (996.6KB, .pdf)
│   │   │   │   │   ├── 2024-140 - 06_11_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-141 - 06_11_2024.pdf (13.2MB, .pdf)
│   │   │   │   │   ├── 2024-142 - 06_11_2024.pdf (790.5KB, .pdf)
│   │   │   │   │   ├── 2024-143 -06_11_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-15 - 01_23_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-16 - 01_23_2024.pdf (1.8MB, .pdf)
│   │   │   │   │   ├── 2024-17 - 01_23_2024.pdf (488.0KB, .pdf)
│   │   │   │   │   ├── 2024-18 - 01_23_2024.pdf (849.1KB, .pdf)
│   │   │   │   │   ├── 2024-19 - 02_19_2024.pdf (1019.5KB, .pdf)
│   │   │   │   │   ├── 2024-20 - 02_13_2024.pdf (824.5KB, .pdf)
│   │   │   │   │   ├── 2024-21 - 02_13_2024.pdf (502.6KB, .pdf)
│   │   │   │   │   ├── 2024-22 - 02_13_2024.pdf (450.6KB, .pdf)
│   │   │   │   │   ├── 2024-23 - 02_13_2024.pdf (447.7KB, .pdf)
│   │   │   │   │   ├── 2024-24 - 02_13_2024.pdf (464.1KB, .pdf)
│   │   │   │   │   ├── 2024-25 - 02_13_2024.pdf (458.3KB, .pdf)
│   │   │   │   │   ├── 2024-26 - 02_13_2024.pdf (465.7KB, .pdf)
│   │   │   │   │   ├── 2024-27 - 02_13_2024.pdf (754.2KB, .pdf)
│   │   │   │   │   ├── 2024-28 - 02_13_2024.pdf (761.7KB, .pdf)
│   │   │   │   │   ├── 2024-29 - 02_13_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-30 - 02_13_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-31 - 02_13_2024.pdf (936.6KB, .pdf)
│   │   │   │   │   ├── 2024-32 - 02_13_2024.pdf (1.9MB, .pdf)
│   │   │   │   │   ├── 2024-33 - 02_13_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-34 - 02_13_2024.pdf (1.9MB, .pdf)
│   │   │   │   │   ├── 2024-35 - 02_27_2024.pdf (480.7KB, .pdf)
│   │   │   │   │   ├── 2024-36 - 02_27_2024.pdf (1.3MB, .pdf)
│   │   │   │   │   ├── 2024-37 - 02_27_2024.pdf (940.3KB, .pdf)
│   │   │   │   │   ├── 2024-38 - 02_27_2024.pdf (829.6KB, .pdf)
│   │   │   │   │   ├── 2024-39 - 02_27_2024.pdf (816.5KB, .pdf)
│   │   │   │   │   ├── 2024-41 - 02_27_2024.pdf (833.1KB, .pdf)
│   │   │   │   │   ├── 2024-42 - 02_27_2024.pdf (473.4KB, .pdf)
│   │   │   │   │   ├── 2024-43 - 02_27_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-44 - 02_27_2024.pdf (1.8MB, .pdf)
│   │   │   │   │   ├── 2024-45 - 03_12_2024.pdf (1002.1KB, .pdf)
│   │   │   │   │   ├── 2024-46 - 03_12_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-47 - 03_12_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-48 - 03_12_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-49 - 03_12_2024.pdf (899.6KB, .pdf)
│   │   │   │   │   ├── 2024-50 - 03_12_2024.pdf (462.1KB, .pdf)
│   │   │   │   │   ├── 2024-51 - 03_12_2024.pdf (538.1KB, .pdf)
│   │   │   │   │   ├── 2024-52 - 03_12_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-53 - 03_12_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-54 - 03_12_2024.pdf (2.0MB, .pdf)
│   │   │   │   │   ├── 2024-55 - 03_12_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-56 - 03_12_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-57 - 03_12_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-58 - 03_12_2024.pdf (775.8KB, .pdf)
│   │   │   │   │   ├── 2024-60 - 03_12_2024.pdf (2.3MB, .pdf)
│   │   │   │   │   ├── 2024-61 - 04_16_2024.pdf (6.4MB, .pdf)
│   │   │   │   │   ├── 2024-62 - 04_16_2024.pdf (2.7MB, .pdf)
│   │   │   │   │   ├── 2024-63 -  04_16_2024.pdf (832.3KB, .pdf)
│   │   │   │   │   ├── 2024-64 - 04_16_2024.pdf (452.6KB, .pdf)
│   │   │   │   │   ├── 2024-65 - 04_16_2024.pdf (894.5KB, .pdf)
│   │   │   │   │   ├── 2024-66 - 04_16_2024.pdf (446.6KB, .pdf)
│   │   │   │   │   ├── 2024-67 - 04_16_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-68 - 04_16_2024.pdf (5.6MB, .pdf)
│   │   │   │   │   ├── 2024-69- -04_16_2024.pdf (443.1KB, .pdf)
│   │   │   │   │   ├── 2024-70 - 04_16_2024.pdf (878.9KB, .pdf)
│   │   │   │   │   ├── 2024-71 - 04_16_2024.pdf (951.8KB, .pdf)
│   │   │   │   │   ├── 2024-72 - 04_16_2024.pdf (821.7KB, .pdf)
│   │   │   │   │   ├── 2024-73 - 04_16_2024.pdf (810.8KB, .pdf)
│   │   │   │   │   ├── 2024-74 - 04_16_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-75 - 04_16_2024.pdf (1.6MB, .pdf)
│   │   │   │   │   ├── 2024-76 - 04_16_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-77 - 04_16_2024.pdf (1013.9KB, .pdf)
│   │   │   │   │   ├── 2024-78 - 04_16_2024.pdf (1007.5KB, .pdf)
│   │   │   │   │   ├── 2024-79 - 04_16_2024.pdf (997.7KB, .pdf)
│   │   │   │   │   ├── 2024-80 - 04_16_2024.pdf (525.7KB, .pdf)
│   │   │   │   │   ├── 2024-81 - 04_16_2024.pdf (923.2KB, .pdf)
│   │   │   │   │   ├── 2024-82 - 04_16_2024.pdf (473.6KB, .pdf)
│   │   │   │   │   ├── 2024-83 - 04_16_2024.pdf (915.3KB, .pdf)
│   │   │   │   │   ├── 2024-84 - 05_07_2024.pdf (992.7KB, .pdf)
│   │   │   │   │   ├── 2024-85 - 05_07_2024.pdf (2.0MB, .pdf)
│   │   │   │   │   ├── 2024-86 - 05_07_2024.pdf (503.1KB, .pdf)
│   │   │   │   │   ├── 2024-87 - 05_07_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-88 - 05_07_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-89 - 05_07_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-90 - 05_07_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-91 - 05_07_2024.pdf (1.8MB, .pdf)
│   │   │   │   │   ├── 2024-92 - 05_07_2024.pdf (452.4KB, .pdf)
│   │   │   │   │   ├── 2024-93 - 05_07_2024.pdf (1014.3KB, .pdf)
│   │   │   │   │   ├── 2024-94 - 05_05_2024.pdf (1.3MB, .pdf)
│   │   │   │   │   ├── 2024-95 - 05_07_2024.pdf (974.5KB, .pdf)
│   │   │   │   │   ├── 2024-96 - 05_07_2024.pdf (1.5MB, .pdf)
│   │   │   │   │   ├── 2024-97 - 05_07_2024.pdf (1023.3KB, .pdf)
│   │   │   │   │   ├── 2024-98 - 05_07_2024.pdf (2.2MB, .pdf)
│   │   │   │   │   └── 2024-99 - 05_07_2024.pdf (792.7KB, .pdf)
│   │   │   │   ├── [EXCLUDED] 1 items: .DS_Store (excluded file)
│   │   │   ├── Verbating Items/
│   │   │   │   ├── 2024/
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - E-4.pdf (2.4MB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - E-5.pdf (735.6KB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - E-7.pdf (306.9KB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - E-8.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - F-10.pdf (925.3KB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - F-2.pdf (288.2KB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - F-5.pdf (360.6KB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - F-6.pdf (212.8KB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - Public Comment.pdf (762.7KB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - C- Public Comment.pdf (1.3MB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - E-3 and E-4.pdf (2.4MB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - E-5.pdf (290.7KB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - F-1.pdf (855.5KB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - F-5.pdf (3.7MB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - F-7.pdf (768.6KB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - H-1.pdf (1.5MB, .pdf)
│   │   │   │   │   ├── 02_13_2024 - Meeting Minutes - Public.pdf (363.5KB, .pdf)
│   │   │   │   │   ├── 02_13_2024 - Verbatim Transcripts - F-12.pdf (7.2MB, .pdf)
│   │   │   │   │   ├── 02_13_2024 - Verbatim Transcripts - H-1.pdf (570.8KB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - E-1.pdf (4.2MB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - E-2.pdf (281.3KB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - E-3.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - F-10.pdf (2.5MB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - F-12.pdf (448.2KB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - H-1.pdf (794.2KB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - Public Comment.pdf (439.4KB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - E-1.pdf (124.3KB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - E-2.pdf (487.9KB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - E-4 and E-11.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - F-1.pdf (416.0KB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - F-2.pdf (1.5MB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - F-4 and F-5 and F-11 and F-12.pdf (1.4MB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - H-1.pdf (121.5KB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - K - Discussion Items.pdf (285.0KB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - Public Comment.pdf (412.4KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - 2-1 AND 2-2.pdf (652.3KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - E-11.pdf (365.8KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - E-2.pdf (369.8KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - E-4.pdf (410.2KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - E-5 E-6 E-7 E-8 E-9 E-10.pdf (3.6MB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-1.pdf (751.9KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-10.pdf (1.3MB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-11.pdf (2.4MB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-12.pdf (2.8MB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-15.pdf (255.0KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-5.pdf (205.0KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-8.pdf (961.1KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-9.pdf (330.1KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - Public Comment.pdf (1.5MB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - 2-1.pdf (1.7MB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - E-1.pdf (4.6MB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - E-10.pdf (128.5KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - E-11.pdf (559.6KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - E-12.pdf (156.1KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - E-8.pdf (288.5KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - E-9.pdf (263.4KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - F-1.pdf (561.1KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - F-13.pdf (151.9KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - F-16.pdf (782.8KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - F-3.pdf (352.6KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - F-9.pdf (736.0KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - H-1.pdf (455.3KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - Public Comment.pdf (1.6MB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - D-3.pdf (539.3KB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - E-10.pdf (629.7KB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - E-3.pdf (350.4KB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - F-2.pdf (1.7MB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - F-3.pdf (2.4MB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - F-4.pdf (3.4MB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - F-5.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - F-7 and F-10.pdf (1.6MB, .pdf)
│   │   │   │   │   └── 06_11_2024 - Verbatim Transcripts - Public Comment.pdf (513.4KB, .pdf)
│   │   │   │   ├── [EXCLUDED] 1 items: .DS_Store (excluded file)
│   │   │   ├── [EXCLUDED] 1 items: .DS_Store (excluded file)
│   │   ├── [EXCLUDED] 1 items: .DS_Store (excluded file)
│   ├── graph_json/
│   │   ├── Agenda 01.9.2024_extracted.json (48.1KB, .json)
│   │   ├── Agenda 01.9.2024_ontology.json (19.6KB, .json)
│   │   ├── debug/
│   │   │   ├── Agenda 01.9.2024_full_text.txt (15.7KB, .txt)
│   │   │   ├── agenda_structure_chunk0_llm_response.txt (16.9KB, .txt)
│   │   │   ├── agenda_structure_chunk0_parsed.json (4.6KB, .json)
│   │   │   ├── all_agenda_items.json (4.6KB, .json)
│   │   │   ├── all_ordinance_files.txt (170.0B, .txt)
│   │   │   ├── document_search_info.txt (174.0B, .txt)
│   │   │   ├── entities_llm_response.txt (6.5KB, .txt)
│   │   │   ├── entities_parsed.json (7.7KB, .json)
│   │   │   ├── linked_documents.json (1013.0B, .json)
│   │   │   ├── matched_documents.txt (123.0B, .txt)
│   │   │   ├── meeting_info_llm_response.txt (536.0B, .txt)
│   │   │   └── meeting_info_parsed.json (516.0B, .json)
│   │   ├── pipeline_report_20250602_164208.json (6.6KB, .json)
│   │   ├── pipeline_report_20250602_165519.json (4.6KB, .json)
│   │   └── pipeline_report_20250602_165757.json (6.6KB, .json)
│   ├── [EXCLUDED] 2 items: .DS_Store (excluded file), global (excluded dir)
├── clear_database.py (963.0B, .py)
├── config.py (1.7KB, .py)
├── graph_visualizer.py (23.6KB, .py)
├── repository_directory_structure.txt (5.7KB, .txt)
├── requirements.txt (966.0B, .txt)
├── run_city_clerk_pipeline.sh (440.0B, .sh)
├── run_graph_pipeline.sh (1.3KB, .sh)
├── run_graph_visualizer.sh (3.6KB, .sh)
├── scripts/
│   ├── check_pipeline_setup.py (3.1KB, .py)
│   ├── clear_database.py (6.0KB, .py)
│   ├── debug_pipeline_output.py (1.5KB, .py)
│   ├── find_duplicates.py (5.2KB, .py)
│   ├── graph_pipeline.py (16.3KB, .py)
│   ├── graph_stages/
│   │   ├── __init__.py (525.0B, .py)
│   │   ├── agenda_graph_builder.py (30.7KB, .py)
│   │   ├── agenda_ontology_extractor.py (23.2KB, .py)
│   │   ├── agenda_pdf_extractor.py (6.9KB, .py)
│   │   ├── cosmos_db_client.py (6.8KB, .py)
│   │   └── document_linker.py (10.8KB, .py)
│   ├── pipeline_modular_optimized.py (12.7KB, .py)
│   ├── rag_local_web_app.py (18.4KB, .py)
│   ├── stages/
│   │   ├── __init__.py (378.0B, .py)
│   │   ├── acceleration_utils.py (3.8KB, .py)
│   │   ├── chunk_text.py (18.6KB, .py)
│   │   ├── db_upsert.py (8.9KB, .py)
│   │   ├── embed_vectors.py (27.0KB, .py)
│   │   ├── extract_clean.py (21.7KB, .py)
│   │   └── llm_enrich.py (5.9KB, .py)
│   ├── test_graph_pipeline.py (2.1KB, .py)
│   ├── test_vector_search.py (5.5KB, .py)
│   ├── topic_filter_and_title.py (5.6KB, .py)
│   ├── [EXCLUDED] 1 items: .DS_Store (excluded file)
├── [EXCLUDED] 6 items: .DS_Store (excluded file), .env (excluded file), .git (excluded dir)
    ... and 3 more excluded items


================================================================================


# File Index - Which Files Are in Which Parts
################################################################################

## Part 1 (10 files):
  - scripts/graph_stages/agenda_graph_builder.py
  - scripts/rag_local_web_app.py
  - scripts/graph_stages/document_linker.py
  - city_clerk_documents/graph_json/debug/entities_parsed.json
  - scripts/graph_stages/cosmos_db_client.py
  - scripts/topic_filter_and_title.py
  - scripts/stages/acceleration_utils.py
  - scripts/check_pipeline_setup.py
  - clear_database.py
  - city_clerk_documents/graph_json/debug/meeting_info_parsed.json

## Part 2 (11 files):
  - scripts/stages/embed_vectors.py
  - scripts/stages/chunk_text.py
  - scripts/pipeline_modular_optimized.py
  - scripts/stages/db_upsert.py
  - scripts/stages/llm_enrich.py
  - scripts/find_duplicates.py
  - city_clerk_documents/graph_json/debug/agenda_structure_chunk0_parsed.json
  - config.py
  - scripts/debug_pipeline_output.py
  - scripts/graph_stages/__init__.py
  - scripts/stages/__init__.py

## Part 3 (10 files):
  - scripts/graph_stages/agenda_ontology_extractor.py
  - scripts/stages/extract_clean.py
  - scripts/graph_pipeline.py
  - scripts/graph_stages/agenda_pdf_extractor.py
  - scripts/clear_database.py
  - scripts/test_vector_search.py
  - city_clerk_documents/graph_json/debug/all_agenda_items.json
  - scripts/test_graph_pipeline.py
  - city_clerk_documents/graph_json/debug/linked_documents.json
  - requirements.txt


================================================================================


################################################################################
# File: scripts/graph_stages/agenda_graph_builder.py
################################################################################

# File: scripts/graph_stages/agenda_graph_builder.py

"""
Agenda Graph Builder - FIXED VERSION
Builds graph representation from extracted agenda ontology with proper date handling.
"""
import logging
from typing import Dict, List, Optional
from pathlib import Path
import hashlib
import json
import calendar
import re

from .cosmos_db_client import CosmosGraphClient

log = logging.getLogger('pipeline_debug.graph_builder')


class AgendaGraphBuilder:
    """Build comprehensive graph representation from agenda ontology."""
    
    def __init__(self, cosmos_client: CosmosGraphClient):
        self.cosmos = cosmos_client
        self.entity_id_cache = {}  # Cache for entity IDs
        self.partition_value = 'demo'  # Partition value property
    
    @staticmethod
    def normalize_item_code(code: str) -> str:
        """Normalize item codes to consistent format for matching with ordinances."""
        if not code:
            return code
        
        # Remove trailing dots: "E.-1." -> "E.-1"
        code = code.rstrip('.')
        
        # Remove dots between letter and dash: "E.-1" -> "E-1"
        code = re.sub(r'([A-Z])\.(-)', r'\1\2', code)
        
        # Also handle cases without dash: "E.1" -> "E-1"
        code = re.sub(r'([A-Z])\.(\d)', r'\1-\2', code)
        
        # Ensure we have a dash between letter and number
        code = re.sub(r'([A-Z])(\d)', r'\1-\2', code)
        
        return code
    
    @staticmethod
    def ensure_us_date_format(date_str: str) -> str:
        """Ensure date is in US format MM-DD-YYYY with dashes."""
        # Handle different input formats
        if '.' in date_str:
            # Format: 01.23.2024 -> 01-23-2024
            return date_str.replace('.', '-')
        elif '/' in date_str:
            # Format: 01/23/2024 -> 01-23-2024
            return date_str.replace('/', '-')
        elif re.match(r'^\d{4}-\d{2}-\d{2}$', date_str):
            # ISO format: 2024-01-23 -> 01-23-2024
            parts = date_str.split('-')
            return f"{parts[1]}-{parts[2]}-{parts[0]}"
        else:
            # Already in correct format or unknown
            return date_str
    
    async def build_graph_from_ontology(self, ontology: Dict, agenda_path: Path) -> Dict:
        """Build graph representation from extracted ontology."""
        log.info(f"🔨 Starting graph build for {agenda_path.name}")
        
        try:
            # Store hyperlinks for reference
            hyperlinks = ontology.get('hyperlinks', {})
            
            graph_data = {
                'nodes': {},
                'edges': [],
                'statistics': {
                    'entities': {},
                    'relationships': 0,
                    'hyperlinks': len(hyperlinks)
                }
            }
            
            # CRITICAL: Ensure meeting date is in US format
            meeting_date_original = ontology['meeting_date']
            meeting_date_us = self.ensure_us_date_format(meeting_date_original)
            meeting_info = ontology['meeting_info']
            
            log.info(f"📅 Meeting date: {meeting_date_original} -> {meeting_date_us}")
            
            # 1. Create Meeting node as the root
            meeting_id = f"meeting-{meeting_date_us}"
            await self._create_meeting_node(meeting_date_us, meeting_info, agenda_path.name)
            log.info(f"✅ Created meeting node: {meeting_id}")
            
            # 1.5 Create Date node and link to meeting
            try:
                date_id = await self._create_date_node(meeting_date_original, meeting_id)
                graph_data['nodes'][date_id] = {
                    'type': 'Date',
                    'date': meeting_date_original
                }
            except Exception as e:
                log.error(f"Failed to create date node: {e}")
            
            graph_data['nodes'][meeting_id] = {
                'type': 'Meeting',
                'date': meeting_date_us,
                'info': meeting_info
            }
            
            # 2. Create nodes for officials present
            await self._create_official_nodes(meeting_info.get('officials_present', {}), meeting_id)
            
            # 3. Process agenda structure
            section_count = 0
            item_count = 0
            
            log.info(f"📑 Processing {len(ontology['agenda_structure'])} sections")
            
            for section_idx, section in enumerate(ontology['agenda_structure']):
                try:
                    section_count += 1
                    section_id = f"section-{meeting_date_us}-{section_idx}"
                    
                    # Create AgendaSection node
                    await self._create_section_node(section_id, section, section_idx)
                    log.info(f"✅ Created section {section_idx}: {section.get('section_name', 'Unknown')}")
                    
                    graph_data['nodes'][section_id] = {
                        'type': 'AgendaSection',
                        'name': section['section_name'],
                        'order': section_idx
                    }
                    
                    # Link section to meeting
                    await self.cosmos.create_edge(
                        from_id=meeting_id,
                        to_id=section_id,
                        edge_type='HAS_SECTION',
                        properties={'order': section_idx}
                    )
                    
                    # Process items in section
                    previous_item_id = None
                    items = section.get('items', [])
                    
                    for item_idx, item in enumerate(items):
                        try:
                            if not item.get('item_code'):
                                log.warning(f"Skipping item without code in section {section['section_name']}")
                                continue
                                
                            item_count += 1
                            # Normalize the item code
                            normalized_code = self.normalize_item_code(item['item_code'])
                            # Use US date format for item ID
                            item_id = f"item-{meeting_date_us}-{normalized_code}"
                            
                            log.info(f"Creating item: {item_id} (from code: {item['item_code']})")
                            
                            # Create AgendaItem node
                            await self._create_agenda_item_node(item_id, item, section.get('section_type', 'Unknown'))
                            
                            graph_data['nodes'][item_id] = {
                                'type': 'AgendaItem',
                                'code': normalized_code,
                                'original_code': item['item_code'],
                                'title': item.get('title', 'Unknown')
                            }
                            
                            # Link item to section
                            await self.cosmos.create_edge(
                                from_id=section_id,
                                to_id=item_id,
                                edge_type='CONTAINS_ITEM',
                                properties={'order': item_idx}
                            )
                            
                            # Create sequential relationships
                            if previous_item_id:
                                await self.cosmos.create_edge(
                                    from_id=previous_item_id,
                                    to_id=item_id,
                                    edge_type='FOLLOWS',
                                    properties={'sequence': item_idx}
                                )
                            
                            previous_item_id = item_id
                            
                            # Create sponsor relationship if exists
                            if item.get('sponsor'):
                                await self._create_sponsor_relationship(item_id, item['sponsor'])
                            
                            # Create department relationship if exists
                            if item.get('department'):
                                await self._create_department_relationship(item_id, item['department'])
                                
                        except Exception as e:
                            log.error(f"Failed to process item {item.get('item_code', 'unknown')}: {e}")
                            
                except Exception as e:
                    log.error(f"Failed to process section {section.get('section_name', 'unknown')}: {e}")
            
            # 4. Create entity nodes
            entity_count = await self._create_entity_nodes(ontology['entities'], meeting_id)
            
            # 5. Create relationships
            relationship_count = 0
            for rel in ontology['relationships']:
                try:
                    await self._create_item_relationship(rel, meeting_date_us)
                    relationship_count += 1
                except Exception as e:
                    log.error(f"Failed to create relationship: {e}")
            
            # Update statistics
            graph_data['statistics'] = {
                'sections': section_count,
                'items': item_count,
                'entities': entity_count,
                'relationships': relationship_count,
                'meeting_date': meeting_date_us
            }
            
            log.info(f"🎉 Graph build complete for {agenda_path.name}")
            log.info(f"   - Sections: {section_count}")
            log.info(f"   - Items: {item_count}")
            log.info(f"   - Entities: {entity_count}")
            log.info(f"   - Relationships: {relationship_count}")
            
            return graph_data
            
        except Exception as e:
            log.error(f"CRITICAL ERROR in build_graph_from_ontology: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    async def _create_meeting_node(self, meeting_date: str, meeting_info: Dict, source_file: str = None) -> str:
        """Create Meeting node with comprehensive metadata."""
        meeting_id = f"meeting-{meeting_date}"
        
        # Check if already exists
        if await self.cosmos.vertex_exists(meeting_id):
            log.info(f"Meeting {meeting_id} already exists")
            return meeting_id
        
        location = meeting_info.get('location', {})
        if isinstance(location, dict):
            location_str = f"{location.get('name', '')} - {location.get('address', '')}"
        else:
            location_str = "405 Biltmore Way, Coral Gables, FL"
        
        properties = {
            'nodeType': 'Meeting',
            'date': meeting_date,
            'type': meeting_info.get('meeting_type', 'Regular Meeting'),
            'time': meeting_info.get('meeting_time', ''),
            'location': location_str
        }
        
        if source_file:
            properties['source_file'] = source_file
        
        await self.cosmos.create_vertex('Meeting', meeting_id, properties)
        log.info(f"✅ Created Meeting node: {meeting_id}")
        return meeting_id
    
    async def _create_date_node(self, date_str: str, meeting_id: str) -> str:
        """Create a Date node and link it to the meeting."""
        from datetime import datetime
        
        # Parse date from MM.DD.YYYY format
        parts = date_str.split('.')
        if len(parts) != 3:
            log.error(f"Invalid date format: {date_str}")
            return None
            
        month, day, year = int(parts[0]), int(parts[1]), int(parts[2])
        
        # Create consistent date ID in ISO format
        date_id = f"date-{year:04d}-{month:02d}-{day:02d}"
        
        # Check if date already exists
        if await self.cosmos.vertex_exists(date_id):
            log.info(f"Date {date_id} already exists")
            # Still create the relationship
            await self.cosmos.create_edge(
                from_id=meeting_id,
                to_id=date_id,
                edge_type='OCCURRED_ON',
                properties={'primary_date': True}
            )
            return date_id
        
        # Get day of week
        date_obj = datetime(year, month, day)
        day_of_week = date_obj.strftime('%A')
        
        # Create date node
        properties = {
            'nodeType': 'Date',
            'full_date': date_str,
            'year': year,
            'month': month,
            'day': day,
            'quarter': (month - 1) // 3 + 1,
            'month_name': calendar.month_name[month],
            'day_of_week': day_of_week,
            'iso_date': f'{year:04d}-{month:02d}-{day:02d}'
        }
        
        await self.cosmos.create_vertex('Date', date_id, properties)
        log.info(f"✅ Created Date node: {date_id}")
        
        # Create relationship: Meeting -> OCCURRED_ON -> Date
        await self.cosmos.create_edge(
            from_id=meeting_id,
            to_id=date_id,
            edge_type='OCCURRED_ON',
            properties={'primary_date': True}
        )
        
        return date_id
    
    async def _create_section_node(self, section_id: str, section: Dict, order: int) -> str:
        """Create AgendaSection node."""
        # Check if already exists
        if await self.cosmos.vertex_exists(section_id):
            log.info(f"Section {section_id} already exists, skipping creation")
            return section_id
        
        properties = {
            'title': section.get('section_name', 'Unknown'),
            'type': section.get('section_type', 'OTHER'),
            'order': order
        }
        
        # Add page range if available
        if 'page_start' in section:
            properties['page_start'] = section.get('page_start', 1)
        if 'page_end' in section:
            properties['page_end'] = section.get('page_end', section.get('page_start', 1))
        
        await self.cosmos.create_vertex('AgendaSection', section_id, properties)
        return section_id
    
    async def _create_agenda_item_node(self, item_id: str, item: Dict, section_type: str) -> str:
        """Create AgendaItem node with all metadata."""
        # Store both original and normalized codes
        original_code = item.get('item_code', '')
        normalized_code = self.normalize_item_code(original_code)
        
        properties = {
            'code': normalized_code,
            'original_code': original_code,
            'title': item.get('title', 'Unknown'),
            'type': item.get('item_type', 'Item'),
            'section_type': section_type
        }
        
        # Add page range if available
        if 'page_start' in item:
            properties['page_start'] = item['page_start']
        if 'page_end' in item:
            properties['page_end'] = item['page_end']
        
        # Add summary if available
        if item.get('summary'):
            properties['summary'] = item['summary'][:500]
        
        # Add document reference and URL if available
        if item.get('document_reference'):
            properties['document_reference'] = item['document_reference']
        
        if item.get('document_url'):
            properties['document_url'] = item['document_url']
            properties['has_hyperlink'] = True
        
        # Add all hyperlinks as JSON if multiple exist
        if item.get('hyperlinks') and len(item['hyperlinks']) > 0:
            properties['hyperlinks_json'] = json.dumps(item['hyperlinks'])
        
        await self.cosmos.create_vertex('AgendaItem', item_id, properties)
        return item_id
    
    async def _create_official_nodes(self, officials: Dict, meeting_id: str):
        """Create nodes for city officials and link to meeting."""
        if not officials:
            return
            
        roles_mapping = {
            'mayor': 'Mayor',
            'vice_mayor': 'Vice Mayor',
            'city_attorney': 'City Attorney',
            'city_manager': 'City Manager',
            'city_clerk': 'City Clerk'
        }
        
        # Process standard officials
        for key, role in roles_mapping.items():
            if officials.get(key) and officials[key] != 'null':
                person_id = await self._ensure_person_node(officials[key], role)
                await self.cosmos.create_edge(
                    from_id=person_id,
                    to_id=meeting_id,
                    edge_type='ATTENDED',
                    properties={'role': role}
                )
        
        # Process commissioners
        commissioners = officials.get('commissioners', [])
        if isinstance(commissioners, list):
            for idx, commissioner in enumerate(commissioners):
                if commissioner and commissioner != 'null':
                    person_id = await self._ensure_person_node(commissioner, 'Commissioner')
                    await self.cosmos.create_edge(
                        from_id=person_id,
                        to_id=meeting_id,
                        edge_type='ATTENDED',
                        properties={'role': 'Commissioner', 'seat': idx + 1}
                    )
    
    async def _create_entity_nodes(self, entities: Dict[str, List[Dict]], meeting_id: str) -> int:
        """Create nodes for all extracted entities."""
        entity_counts = 0
        
        # Create Person nodes
        for person in entities.get('people', []):
            if person.get('name'):
                person_id = await self._ensure_person_node(person['name'], person.get('role', 'Participant'))
                # Link to meeting if they're mentioned
                await self.cosmos.create_edge(
                    from_id=person_id,
                    to_id=meeting_id,
                    edge_type='MENTIONED_IN',
                    properties={'context': person.get('context', '')[:100]}
                )
                entity_counts += 1
        
        # Create Organization nodes
        for org in entities.get('organizations', []):
            if org.get('name'):
                org_id = await self._ensure_organization_node(org['name'], org.get('type', 'Organization'))
                await self.cosmos.create_edge(
                    from_id=org_id,
                    to_id=meeting_id,
                    edge_type='MENTIONED_IN',
                    properties={'context': org.get('context', '')[:100]}
                )
                entity_counts += 1
        
        # Create Location nodes
        for location in entities.get('locations', []):
            if location.get('name'):
                loc_id = await self._ensure_location_node(
                    location['name'], 
                    location.get('address', ''),
                    location.get('type', 'Location')
                )
                await self.cosmos.create_edge(
                    from_id=loc_id,
                    to_id=meeting_id,
                    edge_type='REFERENCED_IN',
                    properties={'context': location.get('context', '')[:100]}
                )
                entity_counts += 1
        
        # Create FinancialItem nodes
        for amount in entities.get('monetary_amounts', []):
            if amount.get('amount'):
                fin_id = await self._create_financial_node(
                    amount['amount'],
                    amount.get('purpose', ''),
                    meeting_id
                )
                entity_counts += 1
        
        return entity_counts
    
    async def _ensure_person_node(self, name: str, role: str) -> str:
        """Create or retrieve person node."""
        clean_name = name.strip()
        # Clean the ID by removing invalid characters
        cleaned_id_part = clean_name.lower().replace(' ', '-').replace('.', '').replace("'", '').replace('"', '').replace('/', '-')
        person_id = f"person-{cleaned_id_part}"
        
        # Check cache first
        if person_id in self.entity_id_cache:
            return person_id
        
        # Check if exists in database
        if await self.cosmos.vertex_exists(person_id):
            self.entity_id_cache[person_id] = True
            return person_id
        
        # Create new person
        properties = {
            'name': clean_name,
            'roles': role
        }
        
        await self.cosmos.create_vertex('Person', person_id, properties)
        self.entity_id_cache[person_id] = True
        return person_id
    
    async def _ensure_organization_node(self, name: str, org_type: str) -> str:
        """Create or retrieve organization node."""
        # Clean the ID
        cleaned_org_name = name.lower().replace(' ', '-').replace('.', '').replace("'", '').replace('"', '').replace('/', '-').replace(',', '')
        org_id = f"org-{cleaned_org_name}"
        
        if org_id in self.entity_id_cache:
            return org_id
        
        if await self.cosmos.vertex_exists(org_id):
            self.entity_id_cache[org_id] = True
            return org_id
        
        properties = {
            'name': name,
            'type': org_type
        }
        
        await self.cosmos.create_vertex('Organization', org_id, properties)
        self.entity_id_cache[org_id] = True
        return org_id
    
    async def _ensure_location_node(self, name: str, address: str, loc_type: str) -> str:
        """Create or retrieve location node."""
        # Clean the ID
        cleaned_loc_name = name.lower().replace(' ', '-').replace('.', '').replace("'", '').replace('"', '').replace('/', '-').replace(',', '')
        loc_id = f"location-{cleaned_loc_name}"
        
        if loc_id in self.entity_id_cache:
            return loc_id
        
        if await self.cosmos.vertex_exists(loc_id):
            self.entity_id_cache[loc_id] = True
            return loc_id
        
        properties = {
            'name': name,
            'address': address,
            'type': loc_type
        }
        
        await self.cosmos.create_vertex('Location', loc_id, properties)
        self.entity_id_cache[loc_id] = True
        return loc_id
    
    async def _create_financial_node(self, amount: str, purpose: str, meeting_id: str) -> str:
        """Create financial item node."""
        fin_id = f"financial-{hashlib.md5(f'{amount}-{purpose}'.encode()).hexdigest()[:8]}"
        
        properties = {
            'amount': amount,
            'purpose': purpose
        }
        
        await self.cosmos.create_vertex('FinancialItem', fin_id, properties)
        
        # Link to meeting
        await self.cosmos.create_edge(
            from_id=fin_id,
            to_id=meeting_id,
            edge_type='DISCUSSED_IN'
        )
        
        return fin_id
    
    async def _create_sponsor_relationship(self, item_id: str, sponsor_name: str):
        """Create sponsorship relationship."""
        person_id = await self._ensure_person_node(sponsor_name, 'Sponsor')
        await self.cosmos.create_edge(
            from_id=person_id,
            to_id=item_id,
            edge_type='SPONSORS',
            properties={'role': 'sponsor'}
        )
    
    async def _create_department_relationship(self, item_id: str, department_name: str):
        """Create department origination relationship."""
        dept_id = await self._ensure_organization_node(department_name, 'Department')
        await self.cosmos.create_edge(
            from_id=dept_id,
            to_id=item_id,
            edge_type='ORIGINATES',
            properties={'role': 'originating_department'}
        )
    
    async def _create_item_relationship(self, rel: Dict, meeting_date: str):
        """Create relationship between agenda items."""
        from_code = self.normalize_item_code(rel['from_code'])
        to_code = self.normalize_item_code(rel['to_code'])
        
        from_id = f"item-{meeting_date}-{from_code}"
        to_id = f"item-{meeting_date}-{to_code}"
        
        # Check if both items exist
        try:
            from_exists = await self.cosmos.vertex_exists(from_id)
            to_exists = await self.cosmos.vertex_exists(to_id)
            
            if from_exists and to_exists:
                await self.cosmos.create_edge(
                    from_id=from_id,
                    to_id=to_id,
                    edge_type=rel['relationship_type'],
                    properties={
                        'description': rel.get('description', ''),
                        'strength': rel.get('strength', 'medium')
                    }
                )
        except Exception as e:
            log.warning(f"Could not create relationship {from_id} -> {to_id}: {e}")
    
    async def process_linked_documents(self, linked_docs: Dict, meeting_id: str, meeting_date: str):
        """Process and create nodes for linked documents with FIXED date format."""
        # CRITICAL FIX: Extract US date format from meeting_id
        # meeting_id is ALWAYS "meeting-01-23-2024" format
        date_from_meeting_id = meeting_id.replace("meeting-", "")  # "01-23-2024"
        
        log.info(f"\n🔗 Processing linked documents")
        log.info(f"   Meeting ID: {meeting_id}")
        log.info(f"   Date format for items: {date_from_meeting_id}")
        
        missing_items = []
        created_count = 0
        
        for doc_type, documents in linked_docs.items():
            log.info(f"\n📄 Processing {len(documents)} {doc_type}")
            
            for doc in documents:
                if doc_type in ['ordinances', 'resolutions']:
                    log.info(f"\n   Processing {doc_type[:-1]} {doc.get('document_number', 'unknown')}")
                    log.info(f"      Item code: {doc.get('item_code', 'MISSING')}")
                    
                    # Create document node
                    doc_id = await self._create_document_node(doc, doc_type, meeting_date)
                    
                    if doc_id:
                        created_count += 1
                        log.info(f"      ✅ Created document node: {doc_id}")
                        
                        # Link to meeting
                        await self.cosmos.create_edge(
                            from_id=doc_id,
                            to_id=meeting_id,
                            edge_type='PRESENTED_AT',
                            properties={'date': meeting_date}
                        )
                        
                        # Check if agenda item exists
                        if doc.get('item_code'):
                            # Normalize the item code
                            normalized_code = self.normalize_item_code(doc['item_code'])
                            
                            # USE THE FIXED DATE FORMAT
                            item_id = f"item-{date_from_meeting_id}-{normalized_code}"
                            
                            log.info(f"      Looking for item: {item_id}")
                            
                            # Check if item exists
                            item_exists = await self.cosmos.vertex_exists(item_id)
                            
                            if not item_exists:
                                log.warning(f"      ❌ Agenda item NOT FOUND: {item_id}")
                                
                                missing_item_info = {
                                    'item_code': normalized_code,
                                    'original_code': doc['item_code'],
                                    'document_number': doc.get('document_number'),
                                    'document_type': doc_type,
                                    'title': doc.get('title', 'Unknown'),
                                    'expected_item_id': item_id
                                }
                                missing_items.append(missing_item_info)
                            else:
                                log.info(f"      ✅ Agenda item found, creating link")
                                # Item exists, create the edge
                                await self.cosmos.create_edge(
                                    from_id=item_id,
                                    to_id=doc_id,
                                    edge_type='REFERENCES_DOCUMENT',
                                    properties={'document_type': doc_type}
                                )
                                log.info(f"      ✅ Linked agenda item to document")
        
        log.info(f"\n📊 Document processing complete")
        log.info(f"   Documents created: {created_count}")
        log.info(f"   Missing items: {len(missing_items)}")
        
        return missing_items

    async def _create_document_node(self, doc_info: Dict, doc_type: str, meeting_date: str) -> str:
        """Create an Ordinance or Resolution node."""
        doc_number = doc_info.get('document_number', 'unknown')
        doc_id = f"{doc_type[:-1]}-{doc_number}"  # Remove 's' from type
        
        # Get full title without truncation
        title = doc_info.get('title', '')
        if not title and doc_info.get('parsed_data', {}).get('title'):
            title = doc_info['parsed_data']['title']
        
        if title is None:
            title = f"Untitled {doc_type.capitalize()} {doc_number}"
            log.warning(f"No title found for {doc_type} {doc_number}, using default")
        
        properties = {
            'document_number': doc_number,
            'full_title': title,
            'title': title[:200] if len(title) > 200 else title,
            'document_type': doc_type[:-1],  # Remove 's'
            'meeting_date': meeting_date
        }
        
        # Add parsed metadata
        parsed_data = doc_info.get('parsed_data', {})
        
        if parsed_data.get('date_passed'):
            properties['date_passed'] = parsed_data['date_passed']
        
        if parsed_data.get('agenda_item'):
            properties['agenda_item'] = parsed_data['agenda_item']
        
        # Add vote details as JSON
        if parsed_data.get('vote_details'):
            properties['vote_details'] = json.dumps(parsed_data['vote_details'])
        
        # Add signatories
        if parsed_data.get('signatories', {}).get('mayor'):
            properties['mayor_signature'] = parsed_data['signatories']['mayor']
        
        await self.cosmos.create_vertex(doc_type[:-1].capitalize(), doc_id, properties)
        
        # Create edges for sponsors
        if parsed_data.get('motion', {}).get('moved_by'):
            person_id = await self._ensure_person_node(
                parsed_data['motion']['moved_by'], 
                'Commissioner'
            )
            await self.cosmos.create_edge(
                from_id=person_id,
                to_id=doc_id,
                edge_type='MOVED',
                properties={'role': 'mover'}
            )
        
        return doc_id


================================================================================


################################################################################
# File: scripts/rag_local_web_app.py
################################################################################

# File: scripts/rag_local_web_app.py

#!/usr/bin/env python3
################################################################################
################################################################################
"""
Mini Flask app that answers misophonia questions with Retrieval‑Augmented
Generation (gpt-4.1-mini-2025-04-14 + Supabase pgvector).

### Patch 2  (2025‑05‑06)
• **Embeddings** now created with **text‑embedding‑ada‑002** (1536‑D).  
• Similarity is re‑computed client‑side with a **plain cosine function** so the
  ranking no longer depends on pgvector's built‑in distance or any RPC
  threshold quirks.

The rest of the grounded‑answer logic (added in Patch 1) is unchanged.
"""
from __future__ import annotations

import logging
import math
import os
import re
from pathlib import Path        # (unused but left in to mirror original)
from typing import Dict, List
import json

from dotenv import load_dotenv
from flask import Flask, jsonify, request, make_response
from openai import OpenAI
from supabase import create_client
from flask_compress import Compress
from flask_cors import CORS

# ────────────────────────────── configuration ───────────────────────────── #

load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
SUPABASE_URL   = os.getenv("SUPABASE_URL")
SUPABASE_KEY   = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
PORT           = int(os.getenv("PORT", 8080))

if not (OPENAI_API_KEY and SUPABASE_URL and SUPABASE_KEY):
    raise SystemExit(
        "❌  Required env vars: OPENAI_API_KEY, SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY"
    )

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s — %(levelname)s — %(message)s",
)
log = logging.getLogger("rag_app")

sb = create_client(SUPABASE_URL, SUPABASE_KEY)
oa = OpenAI(api_key=OPENAI_API_KEY)

app = Flask(__name__)
CORS(app)
app.config['COMPRESS_ALGORITHM'] = 'gzip'
Compress(app)

# ────────────────────────────── helper functions ────────────────────────── #


def embed(text: str) -> List[float]:
    """
    Return OpenAI embedding vector for *text* using text‑embedding‑ada‑002.

    ada‑002 has 1536 dimensions and is inexpensive yet solid for similarity.
    """
    resp = oa.embeddings.create(
        model="text-embedding-ada-002",
        input=text[:8192],  # safety slice
    )
    return resp.data[0].embedding


def cosine_similarity(a: List[float], b: List[float]) -> float:
    """Plain cosine similarity between two equal‑length vectors."""
    dot = sum(x * y for x, y in zip(a, b))
    na = math.sqrt(sum(x * x for x in a))
    nb = math.sqrt(sum(y * y for y in b))
    return dot / (na * nb + 1e-9)


# Add in-memory embedding cache
_qcache = {}
def embed_cached(text):
    if text in _qcache: return _qcache[text]
    vec = embed(text)
    _qcache[text] = vec
    return vec


# Add regex patterns for bibliography detection
_DOI_RE   = re.compile(r'\b10\.\d{4,9}/[-._;()/:A-Z0-9]+\b', re.I)
_YEAR_RE  = re.compile(r'\b(19|20)\d{2}\b')

def looks_like_refs(text: str) -> bool:
    """
    Return True if this chunk is likely just a bibliography list:
      • more than 12 DOIs, or
      • more than 15 year mentions.
    """
    doi_count  = len(_DOI_RE.findall(text))
    year_count = len(_YEAR_RE.findall(text))
    return doi_count > 12 or year_count > 15


def semantic_search(
    query: str,
    *,
    limit: int = 8,
    threshold: float = 0.0,
) -> List[Dict]:
    """
    Retrieve candidate chunks via the pgvector RPC, then re-rank with an
    **explicit cosine similarity** so the final score is always in
    **[-100 … +100] percent**.

    Why the extra work?
    -------------------
    •  The SQL function returns a raw inner-product that can be > 1.  
       (embeddings are *not* unit-length.)  
    •  By pulling the real 1 536-D vectors and re-computing a cosine we get a
       true, bounded similarity that front-end code can safely show.

    The -100 … +100 range is produced by:  
        pct = clamp(cosine × 100, -100, 100)
    """
    # 1. Embed the query once and keep it cached
    q_vec = embed_cached(query)

    # 2. Fast ANN search in Postgres (over-fetch 4× so we can re-rank)
    rows = (
        sb.rpc(
            "match_documents_chunks",
            {
                "query_embedding": q_vec,
                "match_threshold": threshold,
                "match_count": limit * 4,
            },
        )
        .execute()
        .data
    ) or []

    # 3. Filter out bibliography-only chunks
    rows = [r for r in rows if not looks_like_refs(r["text"])]

    if not rows:
        return []

    # 4. Fetch document metadata (title, authors …) in one round-trip
    doc_ids = {r["document_id"] for r in rows}
    meta = {
        d["id"]: d
        for d in (
            sb.table("city_clerk_documents")
              .select("id,document_type,title,date,year,month,day,mayor,vice_mayor,commissioners,city_attorney,city_manager,city_clerk,public_works_director,agenda,keywords,source_pdf")
              .in_("id", list(doc_ids))
              .execute()
              .data
            or []
        )
    }

    # 5. Pull embeddings and page info once and compute **plain cosine** (no scaling)
    chunk_ids = [r["id"] for r in rows]

    emb_rows = (
        sb.table("documents_chunks")
          .select("id, embedding, page_start, page_end")
          .in_("id", chunk_ids)
          .execute()
          .data
    ) or []

    emb_map: Dict[str, List[float]] = {}
    page_map: Dict[str, Dict] = {}
    for e in emb_rows:
        raw = e["embedding"]
        if isinstance(raw, list):                    # list[Decimal]
            emb_map[e["id"]] = [float(x) for x in raw]
        elif isinstance(raw, str) and raw.startswith('['):   # TEXT  "[…]"
            emb_map[e["id"]] = [float(x) for x in raw.strip('[]').split(',')]
        
        # Store page info
        page_map[e["id"]] = {
            "page_start": e.get("page_start", 1),
            "page_end": e.get("page_end", 1)
        }

    for r in rows:
        vec = emb_map.get(r["id"])
        if vec:                                     # we now have the real vector
            cos = cosine_similarity(q_vec, vec)
            r["similarity"] = round(cos * 100, 1)   # –100…+100 % (or 0…100 %)
        else:                                       # fallback if something failed
            dist = float(r.get("similarity", 1.0))  # 0…2 cosine-distance
            r["similarity"] = round((1.0 - dist) * 100, 1)

        r["doc"] = meta.get(r["document_id"], {})
        
        # Add page info to the row
        page_info = page_map.get(r["id"], {"page_start": 1, "page_end": 1})
        r["page_start"] = page_info["page_start"]
        r["page_end"] = page_info["page_end"]

    # 6. Keep the top *limit* rows after proper re-ranking
    ranked = sorted(rows, key=lambda x: x["similarity"], reverse=True)[:limit]
    return ranked


# ──────────────────────── NEW RAG‑PROMPT HELPERS ───────────────────────── #

MAX_PROMPT_CHARS: int = 24_000  # ~6 k tokens @ 4 chars/token heuristic


def trim_chunks(chunks: List[Dict]) -> List[Dict]:
    """
    Fail‑safe guard: ensure concatenated chunk texts remain under the
    MAX_PROMPT_CHARS budget.  Keeps highest‑similarity chunks first.
    """
    sorted_chunks = sorted(chunks, key=lambda c: c.get("similarity", 0), reverse=True)
    output: List[Dict] = []
    total_chars = 0
    for c in sorted_chunks:
        chunk_len = len(c["text"])
        if total_chars + chunk_len > MAX_PROMPT_CHARS:
            break
        output.append(c)
        total_chars += chunk_len
    return output


def build_prompt(question: str, chunks: List[Dict]) -> str:
    """
    Build a structured prompt that asks GPT to:
      • answer in Markdown with short intro + numbered list of key points
      • cite inline like [1], [2] …
      • finish with a Bibliography that includes the document title and type
    """
    snippet_lines, biblio_lines = [], []
    for i, c in enumerate(chunks, 1):
        page_start = c.get('page_start', 1)
        page_end = c.get('page_end', 1)
        snippet_lines.append(
            f"[{i}] \"{c['text'].strip()}\" "
            f"(pp. {page_start}-{page_end})"
        )

        d = c["doc"]
        title = d.get("title", "Untitled Document")
        doc_type = d.get("document_type", "Document")
        date = d.get("date", "Unknown date")
        year = d.get("year", "n.d.")
        pages = f"pp. {page_start}-{page_end}"
        source_pdf = d.get("source_pdf", "")

        # City clerk document bibliography format
        biblio_lines.append(
            f"[{i}] *{title}* · {doc_type} · {date} · {pages}"
        )

    prompt_parts = [
        "You are City Clerk Assistant, a knowledgeable AI that helps with questions about city government documents, including resolutions, ordinances, proclamations, contracts, meeting minutes, and agendas.",
        "You draw on evidence from official city documents and municipal records.",
        "Your responses are clear, professional, and grounded in the provided context.",
        "====",
        "QUESTION:",
        question,
        "====",
        "CONTEXT:",
        *snippet_lines,
        "====",
        "INSTRUCTIONS:",
        "• Write your answer in **Markdown**.",
        "• Begin with a concise summary (2–3 sentences).",
        "• Then elaborate on key points using well-structured paragraphs.",
        "• Provide relevant insights about city governance, policies, or procedures.",
        "• If helpful, use lists, subheadings, or clear explanations to enhance understanding.",
        "• Use a professional and informative tone.",
        "• Cite sources inline like [1], [2] etc.",
        "• After the answer, include a 'BIBLIOGRAPHY:' section that lists each source exactly as provided below.",
        "• If none of the context answers the question, reply: \"I'm sorry, I don't have sufficient information to answer that.\"",
        "====",
        "BEGIN OUTPUT",
        "ANSWER:",
        "",  # where the model writes the main response
        "BIBLIOGRAPHY:",
        *biblio_lines,
    ]

    return '\n'.join(prompt_parts)


def extract_citations(answer: str) -> List[str]:
    """
    Parse numeric citations (e.g., "[1]", "[2]") from the answer text.
    Returns unique citation numbers in ascending order.
    """
    citations = re.findall(r"\[(\d+)\]", answer)
    return sorted(set(citations), key=int)


# ──────────────────────────────── routes ────────────────────────────────── #

@app.route("/")
def home():
    """Simple homepage for the City Clerk RAG application."""
    html = """
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>City Clerk RAG Assistant</title>
        <style>
            body { 
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
                max-width: 800px; 
                margin: 0 auto; 
                padding: 2rem;
                line-height: 1.6;
                color: #333;
            }
            .header { 
                text-align: center; 
                margin-bottom: 2rem;
                padding-bottom: 1rem;
                border-bottom: 2px solid #e0e0e0;
            }
            .search-container {
                background: #f8f9fa;
                padding: 2rem;
                border-radius: 8px;
                margin: 2rem 0;
            }
            .search-box {
                width: 100%;
                padding: 1rem;
                border: 2px solid #ddd;
                border-radius: 4px;
                font-size: 16px;
                margin-bottom: 1rem;
            }
            .search-btn {
                background: #007bff;
                color: white;
                padding: 1rem 2rem;
                border: none;
                border-radius: 4px;
                cursor: pointer;
                font-size: 16px;
            }
            .search-btn:hover { background: #0056b3; }
            .results { margin-top: 2rem; }
            .answer { 
                background: white; 
                padding: 1.5rem; 
                border-radius: 8px; 
                border-left: 4px solid #007bff;
                margin: 1rem 0;
            }
            .sources { 
                background: #f8f9fa; 
                padding: 1rem; 
                border-radius: 4px; 
                margin-top: 1rem;
                font-size: 0.9em;
            }
            .loading { color: #666; font-style: italic; }
            .error { color: #dc3545; background: #f8d7da; padding: 1rem; border-radius: 4px; }
        </style>
    </head>
    <body>
        <div class="header">
            <h1>🏛️ City Clerk RAG Assistant</h1>
            <p>Ask questions about city government documents, resolutions, ordinances, and meeting minutes</p>
        </div>
        
        <div class="search-container">
            <input type="text" id="queryInput" class="search-box" 
                   placeholder="Ask a question about city documents..." 
                   onkeypress="if(event.key==='Enter') search()">
            <button onclick="search()" class="search-btn">Search</button>
        </div>
        
        <div id="results" class="results"></div>
        
        <script>
            async function search() {
                const query = document.getElementById('queryInput').value.trim();
                if (!query) return;
                
                const resultsDiv = document.getElementById('results');
                resultsDiv.innerHTML = '<div class="loading">Searching...</div>';
                
                try {
                    const response = await fetch('/search', {
                        method: 'POST',
                        headers: { 'Content-Type': 'application/json' },
                        body: JSON.stringify({ query: query })
                    });
                    
                    const data = await response.json();
                    
                    if (data.error) {
                        resultsDiv.innerHTML = `<div class="error">Error: ${data.error}</div>`;
                        return;
                    }
                    
                    let html = `<div class="answer">${data.answer.replace(/\\n/g, '<br>')}</div>`;
                    
                    if (data.results && data.results.length > 0) {
                        html += '<div class="sources"><strong>Sources:</strong><ul>';
                        data.results.forEach((result, i) => {
                            const doc = result.doc || {};
                            const title = doc.title || 'Untitled Document';
                            const similarity = Math.round(result.similarity || 0);
                            html += `<li>${title} (${similarity}% match)</li>`;
                        });
                        html += '</ul></div>';
                    }
                    
                    resultsDiv.innerHTML = html;
                } catch (error) {
                    resultsDiv.innerHTML = `<div class="error">Error: ${error.message}</div>`;
                }
            }
        </script>
    </body>
    </html>
    """
    return html

@app.post("/search")
def search():
    payload = request.get_json(force=True, silent=True) or {}
    question = (payload.get("query") or "").strip()
    if not question:
        return jsonify({"error": "Missing 'query'"}), 400

    try:
        # Retrieve semantic matches (client‑side cosine re‑ranked)
        raw_matches = semantic_search(question, limit=int(payload.get("limit", 8)))

        if not raw_matches:
            return jsonify(
                {
                    "answer": "I'm sorry, I don't have sufficient information to answer that.",
                    "citations": [],
                    "results": [],
                }
            )

        # ──────────────────── TRIM CHUNKS TO BUDGET ──────────────────── #
        chunks = trim_chunks(raw_matches)

        # ──────────────────── BUILD PROMPT & CALL LLM ─────────────────── #
        prompt = build_prompt(question, chunks)

        completion = oa.chat.completions.create(
            model="gpt-4.1-mini-2025-04-14",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0,
        )
        answer_text: str = completion.choices[0].message.content.strip()

        # ──────────────────── EXTRACT CITATIONS ──────────────────────── #
        citations = extract_citations(answer_text)

        # Remove embedding vectors before sending back to the browser
        for m in raw_matches:
            m.pop("embedding", None)

        # ──────────────────── RETURN JSON ─────────────────────────────── #
        response = jsonify(
            {
                "answer": answer_text,
                "citations": citations,
                "results": raw_matches,
            }
        )
        response.headers['Connection'] = 'keep-alive'
        return response
    except Exception as exc:  # noqa: BLE001
        log.exception("search failed")
        return jsonify({"error": str(exc)}), 500


@app.get("/stats")
def stats():
    """Tiny ops endpoint—count total chunks."""
    resp = sb.table("documents_chunks").select("id", count="exact").execute()
    return jsonify({"total_chunks": resp.count})


# ──────────────────────────────── main ─────────────────────────────────── #

if __name__ == "__main__":
    log.info("Starting Flask on 0.0.0.0:%s …", PORT)
    app.run(host="0.0.0.0", port=PORT, debug=True)


================================================================================


################################################################################
# File: scripts/graph_stages/document_linker.py
################################################################################

# File: scripts/graph_stages/document_linker.py

"""
Document Linker
Links ordinance documents to their corresponding agenda items.
"""

import logging
import re
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import json
from datetime import datetime
import PyPDF2
from groq import Groq
import os
from dotenv import load_dotenv

load_dotenv()

log = logging.getLogger('document_linker')


class DocumentLinker:
    """Links ordinance and resolution documents to agenda items."""
    
    def __init__(self,
                 groq_api_key: Optional[str] = None,
                 model: str = "llama-3.3-70b-versatile"):
        """Initialize the document linker."""
        self.api_key = groq_api_key or os.getenv("GROQ_API_KEY")
        if not self.api_key:
            raise ValueError("GROQ_API_KEY not found in environment")
        
        self.client = Groq(api_key=self.api_key)
        self.model = model
    
    async def link_documents_for_meeting(self, 
                                       meeting_date: str,
                                       documents_dir: Path) -> Dict[str, List[Dict]]:
        """Find and link all documents for a specific meeting date."""
        log.info(f"🔗 Linking documents for meeting date: {meeting_date}")
        log.info(f"📁 Looking for ordinances in: {documents_dir}")
        
        # Create debug directory
        debug_dir = Path("city_clerk_documents/graph_json/debug")
        debug_dir.mkdir(exist_ok=True)
        
        # Convert date format: "01.09.2024" -> "01_09_2024"
        date_underscore = meeting_date.replace(".", "_")
        
        # Log what we're searching for
        with open(debug_dir / "document_search_info.txt", 'w') as f:
            f.write(f"Meeting Date: {meeting_date}\n")
            f.write(f"Date with underscores: {date_underscore}\n")
            f.write(f"Search directory: {documents_dir}\n")
            f.write(f"Search pattern: *{date_underscore}.pdf\n")
        
        # Find all matching ordinance/resolution files
        # Pattern: YYYY-## - MM_DD_YYYY.pdf
        pattern = f"*{date_underscore}.pdf"
        matching_files = list(documents_dir.glob(pattern))
        
        # Also try without spaces in case filenames vary
        pattern2 = f"*{date_underscore}*.pdf"
        additional_files = [f for f in documents_dir.glob(pattern2) if f not in matching_files]
        matching_files.extend(additional_files)
        
        # List all files in directory for debugging
        all_files = list(documents_dir.glob("*.pdf"))
        with open(debug_dir / "all_ordinance_files.txt", 'w') as f:
            f.write(f"Total files in {documents_dir}: {len(all_files)}\n")
            for file in sorted(all_files):
                f.write(f"  - {file.name}\n")
        
        log.info(f"📄 Found {len(matching_files)} documents for date {meeting_date}")
        
        if matching_files:
            log.info("📄 Documents found:")
            for f in matching_files[:5]:
                log.info(f"   - {f.name}")
            if len(matching_files) > 5:
                log.info(f"   ... and {len(matching_files) - 5} more")
        
        # Save matched files list
        with open(debug_dir / "matched_documents.txt", 'w') as f:
            f.write(f"Documents matching date {meeting_date}:\n")
            for file in matching_files:
                f.write(f"  - {file.name}\n")
        
        # Process each document
        linked_documents = {
            "ordinances": [],
            "resolutions": []
        }
        
        for doc_path in matching_files:
            # Extract document info
            doc_info = await self._process_document(doc_path, meeting_date)
            
            if doc_info:
                # Categorize by type
                if "ordinance" in doc_info.get("title", "").lower():
                    linked_documents["ordinances"].append(doc_info)
                else:
                    linked_documents["resolutions"].append(doc_info)
        
        # Save linked documents info
        with open(debug_dir / "linked_documents.json", 'w') as f:
            json.dump(linked_documents, f, indent=2)
        
        log.info(f"✅ Linked {len(linked_documents['ordinances'])} ordinances, {len(linked_documents['resolutions'])} resolutions")
        return linked_documents
    
    async def _process_document(self, doc_path: Path, meeting_date: str) -> Optional[Dict[str, Any]]:
        """Process a single document to extract agenda item reference."""
        try:
            # Extract document number from filename (e.g., "2024-04" from "2024-04 - 01_23_2024.pdf")
            doc_match = re.match(r'^(\d{4}-\d{2})', doc_path.name)
            if not doc_match:
                log.warning(f"Could not parse document number from {doc_path.name}")
                return None
            
            document_number = doc_match.group(1)
            
            # Extract text from PDF
            text = self._extract_pdf_text(doc_path)
            if not text:
                log.warning(f"No text extracted from {doc_path.name}")
                return None
            
            # Extract agenda item code using LLM
            item_code = await self._extract_agenda_item_code(text, document_number)
            
            # Extract additional metadata
            parsed_data = self._parse_document_metadata(text)
            
            doc_info = {
                "path": str(doc_path),
                "filename": doc_path.name,
                "document_number": document_number,
                "item_code": item_code,
                "document_type": "Ordinance" if "ordinance" in text[:500].lower() else "Resolution",
                "title": self._extract_title(text),
                "parsed_data": parsed_data
            }
            
            log.info(f"📄 Processed {doc_path.name}: Item {item_code or 'NOT_FOUND'}")
            return doc_info
            
        except Exception as e:
            log.error(f"Error processing {doc_path.name}: {e}")
            return None
    
    def _extract_pdf_text(self, pdf_path: Path) -> str:
        """Extract text from PDF file."""
        try:
            with open(pdf_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                text_parts = []
                
                # Extract text from all pages
                for page in reader.pages:
                    text = page.extract_text()
                    if text:
                        text_parts.append(text)
                
                return "\n".join(text_parts)
        except Exception as e:
            log.error(f"Failed to extract text from {pdf_path.name}: {e}")
            return ""
    
    async def _extract_agenda_item_code(self, text: str, document_number: str) -> Optional[str]:
        """Extract agenda item code from document text using LLM."""
        prompt = f"""You are analyzing a City of Coral Gables ordinance document (Document #{document_number}).

Your task is to find the AGENDA ITEM CODE referenced in this document.

The agenda item typically appears near the end in formats like:
- (Agenda Item: E-1)
- Agenda Item: E-3)
- (Agenda Item E-1)
- Item H-3
- H.-3. (with periods and dots)

Document text:
{text[-3000:]}

Respond in this EXACT format:
AGENDA_ITEM: [code] or AGENDA_ITEM: NOT_FOUND

Important: Return the code as it appears (e.g., E-1, not E.-1.)

Examples:
- If you find "(Agenda Item: E-1)" → respond: AGENDA_ITEM: E-1
- If you find "H.-3." → respond: AGENDA_ITEM: H-3
- If no agenda item found → respond: AGENDA_ITEM: NOT_FOUND"""
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a precise data extractor. Find and extract only the agenda item code."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_tokens=50
            )
            
            result = response.choices[0].message.content.strip()
            
            # Parse the response
            if "AGENDA_ITEM:" in result:
                code = result.split("AGENDA_ITEM:")[1].strip()
                if code != "NOT_FOUND":
                    # Normalize the code (remove dots, ensure format)
                    code = self._normalize_item_code(code)
                    return code
            
            return None
            
        except Exception as e:
            log.error(f"Failed to extract agenda item: {e}")
            return None
    
    def _normalize_item_code(self, code: str) -> str:
        """Normalize item code to consistent format."""
        if not code:
            return code
        
        # Remove trailing dots and spaces
        code = code.rstrip('. ')
        
        # Remove dots between letter and dash: "E.-1" -> "E-1"
        code = re.sub(r'([A-Z])\.(-)', r'\1\2', code)
        
        # Handle cases without dash: "E.1" -> "E-1"
        code = re.sub(r'([A-Z])\.(\d)', r'\1-\2', code)
        
        # Remove any remaining dots
        code = code.replace('.', '')
        
        return code
    
    def _extract_title(self, text: str) -> str:
        """Extract document title from text."""
        # Look for "AN ORDINANCE" or "A RESOLUTION" pattern
        title_match = re.search(r'(AN?\s+(ORDINANCE|RESOLUTION)[^.]+\.)', text[:2000], re.IGNORECASE)
        if title_match:
            return title_match.group(1).strip()
        
        # Fallback to first substantive line
        lines = text.split('\n')
        for line in lines[:20]:
            if len(line) > 20 and not line.isdigit():
                return line.strip()[:200]
        
        return "Untitled Document"
    
    def _parse_document_metadata(self, text: str) -> Dict[str, Any]:
        """Parse additional metadata from document."""
        metadata = {}
        
        # Extract date passed
        date_match = re.search(r'day\s+of\s+(\w+),?\s+(\d{4})', text)
        if date_match:
            metadata["date_passed"] = date_match.group(0)
        
        # Extract vote information
        vote_match = re.search(r'PASSED\s+AND\s+ADOPTED.*?(\d+).*?(\d+)', text, re.IGNORECASE | re.DOTALL)
        if vote_match:
            metadata["vote_details"] = {
                "ayes": vote_match.group(1),
                "nays": vote_match.group(2) if len(vote_match.groups()) > 1 else "0"
            }
        
        # Extract motion information
        motion_match = re.search(r'motion\s+(?:was\s+)?made\s+by\s+([^,]+)', text, re.IGNORECASE)
        if motion_match:
            metadata["motion"] = {"moved_by": motion_match.group(1).strip()}
        
        # Extract mayor signature
        mayor_match = re.search(r'Mayor[:\s]+([^\n]+)', text[-1000:])
        if mayor_match:
            metadata["signatories"] = {"mayor": mayor_match.group(1).strip()}
        
        return metadata


================================================================================


################################################################################
# File: city_clerk_documents/graph_json/debug/entities_parsed.json
################################################################################

{
  "people": [
    {
      "name": "Vince Lago",
      "role": "Mayor",
      "context": "presiding"
    },
    {
      "name": "Rhonda Anderson",
      "role": "Vice Mayor",
      "context": "vice presiding"
    },
    {
      "name": "Melissa Castro",
      "role": "Commissioner",
      "context": "governing body"
    },
    {
      "name": "Ariel Fernandez",
      "role": "Commissioner",
      "context": "governing body"
    },
    {
      "name": "Kirk R. Menendez",
      "role": "Commissioner",
      "context": "governing body"
    },
    {
      "name": "Peter J. Iglesias",
      "role": "City Manager",
      "context": "city administration"
    },
    {
      "name": "Cristina M. Suarez",
      "role": "City Attorney",
      "context": "city administration"
    },
    {
      "name": "Billy Y. Urquia",
      "role": "City Clerk",
      "context": "city administration"
    },
    {
      "name": "Judith Alexander",
      "role": "Senior Citizens Advisory Board member",
      "context": "board appointment"
    },
    {
      "name": "John 'Miles' Maronto",
      "role": "Transportation Advisory Board member",
      "context": "board appointment"
    },
    {
      "name": "Nicholas Balsera",
      "role": "Firefighter",
      "context": "award recipient"
    },
    {
      "name": "Michael Aleman",
      "role": "Officer",
      "context": "award recipient"
    }
  ],
  "organizations": [
    {
      "name": "City Commission",
      "type": "government",
      "context": "governing body"
    },
    {
      "name": "Coral Gables Congregational United Church of Christ",
      "type": "religious organization",
      "context": "proclamation recipient"
    },
    {
      "name": "Coral Gables Fire Department",
      "type": "fire department",
      "context": "recognition recipient"
    },
    {
      "name": "City of Miami Fire Department",
      "type": "fire department",
      "context": "recognizing organization"
    },
    {
      "name": "Senior Citizens Advisory Board",
      "type": "advisory board",
      "context": "board appointment"
    },
    {
      "name": "Transportation Advisory Board",
      "type": "advisory board",
      "context": "board appointment"
    },
    {
      "name": "Cultural Development Board",
      "type": "advisory board",
      "context": "meeting minutes"
    },
    {
      "name": "Landscape Beautification Advisory Board",
      "type": "advisory board",
      "context": "meeting minutes"
    },
    {
      "name": "Planning and Zoning Board",
      "type": "advisory board",
      "context": "meeting minutes"
    },
    {
      "name": "School Community Relations Committee",
      "type": "committee",
      "context": "meeting minutes"
    },
    {
      "name": "Sustainability Advisory Board",
      "type": "advisory board",
      "context": "meeting minutes"
    },
    {
      "name": "Waterway Advisory Board",
      "type": "advisory board",
      "context": "meeting minutes"
    },
    {
      "name": "Historic Preservation Board",
      "type": "advisory board",
      "context": "resolution request"
    },
    {
      "name": "Landmarks Advisory Board",
      "type": "advisory board",
      "context": "resolution request"
    },
    {
      "name": "FPL",
      "type": "utility company",
      "context": "streetlight removal"
    }
  ],
  "locations": [
    {
      "name": "City Hall",
      "address": "405 Biltmore Way",
      "type": "government building"
    },
    {
      "name": "Commission Chambers",
      "address": "405 Biltmore Way",
      "type": "meeting room"
    },
    {
      "name": "Coral Gables",
      "type": "city",
      "context": "meeting location"
    },
    {
      "name": "Andalusia Avenue",
      "type": "street",
      "context": "site plan location"
    }
  ],
  "monetary_amounts": [],
  "dates": [
    {
      "date": "01/09/2024",
      "event": "meeting date",
      "type": "meeting"
    },
    {
      "date": "12/16/2023",
      "event": "proclamation date",
      "type": "proclamation"
    },
    {
      "date": "11/14/2023",
      "event": "previous meeting date",
      "type": "meeting"
    },
    {
      "date": "06/01/2023",
      "event": "term start date",
      "type": "board appointment"
    },
    {
      "date": "05/31/2025",
      "event": "term end date",
      "type": "board appointment"
    },
    {
      "date": "11/08/2023",
      "event": "meeting date",
      "type": "meeting"
    },
    {
      "date": "12/14/2023",
      "event": "meeting date",
      "type": "meeting"
    },
    {
      "date": "10/25/2023",
      "event": "meeting date",
      "type": "meeting"
    },
    {
      "date": "10/17/2023",
      "event": "meeting date",
      "type": "meeting"
    },
    {
      "date": "11/01/2023",
      "event": "meeting date",
      "type": "meeting"
    },
    {
      "date": "11/17/2023",
      "event": "meeting date",
      "type": "meeting"
    }
  ],
  "legal_references": [
    {
      "type": "Resolution",
      "number": "23-6764",
      "title": "Proclamation declaring December 16, 2023 as 'Coral Gables Congregational United Church of Christ Day'"
    },
    {
      "type": "Resolution",
      "number": "23-6792",
      "title": "Recognition of Coral Gables Fire Department Rescue 4 Crew"
    },
    {
      "type": "Resolution",
      "number": "23-6793",
      "title": "Congratulations to Firefighter Nicholas Balsera"
    },
    {
      "type": "Resolution",
      "number": "23-6863",
      "title": "Congratulations to Officer Michael Aleman"
    },
    {
      "type": "Resolution",
      "number": "23-6735",
      "title": "Approval of minutes"
    },
    {
      "type": "Resolution",
      "number": "23-6830",
      "title": "Appointment of Judith Alexander to Senior Citizens Advisory Board"
    },
    {
      "type": "Resolution",
      "number": "23-6829",
      "title": "Appointment of John 'Miles' Maronto to Transportation Advisory Board"
    },
    {
      "type": "Resolution",
      "number": "23-6797",
      "title": "Cultural Development Board meeting minutes"
    },
    {
      "type": "Resolution",
      "number": "23-6857",
      "title": "Landscape Beautification Advisory Board meeting minutes"
    },
    {
      "type": "Resolution",
      "number": "23-6674",
      "title": "Planning and Zoning Board meeting minutes"
    },
    {
      "type": "Resolution",
      "number": "23-6787",
      "title": "School Community Relations Committee meeting minutes"
    },
    {
      "type": "Resolution",
      "number": "23-6815",
      "title": "Sustainability Advisory Board meeting minutes"
    },
    {
      "type": "Resolution",
      "number": "23-6814",
      "title": "Transportation Advisory Board meeting minutes"
    },
    {
      "type": "Resolution",
      "number": "23-6818",
      "title": "Waterway Advisory Board meeting minutes"
    },
    {
      "type": "Resolution",
      "number": "23-6882",
      "title": "Request to cease removal of silver streetlights"
    },
    {
      "type": "Ordinance",
      "number": "23-6723",
      "title": "Amendment to Cocoplum Phase 1 Security Guard District"
    },
    {
      "type": "Ordinance",
      "number": "23-6785",
      "title": "Amendment to City Code regarding publication requirements"
    },
    {
      "type": "Ordinance",
      "number": "23-6786",
      "title": "Amendment to Zoning Code regarding publication requirements"
    },
    {
      "type": "Ordinance",
      "number": "23-6567",
      "title": "Amendment to St. Philip's School site plan"
    },
    {
      "type": "Ordinance",
      "number": "23-6826",
      "title": "Text amendments to Zoning Code regarding parking requirements"
    },
    {
      "type": "Ordinance",
      "number": "23-6827",
      "title": "Text amendments to Zoning Code regarding workforce housing"
    }
  ]
}


================================================================================


################################################################################
# File: scripts/graph_stages/cosmos_db_client.py
################################################################################

# File: scripts/graph_stages/cosmos_db_client.py

"""
Azure Cosmos DB Gremlin client for city clerk graph database.
Provides async operations for graph manipulation.
"""

from __future__ import annotations
import asyncio
import logging
from typing import Any, Dict, List, Optional, Union
import os
from gremlin_python.driver import client, serializer
from gremlin_python.driver.driver_remote_connection import DriverRemoteConnection
from gremlin_python.structure.graph import Graph
from dotenv import load_dotenv
import json

load_dotenv()

log = logging.getLogger('cosmos_graph_client')


class CosmosGraphClient:
    """Async client for Azure Cosmos DB Gremlin API."""
    
    def __init__(self, 
                 endpoint: Optional[str] = None,
                 key: Optional[str] = None,
                 database: Optional[str] = None,
                 container: Optional[str] = None,
                 partition_value: str = "demo"):
        """Initialize Cosmos DB client."""
        self.endpoint = endpoint or os.getenv("COSMOS_ENDPOINT")
        self.key = key or os.getenv("COSMOS_KEY")
        self.database = database or os.getenv("COSMOS_DATABASE", "cgGraph")
        self.container = container or os.getenv("COSMOS_CONTAINER", "cityClerk")
        self.partition_value = partition_value
        
        if not all([self.endpoint, self.key, self.database, self.container]):
            raise ValueError("Missing required Cosmos DB configuration")
        
        self._client = None
        self._loop = asyncio.get_event_loop()
    
    async def connect(self) -> None:
        """Establish connection to Cosmos DB."""
        try:
            self._client = client.Client(
                f"{self.endpoint}/gremlin",
                "g",
                username=f"/dbs/{self.database}/colls/{self.container}",
                password=self.key,
                message_serializer=serializer.GraphSONSerializersV2d0()
            )
            log.info(f"✅ Connected to Cosmos DB: {self.database}/{self.container}")
        except Exception as e:
            log.error(f"❌ Failed to connect to Cosmos DB: {e}")
            raise
    
    async def _execute_query(self, query: str, bindings: Optional[Dict] = None) -> List[Any]:
        """Execute a Gremlin query asynchronously."""
        if not self._client:
            await self.connect()
        
        try:
            # Run synchronous operation in thread pool
            future = self._loop.run_in_executor(
                None,
                lambda: self._client.submit(query, bindings or {})
            )
            callback = await future
            
            # Collect results
            results = []
            for result in callback:
                results.extend(result)
            
            return results
        except Exception as e:
            log.error(f"Query execution failed: {query[:100]}... Error: {e}")
            raise
    
    async def clear_graph(self) -> None:
        """Clear all vertices and edges from the graph."""
        log.warning("🗑️  Clearing entire graph...")
        try:
            # Drop all vertices (edges are automatically removed)
            await self._execute_query("g.V().drop()")
            log.info("✅ Graph cleared successfully")
        except Exception as e:
            log.error(f"Failed to clear graph: {e}")
            raise
    
    async def create_vertex(self, 
                          label: str,
                          vertex_id: str,
                          properties: Dict[str, Any]) -> None:
        """Create a vertex with properties."""
        # Build property chain
        prop_chain = ""
        for key, value in properties.items():
            if value is not None:
                # Handle different value types
                if isinstance(value, bool):
                    prop_chain += f".property('{key}', {str(value).lower()})"
                elif isinstance(value, (int, float)):
                    prop_chain += f".property('{key}', {value})"
                elif isinstance(value, list):
                    # Convert list to JSON string
                    json_val = json.dumps(value).replace("'", "\\'")
                    prop_chain += f".property('{key}', '{json_val}')"
                else:
                    # Escape string values
                    escaped_val = str(value).replace("'", "\\'").replace('"', '\\"')
                    prop_chain += f".property('{key}', '{escaped_val}')"
        
        # Always add partition key
        prop_chain += f".property('partitionKey', '{self.partition_value}')"
        
        query = f"g.addV('{label}').property('id', '{vertex_id}'){prop_chain}"
        
        await self._execute_query(query)
    
    async def create_edge(self,
                         from_id: str,
                         to_id: str,
                         edge_type: str,
                         properties: Optional[Dict[str, Any]] = None) -> None:
        """Create an edge between two vertices."""
        # Build property chain for edge
        prop_chain = ""
        if properties:
            for key, value in properties.items():
                if value is not None:
                    if isinstance(value, bool):
                        prop_chain += f".property('{key}', {str(value).lower()})"
                    elif isinstance(value, (int, float)):
                        prop_chain += f".property('{key}', {value})"
                    else:
                        escaped_val = str(value).replace("'", "\\'")
                        prop_chain += f".property('{key}', '{escaped_val}')"
        
        query = f"g.V('{from_id}').addE('{edge_type}').to(g.V('{to_id}')){prop_chain}"
        
        try:
            await self._execute_query(query)
        except Exception as e:
            log.error(f"Failed to create edge {from_id} -> {to_id}: {e}")
            raise
    
    async def vertex_exists(self, vertex_id: str) -> bool:
        """Check if a vertex exists."""
        result = await self._execute_query(f"g.V('{vertex_id}').count()")
        return result[0] > 0 if result else False
    
    async def get_vertex(self, vertex_id: str) -> Optional[Dict]:
        """Get a vertex by ID."""
        result = await self._execute_query(f"g.V('{vertex_id}').valueMap(true)")
        return result[0] if result else None
    
    async def close(self) -> None:
        """Close the connection."""
        if self._client:
            try:
                # Don't use run_until_complete in an async context
                # Just close the client synchronously
                self._client.close()
            except Exception as e:
                log.warning(f"Error closing client: {e}")
            finally:
                self._client = None
                log.info("Connection closed")
    
    async def __aenter__(self):
        await self.connect()
        return self
    
    async def __aexit__(self, *args):
        await self.close()


================================================================================


################################################################################
# File: scripts/topic_filter_and_title.py
################################################################################

#!/usr/bin/env python3
# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------
"""
Stage 3 — *Topic filter + title extraction*

Loop over every TXT file in **city_clerk_documents/txt/**, send a fixed-length
prefix to GPT-4 to extract a title and decide if it's relevant to misophonia.

The script writes **two helper files in the same */scripts/* folder** (the
"script library", as requested):

  1.  not_about_<topic>.txt    — one TXT filename per line (safe to delete)
  2.  rename_map_<topic>.tsv   — "current_filename<TAB>inferred_title"

Change *TOPIC*, *MAX_WORDS* or *MODEL* below as needed.
"""
from __future__ import annotations

import json
import logging
import os
import time
from pathlib import Path
from typing import Any, Dict, List

from dotenv import load_dotenv
from openai import OpenAI

# ────────────────────────── user-tunable settings ───────────────────────────

TOPIC: str = "Misophonia"            # ← change for other topics
MAX_WORDS: int = 300                 # ≈ two paragraphs
MODEL: str = "gpt-4.1-mini-2025-04-14"
RATE_LIMIT_SLEEP: float = 1.2        # s between API calls to stay polite

# ───────────────────────────── paths & client ───────────────────────────────

SCRIPT_DIR = Path(__file__).resolve().parent            # /scripts/
REPO_ROOT  = SCRIPT_DIR.parent                          # project root
TXT_DIR    = REPO_ROOT / "documents" / "research" / "txt"

OUT_NO_TOPIC = SCRIPT_DIR / f"not_about_{TOPIC.lower()}.txt"
OUT_RENAME   = SCRIPT_DIR / f"rename_map_{TOPIC.lower()}.tsv"

load_dotenv()  # picks up OPENAI_API_KEY from .env or environment
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ─────────────────────────── helper functions ──────────────────────────────

def read_excerpt(txt_path: Path, max_words: int = MAX_WORDS) -> str:
    """
    Return the first *max_words* words of the TXT file.
    """
    text = txt_path.read_text(encoding="utf-8", errors="ignore")
    words = text.split()
    return " ".join(words[:max_words])

def query_llm(content: str) -> Dict[str, Any]:
    """
    Ask GPT-4 whether the paper is about *TOPIC* and get its title.
    Returns a dict like:  { "relevant": true/false, "title": "…" }
    """
    system_msg = (
        "You are a scholarly assistant. You will receive an excerpt from a "
        f"scientific paper. Decide whether the paper is about the topic "
        f"'{TOPIC}'. Respond **ONLY** with valid JSON containing two keys:\n"
        '  "relevant": true or false\n'
        '  "title":    full paper title if present, else ""'
    )
    user_msg = f"Excerpt:\n\"\"\"\n{content}\n\"\"\""

    chat = client.chat.completions.create(
        model=MODEL,
        temperature=0,            # deterministic
        top_p=1,
        max_tokens=256,
        stream=False,
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user",   "content": user_msg},
        ],
        response_format={"type": "json_object"},
    )
    return json.loads(chat.choices[0].message.content)

# ──────────────────────────────── main loop ─────────────────────────────────

def main() -> None:
    logging.basicConfig(level=logging.INFO, format="%(message)s")

    if not TXT_DIR.exists():
        logging.error(f"TXT source folder not found: {TXT_DIR}")
        return

    txt_files = sorted(TXT_DIR.glob("*.txt"))
    if not txt_files:
        logging.error(f"No TXT files in {TXT_DIR}")
        return

    not_about: List[str] = []
    rename_rows: List[str] = []

    for fp in txt_files:
        logging.info(f"→ {fp.name}")
        excerpt = read_excerpt(fp)

        # default values in case of any exception
        relevant = False
        title = ""

        try:
            resp = query_llm(excerpt)
            relevant = bool(resp.get("relevant"))
            title    = (resp.get("title") or "").strip()
        except Exception as e:
            logging.warning(f"  ⚠️  LLM error ({e}); treating as NOT relevant")

        if not relevant:
            not_about.append(fp.name)
        else:
            rename_rows.append(f"{fp.name}\t{title}")

        time.sleep(RATE_LIMIT_SLEEP)   # simple client-side rate-limit

    # ───────────────────────────── write outputs ────────────────────────────
    if not_about:
        OUT_NO_TOPIC.write_text("\n".join(not_about) + "\n", encoding="utf-8")
        logging.info(f"✍️  {len(not_about)} non-topic files → {OUT_NO_TOPIC.name}")

    if rename_rows:
        OUT_RENAME.write_text("\n".join(rename_rows) + "\n", encoding="utf-8")
        logging.info(f"✍️  {len(rename_rows)} rename rows → {OUT_RENAME.name}")

    if not not_about and not rename_rows:
        logging.info("✅ No files processed (empty dataset?).")
    else:
        logging.info("✅ Finished.")

# ───────────────────────── entry point ──────────────────────────────────────

if __name__ == "__main__":
    main()


================================================================================


################################################################################
# File: scripts/stages/acceleration_utils.py
################################################################################

# File: scripts/stages/acceleration_utils.py

"""
Hardware acceleration utilities for the pipeline.
Provides Apple Silicon optimization when available, with CPU fallback.
"""
import os
import platform
import multiprocessing as mp
from typing import Optional, Callable, Any, List
import logging
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import asyncio
from functools import partial

log = logging.getLogger(__name__)

class HardwareAccelerator:
    """Detects and manages hardware acceleration capabilities."""
    
    def __init__(self):
        self.is_apple_silicon = self._detect_apple_silicon()
        self.cpu_count = mp.cpu_count()
        self.optimal_workers = self._calculate_optimal_workers()
        
        # Set environment for better performance on macOS
        if self.is_apple_silicon:
            os.environ['OPENBLAS_NUM_THREADS'] = '1'
            os.environ['MKL_NUM_THREADS'] = '1'
            os.environ['OMP_NUM_THREADS'] = '1'
        
        log.info(f"Hardware: {'Apple Silicon' if self.is_apple_silicon else platform.processor()}")
        log.info(f"CPU cores: {self.cpu_count}, Optimal workers: {self.optimal_workers}")
    
    def _detect_apple_silicon(self) -> bool:
        """Detect if running on Apple Silicon."""
        if platform.system() != 'Darwin':
            return False
        try:
            import subprocess
            result = subprocess.run(['sysctl', '-n', 'hw.optional.arm64'], 
                                  capture_output=True, text=True)
            return result.stdout.strip() == '1'
        except:
            return False
    
    def _calculate_optimal_workers(self) -> int:
        """Calculate optimal number of workers based on hardware."""
        if self.is_apple_silicon:
            # Apple Silicon has efficiency and performance cores
            # Use 75% of cores to leave room for system
            return max(1, int(self.cpu_count * 0.75))
        else:
            # Traditional CPU - use all but one core
            return max(1, self.cpu_count - 1)
    
    def get_process_pool(self, max_workers: Optional[int] = None) -> ProcessPoolExecutor:
        """Get optimized process pool executor."""
        workers = max_workers or self.optimal_workers
        return ProcessPoolExecutor(
            max_workers=workers,
            mp_context=mp.get_context('spawn')  # Required for macOS
        )
    
    def get_thread_pool(self, max_workers: Optional[int] = None) -> ThreadPoolExecutor:
        """Get optimized thread pool executor for I/O operations."""
        workers = max_workers or min(32, self.cpu_count * 4)
        return ThreadPoolExecutor(max_workers=workers)

# Global instance
hardware = HardwareAccelerator()

async def run_cpu_bound_concurrent(func: Callable, items: List[Any], 
                                 max_workers: Optional[int] = None,
                                 desc: str = "Processing") -> List[Any]:
    """Run CPU-bound function concurrently on multiple items."""
    loop = asyncio.get_event_loop()
    with hardware.get_process_pool(max_workers) as executor:
        futures = [loop.run_in_executor(executor, func, item) for item in items]
        
        from tqdm.asyncio import tqdm_asyncio
        results = await tqdm_asyncio.gather(*futures, desc=desc)
        return results

async def run_io_bound_concurrent(func: Callable, items: List[Any],
                                max_workers: Optional[int] = None,
                                desc: str = "Processing") -> List[Any]:
    """Run I/O-bound function concurrently on multiple items."""
    loop = asyncio.get_event_loop()
    with hardware.get_thread_pool(max_workers) as executor:
        futures = [loop.run_in_executor(executor, func, item) for item in items]
        
        from tqdm.asyncio import tqdm_asyncio
        results = await tqdm_asyncio.gather(*futures, desc=desc)
        return results


================================================================================


################################################################################
# File: scripts/check_pipeline_setup.py
################################################################################

# File: scripts/check_pipeline_setup.py

"""
Check City Clerk Pipeline Setup
"""
from pathlib import Path
import os

def check_setup():
    """Check if the pipeline environment is properly set up."""
    print("🔍 Checking City Clerk Pipeline Setup\n")
    
    # Check current directory
    cwd = Path.cwd()
    print(f"📂 Current directory: {cwd}")
    print(f"📂 Script location: {Path(__file__).parent}")
    
    # Check for agenda directory
    print("\n📁 Checking for agenda files:")
    possible_dirs = [
        Path("city_clerk_documents/global"),
        Path("../city_clerk_documents/global"),
        Path("../../city_clerk_documents/global"),
        cwd / "city_clerk_documents" / "global"
    ]
    
    found_dir = None
    for dir_path in possible_dirs:
        abs_path = dir_path.absolute()
        exists = dir_path.exists()
        print(f"   {dir_path} -> {abs_path}")
        print(f"   Exists: {exists}")
        
        if exists:
            files = list(dir_path.glob("*.pdf"))
            agenda_files = list(dir_path.glob("*genda*.pdf"))
            print(f"   Total PDFs: {len(files)}")
            print(f"   Agenda PDFs: {len(agenda_files)}")
            
            if agenda_files:
                print(f"   Found agenda files:")
                for f in agenda_files[:5]:
                    print(f"      - {f.name}")
                found_dir = dir_path
                break
        print()
    
    if found_dir:
        print(f"✅ Found agenda directory: {found_dir}")
        print(f"\n💡 Run the pipeline with:")
        print(f"   python scripts/graph_pipeline.py --agenda-dir '{found_dir}'")
    else:
        print("❌ Could not find agenda directory!")
        print("\n💡 Please ensure:")
        print("   1. You have the city_clerk_documents/global directory")
        print("   2. It contains PDF files with 'Agenda' in the name")
        print("   3. You're running from the correct directory")
    
    # Check environment variables
    print("\n🔑 Checking environment variables:")
    env_vars = ['COSMOS_ENDPOINT', 'COSMOS_KEY', 'GROQ_API_KEY']
    all_set = True
    for var in env_vars:
        value = os.getenv(var)
        if value:
            print(f"   ✅ {var}: {'*' * 10} (set)")
        else:
            print(f"   ❌ {var}: NOT SET")
            all_set = False
    
    if not all_set:
        print("\n💡 Create a .env file with the missing variables")
    
    # Check Python imports
    print("\n📦 Checking Python imports:")
    try:
        import gremlin_python
        print("   ✅ gremlin_python")
    except ImportError:
        print("   ❌ gremlin_python - run: pip install gremlinpython")
    
    try:
        import groq
        print("   ✅ groq")
    except ImportError:
        print("   ❌ groq - run: pip install groq")
    
    try:
        import fitz
        print("   ✅ PyMuPDF (fitz)")
    except ImportError:
        print("   ❌ PyMuPDF - run: pip install PyMuPDF")
    
    try:
        import unstructured
        print("   ✅ unstructured")
    except ImportError:
        print("   ❌ unstructured - run: pip install unstructured")

if __name__ == "__main__":
    check_setup()


================================================================================


################################################################################
# File: clear_database.py
################################################################################

# File: clear_database.py

#!/usr/bin/env python3
"""
Clear Cosmos DB Graph Database
This script will clear all vertices and edges from the graph database.
"""

import asyncio
import sys
import os
from pathlib import Path

# Add scripts directory to path
script_dir = Path(__file__).parent / 'scripts'
sys.path.append(str(script_dir))

from graph_stages.cosmos_db_client import CosmosGraphClient

async def clear_database():
    """Clear the entire graph database."""
    print('🗑️  Clearing Cosmos DB graph database...')
    print('⚠️  This will delete ALL vertices and edges!')
    
    try:
        async with CosmosGraphClient() as client:
            await client.clear_graph()
        print('✅ Graph database cleared successfully!')
        return True
        
    except Exception as e:
        print(f'❌ Error clearing database: {e}')
        return False

if __name__ == "__main__":
    success = asyncio.run(clear_database())
    if not success:
        sys.exit(1)


================================================================================


################################################################################
# File: city_clerk_documents/graph_json/debug/meeting_info_parsed.json
################################################################################

{
  "meeting_type": "Regular Meeting",
  "meeting_time": "9:00 AM",
  "location": {
    "name": "City Hall, Commission Chambers",
    "address": "405 Biltmore Way Coral Gables, FL 33134"
  },
  "officials_present": {
    "mayor": "Vince Lago",
    "vice_mayor": "Rhonda Anderson",
    "commissioners": [
      "Melissa Castro",
      "Ariel Fernandez",
      "Kirk R. Menendez"
    ],
    "city_attorney": "Cristina M. Suarez",
    "city_manager": "Peter J. Iglesias, P.E.",
    "city_clerk": "Billy Y. Urquia"
  }
}


================================================================================

