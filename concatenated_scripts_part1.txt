# Concatenated Project Code - Part 1 of 3
# Generated: 2025-06-12 09:30:51
# Root Directory: /Users/gianmariatroiani/Documents/knologi/graph_database
================================================================================

# Directory Structure
################################################################################
├── .gitignore (939.0B, no ext)
├── check_enhanced_results.py (1.9KB, .py)
├── check_ordinances.py (1.5KB, .py)
├── check_status.py (3.5KB, .py)
├── city_clerk_documents/
│   └── [EXCLUDED] 6 items: .DS_Store (excluded file), extracted_markdown (excluded dir), extracted_text (excluded dir)
│       ... and 3 more excluded items
├── concatenate_scripts_broken.py (39.5KB, .py)
├── config.py (1.7KB, .py)
├── debug_enhanced_query.py (1.8KB, .py)
├── explore_graphrag_sources.py (3.0KB, .py)
├── extract_documents_for_graphrag.py (3.5KB, .py)
├── graph_clear_database.py (963.0B, .py)
├── graph_visualizer.py (25.1KB, .py)
├── graphrag_query_ui.py (30.0KB, .py)
├── graphrag_visualization.html (6.1MB, .html)
├── investigate_graph.py (2.8KB, .py)
├── ontology_model.txt (6.6KB, .txt)
├── ontology_modelv2.txt (10.2KB, .txt)
├── pipeline_summary.py (7.9KB, .py)
├── repository_directory_structure.txt (5.7KB, .txt)
├── requirements.txt (1.2KB, .txt)
├── run_city_clerk_pipeline.sh (440.0B, .sh)
├── run_enhanced_dedup.py (415.0B, .py)
├── run_graph_pipeline.sh (1.3KB, .sh)
├── run_graph_visualizer.sh (3.6KB, .sh)
├── run_graphrag.sh (5.6KB, .sh)
├── run_query_ui.sh (761.0B, .sh)
├── scripts/
│   ├── extract_all_pdfs_direct.py (5.8KB, .py)
│   ├── extract_all_to_markdown.py (8.2KB, .py)
│   ├── graph_pipeline.py (21.9KB, .py)
│   ├── graph_stages/
│   │   ├── __init__.py (647.0B, .py)
│   │   ├── agenda_graph_builder.py (63.2KB, .py)
│   │   ├── agenda_ontology_extractor.py (31.4KB, .py)
│   │   ├── agenda_pdf_extractor.py (32.8KB, .py)
│   │   ├── cosmos_db_client.py (10.2KB, .py)
│   │   ├── document_linker.py (13.8KB, .py)
│   │   ├── enhanced_document_linker.py (26.6KB, .py)
│   │   ├── ontology_extractor.py (42.3KB, .py)
│   │   ├── pdf_extractor.py (6.5KB, .py)
│   │   ├── verbatim_transcript_linker.py (19.9KB, .py)
│   │   ├── [EXCLUDED] 1 items: __pycache__ (excluded dir)
│   ├── json_to_markdown_converter.py (1.4KB, .py)
│   ├── microsoft_framework/
│   │   ├── README.md (6.9KB, .md)
│   │   ├── __init__.py (1.3KB, .py)
│   │   ├── city_clerk_settings_template.yaml (2.1KB, .yaml)
│   │   ├── cosmos_synchronizer.py (3.5KB, .py)
│   │   ├── create_custom_prompts.py (4.3KB, .py)
│   │   ├── document_adapter.py (26.0KB, .py)
│   │   ├── enhanced_entity_deduplicator.py (56.5KB, .py)
│   │   ├── entity_deduplicator.py (14.6KB, .py)
│   │   ├── graph_visualizer.py (8.5KB, .py)
│   │   ├── graph_visualizer_microsoft_framework.py (12.1KB, .py)
│   │   ├── graphrag_initializer.py (5.1KB, .py)
│   │   ├── graphrag_output_processor.py (7.7KB, .py)
│   │   ├── graphrag_pipeline.py (2.0KB, .py)
│   │   ├── incremental_processor.py (1.9KB, .py)
│   │   ├── prompt_tuner.py (7.9KB, .py)
│   │   ├── query_engine.py (64.0KB, .py)
│   │   ├── query_graphrag.py (561.0B, .py)
│   │   ├── query_router.py (16.1KB, .py)
│   │   ├── run_graphrag.sh (5.6KB, .sh)
│   │   ├── run_graphrag_direct.py (783.0B, .py)
│   │   ├── run_graphrag_pipeline.py (20.7KB, .py)
│   │   ├── source_tracker.py (2.6KB, .py)
│   │   ├── test_enhanced_query.py (4.6KB, .py)
│   │   ├── test_queries.py (5.7KB, .py)
│   │   ├── test_routing.py (4.7KB, .py)
│   │   ├── [EXCLUDED] 1 items: __pycache__ (excluded dir)
│   ├── [EXCLUDED] 4 items: pipeline_modular_optimized.py (excluded file), rag_local_web_app.py (excluded file), stages (excluded dir)
│       ... and 1 more excluded items
├── settings.yaml (3.4KB, .yaml)
├── test_deduplication.py (7.9KB, .py)
├── test_enhanced_deduplication.py (2.3KB, .py)
├── test_enhanced_pipeline.py (8.6KB, .py)
├── test_query_functionality.py (8.1KB, .py)
├── test_query_system.py (8.6KB, .py)
├── verify_deduplication.py (1.6KB, .py)
├── [EXCLUDED] 16 items: .DS_Store (excluded file), .env (excluded file), .git (excluded dir)
    ... and 13 more excluded items


================================================================================


# File Index - Which Files Are in Which Parts
################################################################################

## Part 1 (15 files):
  - scripts/microsoft_framework/query_engine.py
  - scripts/graph_stages/agenda_pdf_extractor.py
  - scripts/graph_stages/agenda_ontology_extractor.py
  - scripts/microsoft_framework/document_adapter.py
  - scripts/graph_stages/document_linker.py
  - scripts/microsoft_framework/prompt_tuner.py
  - scripts/graph_stages/pdf_extractor.py
  - scripts/microsoft_framework/create_custom_prompts.py
  - check_status.py
  - investigate_graph.py
  - scripts/microsoft_framework/city_clerk_settings_template.yaml
  - scripts/microsoft_framework/incremental_processor.py
  - verify_deduplication.py
  - scripts/microsoft_framework/__init__.py
  - run_enhanced_dedup.py

## Part 2 (15 files):
  - scripts/graph_stages/agenda_graph_builder.py
  - concatenate_scripts_broken.py
  - scripts/graph_stages/enhanced_document_linker.py
  - scripts/graph_stages/verbatim_transcript_linker.py
  - scripts/microsoft_framework/query_router.py
  - scripts/extract_all_to_markdown.py
  - scripts/microsoft_framework/graphrag_output_processor.py
  - scripts/microsoft_framework/graphrag_initializer.py
  - scripts/microsoft_framework/cosmos_synchronizer.py
  - settings.yaml
  - scripts/microsoft_framework/graphrag_pipeline.py
  - check_enhanced_results.py
  - check_ordinances.py
  - requirements.txt
  - scripts/microsoft_framework/query_graphrag.py

## Part 3 (15 files):
  - scripts/microsoft_framework/enhanced_entity_deduplicator.py
  - scripts/graph_stages/ontology_extractor.py
  - graphrag_query_ui.py
  - scripts/microsoft_framework/run_graphrag_pipeline.py
  - scripts/microsoft_framework/entity_deduplicator.py
  - scripts/graph_stages/cosmos_db_client.py
  - scripts/microsoft_framework/README.md
  - scripts/extract_all_pdfs_direct.py
  - extract_documents_for_graphrag.py
  - explore_graphrag_sources.py
  - scripts/microsoft_framework/source_tracker.py
  - config.py
  - scripts/json_to_markdown_converter.py
  - scripts/microsoft_framework/run_graphrag_direct.py
  - scripts/graph_stages/__init__.py


================================================================================


################################################################################
# File: scripts/microsoft_framework/query_engine.py
################################################################################

# File: scripts/microsoft_framework/query_engine.py

import subprocess
import sys
import os
import json
import re
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Set
from enum import Enum
import logging
from .query_router import SmartQueryRouter, QueryIntent
from .source_tracker import SourceTracker

logger = logging.getLogger(__name__)

class QueryType(Enum):
    LOCAL = "local"
    GLOBAL = "global"
    DRIFT = "drift"

class CityClerkQueryEngine:
    """Enhanced query engine with inline source citations."""
    
    def __init__(self, graphrag_root: Path):
        self.graphrag_root = Path(graphrag_root)
        # Check for deduplicated data and use it if available
        output_dir = self.graphrag_root / "output"
        dedup_dir = output_dir / "deduplicated"
        if dedup_dir.exists() and list(dedup_dir.glob("*.parquet")):
            self.output_dir = dedup_dir
            logger.info("Using deduplicated GraphRAG data")
            
            # Load aliases for better query matching
            import pandas as pd
            entities_df = pd.read_parquet(dedup_dir / "entities.parquet")
            if 'aliases' in entities_df.columns:
                self.entity_aliases = {}
                for idx, row in entities_df.iterrows():
                    if row.get('aliases'):
                        for alias in row['aliases'].split('|'):
                            self.entity_aliases[alias.lower()] = row['title']
        else:
            self.output_dir = output_dir
            self.entity_aliases = {}
        self.source_tracker = SourceTracker()  # New component
        
    def _get_python_executable(self):
        """Get the correct Python executable."""
        from pathlib import Path
        
        current_file = Path(__file__)
        project_root = current_file.parent.parent.parent
        
        venv_python = project_root / "venv" / "bin" / "python3"
        if venv_python.exists():
            return str(venv_python)
        
        return sys.executable
        
    async def query(self, query: str, method: Optional[str] = None, **kwargs) -> Dict[str, Any]:
        """Execute query with source tracking and inline citations."""
        
        # Enable source tracking
        kwargs['track_sources'] = True
        
        # Route query
        if not method:
            router = SmartQueryRouter()
            route_info = router.determine_query_method(query)
            method = route_info['method']
            kwargs.update(route_info.get('params', {}))
        
        # Execute query with source tracking
        if method == 'local':
            result = await self._local_search_with_sources(query, **kwargs)
        elif method == 'global':
            result = await self._global_search_with_sources(query, **kwargs)
        elif method == 'drift':
            result = await self._drift_search_with_sources(query, **kwargs)
        else:
            raise ValueError(f"Unknown method: {method}")
        
        # Clean up any JSON artifacts from the answer
        result['answer'] = self._clean_json_artifacts(result['answer'])
        
        # Process answer to add inline citations
        result['answer'] = self._add_inline_citations(result['answer'], result['sources_used'])
        
        return result
    
    async def _local_search_with_sources(self, query: str, **kwargs) -> Dict[str, Any]:
        """Local search with comprehensive source tracking."""
        
        # Use the existing working local search implementation
        result = await self._execute_local_query(query, kwargs)
        
        # Extract sources from the GraphRAG response
        sources_used = self._extract_sources_from_local_response(result['answer'])
        
        # Add source tracking to the result
        result['sources_used'] = sources_used
        result['data_sources'] = self._format_data_sources(sources_used)
        
        return result
    
    async def _global_search_with_sources(self, query: str, **kwargs) -> Dict[str, Any]:
        """Global search with source tracking."""
        
        # Use the existing working global search implementation
        result = await self._execute_global_query(query, kwargs)
        
        # Extract sources from the GraphRAG response
        sources_used = self._extract_sources_from_global_response(result['answer'])
        
        # Add source tracking to the result
        result['sources_used'] = sources_used
        result['data_sources'] = self._format_data_sources(sources_used)
        
        return result
    
    async def _drift_search_with_sources(self, query: str, **kwargs) -> Dict[str, Any]:
        """DRIFT search with source tracking."""
        
        # Use the existing working drift search implementation
        result = await self._execute_drift_query(query, kwargs)
        
        # Extract sources from the GraphRAG response
        sources_used = self._extract_sources_from_drift_response(result['answer'])
        
        # Add source tracking to the result
        result['sources_used'] = sources_used
        result['data_sources'] = self._format_data_sources(sources_used)
        
        return result
    
    def _clean_json_artifacts(self, answer: str) -> str:
        """Clean JSON artifacts and metadata from GraphRAG response."""
        if not answer:
            return answer
            
        import re
        import json
        
        # Remove JSON blocks that might appear in the response
        # Pattern 1: Remove standalone JSON objects
        json_pattern = r'\{[^{}]*"[^"]*":\s*[^{}]*\}'
        answer = re.sub(json_pattern, '', answer)
        
        # Pattern 2: Remove array-like structures
        array_pattern = r'\[[^\[\]]*"[^"]*"[^\[\]]*\]'
        answer = re.sub(array_pattern, '', answer)
        
        # Pattern 3: Remove configuration-like strings
        config_patterns = [
            r'"[^"]*":\s*"[^"]*"',  # "key": "value"
            r'"[^"]*":\s*\d+',      # "key": 123
            r'"[^"]*":\s*true|false', # "key": true/false
            r'"[^"]*":\s*null',     # "key": null
        ]
        
        for pattern in config_patterns:
            answer = re.sub(pattern, '', answer)
        
        # Remove metadata headers that sometimes appear
        metadata_patterns = [
            r'SUCCESS:\s*.*?\n',
            r'INFO:\s*.*?\n',
            r'DEBUG:\s*.*?\n',
            r'WARNING:\s*.*?\n',
            r'ERROR:\s*.*?\n',
            r'METADATA:\s*.*?\n',
            r'RESPONSE:\s*',
            r'QUERY:\s*.*?\n',
        ]
        
        for pattern in metadata_patterns:
            answer = re.sub(pattern, '', answer, flags=re.IGNORECASE)
        
        # Remove any lines that look like JSON structure
        lines = answer.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            # Skip lines that are purely JSON-like
            if (line.startswith('{') and line.endswith('}')) or \
               (line.startswith('[') and line.endswith(']')) or \
               (line.startswith('"') and line.endswith('"') and ':' in line):
                continue
            # Skip empty lines created by removal
            if line:
                cleaned_lines.append(line)
        
        # Rejoin and clean up extra whitespace
        cleaned_answer = '\n'.join(cleaned_lines)
        
        # Remove multiple consecutive newlines
        cleaned_answer = re.sub(r'\n\s*\n\s*\n', '\n\n', cleaned_answer)
        
        # Remove leading/trailing whitespace
        cleaned_answer = cleaned_answer.strip()
        
        return cleaned_answer
    
    def _add_inline_citations(self, answer: str, sources_used: Dict[str, Any]) -> str:
        """Add inline citations to answer text."""
        
        # Extract entity and relationship IDs for citation
        entity_ids = list(sources_used.get('entities', {}).keys())
        rel_ids = list(sources_used.get('relationships', {}).keys())
        source_ids = list(sources_used.get('sources', {}).keys())
        
        # Split answer into paragraphs
        paragraphs = answer.split('\n\n')
        cited_paragraphs = []
        
        for para in paragraphs:
            if not para.strip():
                cited_paragraphs.append(para)
                continue
            
            # Determine which sources are relevant to this paragraph
            relevant_entities = []
            relevant_rels = []
            relevant_sources = []
            
            # Simple relevance check based on entity mentions
            para_lower = para.lower()
            
            for eid, entity in sources_used.get('entities', {}).items():
                if entity['title'].lower() in para_lower or \
                   any(word in para_lower for word in entity.get('description', '').lower().split()[:10]):
                    relevant_entities.append(str(eid))
            
            for rid, rel in sources_used.get('relationships', {}).items():
                if any(word in para_lower for word in rel.get('description', '').lower().split()[:10]):
                    relevant_rels.append(str(rid))
            
            # Add generic source references
            if relevant_entities or relevant_rels:
                relevant_sources = source_ids[:3]  # Use first few sources
            
            # Build citation
            if relevant_entities or relevant_rels or relevant_sources:
                citation_parts = []
                
                if relevant_sources:
                    citation_parts.append(f"Sources ({', '.join(map(str, relevant_sources[:5]))})")
                
                if relevant_entities:
                    citation_parts.append(f"Entities ({', '.join(relevant_entities[:7])})")
                
                if relevant_rels:
                    citation_parts.append(f"Relationships ({', '.join(relevant_rels[:5])})")
                
                citation = f" Data: {'; '.join(citation_parts)}."
                cited_paragraphs.append(para + citation)
            else:
                cited_paragraphs.append(para)
        
        return '\n\n'.join(cited_paragraphs)
    
    def _format_data_sources(self, sources_used: Dict[str, Any]) -> Dict[str, List[Any]]:
        """Format sources for display."""
        return {
            'entities': list(sources_used.get('entities', {}).values()),
            'relationships': list(sources_used.get('relationships', {}).values()),
            'sources': list(sources_used.get('sources', {}).values()),
            'text_units': list(sources_used.get('text_units', {}).values())
        }
    
    def _extract_sources_from_global_response(self, response: str) -> Dict[str, Any]:
        """Extract sources from global search response with actual data lookup."""
        sources_used = {
            'entities': {},
            'relationships': {},
            'sources': {},
            'text_units': {}
        }
        
        try:
            import pandas as pd
            
            # Load GraphRAG output files
            entities_path = self.graphrag_root / "output" / "entities.parquet"
            community_reports_path = self.graphrag_root / "output" / "community_reports.parquet"
            
            entities_df = None
            reports_df = None
            
            if entities_path.exists():
                entities_df = pd.read_parquet(entities_path)
            if community_reports_path.exists():
                reports_df = pd.read_parquet(community_reports_path)
            
            # Parse community references and entity IDs from response
            import re
            entity_matches = re.findall(r'Entities\s*\(([^)]+)\)', response)
            
            for match in entity_matches:
                ids = [id.strip() for id in match.split(',') if id.strip().replace('+more', '').strip().isdigit()]
                for entity_id_str in ids:
                    entity_id = int(entity_id_str)
                    
                    # Look up actual entity data
                    if entities_df is not None and entity_id in entities_df.index:
                        entity_row = entities_df.loc[entity_id]
                        sources_used['entities'][entity_id] = {
                            'id': entity_id,
                            'title': entity_row.get('title', f'Entity {entity_id}'),
                            'type': entity_row.get('type', 'Unknown'),
                            'description': entity_row.get('description', 'No description available')[:200]
                        }
                    else:
                        # Fallback if not found
                        sources_used['entities'][entity_id] = {
                            'id': entity_id,
                            'title': f'Entity {entity_id}',
                            'type': 'Unknown',
                            'description': 'Entity not found in output'
                        }
            
            # Parse community report references
            report_matches = re.findall(r'Reports\s*\(([^)]+)\)', response)
            
            for match in report_matches:
                ids = [id.strip() for id in match.split(',') if id.strip().replace('+more', '').strip().isdigit()]
                for report_id_str in ids:
                    report_id = int(report_id_str)
                    
                    # Look up actual community report data
                    if reports_df is not None and report_id in reports_df.index:
                        report_row = reports_df.loc[report_id]
                        sources_used['sources'][report_id] = {
                            'id': report_id,
                            'title': f"Community Report #{report_id}",
                            'type': 'community_report',
                            'text_preview': report_row.get('summary', 'No summary available')[:100]
                        }
                    else:
                        # Fallback if not found
                        sources_used['sources'][report_id] = {
                            'id': report_id,
                            'title': f'Community Report {report_id}',
                            'type': 'community_report',
                            'text_preview': 'Report not found in output'
                        }
        
        except Exception as e:
            logger.error(f"Error loading GraphRAG data for global source extraction: {e}")
            import traceback
            traceback.print_exc()
        
        return sources_used
    
    def _extract_sources_from_drift_response(self, response: str) -> Dict[str, Any]:
        """Extract sources from DRIFT search response with actual data lookup."""
        sources_used = {
            'entities': {},
            'relationships': {},
            'sources': {},
            'text_units': {}
        }
        
        try:
            import pandas as pd
            
            # Load GraphRAG output files
            entities_path = self.graphrag_root / "output" / "entities.parquet"
            relationships_path = self.graphrag_root / "output" / "relationships.parquet"
            text_units_path = self.graphrag_root / "output" / "text_units.parquet"
            
            entities_df = None
            relationships_df = None
            text_units_df = None
            
            if entities_path.exists():
                entities_df = pd.read_parquet(entities_path)
            if relationships_path.exists():
                relationships_df = pd.read_parquet(relationships_path)
            if text_units_path.exists():
                text_units_df = pd.read_parquet(text_units_path)
            
            # Parse entity references from DRIFT response
            import re
            entity_matches = re.findall(r'Entities\s*\(([^)]+)\)', response)
            
            for match in entity_matches:
                ids = [id.strip() for id in match.split(',') if id.strip().replace('+more', '').strip().isdigit()]
                for entity_id_str in ids:
                    entity_id = int(entity_id_str)
                    
                    # Look up actual entity data
                    if entities_df is not None and entity_id in entities_df.index:
                        entity_row = entities_df.loc[entity_id]
                        sources_used['entities'][entity_id] = {
                            'id': entity_id,
                            'title': entity_row.get('title', f'Entity {entity_id}'),
                            'type': entity_row.get('type', 'Unknown'),
                            'description': entity_row.get('description', 'No description available')[:200]
                        }
                    else:
                        # Fallback if not found
                        sources_used['entities'][entity_id] = {
                            'id': entity_id,
                            'title': f'Entity {entity_id}',
                            'type': 'Unknown',
                            'description': 'Entity not found in output'
                        }
            
            # Parse relationship references
            rel_matches = re.findall(r'Relationships\s*\(([^)]+)\)', response)
            
            for match in rel_matches:
                ids = [id.strip() for id in match.split(',') if id.strip().replace('+more', '').strip().isdigit()]
                for rel_id_str in ids:
                    rel_id = int(rel_id_str)
                    
                    # Look up actual relationship data
                    if relationships_df is not None and rel_id in relationships_df.index:
                        rel_row = relationships_df.loc[rel_id]
                        sources_used['relationships'][rel_id] = {
                            'id': rel_id,
                            'source': rel_row.get('source', f'Relationship {rel_id}'),
                            'target': rel_row.get('target', 'Unknown'),
                            'description': rel_row.get('description', 'No description available')[:200],
                            'weight': rel_row.get('weight', 0.0)
                        }
                    else:
                        # Fallback if not found
                        sources_used['relationships'][rel_id] = {
                            'id': rel_id,
                            'source': f'Relationship {rel_id}',
                            'target': 'Unknown',
                            'description': 'Relationship not found in output',
                            'weight': 0.0
                        }
            
            # Parse source references (text units)
            source_matches = re.findall(r'Sources\s*\(([^)]+)\)', response)
            
            for match in source_matches:
                ids = [id.strip() for id in match.split(',') if id.strip().replace('+more', '').strip().isdigit()]
                for source_id_str in ids:
                    source_id = int(source_id_str)
                    
                    # Look up actual text unit data
                    if text_units_df is not None and source_id in text_units_df.index:
                        source_row = text_units_df.loc[source_id]
                        sources_used['sources'][source_id] = {
                            'id': source_id,
                            'title': source_row.get('document_ids', f'Source {source_id}'),
                            'type': 'text_unit',
                            'text_preview': source_row.get('text', 'No text available')[:100]
                        }
                    else:
                        # Fallback if not found
                        sources_used['sources'][source_id] = {
                            'id': source_id,
                            'title': f'Source {source_id}',
                            'type': 'document',
                            'text_preview': 'Source not found in output'
                        }
        
        except Exception as e:
            logger.error(f"Error loading GraphRAG data for DRIFT source extraction: {e}")
            import traceback
            traceback.print_exc()
        
        return sources_used
    
    def _extract_sources_from_local_response(self, response: str) -> Dict[str, Any]:
        """Extract sources from local search response with actual data lookup."""
        sources_used = {
            'entities': {},
            'relationships': {},
            'sources': {},
            'text_units': {}
        }
        
        try:
            import pandas as pd
            
            # Load GraphRAG output files
            entities_path = self.graphrag_root / "output" / "entities.parquet"
            relationships_path = self.graphrag_root / "output" / "relationships.parquet"
            text_units_path = self.graphrag_root / "output" / "text_units.parquet"
            
            entities_df = None
            relationships_df = None
            text_units_df = None
            
            if entities_path.exists():
                entities_df = pd.read_parquet(entities_path)
            if relationships_path.exists():
                relationships_df = pd.read_parquet(relationships_path)
            if text_units_path.exists():
                text_units_df = pd.read_parquet(text_units_path)
            
            # Parse entity references from local response
            import re
            entity_matches = re.findall(r'Entities\s*\(([^)]+)\)', response)
            
            for match in entity_matches:
                ids = [id.strip() for id in match.split(',') if id.strip().replace('+more', '').strip().isdigit()]
                for entity_id_str in ids:
                    entity_id = int(entity_id_str)
                    
                    # Look up actual entity data
                    if entities_df is not None and entity_id in entities_df.index:
                        entity_row = entities_df.loc[entity_id]
                        sources_used['entities'][entity_id] = {
                            'id': entity_id,
                            'title': entity_row.get('title', f'Entity {entity_id}'),
                            'type': entity_row.get('type', 'Unknown'),
                            'description': entity_row.get('description', 'No description available')[:200]
                        }
                    else:
                        # Fallback if not found
                        sources_used['entities'][entity_id] = {
                            'id': entity_id,
                            'title': f'Entity {entity_id}',
                            'type': 'Unknown',
                            'description': 'Entity not found in output'
                        }
            
            # Parse relationship references
            rel_matches = re.findall(r'Relationships\s*\(([^)]+)\)', response)
            
            for match in rel_matches:
                ids = [id.strip() for id in match.split(',') if id.strip().replace('+more', '').strip().isdigit()]
                for rel_id_str in ids:
                    rel_id = int(rel_id_str)
                    
                    # Look up actual relationship data
                    if relationships_df is not None and rel_id in relationships_df.index:
                        rel_row = relationships_df.loc[rel_id]
                        sources_used['relationships'][rel_id] = {
                            'id': rel_id,
                            'source': rel_row.get('source', f'Relationship {rel_id}'),
                            'target': rel_row.get('target', 'Unknown'),
                            'description': rel_row.get('description', 'No description available')[:200],
                            'weight': rel_row.get('weight', 0.0)
                        }
                    else:
                        # Fallback if not found
                        sources_used['relationships'][rel_id] = {
                            'id': rel_id,
                            'source': f'Relationship {rel_id}',
                            'target': 'Unknown',
                            'description': 'Relationship not found in output',
                            'weight': 0.0
                        }
            
            # Parse source references (text units)
            source_matches = re.findall(r'Sources\s*\(([^)]+)\)', response)
            
            for match in source_matches:
                ids = [id.strip() for id in match.split(',') if id.strip().replace('+more', '').strip().isdigit()]
                for source_id_str in ids:
                    source_id = int(source_id_str)
                    
                    # Look up actual text unit data
                    if text_units_df is not None and source_id in text_units_df.index:
                        source_row = text_units_df.loc[source_id]
                        sources_used['sources'][source_id] = {
                            'id': source_id,
                            'title': source_row.get('document_ids', f'Source {source_id}'),
                            'type': 'text_unit',
                            'text_preview': source_row.get('text', 'No text available')[:100]
                        }
                    else:
                        # Fallback if not found
                        sources_used['sources'][source_id] = {
                            'id': source_id,
                            'title': f'Source {source_id}',
                            'type': 'document',
                            'text_preview': 'Source not found in output'
                        }
            
        except Exception as e:
            logger.error(f"Error loading GraphRAG data for source extraction: {e}")
            import traceback
            traceback.print_exc()
        
        return sources_used
    

    
    async def _execute_query(self, question: str, method: str, **kwargs) -> Dict[str, Any]:
        """Execute the actual query with original functionality."""
        
        # Auto-route if method not specified
        if method is None:
            route_info = self.router.determine_query_method(question)
            method = route_info['method']
            params = route_info['params']
            intent = route_info['intent']
            
            # Log the routing decision
            logger.info(f"Query: {question}")
            logger.info(f"Routed to: {method} (intent: {intent.value})")
            
            # Check if multiple entities detected
            if 'multiple_entities' in params:
                entity_count = len(params['multiple_entities'])
                logger.info(f"Detected {entity_count} entities in query")
                logger.info(f"Query focus: {'comparison' if params.get('comparison_mode') else 'specific' if params.get('strict_entity_focus') else 'contextual'}")
        else:
            params = kwargs
            intent = None
        
        # Execute query based on method
        if method == "global":
            result = await self._execute_global_query(question, params)
        elif method == "local":
            result = await self._execute_local_query(question, params)
        elif method == "drift":
            result = await self._execute_drift_query(question, params)
        else:
            raise ValueError(f"Unknown query method: {method}")
        
        # Add routing metadata to result
        result['routing_metadata'] = {
            'detected_intent': self._get_intent_type(params),
            'community_context_enabled': params.get('include_community_context', True),
            'query_method': method,
            'entity_count': len(params.get('multiple_entities', [])) if 'multiple_entities' in params else 1 if 'entity_filter' in params else 0
        }
        
        # Extract comprehensive source information
        sources_info = self._extract_sources_from_response(
            result.get('answer', ''), 
            result.get('query_type', method)
        )
        
        # Add both the sources info and the entity chunks if this is a local search
        result['sources_info'] = sources_info
        
        # For local searches, also get the actual entity chunks that were used
        if method == "local" or result.get('query_type') == 'local':
            result['entity_chunks'] = await self._get_entity_chunks(question, params)
        
        return result
    
    async def _extract_data_sources(self, result: Dict[str, Any], method: str) -> Dict[str, Any]:
        """Extract entities, relationships, and sources from query result."""
        data_sources = {
            'entities': [],
            'relationships': [],
            'sources': [],
            'communities': [],
            'text_units': []
        }
        
        try:
            # For local search
            if method == 'local' and 'context_data' in result:
                context = result['context_data']
                
                # Extract entity IDs and details
                if 'entities' in context:
                    entities_df = context['entities']
                    data_sources['entities'] = [
                        {
                            'id': idx,
                            'title': row.get('title', 'Unknown'),
                            'type': row.get('type', 'Unknown'),
                            'description': row.get('description', '')[:100] + '...' if len(row.get('description', '')) > 100 else row.get('description', '')
                        }
                        for idx, row in entities_df.iterrows()
                    ]
                
                # Extract relationship IDs and details
                if 'relationships' in context:
                    relationships_df = context['relationships']
                    data_sources['relationships'] = [
                        {
                            'id': idx,
                            'source': row.get('source', ''),
                            'target': row.get('target', ''),
                            'description': row.get('description', '')[:100] + '...',
                            'weight': row.get('weight', 0)
                        }
                        for idx, row in relationships_df.iterrows()
                    ]
                
                # Extract source documents
                if 'sources' in context:
                    sources_df = context['sources']
                    data_sources['sources'] = [
                        {
                            'id': idx,
                            'title': row.get('title', 'Unknown'),
                            'chunk_id': row.get('chunk_id', ''),
                            'document_type': row.get('document_type', 'Unknown')
                        }
                        for idx, row in sources_df.iterrows()
                    ]
            
            # For global search
            elif method == 'global' and 'context_data' in result:
                context = result['context_data']
                
                # Extract community information
                if 'communities' in context:
                    communities = context['communities']
                    data_sources['communities'] = [
                        {
                            'id': comm.get('id', ''),
                            'title': comm.get('title', 'Community'),
                            'level': comm.get('level', 0),
                            'entity_count': len(comm.get('entities', []))
                        }
                        for comm in communities
                    ]
                
                # Extract entities from communities
                for comm in context.get('communities', []):
                    for entity_id in comm.get('entities', []):
                        # Load entity details
                        entity = await self._get_entity_by_id(entity_id)
                        if entity:
                            data_sources['entities'].append({
                                'id': entity_id,
                                'title': entity.get('title', 'Unknown'),
                                'type': entity.get('type', 'Unknown'),
                                'from_community': comm.get('id', '')
                            })
            
            # Extract text units if available
            if 'text_units' in result.get('context_data', {}):
                text_units = result['context_data']['text_units']
                data_sources['text_units'] = [
                    {
                        'id': unit.get('id', ''),
                        'chunk_id': unit.get('chunk_id', ''),
                        'document': unit.get('document', ''),
                        'text_preview': unit.get('text', '')[:100] + '...'
                    }
                    for unit in text_units[:10]  # Limit to first 10
                ]
                
        except Exception as e:
            logger.error(f"Error extracting data sources: {e}")
            import traceback
            traceback.print_exc()
        
        return data_sources
    
    async def _get_entity_by_id(self, entity_id: int):
        """Get entity details by ID."""
        try:
            entities_path = self.graphrag_root / "output" / "entities.parquet"
            if entities_path.exists():
                import pandas as pd
                entities_df = pd.read_parquet(entities_path)
                if entity_id in entities_df.index:
                    return entities_df.loc[entity_id].to_dict()
        except Exception as e:
            logger.error(f"Error loading entity {entity_id}: {e}")
        return None

    async def _extract_local_context(self, query: str, **kwargs) -> Dict[str, Any]:
        """Manually extract context data for local search."""
        context = {
            'entities': None,
            'relationships': None,
            'sources': None,
            'text_units': []
        }
        
        try:
            import pandas as pd
            
            # Load data files
            entities_path = self.graphrag_root / "output" / "entities.parquet"
            relationships_path = self.graphrag_root / "output" / "relationships.parquet"
            text_units_path = self.graphrag_root / "output" / "text_units.parquet"
            
            # Get top-k entities
            if entities_path.exists():
                entities_df = pd.read_parquet(entities_path)
                
                # Filter based on query relevance (simple keyword matching for now)
                query_terms = query.lower().split()
                relevant_entities = []
                
                for idx, entity in entities_df.iterrows():
                    title = str(entity.get('title', '')).lower()
                    description = str(entity.get('description', '')).lower()
                    
                    # Check if any query term matches
                    if any(term in title or term in description for term in query_terms):
                        relevant_entities.append(idx)
                
                # Get top-k relevant entities
                top_k = kwargs.get('top_k_entities', 10)
                context['entities'] = entities_df.loc[relevant_entities[:top_k]]
                
                # Get relationships for these entities
                if relationships_path.exists() and len(relevant_entities) > 0:
                    relationships_df = pd.read_parquet(relationships_path)
                    
                    # Filter relationships involving our entities
                    entity_set = set(relevant_entities[:top_k])
                    relevant_rels = relationships_df[
                        relationships_df['source'].isin(entity_set) | 
                        relationships_df['target'].isin(entity_set)
                    ]
                    
                    context['relationships'] = relevant_rels
            
            # Get text units
            if text_units_path.exists():
                text_units_df = pd.read_parquet(text_units_path)
                
                # Get relevant text units (simplified - in practice would use embeddings)
                relevant_units = []
                for idx, unit in text_units_df.iterrows():
                    text = str(unit.get('text', '')).lower()
                    if any(term in text for term in query.lower().split()):
                        relevant_units.append({
                            'id': idx,
                            'text': unit.get('text', ''),
                            'chunk_id': unit.get('chunk_id', ''),
                            'document': unit.get('document', '')
                        })
                
                context['text_units'] = relevant_units[:10]
            
            # Extract source documents
            if context['entities'] is not None and not context['entities'].empty:
                # Get unique source documents from entities
                sources = []
                for idx, entity in context['entities'].iterrows():
                    if 'source_document' in entity:
                        sources.append({
                            'id': len(sources),
                            'title': entity['source_document'],
                            'document_type': entity.get('document_type', 'Unknown'),
                            'chunk_id': entity.get('chunk_id', '')
                        })
                
                # Deduplicate sources
                seen = set()
                unique_sources = []
                for source in sources:
                    key = source['title']
                    if key not in seen:
                        seen.add(key)
                        unique_sources.append(source)
                
                context['sources'] = pd.DataFrame(unique_sources)
                
        except Exception as e:
            logger.error(f"Error extracting context: {e}")
            import traceback
            traceback.print_exc()
        
        return context

    def _get_intent_type(self, params: Dict) -> str:
        """Determine intent type from parameters."""
        if params.get('comparison_mode'):
            return 'comparison'
        elif params.get('strict_entity_focus'):
            return 'specific_entity'
        elif params.get('focus_on_relationships'):
            return 'relationships'
        else:
            return 'contextual'
    
    async def _execute_global_query(self, question: str, params: Dict) -> Dict[str, Any]:
        """Execute a global search query."""
        cmd = [
            self._get_python_executable(),
            "-m", "graphrag", "query",
            "--root", str(self.graphrag_root),
            "--method", "global"
        ]
        
        if "community_level" in params:
            cmd.extend(["--community-level", str(params["community_level"])])
        
        cmd.extend(["--query", question])
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        # Clean JSON artifacts from the response
        answer = self._clean_json_artifacts(result.stdout)
        
        return {
            "query": question,
            "query_type": "global",
            "answer": answer,
            "context": self._extract_context(result.stdout),
            "parameters": params
        }
    
    async def _execute_local_query(self, question: str, params: Dict) -> Dict[str, Any]:
        """Execute a local search query with available GraphRAG options."""
        
        # Handle multiple entities
        if "multiple_entities" in params:
            return await self._execute_multi_entity_query(question, params)
        
        # Single entity query - use available GraphRAG options
        cmd = [
            self._get_python_executable(),
            "-m", "graphrag", "query",
            "--root", str(self.graphrag_root),
            "--method", "local"
        ]
        
        # Use community-level to control context (available option)
        if params.get("disable_community", False):
            # Use highest community level to get most specific results
            cmd.extend(["--community-level", "3"])  
            logger.info("Using high community level (3) for specific entity query")
        else:
            # Use default community level for broader context
            cmd.extend(["--community-level", "2"])
            logger.info("Using default community level (2) for contextual query")
        
        # If we have entity filtering request, modify the query to be more specific
        if "entity_filter" in params:
            filter_info = params["entity_filter"]
            entity_type = filter_info['type'].replace('_', ' ').lower()
            entity_value = filter_info['value']
            
            if params.get("strict_entity_focus", False):
                # Make query more specific to focus on just this entity
                enhanced_question = f"Tell me specifically about {entity_type} {entity_value}. Focus only on {entity_value} and do not include information about other items."
                logger.info(f"Enhanced query for strict focus on {entity_value}")
            else:
                # Keep original query but mention the entity
                enhanced_question = f"{question} (specifically about {entity_type} {entity_value})"
                logger.info(f"Enhanced query for contextual information about {entity_value}")
            
            question = enhanced_question
        
        cmd.extend(["--query", question])
        
        logger.debug(f"Executing command: {' '.join(cmd)}")
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        answer = result.stdout
        
        # Clean JSON artifacts from the response
        answer = self._clean_json_artifacts(answer)
        
        # Post-process if strict entity focus is requested
        if params.get("strict_entity_focus", False) and "entity_filter" in params:
            answer = self._filter_to_specific_entity(answer, params["entity_filter"]["value"])
        
        return {
            "query": question,
            "query_type": "local",
            "answer": answer,
            "context": self._extract_context(answer),
            "parameters": params,
            "intent_detection": {
                "specific_entity_focus": params.get("strict_entity_focus", False),
                "community_level_used": 3 if params.get("disable_community") else 2,
                "query_enhanced": "entity_filter" in params
            }
        }
    
    async def _execute_multi_entity_query(self, question: str, params: Dict) -> Dict[str, Any]:
        """Execute queries for multiple entities using available GraphRAG options."""
        entities = params["multiple_entities"]
        all_results = []
        
        # Determine query strategy based on intent
        if params.get("aggregate_results") and params.get("strict_entity_focus"):
            # Query each entity separately with high community level
            logger.info(f"Executing separate queries for {len(entities)} entities")
            
            for entity in entities:
                cmd = [
                    self._get_python_executable(),
                    "-m", "graphrag", "query",
                    "--root", str(self.graphrag_root),
                    "--method", "local",
                    "--community-level", "3"  # High level for specific results
                ]
                
                # Create entity-specific query
                entity_type = entity['type'].replace('_', ' ').lower()
                entity_query = f"Tell me specifically about {entity_type} {entity['value']}. Focus only on {entity['value']}."
                cmd.extend(["--query", entity_query])
                
                result = subprocess.run(cmd, capture_output=True, text=True)
                all_results.append({
                    "entity": entity,
                    "answer": result.stdout
                })
            
            # Combine results
            combined_answer = self._format_multiple_entity_results(all_results, params)
            
        elif params.get("comparison_mode"):
            # Query with all entities for comparison
            logger.info(f"Executing comparison query for entities: {[e['value'] for e in entities]}")
            
            entity_values = [e['value'] for e in entities]
            comparison_query = f"Compare and contrast {' and '.join(entity_values)}. What are the similarities and differences between these items?"
            
            cmd = [
                self._get_python_executable(),
                "-m", "graphrag", "query",
                "--root", str(self.graphrag_root),
                "--method", "local",
                "--community-level", "2",  # Medium level for comparison context
                "--query", comparison_query
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True)
            combined_answer = self._format_comparison_results(result.stdout, entities)
            
        else:
            # Query for relationships between entities
            logger.info(f"Executing relationship query for entities: {[e['value'] for e in entities]}")
            
            entity_values = [e['value'] for e in entities]
            relationship_query = f"How do {' and '.join(entity_values)} relate to each other? What connections exist between these items?"
            
            cmd = [
                self._get_python_executable(),
                "-m", "graphrag", "query",
                "--root", str(self.graphrag_root),
                "--method", "local",
                "--community-level", "1",  # Lower level for broader relationships
                "--query", relationship_query
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True)
            combined_answer = result.stdout
        
        # Clean JSON artifacts from the combined answer
        combined_answer = self._clean_json_artifacts(combined_answer)
        
        return {
            "query": question,
            "query_type": "local",
            "answer": combined_answer,
            "context": self._extract_context(combined_answer),
            "parameters": params,
            "intent_detection": {
                "multi_entity_query": True,
                "entity_count": len(entities),
                "query_mode": "comparison" if params.get("comparison_mode") else "aggregate" if params.get("aggregate_results") else "relationships"
            }
        }
    
    def _format_multiple_entity_results(self, results: List[Dict], params: Dict) -> str:
        """Format results from multiple individual entity queries."""
        formatted = []
        
        formatted.append(f"Information about {len(results)} requested items:\n")
        
        for i, result in enumerate(results, 1):
            entity = result['entity']
            answer = result['answer'].strip()
            
            formatted.append(f"\n{i}. {entity['type'].replace('_', ' ').title()} {entity['value']}:")
            formatted.append("-" * 50)
            
            # Clean and format the answer
            if answer:
                # Remove any GraphRAG metadata/headers and JSON artifacts
                clean_answer = self._clean_graphrag_output(answer)
                clean_answer = self._clean_json_artifacts(clean_answer)
                formatted.append(clean_answer)
            else:
                formatted.append(f"No information found for {entity['value']}")
        
        return "\n".join(formatted)
    
    def _format_comparison_results(self, raw_answer: str, entities: List[Dict]) -> str:
        """Format comparison results to highlight differences and similarities."""
        # Clean JSON artifacts from the raw answer
        clean_answer = self._clean_json_artifacts(raw_answer)
        
        # This could be enhanced with more sophisticated formatting
        formatted = [f"Comparison of {', '.join([e['value'] for e in entities])}:\n"]
        formatted.append(clean_answer)
        
        return "\n".join(formatted)
    
    def _clean_graphrag_output(self, output: str) -> str:
        """Remove GraphRAG metadata and format output cleanly."""
        # Remove common GraphRAG headers/footers
        lines = output.split('\n')
        cleaned_lines = []
        
        for line in lines:
            # Skip metadata lines
            if line.startswith('INFO:') or line.startswith('WARNING:') or line.startswith('DEBUG:'):
                continue
            # Skip empty lines at start/end
            if not line.strip() and (not cleaned_lines or len(cleaned_lines) == len(lines) - 1):
                continue
            cleaned_lines.append(line)
        
        return '\n'.join(cleaned_lines).strip()
    
    async def _execute_drift_query(self, question: str, params: Dict) -> Dict[str, Any]:
        """Execute a DRIFT search query."""
        cmd = [
            self._get_python_executable(),
            "-m", "graphrag", "query",
            "--root", str(self.graphrag_root),
            "--method", "drift"
        ]
        
        cmd.extend(["--query", question])
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        # Clean JSON artifacts from the response
        answer = self._clean_json_artifacts(result.stdout)
        
        return {
            "query": question,
            "query_type": "drift",
            "answer": answer,
            "context": self._extract_context(result.stdout),
            "parameters": params
        }
    
    def _filter_to_specific_entity(self, response: str, target_entity: str) -> str:
        """Aggressively filter response to ONLY information about the target entity."""
        if not target_entity or not response:
            return response
        
        # Split response into sentences
        sentences = response.split('.')
        filtered_sentences = []
        
        for sentence in sentences:
            # Only keep sentences that explicitly mention the target entity
            if target_entity in sentence:
                # Check if any other entity codes are mentioned
                other_entities = re.findall(r'\b[A-Z]-\d+\b', sentence)
                other_entities = [e for e in other_entities if e != target_entity]
                
                # Only keep if no other entities are mentioned
                if not other_entities:
                    filtered_sentences.append(sentence.strip())
        
        if filtered_sentences:
            filtered_response = '. '.join(filtered_sentences) + '.'
            filtered_response = f"Information specifically about {target_entity}:\n\n{filtered_response}"
        else:
            filtered_response = f"Specific information about {target_entity} only."
        
        return filtered_response
    
    def _is_paragraph_about_target(self, paragraph: str, target: str, all_entities: set) -> bool:
        """Determine if a paragraph should be kept in filtered response."""
        if target not in paragraph:
            return False
        
        other_entities = all_entities - {target}
        if not any(entity in paragraph for entity in other_entities):
            return True
        
        target_count = paragraph.count(target)
        other_counts = sum(paragraph.count(entity) for entity in other_entities)
        
        return target_count >= other_counts
    
    def _extract_sources_from_response(self, response: str, method: str) -> Dict[str, Any]:
        """Extract and resolve all source references from GraphRAG response."""
        sources_info = {
            'entities': [],
            'reports': [],
            'raw_references': {},
            'resolved_sources': []
        }
        
        import re
        
        # Parse all reference patterns
        entities_pattern = r'Entities\s*\(([^)]+)\)'
        reports_pattern = r'Reports\s*\(([^)]+)\)'
        sources_pattern = r'Sources\s*\(([^)]+)\)'
        data_pattern = r'Data:\s*(?:Sources\s*\([^)]+\);\s*)?(?:Entities\s*\([^)]+\)|Reports\s*\([^)]+\))'
        
        # Extract all matches
        entities_matches = re.findall(entities_pattern, response)
        reports_matches = re.findall(reports_pattern, response)
        sources_matches = re.findall(sources_pattern, response)
        
        # Store raw references
        sources_info['raw_references'] = {
            'entities': entities_matches,
            'reports': reports_matches,
            'sources': sources_matches
        }
        
        # Parse entity IDs
        all_entity_ids = []
        for match in entities_matches:
            ids = [id.strip() for id in match.split(',') if id.strip().replace('+more', '').strip().isdigit()]
            all_entity_ids.extend(ids)
        
        # Parse report IDs
        all_report_ids = []
        for match in reports_matches:
            ids = [id.strip() for id in match.split(',') if id.strip().replace('+more', '').strip().isdigit()]
            all_report_ids.extend(ids)
        
        # Load and resolve entities
        if all_entity_ids:
            try:
                entities_path = self.graphrag_root / "output/entities.parquet"
                if entities_path.exists():
                    import pandas as pd
                    entities_df = pd.read_parquet(entities_path)
                    
                    for entity_id in all_entity_ids:
                        try:
                            entity_idx = int(entity_id)
                            if entity_idx in entities_df.index:
                                entity = entities_df.loc[entity_idx]
                                sources_info['entities'].append({
                                    'id': entity_idx,
                                    'title': entity['title'],
                                    'type': entity['type'],
                                    'description': entity.get('description', '')[:300]
                                })
                        except Exception as e:
                            logger.error(f"Failed to resolve entity {entity_id}: {e}")
            except Exception as e:
                logger.error(f"Failed to load entities: {e}")
        
        # Load and resolve community reports
        if all_report_ids:
            try:
                reports_path = self.graphrag_root / "output/community_reports.parquet"
                if reports_path.exists():
                    import pandas as pd
                    reports_df = pd.read_parquet(reports_path)
                    
                    for report_id in all_report_ids:
                        try:
                            report_idx = int(report_id)
                            if report_idx in reports_df.index:
                                report = reports_df.loc[report_idx]
                                sources_info['reports'].append({
                                    'id': report_idx,
                                    'title': f"Community Report #{report_idx}",
                                    'summary': report.get('summary', '')[:300] if 'summary' in report else str(report)[:300],
                                    'level': report.get('level', 'unknown') if 'level' in report else 'unknown'
                                })
                        except Exception as e:
                            logger.error(f"Failed to resolve report {report_id}: {e}")
            except Exception as e:
                logger.error(f"Failed to load reports: {e}")
        
        # Create resolved sources list combining everything
        sources_info['resolved_sources'] = sources_info['entities'] + sources_info['reports']
        
        return sources_info

    async def _get_retrieved_entities(self, query: str, params: Dict) -> List[Dict]:
        """Get the actual entities that GraphRAG retrieved for this query."""
        source_entities = []
        
        try:
            entities_path = self.graphrag_root / "output/entities.parquet"
            text_units_path = self.graphrag_root / "output/text_units.parquet"
            relationships_path = self.graphrag_root / "output/relationships.parquet"
            
            if entities_path.exists():
                import pandas as pd
                entities_df = pd.read_parquet(entities_path)
                
                # If we have an entity filter, find that specific entity
                if 'entity_filter' in params:
                    filter_info = params['entity_filter']
                    entity_value = filter_info['value']
                    entity_type = filter_info['type']
                    
                    # Find matching entities by type and value
                    matches = entities_df[
                        (entities_df['type'].str.upper() == entity_type.upper()) &
                        (entities_df['title'].str.contains(entity_value, case=False, na=False))
                    ]
                    
                    # Also find related entities through relationships
                    if relationships_path.exists() and not matches.empty:
                        relationships_df = pd.read_parquet(relationships_path)
                        
                        for _, entity in matches.iterrows():
                            entity_id = entity.name  # Index is the entity ID
                            
                            # Find all relationships involving this entity
                            related_rels = relationships_df[
                                (relationships_df['source'] == entity_id) | 
                                (relationships_df['target'] == entity_id)
                            ]
                            
                            # Add the main entity
                            source_entities.append({
                                'entity_id': entity_id,
                                'title': entity['title'],
                                'type': entity['type'],
                                'description': entity.get('description', '')[:500],
                                'is_primary': True,
                                'source_document': self._trace_entity_to_document(entity_id, entity['title'])
                            })
                            
                            # Add related entities
                            for _, rel in related_rels.iterrows():
                                other_id = rel['target'] if rel['source'] == entity_id else rel['source']
                                if other_id in entities_df.index:
                                    related_entity = entities_df.loc[other_id]
                                    source_entities.append({
                                        'entity_id': other_id,
                                        'title': related_entity['title'],
                                        'type': related_entity['type'],
                                        'description': related_entity.get('description', '')[:300],
                                        'is_primary': False,
                                        'relationship': rel['description'],
                                        'source_document': self._trace_entity_to_document(other_id, related_entity['title'])
                                    })
        
        except Exception as e:
            logger.error(f"Failed to get retrieved entities: {e}")
        
        return source_entities

    async def _get_entity_chunks(self, query: str, params: Dict) -> List[Dict]:
        """Get the actual text chunks/entities that were retrieved."""
        chunks = []
        
        try:
            # Load entities and text units
            entities_path = self.graphrag_root / "output/entities.parquet"
            text_units_path = self.graphrag_root / "output/text_units.parquet"
            
            if entities_path.exists():
                import pandas as pd
                entities_df = pd.read_parquet(entities_path)
                
                # If text units exist, load them too
                text_units_df = None
                if text_units_path.exists():
                    text_units_df = pd.read_parquet(text_units_path)
                
                # Get entities based on the query parameters
                if 'entity_filter' in params:
                    filter_info = params['entity_filter']
                    entity_value = filter_info['value']
                    entity_type = filter_info['type']
                    
                    # Find all matching entities
                    matches = entities_df[
                        (entities_df['type'].str.upper() == entity_type.upper()) & 
                        (entities_df['title'].str.contains(entity_value, case=False, na=False))
                    ]
                    
                    # Also get related entities by looking at descriptions
                    related = entities_df[
                        entities_df['description'].str.contains(entity_value, case=False, na=False)
                    ]
                    
                    all_matches = pd.concat([matches, related]).drop_duplicates()
                    
                    # Convert to chunks format
                    for _, entity in all_matches.iterrows():
                        chunk = {
                            'entity_id': entity.name,
                            'type': entity['type'],
                            'title': entity['title'],
                            'description': entity.get('description', ''),
                            'source': self._trace_entity_to_document(entity.name, entity['title'])
                        }
                        chunks.append(chunk)
        
        except Exception as e:
            logger.error(f"Failed to get entity chunks: {e}")
        
        return chunks

    def _trace_entity_to_document(self, entity_id, entity_title: str) -> Dict:
        """Trace an entity back to its source document."""
        try:
            csv_path = self.graphrag_root / "city_clerk_documents.csv"
            if csv_path.exists():
                import pandas as pd
                docs_df = pd.read_csv(csv_path)
                
                # Extract potential item code from entity title
                import re
                item_match = re.search(r'([A-Z]-\d+)', entity_title)
                doc_match = re.search(r'(\d{4}-\d+)', entity_title)
                
                # Try to find matching document
                for _, doc in docs_df.iterrows():
                    # Check if entity matches document identifiers
                    if (item_match and item_match.group(1) == doc.get('item_code')) or \
                       (doc_match and doc_match.group(1) in str(doc.get('document_number', ''))) or \
                       (entity_title.lower() in str(doc.get('title', '')).lower()):
                        return {
                            'document_id': doc['id'],
                            'title': doc.get('title', ''),
                            'type': doc.get('document_type', ''),
                            'meeting_date': doc.get('meeting_date', ''),
                            'source_file': doc.get('source_file', '')
                        }
        except Exception as e:
            logger.error(f"Failed to trace entity to document: {e}")
        
        return {}
    
    def _extract_context(self, response: str) -> List[Dict]:
        """Extract context and sources from response."""
        context = []
        # Parse response for entity references and sources
        return context

# Legacy compatibility class
class CityClerkGraphRAGQuery(CityClerkQueryEngine):
    """Legacy compatibility wrapper."""
    
    async def query(self, 
                    question: str, 
                    query_type: QueryType = QueryType.LOCAL,
                    community_level: int = 0) -> Dict[str, Any]:
        """Legacy query method for backward compatibility."""
        return await super().query(
            question=question,
            method=query_type.value,
            community_level=community_level
        )

# Example usage function
async def handle_user_query(question: str, graphrag_root: Path = None):
    """Handle user query with intelligent routing."""
    if graphrag_root is None:
        graphrag_root = Path("./graphrag_data")
    
    engine = CityClerkQueryEngine(graphrag_root)
    result = await engine.query(question)
    
    print(f"Query: {question}")
    print(f"Selected method: {result['query_type']}")
    print(f"Detected entities: {result['routing_metadata'].get('entity_count', 0)}")
    print(f"Query intent: {result['routing_metadata'].get('detected_intent', 'unknown')}")
    
    return result


================================================================================


################################################################################
# File: scripts/graph_stages/agenda_pdf_extractor.py
################################################################################

# scripts/graph_stages/agenda_pdf_extractor.py
"""
PDF Extractor for City Clerk Agenda Documents
Extracts text, structure, and hyperlinks from agenda PDFs.
"""

import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import json
import re
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import PdfPipelineOptions
from groq import Groq
import os
import fitz  # PyMuPDF for hyperlink extraction
from functools import lru_cache
import hashlib
import asyncio

log = logging.getLogger(__name__)


class AgendaPDFExtractor:
    """Extract structured content from agenda PDFs using Docling and LLM."""
    
    def __init__(self, output_dir: Optional[Path] = None):
        """Initialize the agenda PDF extractor."""
        self.output_dir = output_dir or Path("city_clerk_documents/extracted_text")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize Docling converter with OCR enabled
        pipeline_options = PdfPipelineOptions()
        pipeline_options.do_ocr = True  # Enable OCR for better text extraction
        pipeline_options.do_table_structure = True  # Better table extraction
        
        self.converter = DocumentConverter(
            format_options={
                InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
            }
        )
        
        # Initialize OpenAI client for LLM extraction
        self.client = Groq()
        self.model = "gpt-4.1-mini-2025-04-14"
        
        # Initialize extraction cache
        self._extraction_cache = {}
    
    async def extract_agenda(self, pdf_path: Path) -> Dict[str, any]:
        """Extract agenda with caching."""
        log.info(f"📄 Extracting agenda from {pdf_path.name}")
        
        # Check cache first
        file_hash = self._get_file_hash(pdf_path)
        if file_hash in self._extraction_cache:
            log.info(f"📋 Using cached extraction for {pdf_path.name}")
            return self._extraction_cache[file_hash]
        
        # Convert with Docling - pass path directly
        result = self.converter.convert(str(pdf_path))
        
        # Get the document
        doc = result.document
        
        # Get full text and markdown
        full_text = doc.export_to_markdown() or ""
        
        # Use LLM to extract structured agenda items
        log.info("🧠 Using LLM to extract agenda structure...")
        extracted_items = self._extract_agenda_items_with_llm(full_text)
        
        # Build sections from extracted items
        sections = self._build_sections_from_items(extracted_items, full_text)
        
        # Extract hyperlinks using PyMuPDF
        hyperlinks = self._extract_hyperlinks_pymupdf(pdf_path)
        
        # Parallelize item extraction
        agenda_items_with_urls = extracted_items
        if extracted_items:
            # Process items in parallel batches
            batch_size = 5
            all_enhanced_items = []
            
            for i in range(0, len(extracted_items), batch_size):
                batch = extracted_items[i:i + batch_size]
                
                # Create tasks for parallel execution
                tasks = [
                    self._extract_item_details_async(item, sections)
                    for item in batch
                ]
                
                # Execute batch in parallel
                batch_results = await asyncio.gather(*tasks, return_exceptions=True)
                
                for result, original_item in zip(batch_results, batch):
                    if isinstance(result, Exception):
                        log.error(f"Error extracting item {original_item.get('item_code')}: {result}")
                        all_enhanced_items.append(original_item)
                    else:
                        all_enhanced_items.append(result)
            
            extracted_items = all_enhanced_items
        
        # Associate hyperlinks with agenda items
        agenda_items_with_urls = self._associate_urls_with_items(extracted_items, hyperlinks, full_text)
        
        # Create agenda data structure with both raw and structured data
        agenda_data = {
            'source_file': pdf_path.name,
            'full_text': full_text,
            'sections': sections,
            'agenda_items': agenda_items_with_urls,  # Updated with URLs
            'hyperlinks': hyperlinks,
            'meeting_info': self._extract_meeting_info(pdf_path, full_text),
            'metadata': {
                'extraction_method': 'docling+llm+pymupdf',
                'num_sections': len(sections),
                'num_items': self._count_items(agenda_items_with_urls),
                'num_hyperlinks': len(hyperlinks)
            }
        }
        
        # Cache result
        self._extraction_cache[file_hash] = agenda_data
        
        # Save using the new save method
        output_file = self.output_dir / f"{pdf_path.stem}_extracted.json"
        self.save_extracted_agenda(agenda_data, output_file)
        
        log.info(f"✅ Extraction complete: {len(sections)} sections, {self._count_items(agenda_items_with_urls)} items, {len(hyperlinks)} hyperlinks")
        log.info(f"✅ Saved extracted data to: {output_file}")
        
        return agenda_data
    
    # Add async version of item extraction
    async def _extract_item_details_async(self, item: Dict, pages_dict: Dict[int, str]) -> Dict:
        """Async version of item detail extraction."""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            None, 
            self._extract_item_details, 
            item, 
            pages_dict
        )
    
    def _extract_item_details(self, item: Dict, pages_dict: Dict[int, str]) -> Dict:
        """Extract detailed information for an agenda item."""
        # This is a placeholder implementation - enhance with actual detail extraction logic
        # For now, just return the item as-is
        return item
    
    def _get_file_hash(self, file_path: Path) -> str:
        """Get hash of file for caching."""
        with open(file_path, 'rb') as f:
            return hashlib.md5(f.read()).hexdigest()

    def save_extracted_agenda(self, agenda_data: dict, output_path: Path):
        """Save extracted agenda data to JSON file."""
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(agenda_data, f, indent=2, ensure_ascii=False)
        
        log.info(f"✅ Saved extracted agenda to: {output_path}")
        
        # NEW: Also save as markdown
        self._save_agenda_as_markdown(agenda_data, output_path)

    def _save_agenda_as_markdown(self, agenda_data: dict, json_path: Path):
        """Save agenda as enhanced markdown for GraphRAG."""
        markdown_dir = json_path.parent.parent / "extracted_markdown"
        markdown_dir.mkdir(exist_ok=True)
        
        meeting_info = agenda_data.get('meeting_info', {})
        meeting_date = meeting_info.get('date', None)
        
        # If date not found in meeting_info, extract from filename
        if not meeting_date or meeting_date == 'N/A':
            import re
            # Try to extract from the JSON filename first
            # Pattern: "Agenda 01.9.2024_extracted.json"
            filename_match = re.search(r'Agenda[_ ](\d{1,2})\.(\d{1,2})\.(\d{4})', json_path.name)
            if filename_match:
                month = filename_match.group(1).zfill(2)
                day = filename_match.group(2).zfill(2)
                year = filename_match.group(3)
                meeting_date = f"{month}.{day}.{year}"
                log.info(f"📅 Extracted date from filename: {meeting_date}")
            else:
                # Last resort: check the original PDF name in agenda_data
                source_file = agenda_data.get('source_file', '')
                pdf_match = re.search(r'(\d{1,2})\.(\d{1,2})\.(\d{4})', source_file)
                if pdf_match:
                    month = pdf_match.group(1).zfill(2)
                    day = pdf_match.group(2).zfill(2)
                    year = pdf_match.group(3)
                    meeting_date = f"{month}.{day}.{year}"
                    log.info(f"📅 Extracted date from source file: {meeting_date}")
                else:
                    meeting_date = 'unknown'
                    log.warning("⚠️ Could not extract meeting date from any source")
        
        # Convert to underscore format for filename
        if meeting_date != 'unknown':
            meeting_date_filename = meeting_date.replace('.', '_')
        else:
            meeting_date_filename = 'unknown'
        
        # Build comprehensive header
        header = self._build_agenda_header(agenda_data)
        
        # Add detailed agenda items section
        items_section = self._build_agenda_items_section(agenda_data)
        
        # Combine with full text
        full_content = header + items_section + "\n\n# FULL AGENDA TEXT\n\n" + agenda_data.get('full_text', '')
        
        # Save markdown
        md_filename = f"agenda_{meeting_date_filename}.md"
        md_path = markdown_dir / md_filename
        
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(full_content)
        
        log.info(f"📝 Saved agenda markdown to: {md_path}")

    def _build_agenda_header(self, agenda_data: dict) -> str:
        """Build comprehensive agenda header."""
        meeting_info = agenda_data.get('meeting_info', {})
        agenda_items = agenda_data.get('agenda_items', [])
        
        all_item_codes = [item.get('item_code', '') for item in agenda_items if item.get('item_code')]
        
        header = f"""---
DOCUMENT METADATA AND CONTEXT
=============================

**DOCUMENT IDENTIFICATION:**
- Document Type: AGENDA
- Meeting Date: {meeting_info.get('date', 'N/A')}

**ENTITIES IN THIS DOCUMENT:**
{self._format_agenda_entities(all_item_codes)}

**SEARCHABLE IDENTIFIERS:**
- DOCUMENT_TYPE: AGENDA
{self._format_item_identifiers(all_item_codes)}

---

"""
        return header

    def _format_agenda_entities(self, item_codes: list) -> str:
        """Format agenda item entities."""
        lines = []
        for code in item_codes[:10]:
            lines.append(f"- AGENDA_ITEM: {code}")
        if len(item_codes) > 10:
            lines.append(f"- ... and {len(item_codes) - 10} more items")
        return '\n'.join(lines)

    def _format_item_identifiers(self, item_codes: list) -> str:
        """Format item identifiers."""
        lines = []
        for code in item_codes:
            lines.append(f"- AGENDA_ITEM: {code}")
        return '\n'.join(lines)

    def _build_agenda_items_section(self, agenda_data: dict) -> str:
        """Build agenda items section."""
        lines = ["## AGENDA ITEMS QUICK REFERENCE\n"]
        
        for item in agenda_data.get('agenda_items', []):
            item_code = item.get('item_code', 'UNKNOWN')
            lines.append(f"### Agenda Item {item_code}")
            lines.append(f"**Title:** {item.get('title', 'N/A')}")
            lines.append(f"\n**What is Item {item_code}?**")
            lines.append(f"Item {item_code} is '{item.get('title', 'N/A')}'")
            lines.append("")
        
        return '\n'.join(lines)

    def _extract_meeting_info(self, pdf_path: Path, full_text: str) -> dict:
        """Extract meeting information from the agenda."""
        meeting_info = {
            'date': 'N/A',
            'time': 'N/A',
            'location': 'N/A'
        }
        
        # Try to extract date from filename first
        import re
        date_match = re.search(r'(\d{2})\.(\d{2})\.(\d{4})', pdf_path.name)
        if date_match:
            month, day, year = date_match.groups()
            meeting_info['date'] = f"{month}.{day}.{year}"
        
        # Try to extract time and location from text
        lines = full_text.split('\n')[:50]  # Check first 50 lines
        for line in lines:
            line = line.strip()
            # Look for time patterns
            time_match = re.search(r'(\d{1,2}:\d{2}\s*[AP]M)', line, re.IGNORECASE)
            if time_match and meeting_info['time'] == 'N/A':
                meeting_info['time'] = time_match.group(1)
            
            # Look for location
            if 'city hall' in line.lower() or 'commission chamber' in line.lower():
                meeting_info['location'] = line[:100]  # Limit length
        
        return meeting_info
    
    def _extract_hyperlinks_pymupdf(self, pdf_path: Path) -> List[Dict[str, any]]:
        """Extract hyperlinks from PDF using PyMuPDF."""
        hyperlinks = []
        
        try:
            # Open PDF with PyMuPDF
            pdf_document = fitz.open(str(pdf_path))
            
            for page_num in range(len(pdf_document)):
                page = pdf_document[page_num]
                
                # Get all links on the page
                links = page.get_links()
                
                for link in links:
                    if link.get('uri'):  # External URL
                        # Get the link text by extracting text from the link rectangle
                        rect = fitz.Rect(link['from'])
                        link_text = page.get_text(clip=rect).strip()
                        
                        # Clean up the link text
                        link_text = ' '.join(link_text.split())
                        
                        hyperlinks.append({
                            'url': link['uri'],
                            'text': link_text or 'Click here',
                            'page': page_num + 1,
                            'rect': {
                                'x0': link['from'].x0,
                                'y0': link['from'].y0,
                                'x1': link['from'].x1,
                                'y1': link['from'].y1
                            }
                        })
            
            pdf_document.close()
            
            log.info(f"🔗 Extracted {len(hyperlinks)} hyperlinks from PDF")
            
        except Exception as e:
            log.error(f"Failed to extract hyperlinks with PyMuPDF: {e}")
        
        return hyperlinks
    
    def _associate_urls_with_items(self, items: List[Dict], hyperlinks: List[Dict], full_text: str) -> List[Dict]:
        """Associate extracted URLs with their corresponding agenda items."""
        # Create a mapping of items by their document reference
        items_by_ref = {}
        for item in items:
            if item.get('document_reference'):
                items_by_ref[item['document_reference']] = item
                # Initialize URLs list for each item
                item['urls'] = []
        
        # Try to associate URLs with items based on proximity and context
        for link in hyperlinks:
            # Strategy 1: Check if the link text contains a document reference
            for ref, item in items_by_ref.items():
                if ref in link.get('text', ''):
                    item['urls'].append({
                        'url': link['url'],
                        'text': link['text'],
                        'page': link['page']
                    })
                    log.info(f"🔗 Associated URL with item {item.get('item_code', 'Unknown')}: {link['url'][:50]}...")
                    break
            else:
                # Strategy 2: Check for item codes in the link text
                link_text = link.get('text', '').upper()
                for item in items:
                    item_code = item.get('item_code', '')
                    if item_code and item_code in link_text:
                        item['urls'].append({
                            'url': link['url'],
                            'text': link['text'],
                            'page': link['page']
                        })
                        log.info(f"🔗 Associated URL with item {item_code}: {link['url'][:50]}...")
                        break
        
        # Log summary of URL associations
        items_with_urls = sum(1 for item in items if item.get('urls'))
        total_urls_associated = sum(len(item.get('urls', [])) for item in items)
        log.info(f"📊 Associated {total_urls_associated} URLs with {items_with_urls} agenda items")
        
        return items
    
    def _extract_agenda_items_with_llm(self, text: str) -> List[Dict[str, any]]:
        """Use LLM to extract agenda items from the text."""
        # Split text into chunks if too long
        max_chars = 30000
        chunks = []
        
        if len(text) > max_chars:
            # Split by lines to avoid breaking mid-sentence
            lines = text.split('\n')
            current_chunk = []
            current_length = 0
            
            for line in lines:
                if current_length + len(line) > max_chars and current_chunk:
                    chunks.append('\n'.join(current_chunk))
                    current_chunk = [line]
                    current_length = len(line)
                else:
                    current_chunk.append(line)
                    current_length += len(line) + 1
            
            if current_chunk:
                chunks.append('\n'.join(current_chunk))
        else:
            chunks = [text]
        
        all_items = []
        
        for i, chunk in enumerate(chunks):
            log.info(f"Processing chunk {i+1}/{len(chunks)}")
            
            prompt = """Extract ALL agenda items from this city council agenda document. Look for ALL these formats:

- Letter.-Number. Reference (e.g., H.-1. 23-6819)
- Letter-Number Reference (e.g., H-1 23-6819)
- Empty sections marked as "None"

IMPORTANT: 
1. Extract EVERY section even if it says "None"
2. Look for ALL item formats including H.-1., H.-2., etc.
3. Include items without explicit ordinance/resolution text

For EACH section/item found, extract:
1. section_name: The section name (e.g., "CITY MANAGER ITEMS")
2. item_code: The item code (e.g., "H-1") - normalize to Letter-Number format
3. document_reference: The reference number (e.g., "23-6819")
4. title: The full description
5. has_items: true if section has items, false if "None"

Return a JSON array including both sections and items.

Document text:
""" + chunk
            
            try:
                response = self.client.chat.completions.create(
                    model="meta-llama/llama-4-maverick-17b-128e-instruct",
                    messages=[
                        {"role": "system", "content": "You are an expert at extracting structured data from city government agenda documents. Return only valid JSON."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0,
                    max_completion_tokens=8192,
                    top_p=1,
                    stream=False,
                    stop=None
                )
                
                response_text = response.choices[0].message.content.strip()
                
                # Clean up response to ensure valid JSON
                if response_text.startswith('```json'):
                    response_text = response_text.replace('```json', '').replace('```', '')
                elif response_text.startswith('```'):
                    response_text = response_text.replace('```', '')
                
                response_text = response_text.strip()
                
                # Try to parse JSON
                try:
                    data = json.loads(response_text)
                    if isinstance(data, dict) and 'items' in data:
                        items = data['items']
                    elif isinstance(data, list):
                        items = data
                    else:
                        log.warning(f"Unexpected LLM response format: {type(data)}")
                        items = []
                        
                    all_items.extend(items)
                    log.info(f"Extracted {len(items)} items from chunk {i+1}")
                    
                except json.JSONDecodeError as e:
                    log.error(f"Failed to parse JSON from chunk {i+1}: {e}")
                    log.error(f"Raw response: {response_text[:200]}...")
                    # Try manual extraction as fallback
                    manual_sections = self._manual_extract_items(chunk)
                    # Flatten sections to items for consistency
                    manual_items = []
                    for section in manual_sections:
                        manual_items.extend(section.get('items', []))
                    all_items.extend(manual_items)
                    log.info(f"Manual fallback extracted {len(manual_items)} items from {len(manual_sections)} sections")
                    
            except Exception as e:
                log.error(f"LLM extraction failed for chunk {i+1}: {e}")
                # Fallback to manual extraction
                manual_sections = self._manual_extract_items(chunk)
                # Flatten sections to items for consistency
                manual_items = []
                for section in manual_sections:
                    manual_items.extend(section.get('items', []))
                all_items.extend(manual_items)
                log.info(f"Manual fallback extracted {len(manual_items)} items from {len(manual_sections)} sections")
        
        # Deduplicate items by item_code
        seen_codes = set()
        unique_items = []
        for item in all_items:
            if item.get('item_code') and item['item_code'] not in seen_codes:
                seen_codes.add(item['item_code'])
                unique_items.append(item)
        
        log.info(f"Total unique items extracted: {len(unique_items)}")
        return unique_items
    
    def _manual_extract_items(self, text: str) -> List[Dict[str, any]]:
        """Manually extract agenda items using regex patterns."""
        sections = []
        current_section = None
        
        # Updated section patterns to catch all sections
        section_patterns = [
            (r'^([A-Z])\.\s+(.+)$', 'SECTION'),  # Letter. Section Name
            (r'^(CITY MANAGER ITEMS?)$', 'CITY_MANAGER'),
            (r'^(CITY ATTORNEY ITEMS?)$', 'CITY_ATTORNEY'),
            (r'^(BOARDS?/COMMITTEES? ITEMS?)$', 'BOARDS_COMMITTEES'),
            (r'^(PRESENTATIONS AND PROTOCOL DOCUMENTS)', 'PRESENTATIONS'),
            (r'^(APPROVAL OF MINUTES)', 'MINUTES'),
            (r'^(PUBLIC COMMENTS)', 'PUBLIC_COMMENTS'),
            (r'^(CONSENT AGENDA)', 'CONSENT'),
            (r'^(PUBLIC HEARINGS)', 'PUBLIC_HEARINGS'),
            (r'^(RESOLUTIONS)', 'RESOLUTIONS'),
            (r'^(ORDINANCES.*)', 'ORDINANCES'),
            (r'^(DISCUSSION ITEMS)', 'DISCUSSION'),
            (r'^(BOARDS AND COMMITTEES)', 'BOARDS'),
        ]
        
        # Track if we're in a section that might have "None" as content
        in_section = False
        section_content_lines = []
        
        lines = text.split('\n')
        for i, line in enumerate(lines):
            line_stripped = line.strip()
            
            # Skip empty lines
            if not line_stripped:
                continue
            
            # Check for section headers
            section_found = False
            for pattern, section_type in section_patterns:
                if re.match(pattern, line_stripped, re.IGNORECASE):
                    # Process previous section if it exists
                    if current_section:
                        # Check if section only contains "None"
                        content = ' '.join(section_content_lines).strip()
                        if content.lower() == 'none' or not current_section['items']:
                            current_section['has_items'] = False
                        sections.append(current_section)
                    
                    # Start new section
                    current_section = {
                        'section_name': line_stripped,
                        'section_type': section_type,
                        'items': [],
                        'has_items': True
                    }
                    section_content_lines = []
                    in_section = True
                    section_found = True
                    break
            
            if section_found:
                continue
            
            # Collect section content
            if in_section and current_section:
                section_content_lines.append(line_stripped)
                
            # Updated item patterns to handle multiline items
            if current_section and re.match(r'^[A-Z]\.-\d+\.?\s*$', line_stripped):
                # Item code on its own line
                item_code = line_stripped.strip()
                # Look ahead for document reference
                if i + 1 < len(lines):
                    next_line = lines[i + 1].strip()
                    doc_ref_match = re.match(r'^(\d{2}-\d{4,5})', next_line)
                    if doc_ref_match:
                        doc_ref = doc_ref_match.group(1)
                        # Get title from remaining text or next lines
                        title_start = i + 1
                        title_lines = []
                        for j in range(title_start, min(i + 5, len(lines))):
                            title_line = lines[j].strip()
                            if title_line and not re.match(r'^[A-Z]\.-\d+\.?', title_line):
                                title_lines.append(title_line)
                        
                        title = ' '.join(title_lines)
                        # Remove the document reference from title
                        title = title.replace(doc_ref, '').strip()
                        
                        current_section['items'].append({
                            'item_code': item_code.rstrip('.'),
                            'document_reference': doc_ref,
                            'title': title,
                            'item_type': self._determine_item_type(title, current_section['section_type'])
                        })
                        log.info(f"Extracted multiline item: {item_code.rstrip('.')} - {doc_ref}")
                        continue
            
            # Original item patterns for single-line items
            item_patterns = [
                r'^([A-Z]\.-\d+\.?)\s+(\d{2}-\d{4,5})\s+(.+)$',  # A.-1. 23-6764
                r'^(\d+\.-\d+\.?)\s+(\d{2}-\d{4,5})\s+(.+)$',    # 1.-1. 23-6797
                r'^([A-Z]-\d+)\s+(\d{2}-\d{4,5})\s+(.+)$',       # E-1 23-6784
            ]
            
            for pattern in item_patterns:
                match = re.match(pattern, line_stripped)
                if match and current_section:
                    item_code = match.group(1)
                    doc_ref = match.group(2)
                    title = match.group(3).strip()
                    
                    # Determine item type based on title or section
                    item_type = self._determine_item_type(title, current_section.get("section_type", ""))
                    
                    item = {
                        "item_code": item_code.rstrip('.'),
                        "document_reference": doc_ref,
                        "title": title[:300],
                        "item_type": item_type
                    }
                    
                    current_section["items"].append(item)
                    log.info(f"Extracted single-line {item_type}: {item_code} - {doc_ref}")
                    break
        
        # Don't forget the last section
        if current_section:
            # Check if section only contains "None"
            content = ' '.join(section_content_lines).strip()
            if content.lower() == 'none' or not current_section['items']:
                current_section['has_items'] = False
            sections.append(current_section)
        
        total_items = sum(len(s['items']) for s in sections)
        log.info(f"Manual extraction complete: {total_items} items in {len(sections)} sections")
        
        return sections
    
    def _determine_item_type(self, title: str, section_type: str) -> str:
        """Determine item type from title and section."""
        title_lower = title.lower()
        
        # Check title first for explicit type
        if 'an ordinance' in title_lower:
            return 'Ordinance'
        elif 'a resolution' in title_lower:
            return 'Resolution'
        elif 'proclamation' in title_lower:
            return 'Proclamation'
        elif 'recognition' in title_lower:
            return 'Recognition'
        elif 'congratulations' in title_lower:
            return 'Recognition'
        elif 'presentation' in title_lower:
            return 'Presentation'
        elif section_type == 'PRESENTATIONS':
            return 'Presentation'
        elif section_type == 'MINUTES':
            return 'Minutes Approval'
        elif section_type == 'CITY_MANAGER':
            return 'City Manager Item'
        elif section_type == 'CITY_ATTORNEY':
            return 'City Attorney Item'
        elif section_type == 'BOARDS_COMMITTEES':
            return 'Board/Committee Item'
        else:
            return 'Agenda Item'  # Generic fallback

    def _build_sections_from_items(self, extracted_data: List[Dict], full_text: str) -> List[Dict[str, str]]:
        """Build sections structure from extracted items or sections."""
        if not extracted_data:
            # If no items found, return the full document as one section
            return [{
                'title': 'Full Document',
                'text': full_text
            }]
        
        # Check if we have sections (from manual extraction) or items (from LLM)
        if extracted_data and isinstance(extracted_data[0], dict) and 'section_name' in extracted_data[0]:
            # We have sections from manual extraction
            sections = []
            for section_data in extracted_data:
                section_text_parts = []
                section_text_parts.append(f"=== {section_data['section_name']} ===\n")
                
                if section_data.get('has_items', True) and section_data.get('items'):
                    for item in section_data['items']:
                        item_text = f"{item['item_code']} - {item['document_reference']}\n{item['title']}\n"
                        section_text_parts.append(item_text)
                else:
                    section_text_parts.append("None\n")
                
                sections.append({
                    'title': section_data['section_name'],
                    'text': '\n'.join(section_text_parts)
                })
            
            return sections
        else:
            # We have items from LLM extraction - group them
            sections = []
            
            # Create agenda items section
            agenda_section_text = []
            for item in extracted_data:
                item_text = f"{item.get('item_code', 'Unknown')} - {item.get('document_reference', 'Unknown')}\n{item.get('title', 'Unknown')}\n"
                agenda_section_text.append(item_text)
            
            sections.append({
                'title': 'AGENDA ITEMS',
                'text': '\n'.join(agenda_section_text)
            })
            
            return sections
    
    def _extract_hyperlinks(self, doc) -> Dict[str, Dict[str, any]]:
        """Extract hyperlinks from the document."""
        hyperlinks = {}
        
        # Try to extract links from document structure
        if hasattr(doc, 'links'):
            for link in doc.links:
                if hasattr(link, 'text') and hasattr(link, 'url'):
                    hyperlinks[link.text] = {
                        'url': link.url,
                        'page': getattr(link, 'page', 0)
                    }
        
        # Try to extract from markdown if links are preserved there
        if hasattr(doc, 'export_to_markdown'):
            markdown = doc.export_to_markdown()
            # Extract markdown links pattern [text](url)
            link_pattern = r'\[([^\]]+)\]\(([^)]+)\)'
            for match in re.finditer(link_pattern, markdown):
                text, url = match.groups()
                if text and url:
                    hyperlinks[text] = {
                        'url': url,
                        'page': 0  # We don't have page info from markdown
                    }
        
        return hyperlinks 

    def _count_items(self, extracted_data: List[Dict]) -> int:
        """Count the number of items in extracted data (items or sections with items)."""
        if not extracted_data:
            return 0
        
        # Check if we have sections or items
        if extracted_data and isinstance(extracted_data[0], dict) and 'section_name' in extracted_data[0]:
            # We have sections - count items within them
            total_items = 0
            for section in extracted_data:
                total_items += len(section.get('items', []))
            return total_items
        else:
            # We have items directly
            return len(extracted_data)


================================================================================


################################################################################
# File: scripts/graph_stages/agenda_ontology_extractor.py
################################################################################

# File: scripts/graph_stages/agenda_ontology_extractor.py

"""
City Clerk Ontology Extractor - FIXED VERSION
Uses OpenAI LLM to extract structured data from city agenda documents.
"""

import logging
import json
import re
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
import os
from groq import Groq
from dotenv import load_dotenv
import asyncio

load_dotenv()

log = logging.getLogger('ontology_extractor')


class CityClerkOntologyExtractor:
    """Extract structured ontology from city clerk documents using LLM."""
    
    def __init__(self, 
                 openai_api_key: Optional[str] = None,
                 model: str = "gpt-4.1-mini-2025-04-14",
                 output_dir: Optional[Path] = None,
                 max_tokens: int = 32768):
        """Initialize the extractor with OpenAI client."""
        self.api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY not found in environment")
        
        self.client = Groq()
        self.model = model
        self.max_tokens = max_tokens
        self.output_dir = output_dir or Path("city_clerk_documents/graph_json")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Create debug directory
        self.debug_dir = self.output_dir / "debug"
        self.debug_dir.mkdir(exist_ok=True)
    
    def extract(self, pdf_path: Path) -> Dict[str, Any]:
        """Extract complete ontology from agenda PDF."""
        log.info(f"🧠 Extracting ontology from {pdf_path.name}")
        
        # First, load the extracted text
        extracted_path = self.output_dir / f"{pdf_path.stem}_extracted.json"
        if extracted_path.exists():
            with open(extracted_path, 'r') as f:
                extracted_data = json.load(f)
        else:
            raise FileNotFoundError(f"No extracted data found for {pdf_path.name}. Run PDF extraction first.")
        
        # Get full text from sections
        full_text = "\n".join(section.get("text", "") for section in extracted_data.get("sections", []))
        
        # Save full text for debugging
        with open(self.debug_dir / f"{pdf_path.stem}_full_text.txt", 'w', encoding='utf-8') as f:
            f.write(full_text)
        
        # Extract meeting date from filename first (more reliable)
        meeting_date = self._extract_meeting_date_from_filename(pdf_path.stem)
        if not meeting_date:
            meeting_date = self._extract_meeting_date(pdf_path.stem, full_text[:1000])
        
        log.info(f"📅 Extracted meeting date: {meeting_date}")
        
        # Step 1: Extract meeting information
        meeting_info = self._extract_meeting_info(full_text[:4000])
        
        # Step 2: Extract complete agenda structure
        agenda_structure = self._extract_agenda_structure(full_text)
        
        # Step 3: Extract entities
        entities = self._extract_entities(full_text[:15000])
        
        # Step 4: Extract relationships between items
        relationships = self._extract_relationships(agenda_structure)
        
        # Build complete ontology
        ontology = {
            "meeting_date": meeting_date,
            "meeting_info": meeting_info,
            "agenda_structure": agenda_structure,
            "entities": entities,
            "relationships": relationships,
            "hyperlinks": extracted_data.get("hyperlinks", {}),
            "metadata": {
                "source_pdf": str(pdf_path.absolute()),
                "extraction_date": datetime.utcnow().isoformat() + "Z",
                "model": self.model
            }
        }
        
        # Save ontology
        output_path = self.output_dir / f"{pdf_path.stem}_ontology.json"
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(ontology, f, indent=2, ensure_ascii=False)
        
        log.info(f"✅ Ontology extraction complete: {len(agenda_structure)} sections, {sum(len(s.get('items', [])) for s in agenda_structure)} items")
        return ontology
    
    def _extract_meeting_date_from_filename(self, filename: str) -> Optional[str]:
        """Extract meeting date from filename like 'Agenda 01.9.2024'."""
        # Try to extract date from filename
        date_patterns = [
            r'(\d{1,2})\.(\d{1,2})\.(\d{4})',  # 01.9.2024
            r'(\d{1,2})-(\d{1,2})-(\d{4})',    # 01-9-2024
            r'(\d{1,2})_(\d{1,2})_(\d{4})',    # 01_9_2024
        ]
        
        for pattern in date_patterns:
            match = re.search(pattern, filename)
            if match:
                month, day, year = match.groups()
                return f"{month.zfill(2)}.{day.zfill(2)}.{year}"
        
        return None
    
    def _extract_meeting_date(self, filename: str, text: str) -> str:
        """Extract meeting date in MM.DD.YYYY format."""
        # Try filename first
        date = self._extract_meeting_date_from_filename(filename)
        if date:
            return date
        
        # Try MM/DD/YYYY format in text
        date_match = re.search(r'(\d{1,2})/(\d{1,2})/(\d{4})', text)
        if date_match:
            month, day, year = date_match.groups()
            return f"{month.zfill(2)}.{day.zfill(2)}.{year}"
        
        # Default fallback
        return "01.09.2024"  # Based on the actual filename
    
    def _clean_json_response(self, json_text: str) -> str:
        """Clean and fix common JSON formatting issues from LLM responses."""
        # Remove markdown code blocks
        json_text = re.sub(r'```json\s*', '', json_text)
        json_text = re.sub(r'\s*```', '', json_text)
        
        # Remove any text before the first { or [
        json_start = json_text.find('{')
        array_start = json_text.find('[')
        
        if json_start == -1 and array_start == -1:
            return json_text
        
        if json_start == -1:
            start_pos = array_start
        elif array_start == -1:
            start_pos = json_start
        else:
            start_pos = min(json_start, array_start)
        
        json_text = json_text[start_pos:]
        
        # Fix common escape issues
        json_text = json_text.replace('\\n', ' ')
        json_text = json_text.replace('\\"', '"')
        json_text = json_text.replace('\\/', '/')
        
        # Fix truncated strings by closing them
        # Count quotes to detect unclosed strings
        in_string = False
        escape_next = False
        cleaned_chars = []
        
        for i, char in enumerate(json_text):
            if escape_next:
                escape_next = False
                cleaned_chars.append(char)
                continue
                
            if char == '\\':
                escape_next = True
                cleaned_chars.append(char)
                continue
                
            if char == '"':
                in_string = not in_string
                
            cleaned_chars.append(char)
        
        # If we end while still in a string, close it
        if in_string:
            cleaned_chars.append('"')
            # Also close any open braces/brackets
            open_braces = cleaned_chars.count('{') - cleaned_chars.count('}')
            open_brackets = cleaned_chars.count('[') - cleaned_chars.count(']')
            
            if open_braces > 0:
                cleaned_chars.append('}' * open_braces)
            if open_brackets > 0:
                cleaned_chars.append(']' * open_brackets)
        
        return ''.join(cleaned_chars)
    
    def _extract_meeting_info(self, text: str) -> Dict[str, Any]:
        """Extract meeting metadata using LLM."""
        prompt = """Analyze this city commission meeting agenda and extract meeting details.

Text:
{text}

IMPORTANT: Return ONLY the JSON object below. Do not include any other text, markdown formatting, or code blocks.

{{
    "meeting_type": "Regular Meeting or Special Meeting or Workshop",
    "meeting_time": "time if mentioned",
    "location": {{
        "name": "venue name",
        "address": "full address"
    }},
    "officials_present": {{
        "mayor": "name or null",
        "vice_mayor": "name or null",
        "commissioners": ["names"],
        "city_attorney": "name or null",
        "city_manager": "name or null",
        "city_clerk": "name or null"
    }}
}}""".format(text=text)
        
        try:
            response = self.client.chat.completions.create(
                model="meta-llama/llama-4-maverick-17b-128e-instruct",
                messages=[
                    {"role": "system", "content": "You are a JSON extractor. Return only valid JSON, no markdown or other formatting."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_completion_tokens=8192,
                top_p=1,
                stream=False,
                stop=None
            )
            
            # Save LLM response for debugging
            raw_response = response.choices[0].message.content.strip()
            with open(self.debug_dir / "meeting_info_llm_response.txt", 'w', encoding='utf-8') as f:
                f.write(raw_response)
            
            # Clean and parse JSON
            json_text = self._clean_json_response(raw_response)
            result = json.loads(json_text)
            
            # Save parsed result
            with open(self.debug_dir / "meeting_info_parsed.json", 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2)
            
            return result
            
        except json.JSONDecodeError as e:
            log.error(f"JSON decode error in meeting info: {e}")
            log.error(f"Raw response saved to debug/meeting_info_llm_response.txt")
            return self._default_meeting_info()
        except Exception as e:
            log.error(f"Failed to extract meeting info: {e}")
            return self._default_meeting_info()
    
    def _extract_agenda_structure(self, text: str) -> List[Dict[str, Any]]:
        """Extract complete agenda structure with all items."""
        # Split text into smaller chunks to avoid token limits
        max_chunk_size = 30000  # characters
        
        if len(text) > max_chunk_size:
            # Process in chunks
            chunks = [text[i:i+max_chunk_size] for i in range(0, len(text), max_chunk_size-1000)]
            all_sections = []
            
            for i, chunk in enumerate(chunks):
                log.info(f"Processing chunk {i+1}/{len(chunks)} for agenda structure")
                sections = self._extract_agenda_structure_chunk(chunk, i)
                all_sections.extend(sections)
            
            return all_sections
        else:
            return self._extract_agenda_structure_chunk(text, 0)
    
    def _extract_agenda_structure_chunk(self, text: str, chunk_num: int) -> List[Dict[str, Any]]:
        """Extract agenda structure from a text chunk."""
        prompt = """Extract the complete agenda structure from this city commission agenda.

Extract ALL sections and their items, including:
- PRESENTATIONS AND PROTOCOL DOCUMENTS
- APPROVAL OF MINUTES
- PUBLIC COMMENTS
- CONSENT AGENDA
- PUBLIC HEARINGS
- RESOLUTIONS
- CITY MANAGER ITEMS
- CITY ATTORNEY ITEMS
- BOARDS AND COMMITTEES
- DISCUSSION ITEMS

Text:
{text}

Return ONLY a JSON array. Each section should have this structure:
[
    {{
        "section_name": "PRESENTATIONS AND PROTOCOL DOCUMENTS",
        "section_type": "PRESENTATIONS",
        "order": 1,
        "items": [
            {{
                "item_code": "A.-1.",
                "document_reference": "23-6764",
                "title": "Presentation of a Proclamation declaring...",
                "item_type": "Presentation"
            }}
        ]
    }},
    {{
        "section_name": "CONSENT AGENDA",
        "section_type": "CONSENT",
        "order": 4,
        "items": [
            {{
                "item_code": "D.-1.",
                "document_reference": "23-6830",
                "title": "A Resolution of the City Commission appointing...",
                "item_type": "Resolution"
            }}
        ]
    }}
]""".format(text=text)
        
        try:
            response = self.client.chat.completions.create(
                model="meta-llama/llama-4-maverick-17b-128e-instruct",
                messages=[
                    {"role": "system", "content": "Extract ALL agenda items. Do not skip any items in the sequence. If you see E-1 and E-3, look carefully for E-2."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_completion_tokens=8192,
                top_p=1,
                stream=False,
                stop=None
            )
            
            raw_response = response.choices[0].message.content.strip()
            
            # Save LLM response for debugging
            with open(self.debug_dir / f"agenda_structure_chunk{chunk_num}_llm_response.txt", 'w', encoding='utf-8') as f:
                f.write(raw_response)
            
            # For OpenAI, just clean the JSON response
            json_text = self._clean_json_response(raw_response)
            
            # Try to parse
            try:
                agenda_structure = json.loads(json_text)
                
                # Validate for missing items in sequence
                all_items = []
                for section in agenda_structure:
                    all_items.extend(section.get('items', []))
                
                # Check for missing E items
                e_items = sorted([item['item_code'] for item in all_items if item['item_code'].startswith('E')])
                if e_items:
                    log.info(f"Found E-section items: {e_items}")
                    # Check for gaps
                    for i in range(len(e_items) - 1):
                        current = e_items[i]
                        next_item = e_items[i + 1]
                        # Extract numbers
                        current_num = int(re.search(r'\d+', current).group())
                        next_num = int(re.search(r'\d+', next_item).group())
                        if next_num - current_num > 1:
                            log.warning(f"⚠️  Gap detected: {current} -> {next_item}. Missing items in between!")
                
            except json.JSONDecodeError:
                # If parsing fails, try to extract items manually from the text
                log.warning(f"Failed to parse LLM response, extracting items manually")
                agenda_structure = self._extract_items_manually(text)
            
            # Save parsed result
            with open(self.debug_dir / f"agenda_structure_chunk{chunk_num}_parsed.json", 'w', encoding='utf-8') as f:
                json.dump(agenda_structure, f, indent=2)
            
            return agenda_structure
            
        except Exception as e:
            log.error(f"Failed to extract agenda structure chunk {chunk_num}: {e}")
            # Try manual extraction as fallback
            return self._extract_items_manually(text)
    
    def _extract_items_manually(self, text: str) -> List[Dict[str, Any]]:
        """Manually extract agenda items using regex patterns."""
        log.info("Attempting manual extraction of agenda items")
        
        sections = []
        current_section = None
        
        # Define section headers (no letter mapping needed)
        section_patterns = [
            (r'^(PRESENTATIONS AND PROTOCOL DOCUMENTS)', 'PRESENTATIONS'),
            (r'^(PUBLIC COMMENTS?)', 'PUBLIC_COMMENTS'),
            (r'^(CONSENT AGENDA)', 'CONSENT'),
            (r'^(ORDINANCES.*)', 'ORDINANCES'),
            (r'^(RESOLUTIONS)', 'RESOLUTIONS'),
            (r'^(BOARDS?/COMMITTEES? ITEMS?)', 'BOARDS_COMMITTEES'),
            (r'^(CITY MANAGER ITEMS?)', 'CITY_MANAGER'),
            (r'^(CITY ATTORNEY ITEMS?)', 'CITY_ATTORNEY'),
            (r'^(CITY CLERK ITEMS?)', 'CITY_CLERK'),
            (r'^(DISCUSSION ITEMS?)', 'DISCUSSION'),
            (r'^([A-Z])\.\s+(.+)$', 'SECTION'),  # Letter-prefixed sections
            (r'^(APPROVAL OF MINUTES)', 'MINUTES'),
            (r'^(PUBLIC HEARINGS)', 'PUBLIC_HEARINGS'),
            (r'^(BOARDS AND COMMITTEES)', 'BOARDS'),
            # Also match letter-prefixed sections like "A. PRESENTATIONS..."
        ]
        
        lines = text.split('\n')
        
        for i, line in enumerate(lines):
            line_stripped = line.strip()
            
            # Skip empty lines
            if not line_stripped:
                continue
            
            # Check for section headers
            section_found = False
            for pattern, section_type in section_patterns:
                match = re.match(pattern, line_stripped, re.IGNORECASE)
                if match:
                    # Save previous section if exists
                    if current_section and current_section["items"]:
                        sections.append(current_section)
                    
                    # Extract section name
                    if section_type == 'SECTION':
                        section_name = match.group(1)
                    else:
                        section_name = match.group(1)
                    
                    current_section = {
                        "section_name": section_name.strip(),
                        "section_type": section_type,
                        "order": len(sections) + 1,
                        "items": []
                    }
                    section_found = True
                    break
            
            if section_found:
                continue
            
            # Check for agenda items
            # Patterns: A.-1. 23-6764, D.-1. 23-6830, 1.-1. 23-6797
            item_patterns = [
                r'^([A-Z]\.-\d+\.?)\s+(\d{2}-\d{4,5})\s+(.+)$',  # A.-1. 23-6764
                r'^(\d+\.-\d+\.?)\s+(\d{2}-\d{4,5})\s+(.+)$',    # 1.-1. 23-6797
                r'^([A-Z]-\d+)\s+(\d{2}-\d{4,5})\s+(.+)$',       # E-1 23-6784
            ]
            
            for pattern in item_patterns:
                match = re.match(pattern, line_stripped)
                if match and current_section:
                    item_code = match.group(1)
                    doc_ref = match.group(2)
                    title = match.group(3).strip()
                    
                    # Determine item type based on title or section
                    item_type = self._determine_item_type(title, current_section.get("section_type", ""))
                    
                    item = {
                        "item_code": item_code,
                        "document_reference": doc_ref,
                        "title": title[:300],
                        "item_type": item_type
                    }
                    
                    current_section["items"].append(item)
                    log.info(f"Extracted {item_type}: {item_code} - {doc_ref}")
                    break
        
        # Don't forget the last section
        if current_section and current_section["items"]:
            sections.append(current_section)
        
        total_items = sum(len(s['items']) for s in sections)
        log.info(f"Manual extraction complete: {total_items} items in {len(sections)} sections")
        
        return sections

    def _normalize_agenda_item_code(self, code: str) -> str:
        """Normalize agenda item code to consistent format for agenda display."""
        # Remove all spaces
        code = code.strip()
        
        # Ensure we have the letter part
        match = re.match(r'([A-Z])\.?-?(\d+)\.?', code)
        if match:
            letter = match.group(1)
            number = match.group(2)
            # Return in consistent format: "E.-9."
            return f"{letter}.-{number}."
        
        return code
    
    def _determine_item_type(self, title: str, section_type: str) -> str:
        """Determine item type from title and section."""
        title_lower = title.lower()
        
        # Check title first for explicit type
        if 'an ordinance' in title_lower:
            return 'Ordinance'
        elif 'a resolution' in title_lower:  # Changed: more specific
            return 'Resolution'
        elif 'proclamation' in title_lower:
            return 'Proclamation'
        elif 'recognition' in title_lower:
            return 'Recognition'
        elif 'congratulations' in title_lower:
            return 'Recognition'
        elif 'presentation' in title_lower:
            return 'Presentation'
        elif section_type == 'PRESENTATIONS':
            return 'Presentation'
        elif section_type == 'MINUTES':
            return 'Minutes Approval'
        else:
            return 'Agenda Item'  # Generic fallback

    def _determine_section_type(self, section_name: str) -> str:
        """Determine section type from section name."""
        section_name_upper = section_name.upper()
        if "RESOLUTION" in section_name_upper:
            return "RESOLUTION"
        elif "ORDINANCE" in section_name_upper:
            return "ORDINANCE"
        elif "COMMISSION" in section_name_upper:
            return "COMMISSION"
        elif "CONSENT" in section_name_upper:
            return "CONSENT"
        else:
            return "GENERAL"
    
    async def extract_entities(self, content: str, context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Extract entities with parallel processing for chunks."""
        # Split content into chunks for parallel processing
        chunk_size = 4000  # Adjust based on model limits
        chunks = [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]
        
        if len(chunks) > 1:
            # Process chunks in parallel
            max_concurrent = min(len(chunks), 5)
            semaphore = asyncio.Semaphore(max_concurrent)
            
            async def extract_from_chunk(chunk, idx):
                async with semaphore:
                    return await self._extract_entities_from_chunk(chunk, context, idx)
            
            # Execute parallel extraction
            chunk_results = await asyncio.gather(
                *[extract_from_chunk(chunk, idx) for idx, chunk in enumerate(chunks)],
                return_exceptions=True
            )
            
            # Merge results
            all_entities = []
            for result in chunk_results:
                if isinstance(result, Exception):
                    log.error(f"Chunk extraction error: {result}")
                elif result:
                    all_entities.extend(result)
            
            # Deduplicate entities
            return self._deduplicate_entities(all_entities)
        else:
            # Single chunk, process normally
            return await self._extract_entities_from_chunk(content, context, 0)

    # Add helper method
    async def _extract_entities_from_chunk(self, chunk: str, context: Dict[str, Any], chunk_idx: int) -> List[Dict[str, Any]]:
        """Extract entities from a single chunk."""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            None,
            self._extract_entities_sync,
            chunk,
            context,
            chunk_idx
        )
    
    def _extract_entities_sync(self, chunk: str, context: Dict[str, Any], chunk_idx: int) -> List[Dict[str, Any]]:
        """Synchronous entity extraction for executor."""
        prompt = """Extract entities from this city agenda document.

Text:
{text}

IMPORTANT: Return ONLY the JSON object below. No markdown, no code blocks, no other text.

{{
    "people": [
        {{"name": "John Smith", "role": "Mayor", "context": "presiding"}}
    ],
    "organizations": [
        {{"name": "City Commission", "type": "government", "context": "governing body"}}
    ],
    "locations": [
        {{"name": "City Hall", "address": "405 Biltmore Way", "type": "government building"}}
    ],
    "monetary_amounts": [
        {{"amount": "$100,000", "purpose": "budget allocation", "context": "parks improvement"}}
    ],
    "dates": [
        {{"date": "01/23/2024", "event": "meeting date", "type": "meeting"}}
    ],
    "legal_references": [
        {{"type": "Resolution", "number": "2024-01", "title": "Budget Amendment"}}
    ]
}}""".format(text=chunk[:10000])  # Limit text to avoid token issues
        
        try:
            response = self.client.chat.completions.create(
                model="meta-llama/llama-4-maverick-17b-128e-instruct",
                messages=[
                    {"role": "system", "content": "Extract entities. Return only JSON, no formatting."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_completion_tokens=8192,
                top_p=1,
                stream=False,
                stop=None
            )
            
            raw_response = response.choices[0].message.content.strip()
            
            # Save LLM response for debugging
            with open(self.debug_dir / f"entities_chunk_{chunk_idx}_llm_response.txt", 'w', encoding='utf-8') as f:
                f.write(raw_response)
            
            # Clean and parse JSON
            json_text = self._clean_json_response(raw_response)
            entities_dict = json.loads(json_text)
            
            # Convert to list format
            entities = []
            for entity_type, entity_list in entities_dict.items():
                for entity in entity_list:
                    entity['type'] = entity_type
                    entities.append(entity)
            
            # Save parsed result
            with open(self.debug_dir / f"entities_chunk_{chunk_idx}_parsed.json", 'w', encoding='utf-8') as f:
                json.dump(entities, f, indent=2)
            
            return entities
            
        except json.JSONDecodeError as e:
            log.error(f"JSON decode error in entities chunk {chunk_idx}: {e}")
            return []
        except Exception as e:
            log.error(f"Failed to extract entities from chunk {chunk_idx}: {e}")
            return []
    
    def _deduplicate_entities(self, entities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Remove duplicate entities based on name/identifier."""
        seen = set()
        unique_entities = []
        
        for entity in entities:
            # Create identifier based on name or other unique field
            identifier = entity.get('name', entity.get('amount', entity.get('date', str(entity))))
            if identifier not in seen:
                seen.add(identifier)
                unique_entities.append(entity)
        
        return unique_entities
    
    def _extract_entities(self, text: str) -> Dict[str, List[Dict[str, Any]]]:
        """Extract all entities mentioned in the document (legacy sync method)."""
        # This is the original synchronous method for backward compatibility
        prompt = """Extract entities from this city agenda document.

Text:
{text}

IMPORTANT: Return ONLY the JSON object below. No markdown, no code blocks, no other text.

{{
    "people": [
        {{"name": "John Smith", "role": "Mayor", "context": "presiding"}}
    ],
    "organizations": [
        {{"name": "City Commission", "type": "government", "context": "governing body"}}
    ],
    "locations": [
        {{"name": "City Hall", "address": "405 Biltmore Way", "type": "government building"}}
    ],
    "monetary_amounts": [
        {{"amount": "$100,000", "purpose": "budget allocation", "context": "parks improvement"}}
    ],
    "dates": [
        {{"date": "01/23/2024", "event": "meeting date", "type": "meeting"}}
    ],
    "legal_references": [
        {{"type": "Resolution", "number": "2024-01", "title": "Budget Amendment"}}
    ]
}}""".format(text=text[:10000])  # Limit text to avoid token issues
        
        try:
            response = self.client.chat.completions.create(
                model="meta-llama/llama-4-maverick-17b-128e-instruct",
                messages=[
                    {"role": "system", "content": "Extract entities. Return only JSON, no formatting."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_completion_tokens=8192,
                top_p=1,
                stream=False,
                stop=None
            )
            
            raw_response = response.choices[0].message.content.strip()
            
            # Save LLM response for debugging
            with open(self.debug_dir / "entities_llm_response.txt", 'w', encoding='utf-8') as f:
                f.write(raw_response)
            
            # Clean and parse JSON
            json_text = self._clean_json_response(raw_response)
            entities = json.loads(json_text)
            
            # Save parsed result
            with open(self.debug_dir / "entities_parsed.json", 'w', encoding='utf-8') as f:
                json.dump(entities, f, indent=2)
            
            return entities
            
        except json.JSONDecodeError as e:
            log.error(f"JSON decode error in entities: {e}")
            log.error(f"Raw response saved to debug/entities_llm_response.txt")
            return self._default_entities()
        except Exception as e:
            log.error(f"Failed to extract entities: {e}")
            return self._default_entities()
    
    def _extract_relationships(self, agenda_structure: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Extract relationships between agenda items."""
        relationships = []
        
        # Find related items based on common references
        all_items = []
        for section in agenda_structure:
            for item in section.get("items", []):
                item["section"] = section.get("section_name")
                all_items.append(item)
        
        # Save all items for debugging
        with open(self.debug_dir / "all_agenda_items.json", 'w', encoding='utf-8') as f:
            json.dump(all_items, f, indent=2)
        
        # Look for items that reference each other
        for i, item1 in enumerate(all_items):
            for j, item2 in enumerate(all_items[i+1:], i+1):
                # Check if items share document references
                if (item1.get("document_reference") and 
                    item1.get("document_reference") == item2.get("document_reference")):
                    relationships.append({
                        "from_code": item1.get("item_code"),
                        "to_code": item2.get("item_code"),
                        "relationship_type": "REFERENCES_SAME_DOCUMENT",
                        "description": f"Both reference document {item1.get('document_reference')}"
                    })
        
        return relationships
    
    def _default_meeting_info(self) -> Dict[str, Any]:
        """Return default meeting info structure."""
        return {
            "meeting_type": "Regular Meeting",
            "meeting_time": "9:00 a.m.",
            "location": {
                "name": "City Hall, Commission Chambers",
                "address": "405 Biltmore Way, Coral Gables, FL 33134"
            },
            "officials_present": {
                "mayor": None,
                "vice_mayor": None,
                "commissioners": [],
                "city_attorney": None,
                "city_manager": None,
                "city_clerk": None
            }
        }
    
    def _default_entities(self) -> Dict[str, List]:
        """Return default empty entities structure."""
        return {
            "people": [],
            "organizations": [],
            "locations": [],
            "monetary_amounts": [],
            "dates": [],
            "legal_references": []
        }


================================================================================


################################################################################
# File: scripts/microsoft_framework/document_adapter.py
################################################################################

# File: scripts/microsoft_framework/document_adapter.py

from pathlib import Path
import json
import pandas as pd
import csv
import re
from typing import List, Dict, Any
from datetime import datetime
import yaml
from concurrent.futures import ThreadPoolExecutor, as_completed
import multiprocessing

class CityClerkDocumentAdapter:
    """Adapt city clerk documents for GraphRAG processing."""
    
    def __init__(self, extracted_text_dir: Path):
        self.extracted_text_dir = Path(extracted_text_dir)
        
    def prepare_documents_for_graphrag(self, output_dir: Path) -> pd.DataFrame:
        """Prepare documents with enhanced source tracking."""
        
        all_documents = []
        
        for json_file in self.extracted_text_dir.glob("*_extracted.json"):
            with open(json_file, 'r') as f:
                doc_data = json.load(f)
            
            # Extract metadata
            doc_type = doc_data.get('document_type', 'unknown')
            meeting_date = doc_data.get('meeting_date', '')
            
            # For agenda items, create separate documents
            if 'items' in doc_data:
                for item in doc_data['items']:
                    doc_dict = {
                        'text': self._format_agenda_item(item),
                        'title': f"Agenda Item {item['item_code']} - {meeting_date}",
                        'document_type': 'agenda_item',
                        'meeting_date': meeting_date,
                        'item_code': item['item_code'],
                        'source_file': json_file.name,
                        'source_id': f"{json_file.stem}_{item['item_code']}",  # Unique source ID
                        'metadata': json.dumps({
                            'original_file': json_file.name,
                            'extraction_method': doc_data.get('metadata', {}).get('extraction_method', 'unknown'),
                            'item_type': item.get('type', 'unknown')
                        })
                    }
                    all_documents.append(doc_dict)
            
            # For other documents
            else:
                doc_dict = {
                    'text': doc_data.get('full_text', ''),
                    'title': self._generate_title(doc_data),
                    'document_type': doc_type,
                    'meeting_date': meeting_date,
                    'item_code': doc_data.get('item_code', ''),
                    'source_file': json_file.name,
                    'source_id': json_file.stem,  # Unique source ID
                    'metadata': json.dumps({
                        'original_file': json_file.name,
                        'document_number': doc_data.get('document_number', ''),
                        'extraction_method': doc_data.get('metadata', {}).get('extraction_method', 'unknown')
                    })
                }
                all_documents.append(doc_dict)
        
        # Create DataFrame with source tracking columns
        df = pd.DataFrame(all_documents)
        
        # Ensure required columns exist
        required_columns = ['text', 'title', 'source_id', 'source_file', 'metadata']
        for col in required_columns:
            if col not in df.columns:
                df[col] = ''
        
        # Save with source tracking preserved
        csv_path = output_dir / "city_clerk_documents.csv"
        df.to_csv(csv_path, index=False, encoding='utf-8')
        
        return df
    
    def _format_agenda_item(self, item: Dict) -> str:
        """Format agenda item text."""
        parts = []
        
        if item.get('item_code'):
            parts.append(f"Item Code: {item['item_code']}")
        
        if item.get('title'):
            parts.append(f"Title: {item['title']}")
        
        if item.get('description'):
            parts.append(f"Description: {item['description']}")
        
        if item.get('sponsors'):
            sponsors = ', '.join(item['sponsors'])
            parts.append(f"Sponsors: {sponsors}")
        
        return "\n\n".join(parts)
    
    def _generate_title(self, doc_data: Dict) -> str:
        """Generate title for document."""
        doc_type = doc_data.get('document_type', 'Document')
        doc_number = doc_data.get('document_number', '')
        
        if doc_number:
            return f"{doc_type.title()} {doc_number}"
        else:
            return doc_type.title()
    
    def _process_json_file(self, json_file: Path) -> List[Dict]:
        """Process a single JSON file and return its documents."""
        documents = []
        
        with open(json_file, 'r') as f:
            data = json.load(f)
        
        doc_type = data.get('document_type', self._determine_document_type(json_file.name))
        meeting_date = data.get('meeting_date', self._extract_meeting_date(json_file.name))
        
        # CRITICAL: Process agenda items as completely separate documents
        if doc_type == 'agenda' and 'agenda_items' in data:
            # Each agenda item becomes its own document with NO references to other items
            for item in data['agenda_items']:
                # Create a unique, isolated document for this specific item
                doc_record = {
                    'id': f"agenda_item_{meeting_date}_{item['item_code']}",
                    'title': f"Agenda Item {item['item_code']} ONLY",
                    'text': self._prepare_isolated_item_text(item, meeting_date),
                    'document_type': 'agenda_item',
                    'meeting_date': meeting_date,
                    'item_code': item['item_code'],
                    'source_file': json_file.name,
                    'isolation_flag': True,  # Flag to indicate this must be treated in isolation
                    'urls': json.dumps(item.get('urls', []))
                }
                documents.append(doc_record)
        
        elif doc_type in ['ordinance', 'resolution']:
            # Process ordinances and resolutions with explicit item code isolation
            doc_record = {
                'id': f"{doc_type}_{data.get('document_number', json_file.stem)}",
                'title': f"{doc_type.title()} {data.get('document_number', '')}",
                'text': self._prepare_isolated_document_text(data),
                'document_type': doc_type,
                'meeting_date': meeting_date,
                'item_code': data.get('item_code', ''),
                'document_number': data.get('document_number', ''),
                'source_file': json_file.name,
                'isolation_flag': True,
                'urls': json.dumps([])
            }
            documents.append(doc_record)
        
        elif doc_type == 'verbatim_transcript':
            # Process transcripts with item code isolation
            item_codes = data.get('item_codes', [])
            # Create separate document for each item code mentioned
            if item_codes:
                for item_code in item_codes:
                    doc_record = {
                        'id': f"transcript_{json_file.stem}_{item_code}",
                        'title': f"Transcript for Item {item_code} ONLY",
                        'text': self._extract_item_specific_transcript(data, item_code),
                        'document_type': 'verbatim_transcript',
                        'meeting_date': meeting_date,
                        'item_code': item_code,
                        'transcript_type': data.get('transcript_type', ''),
                        'source_file': json_file.name,
                        'isolation_flag': True,
                        'urls': json.dumps([])
                    }
                    documents.append(doc_record)
            else:
                # General transcript without specific items
                doc_record = {
                    'id': f"transcript_{json_file.stem}",
                    'title': f"Transcript: {data.get('transcript_type', 'Meeting')}",
                    'text': data.get('full_text', ''),
                    'document_type': 'verbatim_transcript',
                    'meeting_date': meeting_date,
                    'item_code': '',
                    'transcript_type': data.get('transcript_type', ''),
                    'source_file': json_file.name,
                    'urls': json.dumps([])
                }
                documents.append(doc_record)
        
        return documents
    
    def _prepare_isolated_item_text(self, item: Dict, meeting_date: str) -> str:
        """Prepare agenda item text with STRICT isolation - no references to other items."""
        parts = []
        
        # Add isolation header
        parts.append(f"=== ISOLATED ENTITY: AGENDA ITEM {item['item_code']} ===")
        parts.append(f"THIS DOCUMENT CONTAINS INFORMATION ABOUT {item['item_code']} ONLY.")
        parts.append(f"DO NOT CONFUSE WITH OTHER AGENDA ITEMS.\n")
        
        # Add item-specific information
        parts.append(f"Agenda Item Code: {item['item_code']}")
        parts.append(f"Meeting Date: {meeting_date}")
        
        # Add title
        if item.get('title'):
            parts.append(f"Title: {item['title']}")
        
        # Add sponsors
        if item.get('sponsors'):
            sponsors = ', '.join(item['sponsors'])
            parts.append(f"Sponsors: {sponsors}")
        
        # Add description
        if item.get('description'):
            parts.append(f"Description: {item['description']}")
        
        # Add URLs as context
        if item.get('urls'):
            parts.append("\nReferenced Documents:")
            for url in item['urls']:
                parts.append(f"- {url.get('text', 'Link')}: {url.get('url', '')}")
        
        # Add isolation footer
        parts.append(f"\n=== END OF ISOLATED ENTITY {item['item_code']} ===")
        
        return "\n\n".join(parts)
    
    def _prepare_isolated_document_text(self, data: Dict) -> str:
        """Prepare document text with isolation markers."""
        parts = []
        
        doc_number = data.get('document_number', '')
        item_code = data.get('item_code', '')
        
        # Add isolation header
        if item_code:
            parts.append(f"=== ISOLATED ENTITY: {data.get('document_type', '').upper()} FOR ITEM {item_code} ===")
            parts.append(f"THIS DOCUMENT IS SPECIFICALLY ABOUT AGENDA ITEM {item_code}.")
        else:
            parts.append(f"=== ISOLATED ENTITY: {data.get('document_type', '').upper()} {doc_number} ===")
        
        # Add the full text
        parts.append(data.get('full_text', ''))
        
        # Add isolation footer
        parts.append(f"\n=== END OF ISOLATED ENTITY ===")
        
        return "\n\n".join(parts)
    
    def _extract_item_specific_transcript(self, data: Dict, item_code: str) -> str:
        """Extract only the transcript portions relevant to a specific item code."""
        full_text = data.get('full_text', '')
        
        # Try to extract sections mentioning this specific item
        lines = full_text.split('\n')
        relevant_sections = []
        in_relevant_section = False
        context_buffer = []
        
        for i, line in enumerate(lines):
            # Check if this line mentions the specific item code
            if item_code in line:
                in_relevant_section = True
                # Add some context before
                start_idx = max(0, i - 5)
                context_buffer = lines[start_idx:i]
                relevant_sections.extend(context_buffer)
                relevant_sections.append(line)
            elif in_relevant_section:
                # Continue adding lines until we hit another item code or section break
                if re.search(r'[A-Z]-\d+', line) and item_code not in line:
                    # Hit another item code, stop
                    in_relevant_section = False
                else:
                    relevant_sections.append(line)
        
        if relevant_sections:
            isolated_text = f"=== TRANSCRIPT EXCERPT FOR ITEM {item_code} ONLY ===\n\n"
            isolated_text += "\n".join(relevant_sections)
            isolated_text += f"\n\n=== END OF ITEM {item_code} TRANSCRIPT ==="
            return isolated_text
        else:
            return f"No specific transcript content found for item {item_code}"

    def _determine_document_type(self, filename: str) -> str:
        """Determine document type from filename."""
        filename_lower = filename.lower()
        if 'agenda' in filename_lower:
            return 'agenda'
        elif 'ordinance' in filename_lower:
            return 'ordinance'
        elif 'resolution' in filename_lower:
            return 'resolution'
        elif 'verbatim' in filename_lower:
            return 'verbatim_transcript'
        elif 'minutes' in filename_lower:
            return 'minutes'
        else:
            return 'document'
    
    def _extract_meeting_date(self, filename: str) -> str:
        """Extract meeting date from filename."""
        import re
        # Try different date patterns
        patterns = [
            r'(\d{4}-\d{2}-\d{2})',  # YYYY-MM-DD
            r'(\d{2}_\d{2}_\d{4})',  # MM_DD_YYYY
            r'(\d{2}\.\d{2}\.\d{4})'  # MM.DD.YYYY
        ]
        
        for pattern in patterns:
            match = re.search(pattern, filename)
            if match:
                date_str = match.group(1)
                # Normalize to MM.DD.YYYY format
                if '-' in date_str:
                    parts = date_str.split('-')
                    return f"{parts[1]}.{parts[2]}.{parts[0]}"
                elif '_' in date_str:
                    parts = date_str.split('_')
                    return f"{parts[0]}.{parts[1]}.{parts[2]}"
                else:
                    return date_str
        
        return ""

    def prepare_documents_from_markdown(self, output_dir: Path) -> pd.DataFrame:
        """Convert markdown files to GraphRAG CSV format with enhanced metadata."""
        
        markdown_dir = Path("city_clerk_documents/extracted_markdown")
        
        if not markdown_dir.exists():
            raise ValueError(f"Markdown directory not found: {markdown_dir}")
        
        documents = []
        
        # Process all markdown files
        for md_file in markdown_dir.glob("*.md"):
            try:
                with open(md_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # Extract metadata from the header
                metadata = self._extract_enhanced_metadata(content)
                
                # Build enriched text that includes all identifiers
                enriched_text = self._build_enriched_text(content, metadata)
                
                # Clean content for CSV
                enriched_text = self._clean_text_for_graphrag(enriched_text)
                
                # Ensure content is not empty
                if not enriched_text.strip():
                    print(f"⚠️  Skipping empty file: {md_file.name}")
                    continue
                
                # Parse document type from filename
                filename = md_file.stem
                if filename.startswith('agenda_'):
                    doc_type = 'agenda'
                elif filename.startswith('ordinance_'):
                    doc_type = 'ordinance'
                elif filename.startswith('resolution_'):
                    doc_type = 'resolution'
                elif filename.startswith('verbatim_'):
                    doc_type = 'verbatim_transcript'
                else:
                    doc_type = 'document'
                
                # Extract meeting date and item code from filename or content
                meeting_date = self._extract_meeting_date_from_markdown(filename, content)
                item_code = self._extract_item_code_from_markdown(filename, content)
                
                doc_record = {
                    'id': filename,
                    'title': self._build_comprehensive_title(metadata),
                    'text': enriched_text,
                    'document_type': doc_type,
                    'meeting_date': meeting_date,
                    'item_code': metadata.get('item_code', item_code),
                    'document_number': metadata.get('document_number', ''),
                    'related_items': json.dumps(metadata.get('related_items', [])),
                    'source_file': md_file.name
                }
                documents.append(doc_record)
                
            except Exception as e:
                print(f"❌ Error processing {md_file.name}: {e}")
                continue
        
        if not documents:
            raise ValueError("No documents were successfully processed!")
        
        # Convert to DataFrame
        df = pd.DataFrame(documents)
        
        # Ensure no null values in required columns
        df['text'] = df['text'].fillna('')
        df['title'] = df['title'].fillna('Untitled')
        df['meeting_date'] = df['meeting_date'].fillna('')
        df['item_code'] = df['item_code'].fillna('')
        df['document_number'] = df['document_number'].fillna('')
        df['related_items'] = df['related_items'].fillna('[]')
        
        # Ensure text column has content
        empty_texts = df[df['text'].str.strip() == '']
        if len(empty_texts) > 0:
            print(f"⚠️  Warning: {len(empty_texts)} documents have empty text")
            df = df[df['text'].str.strip() != '']
        
        # Log summary
        print(f"📊 Prepared {len(df)} documents from markdown:")
        for doc_type in df['document_type'].unique():
            count = len(df[df['document_type'] == doc_type])
            print(f"   - {doc_type}: {count}")
        
        # Save as CSV with proper escaping
        output_file = output_dir / "city_clerk_documents.csv"
        
        # Use pandas to_csv with specific parameters to handle special characters
        df.to_csv(
            output_file, 
            index=False,
            encoding='utf-8',
            escapechar='\\',
            doublequote=True,
            quoting=csv.QUOTE_MINIMAL
        )
        
        print(f"💾 Saved CSV to: {output_file}")
        
        # Verify the CSV can be read back
        try:
            test_df = pd.read_csv(output_file)
            print(f"✅ CSV verification: {len(test_df)} rows can be read back")
        except Exception as e:
            print(f"❌ CSV verification failed: {e}")
        
        return df

    def _clean_text_for_graphrag(self, text: str) -> str:
        """Clean markdown text for GraphRAG processing."""
        # Remove metadata header if present
        if text.startswith('---'):
            parts = text.split('---', 2)
            if len(parts) >= 3:
                # Keep only the main content after metadata
                text = parts[2].strip()
                
                # Also remove the "ORIGINAL DOCUMENT CONTENT" marker if present
                if "ORIGINAL DOCUMENT CONTENT" in text:
                    text = text.split("ORIGINAL DOCUMENT CONTENT", 1)[1].strip()
        
        # Remove excessive markdown formatting
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)  # Remove headers
        text = re.sub(r'\*\*([^*]+)\*\*', r'\1', text)  # Remove bold
        text = re.sub(r'\n{3,}', '\n\n', text)  # Reduce multiple newlines
        
        # DO NOT TRUNCATE - GraphRAG will handle chunking itself
        # Just ensure the text is clean and complete
        return text.strip()

    def _clean_text_for_csv(self, text: str) -> str:
        """Clean text to be CSV-safe."""
        # Remove markdown metadata header if present
        if text.startswith('---'):
            parts = text.split('---', 2)
            if len(parts) >= 3:
                # Keep only the main content
                text = parts[2]
        
        # Remove problematic characters
        text = text.replace('\x00', '')  # Null bytes
        text = text.replace('\r\n', '\n')  # Windows line endings
        
        # Replace multiple newlines with double newline
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        # Remove or escape quotes that might break CSV
        text = text.replace('"', '""')  # Escape quotes for CSV
        
        # Ensure text is not too long (GraphRAG might have limits)
        max_length = 50000  # Adjust as needed
        if len(text) > max_length:
            text = text[:max_length] + "... [truncated]"
        
        return text.strip()

    def _extract_meeting_date_from_markdown(self, filename: str, content: str) -> str:
        """Extract meeting date from filename or markdown content."""
        # First try from filename
        date_from_filename = self._extract_meeting_date(filename)
        if date_from_filename:
            return date_from_filename
        
        # Try from content metadata
        meeting_date_match = re.search(r'- Meeting Date: (.+)', content)
        if meeting_date_match:
            return meeting_date_match.group(1).strip()
        
        return ""

    def _extract_item_code_from_markdown(self, filename: str, content: str) -> str:
        """Extract item code from filename or markdown content."""
        # Try from filename
        item_match = re.search(r'([A-Z]-\d+)', filename)
        if item_match:
            return item_match.group(1)
        
        # Try from content
        item_match = re.search(r'- Agenda Items Discussed: (.+)', content)
        if item_match:
            return item_match.group(1).strip()
        
        return ""

    def _extract_title_from_markdown(self, content: str, filename: str) -> str:
        """Extract title from markdown content."""
        # Look for title in metadata section
        import re
        title_match = re.search(r'\*\*PARSED INFORMATION:\*\*.*?- Title: (.+)', content, re.DOTALL)
        if title_match:
            return title_match.group(1).strip()
        
        # Fallback to filename
        return filename.replace('_', ' ').title()

    def _parse_custom_metadata(self, metadata_text: str) -> Dict:
        """Parse our custom metadata format from the enhanced PDF extractor."""
        frontmatter = {}
        
        # Extract document type from the metadata
        import re
        
        # Look for document type
        doc_type_match = re.search(r'Document Type:\s*(.+)', metadata_text)
        if doc_type_match:
            frontmatter['document_type'] = doc_type_match.group(1).strip()
        
        # Look for filename
        filename_match = re.search(r'Filename:\s*(.+)', metadata_text)
        if filename_match:
            frontmatter['filename'] = filename_match.group(1).strip()
        
        # Look for document number
        doc_num_match = re.search(r'Document Number:\s*(.+)', metadata_text)
        if doc_num_match and doc_num_match.group(1).strip() != 'N/A':
            frontmatter['document_number'] = doc_num_match.group(1).strip()
        
        # Look for meeting date
        date_match = re.search(r'Meeting Date:\s*(.+)', metadata_text)
        if date_match and date_match.group(1).strip() != 'N/A':
            frontmatter['meeting_date'] = date_match.group(1).strip()
        
        # Look for agenda items
        items_match = re.search(r'Related Agenda Items:\s*(.+)', metadata_text)
        if items_match and items_match.group(1).strip() != 'N/A':
            frontmatter['agenda_items'] = items_match.group(1).strip()
        
        return frontmatter 

    def _extract_enhanced_metadata(self, content: str) -> Dict:
        """Extract all metadata including cross-references."""
        metadata = {}
        
        # Extract all document numbers
        doc_nums = re.findall(r'(?:Ordinance|Resolution)\s*(?:No\.\s*)?(\d{4}-\d+|\d+)', content)
        metadata['all_document_numbers'] = list(set(doc_nums))
        
        # Extract all agenda items
        agenda_items = re.findall(r'(?:Item|Agenda Item)\s*:?\s*([A-Z]-?\d+)', content)
        metadata['all_agenda_items'] = list(set(agenda_items))
        
        # Extract relationships
        metadata['related_items'] = self._extract_relationships(content)
        
        return metadata

    def _build_enriched_text(self, content: str, metadata: Dict) -> str:
        """Build text that prominently features all identifiers."""
        # Add a summary header with all identifiers
        identifier_summary = []
        
        if metadata.get('all_document_numbers'):
            identifier_summary.append(f"Document Numbers: {', '.join(metadata['all_document_numbers'])}")
        
        if metadata.get('all_agenda_items'):
            identifier_summary.append(f"Agenda Items: {', '.join(metadata['all_agenda_items'])}")
        
        if identifier_summary:
            enriched_header = "DOCUMENT IDENTIFIERS:\n" + '\n'.join(identifier_summary) + "\n\n"
            return enriched_header + content
        
        return content

    def _build_comprehensive_title(self, metadata: Dict) -> str:
        """Build a comprehensive title from metadata."""
        title_parts = []
        
        if metadata.get('all_agenda_items'):
            title_parts.append(f"Items: {', '.join(metadata['all_agenda_items'])}")
        
        if metadata.get('all_document_numbers'):
            title_parts.append(f"Docs: {', '.join(metadata['all_document_numbers'])}")
        
        if title_parts:
            return ' | '.join(title_parts)
        
        return "City Document"

    def _extract_relationships(self, content: str) -> List[Dict]:
        """Extract document relationships from content."""
        relationships = []
        
        # Look for patterns that indicate relationships
        relationship_patterns = [
            r'(?:amending|modifying|updating)\s+(?:Ordinance|Resolution)\s*(?:No\.\s*)?(\d+)',
            r'(?:relating to|concerning|regarding)\s+(?:agenda item|item)\s*([A-Z]-?\d+)',
            r'(?:pursuant to|under)\s+(?:agenda item|item)\s*([A-Z]-?\d+)'
        ]
        
        for pattern in relationship_patterns:
            matches = re.finditer(pattern, content, re.IGNORECASE)
            for match in matches:
                relationships.append({
                    'type': 'reference',
                    'target': match.group(1),
                    'context': match.group(0)
                })
        
        return relationships


================================================================================


################################################################################
# File: scripts/graph_stages/document_linker.py
################################################################################

# File: scripts/graph_stages/document_linker.py

"""
Document Linker
Links ordinance documents to their corresponding agenda items.
"""

import logging
import re
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import json
from datetime import datetime
import PyPDF2
from groq import Groq
import os
from dotenv import load_dotenv
import asyncio
from concurrent.futures import ThreadPoolExecutor
import multiprocessing

load_dotenv()

log = logging.getLogger('document_linker')


class DocumentLinker:
    """Links ordinance and resolution documents to agenda items."""
    
    def __init__(self,
                 openai_api_key: Optional[str] = None,
                 model: str = "gpt-4.1-mini-2025-04-14",
                 agenda_extraction_max_tokens: int = 32768):
        """Initialize the document linker."""
        self.api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY not found in environment")
        
        self.client = Groq()
        self.model = model
        self.agenda_extraction_max_tokens = agenda_extraction_max_tokens
    
    async def link_documents_for_meeting(self, 
                                       meeting_date: str,
                                       documents_dir: Path) -> Dict[str, List[Dict]]:
        """Find and link all documents for a specific meeting date."""
        log.info(f"🔗 Linking documents for meeting date: {meeting_date}")
        log.info(f"📁 Looking for ordinances in: {documents_dir}")
        
        # Create debug directory
        debug_dir = Path("city_clerk_documents/graph_json/debug")
        debug_dir.mkdir(exist_ok=True)
        
        # Convert date format: "01.09.2024" -> "01_09_2024"
        date_underscore = meeting_date.replace(".", "_")
        
        # Log what we're searching for
        with open(debug_dir / "document_search_info.txt", 'w') as f:
            f.write(f"Meeting Date: {meeting_date}\n")
            f.write(f"Date with underscores: {date_underscore}\n")
            f.write(f"Search directory: {documents_dir}\n")
            f.write(f"Search pattern: *{date_underscore}.pdf\n")
        
        # Find all matching ordinance/resolution files
        # Pattern: YYYY-## - MM_DD_YYYY.pdf
        pattern = f"*{date_underscore}.pdf"
        matching_files = list(documents_dir.glob(pattern))
        
        # Also try without spaces in case filenames vary
        pattern2 = f"*{date_underscore}*.pdf"
        additional_files = [f for f in documents_dir.glob(pattern2) if f not in matching_files]
        matching_files.extend(additional_files)
        
        # List all files in directory for debugging
        all_files = list(documents_dir.glob("*.pdf"))
        with open(debug_dir / "all_ordinance_files.txt", 'w') as f:
            f.write(f"Total files in {documents_dir}: {len(all_files)}\n")
            for file in sorted(all_files):
                f.write(f"  - {file.name}\n")
        
        log.info(f"📄 Found {len(matching_files)} documents for date {meeting_date}")
        
        if matching_files:
            log.info("📄 Documents found:")
            for f in matching_files[:5]:
                log.info(f"   - {f.name}")
            if len(matching_files) > 5:
                log.info(f"   ... and {len(matching_files) - 5} more")
        
        # Save matched files list
        with open(debug_dir / "matched_documents.txt", 'w') as f:
            f.write(f"Documents matching date {meeting_date}:\n")
            for file in matching_files:
                f.write(f"  - {file.name}\n")
        
        # Process documents in parallel
        linked_documents = {
            "ordinances": [],
            "resolutions": []
        }
        
        if matching_files:
            # Use asyncio.gather for parallel processing
            max_concurrent = min(multiprocessing.cpu_count() * 2, 10)
            semaphore = asyncio.Semaphore(max_concurrent)
            
            async def process_with_semaphore(doc_path):
                async with semaphore:
                    return await self._process_document(doc_path, meeting_date)
            
            # Process all documents concurrently
            results = await asyncio.gather(
                *[process_with_semaphore(doc_path) for doc_path in matching_files],
                return_exceptions=True
            )
            
            # Categorize results
            for doc_info, doc_path in zip(results, matching_files):
                if isinstance(doc_info, Exception):
                    log.error(f"Error processing {doc_path.name}: {doc_info}")
                    continue
                if doc_info:
                    if "ordinance" in doc_info.get("title", "").lower():
                        linked_documents["ordinances"].append(doc_info)
                    else:
                        linked_documents["resolutions"].append(doc_info)
        
        # Save linked documents info
        with open(debug_dir / "linked_documents.json", 'w') as f:
            json.dump(linked_documents, f, indent=2)
        
        log.info(f"✅ Linked {len(linked_documents['ordinances'])} ordinances, {len(linked_documents['resolutions'])} resolutions")
        return linked_documents
    
    async def _process_document(self, doc_path: Path, meeting_date: str) -> Optional[Dict[str, Any]]:
        """Process a single document to extract agenda item reference."""
        try:
            # Extract document number from filename (e.g., "2024-04" from "2024-04 - 01_23_2024.pdf")
            doc_match = re.match(r'^(\d{4}-\d{2})', doc_path.name)
            if not doc_match:
                log.warning(f"Could not parse document number from {doc_path.name}")
                return None
            
            document_number = doc_match.group(1)
            
            # Extract text from PDF
            text = self._extract_pdf_text(doc_path)
            if not text:
                log.warning(f"No text extracted from {doc_path.name}")
                return None
            
            # Extract agenda item code using LLM
            item_code = await self._extract_agenda_item_code(text, document_number)
            
            # Extract additional metadata
            parsed_data = self._parse_document_metadata(text)
            
            doc_info = {
                "path": str(doc_path),
                "filename": doc_path.name,
                "document_number": document_number,
                "item_code": item_code,
                "document_type": "Ordinance" if "ordinance" in text[:500].lower() else "Resolution",
                "title": self._extract_title(text),
                "parsed_data": parsed_data
            }
            
            log.info(f"📄 Processed {doc_path.name}: Item {item_code or 'NOT_FOUND'}")
            return doc_info
            
        except Exception as e:
            log.error(f"Error processing {doc_path.name}: {e}")
            return None
    
    def _extract_pdf_text(self, pdf_path: Path) -> str:
        """Extract text from PDF file."""
        try:
            with open(pdf_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                text_parts = []
                
                # Extract text from all pages
                for page in reader.pages:
                    text = page.extract_text()
                    if text:
                        text_parts.append(text)
                
                return "\n".join(text_parts)
        except Exception as e:
            log.error(f"Failed to extract text from {pdf_path.name}: {e}")
            return ""
    
    async def _extract_agenda_item_code(self, text: str, document_number: str) -> Optional[str]:
        """Extract agenda item code from document text using LLM."""
        # Create debug directory if it doesn't exist
        debug_dir = Path("city_clerk_documents/graph_json/debug")
        debug_dir.mkdir(exist_ok=True)
        
        # Send the entire document to qwen-32b
        text_excerpt = text
        
        # Save the full text being sent to LLM for debugging
        with open(debug_dir / f"llm_input_{document_number}.txt", 'w', encoding='utf-8') as f:
            f.write(f"Document: {document_number}\n")
            f.write(f"Text length: {len(text)} characters\n")
            f.write(f"Using model: {self.model}\n")
            f.write(f"Max tokens: {self.agenda_extraction_max_tokens}\n")
            f.write("\n--- FULL DOCUMENT SENT TO LLM ---\n")
            f.write(text_excerpt)
        
        log.info(f"📄 Sending full document to LLM for {document_number}: {len(text)} characters")
        
        prompt = f"""You are analyzing a City of Coral Gables ordinance document (Document #{document_number}).

Your task is to find the AGENDA ITEM CODE referenced in this document.

IMPORTANT: The agenda item can appear ANYWHERE in the document - on page 3, at the end, or anywhere else. Search the ENTIRE document carefully.

The agenda item typically appears in formats like:
- (Agenda Item: E-1)
- Agenda Item: E-3)
- (Agenda Item E-1)
- Item H-3
- H.-3. (with periods and dots)
- E.-2. (with dots)
- E-2 (without dots)
- Item E-2

Full document text:
{text_excerpt}

Respond in this EXACT format:
AGENDA_ITEM: [code] or AGENDA_ITEM: NOT_FOUND

Important: Return the code as it appears (e.g., E-2, not E.-2.)

Examples:
- If you find "(Agenda Item: E-2)" → respond: AGENDA_ITEM: E-2
- If you find "Item E-2" → respond: AGENDA_ITEM: E-2
- If you find "H.-3." → respond: AGENDA_ITEM: H-3
- If no agenda item found → respond: AGENDA_ITEM: NOT_FOUND"""
        
        try:
            response = self.client.chat.completions.create(
                model="meta-llama/llama-4-maverick-17b-128e-instruct",
                messages=[
                    {"role": "system", "content": "You are a precise data extractor. Find and extract only the agenda item code. Search the ENTIRE document thoroughly."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_completion_tokens=8192,
                top_p=1,
                stream=False,
                stop=None
            )
            
            raw_response = response.choices[0].message.content.strip()
            
            # Save raw LLM response for debugging
            with open(debug_dir / f"llm_response_{document_number}_raw.txt", 'w', encoding='utf-8') as f:
                f.write(raw_response)
            
            # Parse response directly (no qwen parsing needed)
            result = raw_response
            
            # Log the response for debugging
            log.info(f"LLM response for {document_number} (first 200 chars): {result[:200]}")
            
            # Parse the response
            if "AGENDA_ITEM:" in result:
                code = result.split("AGENDA_ITEM:")[1].strip()
                if code != "NOT_FOUND":
                    # Normalize the code (remove dots, ensure format)
                    code = self._normalize_item_code(code)
                    log.info(f"✅ Found agenda item code for {document_number}: {code}")
                    return code
                else:
                    log.warning(f"❌ LLM could not find agenda item in {document_number}")
            else:
                log.error(f"❌ Invalid LLM response format for {document_number}: {result[:100]}")
            
            return None
            
        except Exception as e:
            log.error(f"Failed to extract agenda item for {document_number}: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _normalize_item_code(self, code: str) -> str:
        """Normalize item code to consistent format."""
        if not code:
            return code
        
        # Remove trailing dots and spaces
        code = code.rstrip('. ')
        
        # Remove dots between letter and dash: "E.-1" -> "E-1"
        code = re.sub(r'([A-Z])\.(-)', r'\1\2', code)
        
        # Handle cases without dash: "E.1" -> "E-1"
        code = re.sub(r'([A-Z])\.(\d)', r'\1-\2', code)
        
        # Remove any remaining dots
        code = code.replace('.', '')
        
        return code
    
    def _extract_title(self, text: str) -> str:
        """Extract document title from text."""
        # Look for "AN ORDINANCE" or "A RESOLUTION" pattern
        title_match = re.search(r'(AN?\s+(ORDINANCE|RESOLUTION)[^.]+\.)', text[:2000], re.IGNORECASE)
        if title_match:
            return title_match.group(1).strip()
        
        # Fallback to first substantive line
        lines = text.split('\n')
        for line in lines[:20]:
            if len(line) > 20 and not line.isdigit():
                return line.strip()[:200]
        
        return "Untitled Document"
    
    def _parse_document_metadata(self, text: str) -> Dict[str, Any]:
        """Parse additional metadata from document."""
        metadata = {}
        
        # Extract date passed
        date_match = re.search(r'day\s+of\s+(\w+),?\s+(\d{4})', text)
        if date_match:
            metadata["date_passed"] = date_match.group(0)
        
        # Extract vote information
        vote_match = re.search(r'PASSED\s+AND\s+ADOPTED.*?(\d+).*?(\d+)', text, re.IGNORECASE | re.DOTALL)
        if vote_match:
            metadata["vote_details"] = {
                "ayes": vote_match.group(1),
                "nays": vote_match.group(2) if len(vote_match.groups()) > 1 else "0"
            }
        
        # Extract motion information
        motion_match = re.search(r'motion\s+(?:was\s+)?made\s+by\s+([^,]+)', text, re.IGNORECASE)
        if motion_match:
            metadata["motion"] = {"moved_by": motion_match.group(1).strip()}
        
        # Extract mayor signature
        mayor_match = re.search(r'Mayor[:\s]+([^\n]+)', text[-1000:])
        if mayor_match:
            metadata["signatories"] = {"mayor": mayor_match.group(1).strip()}
        
        return metadata


================================================================================


################################################################################
# File: scripts/microsoft_framework/prompt_tuner.py
################################################################################

# File: scripts/microsoft_framework/prompt_tuner.py

import subprocess
import sys
import os
from pathlib import Path
import shutil

class CityClerkPromptTuner:
    """Auto-tune GraphRAG prompts for city clerk documents."""
    
    def __init__(self, graphrag_root: Path):
        self.graphrag_root = Path(graphrag_root)
        self.prompts_dir = self.graphrag_root / "prompts"
        
    def run_auto_tuning(self):
        """Run GraphRAG auto-tuning for city clerk domain."""
        
        # First, ensure prompts directory is clean
        if self.prompts_dir.exists():
            import shutil
            shutil.rmtree(self.prompts_dir)
        self.prompts_dir.mkdir(parents=True, exist_ok=True)
        
        # Create domain-specific examples file
        examples_file = self.graphrag_root / "domain_examples.txt"
        with open(examples_file, 'w') as f:
            f.write("""
Examples of entities in city clerk documents:

AGENDA_ITEM: E-1, F-10, H-3 (format: letter-number identifying agenda items)
ORDINANCE: 2024-01, 2024-15 (format: year-number for city ordinances)  
RESOLUTION: 2024-123, 2024-45 (format: year-number for city resolutions)
MEETING: January 9, 2024 City Commission Meeting
PERSON: Commissioner Smith, Mayor Johnson
ORGANIZATION: City of Coral Gables, Parks Department
MONEY: $1.5 million, $250,000
PROJECT: Waterfront Development, Parks Renovation

Example text:
"Agenda Item E-1 relates to Ordinance 2024-01 regarding the Cocoplum security district."
"Commissioner Smith moved to approve Resolution 2024-15 for $1.5 million funding."
""")
        
        # Get the correct Python executable
        python_exe = self.get_venv_python()
        print(f"🐍 Using Python: {python_exe}")
        
        # Run tuning with correct arguments
        cmd = [
            python_exe,
            "-m", "graphrag", "prompt-tune",
            "--root", str(self.graphrag_root),
            "--config", str(self.graphrag_root / "settings.yaml"),
            "--domain", "city government meetings, ordinances, resolutions, agenda items like E-1 and F-10",
            "--selection-method", "random",
            "--limit", "50",
            "--language", "English",
            "--max-tokens", "2000",
            "--chunk-size", "1200",
            "--output", str(self.prompts_dir)
        ]
        
        # Note: --examples flag might not exist in this version
        # Remove it if it causes issues
        
        subprocess.run(cmd, check=True)
        
    def get_venv_python(self):
        """Get the correct Python executable."""
        # Check if we're in a venv
        if sys.prefix != sys.base_prefix:
            return sys.executable
        
        # Try common venv locations
        venv_paths = [
            'venv/bin/python3',
            'venv/bin/python',
            '.venv/bin/python3',
            '.venv/bin/python',
            'city_clerk_rag/bin/python3',
            'city_clerk_rag/bin/python'
        ]
        
        for venv_path in venv_paths:
            full_path = os.path.join(os.getcwd(), venv_path)
            if os.path.exists(full_path):
                return full_path
        
        # Fallback
        return sys.executable
        
    def create_manual_prompts(self):
        """Create prompts manually without auto-tuning."""
        self.prompts_dir.mkdir(parents=True, exist_ok=True)
        
        # Entity extraction prompt - GraphRAG expects specific format
        entity_prompt = """
-Goal-
Given a text document from the City of Coral Gables, identify all entities and their relationships with high precision, following strict context rules.

**CRITICAL RULES FOR CONTEXT AND IDENTITY:**
1.  **Strict Association**: When you extract an entity, its description and relationships MUST come from the immediate surrounding text ONLY.
2.  **Detect Aliases and Identity**: If the text states or strongly implies that two different identifiers (e.g., "Agenda Item E-1" and "Ordinance 2024-01") refer to the SAME underlying legislative action, you MUST create a relationship between them.
    *   **Action**: Create both entities (e.g., `AGENDA_ITEM:E-1` and `ORDINANCE:2024-01`).
    *   **Relationship**: Link them with a relationship like `("relationship"<|>E-1<|>2024-01<|>is the same legislative action<|>10)`.
    *   **Description**: The descriptions for both entities should be consistent and reflect their shared identity.

-Steps-
1. Identify all entities. For each identified entity, extract the following information:
- title: Name of the entity, capitalized
- type: One of the following types: [{entity_types}]
- description: Comprehensive description of the entity's attributes and activities, **based ONLY on the immediate context and aliasing rules**.
Format each entity as ("entity"<|><title><|><type><|><description>)

2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly and directly related* to each other in the immediate text.
For each pair of related entities, extract the following information:
- source: name of the source entity, as identified in step 1
- target: name of the target entity, as identified in step 1
- description: explanation as to why you think the source entity and the target entity are related to each other, **citing direct evidence from the text**.
- weight: a numeric score indicating strength of the relationship between the source entity and target entity
Format each relationship as ("relationship"<|><source><|><target><|><description><|><weight>)

3. Return output in English as a single list of all the entities and relationships identified in steps 1 and 2. Use **{record_delimiter}** as the list delimiter.

4. When finished, output {completion_delimiter}

-Examples-
Entity types: AGENDA_ITEM, ORDINANCE, PERSON, ORGANIZATION
Text: "Agenda Item E-1, an ordinance to amend zoning, was passed. This is Ordinance 2024-01."
Output:
("entity"<|>E-1<|>AGENDA_ITEM<|>Agenda item E-1, which is the same as Ordinance 2024-01 and amends zoning.)
{record_delimiter}
("entity"<|>2024-01<|>ORDINANCE<|>Ordinance 2024-01, also known as agenda item E-1, which amends zoning.)
{record_delimiter}
("relationship"<|>E-1<|>2024-01<|>is the same as<|>10)
{completion_delimiter}

-Real Data-
Entity types: {entity_types}
Text: {input_text}
Output:
"""
        
        with open(self.prompts_dir / "entity_extraction.txt", 'w') as f:
            f.write(entity_prompt)
        
        # Community report prompt
        community_prompt = """
You are analyzing a community of related entities from city government documents.
Provide a comprehensive summary of the community, focusing on:
1. Key entities and their roles
2. Main relationships and interactions
3. Important decisions or actions
4. Overall significance to city governance

Community data:
{input_text}

Summary:
"""
        
        with open(self.prompts_dir / "community_report.txt", 'w') as f:
            f.write(community_prompt)
        
        print("✅ Created manual prompts with GraphRAG format")
        
    def customize_prompts(self):
        """Further customize prompts for city clerk specifics."""
        # Load and modify entity extraction prompt
        entity_prompt_path = self.prompts_dir / "entity_extraction.txt"
        
        if entity_prompt_path.exists():
            with open(entity_prompt_path, 'r') as f:
                prompt = f.read()
            
            # Add city clerk specific examples
            custom_additions = """
### City Clerk Specific Instructions:
- Pay special attention to agenda item codes (e.g., E-1, F-10, H-3)
- Extract voting records (who voted yes/no on what)
- Identify ordinance and resolution numbers (e.g., 2024-01, Resolution 2024-123)
- Extract budget amounts and financial figures
- Identify project names and development proposals
- Note public comment speakers and their concerns
"""
            
            # Insert custom additions
            prompt = prompt.replace("-Real Data-", custom_additions + "\n-Real Data-")
            
            with open(entity_prompt_path, 'w') as f:
                f.write(prompt)


================================================================================


################################################################################
# File: scripts/graph_stages/pdf_extractor.py
################################################################################

# scripts/graph_stages/pdf_extractor.py

import os
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import logging
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.datamodel.base_models import InputFormat, DocumentStream
from docling.datamodel.pipeline_options import PdfPipelineOptions
import json
from io import BytesIO

# Set up logging
logging.basicConfig(level=logging.INFO)
log = logging.getLogger(__name__)

class PDFExtractor:
    """Extract text from PDFs using Docling for accurate OCR and text extraction."""
    
    def __init__(self, pdf_dir: Path, output_dir: Optional[Path] = None):
        """Initialize the PDF extractor with Docling."""
        self.pdf_dir = Path(pdf_dir)
        self.output_dir = output_dir or Path("city_clerk_documents/extracted_text")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Create debug directory
        self.debug_dir = self.output_dir / "debug"
        self.debug_dir.mkdir(exist_ok=True)
        
        # Initialize Docling converter with OCR enabled
        # Configure for better OCR and structure detection
        pipeline_options = PdfPipelineOptions()
        pipeline_options.do_ocr = True  # Enable OCR for better text extraction
        pipeline_options.do_table_structure = True  # Better table extraction
        
        self.converter = DocumentConverter(
            format_options={
                InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
            }
        )
    
    def extract_text_from_pdf(self, pdf_path: Path) -> Tuple[str, List[Dict[str, str]]]:
        """
        Extract text from PDF using Docling with OCR support.
        
        Returns:
            Tuple of (full_text, pages) where pages is a list of dicts with 'text' and 'page_num'
        """
        log.info(f"📄 Extracting text from: {pdf_path.name}")
        
        # Convert PDF with Docling - just pass the path directly
        result = self.converter.convert(str(pdf_path))
        
        # Get the document
        doc = result.document
        
        # Get the markdown representation which preserves structure better
        full_markdown = doc.export_to_markdown()
        
        # Extract pages if available
        pages = []
        
        # Try to extract page-level content from the document structure
        if hasattr(doc, 'pages') and doc.pages:
            for page_num, page in enumerate(doc.pages, 1):
                # Get page text
                page_text = ""
                if hasattr(page, 'text'):
                    page_text = page.text
                elif hasattr(page, 'get_text'):
                    page_text = page.get_text()
                else:
                    # Try to extract from elements
                    page_elements = []
                    if hasattr(page, 'elements'):
                        for element in page.elements:
                            if hasattr(element, 'text'):
                                page_elements.append(element.text)
                    page_text = "\n".join(page_elements)
                
                if page_text:
                    pages.append({
                        'text': page_text,
                        'page_num': page_num
                    })
        
        # If we couldn't extract pages, create one page with all content
        if not pages and full_markdown:
            pages = [{
                'text': full_markdown,
                'page_num': 1
            }]
        
        # Use markdown as the full text (better structure preservation)
        full_text = full_markdown if full_markdown else ""
        
        # Save debug information
        debug_info = {
            'file': pdf_path.name,
            'total_pages': len(pages),
            'total_characters': len(full_text),
            'extraction_method': 'docling',
            'has_markdown': bool(full_markdown),
            'doc_attributes': list(dir(doc)) if doc else []
        }
        
        debug_file = self.debug_dir / f"{pdf_path.stem}_extraction_debug.json"
        with open(debug_file, 'w', encoding='utf-8') as f:
            json.dump(debug_info, f, indent=2)
        
        # Save the full extracted text for inspection
        text_file = self.debug_dir / f"{pdf_path.stem}_full_text.txt"
        with open(text_file, 'w', encoding='utf-8') as f:
            f.write(full_text)
        
        # Save markdown version separately for debugging
        if full_markdown:
            markdown_file = self.debug_dir / f"{pdf_path.stem}_markdown.md"
            with open(markdown_file, 'w', encoding='utf-8') as f:
                f.write(full_markdown)
        
        log.info(f"✅ Extracted {len(pages)} pages, {len(full_text)} characters")
        
        return full_text, pages
    
    def extract_all_pdfs(self) -> Dict[str, Dict[str, any]]:
        """Extract text from all PDFs in the directory."""
        pdf_files = list(self.pdf_dir.glob("*.pdf"))
        log.info(f"📚 Found {len(pdf_files)} PDF files to process")
        
        extracted_data = {}
        
        for pdf_path in pdf_files:
            try:
                full_text, pages = self.extract_text_from_pdf(pdf_path)
                
                extracted_data[pdf_path.stem] = {
                    'full_text': full_text,
                    'pages': pages,
                    'metadata': {
                        'filename': pdf_path.name,
                        'num_pages': len(pages),
                        'total_chars': len(full_text),
                        'extraction_method': 'docling'
                    }
                }
                
                # Save extracted text
                output_file = self.output_dir / f"{pdf_path.stem}_extracted.json"
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(extracted_data[pdf_path.stem], f, indent=2, ensure_ascii=False)
                
                log.info(f"✅ Saved extracted text to: {output_file}")
                
            except Exception as e:
                log.error(f"❌ Failed to process {pdf_path.name}: {e}")
                import traceback
                traceback.print_exc()
                # No fallback - if Docling fails, we fail
                raise
        
        return extracted_data

# Add this to requirements.txt:
# unstructured[pdf]==0.10.30
# pytesseract>=0.3.10
# pdf2image>=1.16.3
# poppler-utils (system dependency - install with: brew install poppler)


================================================================================


################################################################################
# File: scripts/microsoft_framework/create_custom_prompts.py
################################################################################

# File: scripts/microsoft_framework/create_custom_prompts.py

#!/usr/bin/env python3
"""Create custom prompts for city clerk entity extraction."""

from pathlib import Path

def create_entity_extraction_prompt():
    """Create custom entity extraction prompt with STRICT isolation rules."""
    
    prompt_dir = Path("graphrag_data/prompts")
    prompt_dir.mkdir(parents=True, exist_ok=True)
    
    prompt = """
You are an AI assistant specialized in analyzing government documents for the City of Coral Gables. Your task is to extract entities and their relationships with EXTREME PRECISION and STRICT ISOLATION.

**CRITICAL ISOLATION RULES:**

1. **ABSOLUTE ENTITY SEPARATION**: Each entity is completely independent. Information about one entity (e.g., E-1) must NEVER be mixed with information about another entity (e.g., E-4).

2. **CONTEXT BOUNDARIES**: When you see isolation markers like "=== ISOLATED ENTITY: AGENDA ITEM E-1 ===" treat everything within those markers as belonging ONLY to that entity.

3. **NO CROSS-CONTAMINATION**: Even if entities appear similar or sequential (E-1, E-2, E-3, E-4), they are COMPLETELY SEPARATE entities with no shared information.

4. **STRICT SCOPE**: When extracting information about an entity, use ONLY the text within its specific section or document. Do not infer or assume relationships unless explicitly stated.

**ENTITY EXTRACTION RULES:**

For each entity, extract ONLY information that is explicitly mentioned in its isolated context:

1. **AGENDA_ITEM**: 
   - Extract ONLY the specific item code mentioned (e.g., "E-1")
   - Use ONLY the description from within that item's section
   - Do NOT reference other agenda items in the description

2. **ORDINANCE/RESOLUTION**:
   - Extract the document number exactly as stated
   - If linked to an agenda item, create a relationship ONLY if explicitly stated

3. **RELATIONSHIPS**:
   - Create relationships ONLY when explicitly stated in the text
   - Example: "Agenda Item E-1 relates to Ordinance 2024-01" → Create relationship
   - Do NOT create relationships based on proximity or sequence

**OUTPUT FORMAT:**

For each entity: ("entity"<|><id><|><type><|><description>)
- id: The exact identifier (E-1, 2024-01, etc.)
- type: AGENDA_ITEM, ORDINANCE, RESOLUTION, PERSON, etc.
- description: Information from ONLY this entity's isolated context

For relationships: ("relationship"<|><source><|><target><|><description><|><weight>)
- Only create when explicitly stated
- description must quote the text that establishes the relationship

**EXAMPLES:**

CORRECT:
Text: "=== ISOLATED ENTITY: AGENDA ITEM E-1 === 
Agenda Item E-1: Zoning amendment for 123 Main St.
=== END ==="
Output: ("entity"<|>E-1<|>AGENDA_ITEM<|>Zoning amendment for 123 Main St.)

INCORRECT (mixing entities):
Text about E-1...
Output: ("entity"<|>E-1<|>AGENDA_ITEM<|>Zoning amendment, similar to E-4's proposal) ❌

Remember: COMPLETE ISOLATION. Each entity stands alone.
"""
    
    with open(prompt_dir / "entity_extraction.txt", 'w') as f:
        f.write(prompt)
    
    print(f"✅ Created strict isolation entity extraction prompt")

def create_entity_specific_prompt():
    """Create a custom prompt for strict entity-specific queries."""
    
    prompt_dir = Path("graphrag_data/prompts")
    prompt_dir.mkdir(parents=True, exist_ok=True)
    
    prompt = """
You are answering a question about a SPECIFIC entity in the City of Coral Gables government documents.

CRITICAL INSTRUCTIONS:
1. Focus EXCLUSIVELY on the SPECIFIC entity mentioned in the question.
2. DO NOT include information about other similar entities, even if they're related.
3. If asked about Agenda Item E-1, ONLY discuss E-1, not E-2, E-3, E-4, etc.
4. If asked about Ordinance 2024-01, ONLY discuss that specific ordinance, not other ordinances.
5. If asked about Resolution 2024-123, ONLY discuss that specific resolution, not other resolutions.

The user has specifically requested information about ONE entity. Keep your response focused ONLY on that entity.

If you're uncertain about details of the specific entity, state this clearly rather than including information about other entities.

Question: {input_query}
"""
    
    with open(prompt_dir / "entity_specific_query.txt", 'w') as f:
        f.write(prompt)
    
    print(f"✅ Created custom entity-specific query prompt")

if __name__ == "__main__":
    create_entity_extraction_prompt()


================================================================================


################################################################################
# File: check_status.py
################################################################################

# File: check_status.py

#!/usr/bin/env python3
"""
Quick status checker for GraphRAG process
"""
import os
import time
from pathlib import Path
from datetime import datetime

def check_status():
    base_dir = Path("/Users/gianmariatroiani/Documents/knologi/graph_database/graphrag_data")
    logs_dir = base_dir / "logs"
    output_dir = base_dir / "output"
    
    print(f"🔍 GraphRAG Status Check - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    # Check if process is running
    import subprocess
    try:
        result = subprocess.run(['pgrep', '-f', 'monitor_graphrag.py'], 
                              capture_output=True, text=True)
        if result.stdout.strip():
            print("✅ GraphRAG monitor process is RUNNING")
            print(f"   Process ID: {result.stdout.strip()}")
        else:
            print("❌ GraphRAG monitor process is NOT running")
    except:
        print("❓ Cannot determine process status")
    
    # Check latest monitor log
    if logs_dir.exists():
        monitor_logs = list(logs_dir.glob("graphrag_monitor_*.log"))
        if monitor_logs:
            latest_log = max(monitor_logs, key=lambda x: x.stat().st_mtime)
            print(f"📋 Latest monitor log: {latest_log.name}")
            
            # Show last few lines
            try:
                with open(latest_log, 'r') as f:
                    lines = f.readlines()
                    if lines:
                        print("📖 Last 3 log entries:")
                        for line in lines[-3:]:
                            print(f"   {line.strip()}")
            except:
                pass
    
    # Check output files
    print(f"\n📁 Output Files Status:")
    expected_files = [
        "entities.parquet", "relationships.parquet", "communities.parquet",
        "community_reports.parquet", "text_units.parquet"
    ]
    
    if output_dir.exists():
        for file_name in expected_files:
            file_path = output_dir / file_name
            if file_path.exists():
                size = file_path.stat().st_size
                mod_time = datetime.fromtimestamp(file_path.stat().st_mtime)
                print(f"   ✅ {file_name}: {size:,} bytes (modified: {mod_time.strftime('%H:%M:%S')})")
            else:
                print(f"   ⏳ {file_name}: Not created yet")
    else:
        print("   ❌ Output directory doesn't exist yet")
    
    # Check GraphRAG engine log
    engine_log = logs_dir / "indexing-engine.log"
    if engine_log.exists():
        mod_time = datetime.fromtimestamp(engine_log.stat().st_mtime)
        print(f"\n📊 GraphRAG engine log last updated: {mod_time.strftime('%H:%M:%S')}")
        
        # Check for errors in recent lines
        try:
            with open(engine_log, 'r') as f:
                lines = f.readlines()
                recent_lines = lines[-10:] if len(lines) > 10 else lines
                errors = [line for line in recent_lines if 'ERROR' in line.upper() or 'FAILED' in line.upper()]
                if errors:
                    print("⚠️  Recent errors found:")
                    for error in errors[-2:]:  # Show last 2 errors
                        print(f"   {error.strip()}")
                else:
                    print("✅ No recent errors detected")
        except:
            pass
    
    print("\n" + "=" * 60)
    print("💡 To check status again, run: python3 check_status.py")
    print("🛑 To stop the process: pkill -f monitor_graphrag.py")

if __name__ == "__main__":
    check_status()


================================================================================


################################################################################
# File: investigate_graph.py
################################################################################

# File: investigate_graph.py

#!/usr/bin/env python3
"""
Investigate the GraphRAG knowledge graph to debug data integrity issues.
This script reads the entities and relationships to find out what the graph knows about a specific item.
"""

import pandas as pd
from pathlib import Path

def investigate_entity(entity_name: str):
    """
    Investigate a specific entity in the GraphRAG output to find its connections.
    """
    print(f"🔍 Investigating entity: '{entity_name}'")
    print("=" * 60)
    
    # Define paths to GraphRAG output
    output_dir = Path("graphrag_data/output")
    entities_path = output_dir / "entities.parquet"
    relationships_path = output_dir / "relationships.parquet"
    
    # Check if files exist
    if not entities_path.exists() or not relationships_path.exists():
        print("❌ GraphRAG output files (entities.parquet, relationships.parquet) not found.")
        return
    
    # Load the data
    try:
        entities_df = pd.read_parquet(entities_path)
        relationships_df = pd.read_parquet(relationships_path)
        print(f"✅ Loaded {len(entities_df)} entities and {len(relationships_df)} relationships.")
    except Exception as e:
        print(f"❌ Error loading parquet files: {e}")
        return
    
    # Find the entity
    target_entity = entities_df[entities_df['title'].str.upper() == entity_name.upper()]
    
    if target_entity.empty:
        print(f"Entity '{entity_name}' not found in the knowledge graph.")
        return
    
    print(f"\n--- Entity Details for '{entity_name}' ---")
    print(target_entity.to_string())
    
    entity_id = target_entity.index[0]
    
    # Find all relationships involving this entity
    related_as_source = relationships_df[relationships_df['source'] == entity_id]
    related_as_target = relationships_df[relationships_df['target'] == entity_id]
    
    all_relations = pd.concat([related_as_source, related_as_target])
    
    if all_relations.empty:
        print(f"\n--- No relationships found for '{entity_name}' ---")
    else:
        print(f"\n--- Found {len(all_relations)} relationships for '{entity_name}' ---")
        
        # Get the names of the connected entities
        connected_entity_ids = set(all_relations['source']).union(set(all_relations['target']))
        connected_entity_ids.discard(entity_id) # Remove the entity itself
        
        connected_entities = entities_df[entities_df.index.isin(connected_entity_ids)]
        
        print("This entity is connected to:")
        for _, row in connected_entities.iterrows():
            print(f"  - {row['title']} (Type: {row['type']})")
        
        print("\nFull Relationship Details:")
        print(all_relations.to_string())

if __name__ == "__main__":
    # Investigate both E-1 and E-4 to see the difference
    investigate_entity("E-1")
    print("\n\n" + "="*80 + "\n\n")
    investigate_entity("E-4")


================================================================================


################################################################################
# File: scripts/microsoft_framework/city_clerk_settings_template.yaml
################################################################################

# File: scripts/microsoft_framework/city_clerk_settings_template.yaml

llm:
  api_type: "openai"
  model: "gpt-4.1-mini-2025-04-14"
  api_key: "${OPENAI_API_KEY}"
  max_tokens: 32768
  temperature: 0
  
chunks:
  size: 1200
  overlap: 200
  group_by_columns: ["document_type", "meeting_date", "item_code"]
  
entity_extraction:
  prompt: "prompts/city_clerk_entity_extraction.txt"
  entity_types: ["person", "organization", "location", "document", 
                 "meeting", "money", "project", "agenda_item",
                 "ordinance", "resolution", "contract"]
  max_gleanings: 2
  
claim_extraction:
  enabled: true
  prompt: "prompts/city_clerk_claims.txt"
  description: "Extract voting records, motions, and decisions"
  
community_reports:
  prompt: "prompts/city_clerk_community_report.txt"
  max_length: 2000
  max_input_length: 32768
  
embeddings:
  model: "text-embedding-3-small"
  batch_size: 16
  batch_max_tokens: 2048
  
cluster_graph:
  max_cluster_size: 10
  
storage:
  type: "file"
  base_dir: "./output/artifacts"

# Query configuration section
query:
  # Global search settings
  global_search:
    community_level: 2  # Which hierarchical level to use
    max_tokens: 32768
    temperature: 0.0
    top_p: 1.0
    n: 1
    use_dynamic_community_selection: true
    relevance_score_threshold: 0.7
    rate_relevancy_model: "gpt-4.1-mini-2025-04-14"  # Same model for consistency
    
  # Local search settings  
  local_search:
    text_unit_prop: 0.5  # Proportion of context window for text units
    community_prop: 0.1  # Proportion for community summaries
    conversation_history_max_turns: 5
    top_k_entities: 10  # Number of related entities to retrieve
    top_k_relationships: 10
    max_tokens: 32768
    temperature: 0.0
    
  # DRIFT search settings
  drift_search:
    initial_community_level: 2
    max_iterations: 5
    follow_up_expansion: 3
    relevance_threshold: 0.7
    max_tokens: 32768
    temperature: 0.0
    primer_queries: 3  # Initial community queries
    follow_up_depth: 5  # Max recursion depth
    similarity_threshold: 0.8
    termination_strategy: "convergence"  # or "max_depth"
    include_global_context: true


================================================================================


################################################################################
# File: scripts/microsoft_framework/incremental_processor.py
################################################################################

# File: scripts/microsoft_framework/incremental_processor.py

from pathlib import Path
import json
from typing import List, Set
import asyncio

class IncrementalGraphRAGProcessor:
    """Handle incremental updates to GraphRAG index."""
    
    def __init__(self, graphrag_root: Path):
        self.graphrag_root = Path(graphrag_root)
        self.processed_files = self._load_processed_files()
        
    async def process_new_documents(self, new_docs_dir: Path):
        """Process only new documents added since last run."""
        
        # Find new documents
        new_files = []
        for doc_path in new_docs_dir.glob("*.pdf"):
            if doc_path.name not in self.processed_files:
                new_files.append(doc_path)
        
        if not new_files:
            print("✅ No new documents to process")
            return
        
        print(f"📄 Processing {len(new_files)} new documents...")
        
        # Extract text using existing Docling pipeline
        from scripts.graph_stages.pdf_extractor import PDFExtractor
        extractor = PDFExtractor(new_docs_dir)
        
        # Process and add to GraphRAG
        # ... implementation details ...
        
        # Update processed files list
        self._update_processed_files(new_files)
    
    def _load_processed_files(self) -> Set[str]:
        """Load list of previously processed files."""
        processed_files_path = self.graphrag_root / "processed_files.json"
        
        if processed_files_path.exists():
            with open(processed_files_path, 'r') as f:
                return set(json.load(f))
        
        return set()
    
    def _update_processed_files(self, new_files: List[Path]):
        """Update the list of processed files."""
        for file_path in new_files:
            self.processed_files.add(file_path.name)
        
        processed_files_path = self.graphrag_root / "processed_files.json"
        with open(processed_files_path, 'w') as f:
            json.dump(list(self.processed_files), f)


================================================================================


################################################################################
# File: verify_deduplication.py
################################################################################

# File: verify_deduplication.py

#!/usr/bin/env python3
"""Verify deduplication results."""

import pandas as pd
from pathlib import Path

def verify_deduplication():
    """Check deduplication results."""
    
    original_dir = Path("graphrag_data/output")
    dedup_dir = original_dir / "deduplicated"
    
    if not dedup_dir.exists():
        print("❌ No deduplicated data found!")
        return
    
    # Load data
    orig_entities = pd.read_parquet(original_dir / "entities.parquet")
    dedup_entities = pd.read_parquet(dedup_dir / "entities.parquet")
    
    print("📊 Deduplication Results:")
    print(f"   Original entities: {len(orig_entities)}")
    print(f"   Deduplicated entities: {len(dedup_entities)}")
    print(f"   Entities merged: {len(orig_entities) - len(dedup_entities)}")
    
    # Check for aliases
    if 'aliases' in dedup_entities.columns:
        entities_with_aliases = dedup_entities[dedup_entities['aliases'].notna() & (dedup_entities['aliases'] != '')]
        print(f"\n📝 Entities with aliases: {len(entities_with_aliases)}")
        
        print("\nExample merged entities:")
        for idx, entity in entities_with_aliases.head(5).iterrows():
            print(f"\n'{entity['title']}'")
            print(f"  Aliases: {entity['aliases']}")
            if '[Also known as:' in entity.get('description', ''):
                # Extract the alias info
                desc_lines = entity['description'].split('\n')
                for line in desc_lines:
                    if '[Also known as:' in line:
                        print(f"  {line.strip()}")

if __name__ == "__main__":
    verify_deduplication()


================================================================================


################################################################################
# File: scripts/microsoft_framework/__init__.py
################################################################################

# File: scripts/microsoft_framework/__init__.py

"""
Microsoft GraphRAG integration for City Clerk document processing.

This package provides components for integrating Microsoft GraphRAG with
the existing city clerk document processing pipeline.
"""

from .graphrag_initializer import GraphRAGInitializer
from .document_adapter import CityClerkDocumentAdapter
from .prompt_tuner import CityClerkPromptTuner
from .graphrag_pipeline import CityClerkGraphRAGPipeline
from .cosmos_synchronizer import GraphRAGCosmosSync
from .query_engine import CityClerkGraphRAGQuery, QueryType, CityClerkQueryEngine, handle_user_query
from .query_router import SmartQueryRouter, QueryIntent, QueryFocus
from .incremental_processor import IncrementalGraphRAGProcessor
from .graphrag_output_processor import GraphRAGOutputProcessor
from .entity_deduplicator import AdvancedEntityDeduplicator
from .enhanced_entity_deduplicator import EnhancedEntityDeduplicator

__all__ = [
    'GraphRAGInitializer',
    'CityClerkDocumentAdapter',
    'CityClerkPromptTuner',
    'CityClerkGraphRAGPipeline',
    'GraphRAGCosmosSync',
    'CityClerkGraphRAGQuery',
    'CityClerkQueryEngine',
    'QueryType',
    'SmartQueryRouter',
    'QueryIntent',
    'QueryFocus',
    'handle_user_query',
    'IncrementalGraphRAGProcessor',
    'GraphRAGOutputProcessor',
    'AdvancedEntityDeduplicator',
    'EnhancedEntityDeduplicator'
]


================================================================================


################################################################################
# File: run_enhanced_dedup.py
################################################################################

# File: run_enhanced_dedup.py

#!/usr/bin/env python3
import sys
sys.path.append('scripts/microsoft_framework')
from enhanced_entity_deduplicator import EnhancedEntityDeduplicator
from pathlib import Path

if __name__ == '__main__':
    # Run with enhanced progress indicators - correct path
    output_dir = Path('graphrag_data/output')
    deduplicator = EnhancedEntityDeduplicator(output_dir)
    results = deduplicator.deduplicate_entities()


================================================================================

