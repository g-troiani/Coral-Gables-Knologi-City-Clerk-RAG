# Concatenated Project Code - Part 2 of 3
# Generated: 2025-06-12 09:30:51
# Root Directory: /Users/gianmariatroiani/Documents/knologi/graph_database
================================================================================

# File Index - Which Files Are in Which Parts
################################################################################

## Part 1 (15 files):
  - scripts/microsoft_framework/query_engine.py
  - scripts/graph_stages/agenda_pdf_extractor.py
  - scripts/graph_stages/agenda_ontology_extractor.py
  - scripts/microsoft_framework/document_adapter.py
  - scripts/graph_stages/document_linker.py
  - scripts/microsoft_framework/prompt_tuner.py
  - scripts/graph_stages/pdf_extractor.py
  - scripts/microsoft_framework/create_custom_prompts.py
  - check_status.py
  - investigate_graph.py
  - scripts/microsoft_framework/city_clerk_settings_template.yaml
  - scripts/microsoft_framework/incremental_processor.py
  - verify_deduplication.py
  - scripts/microsoft_framework/__init__.py
  - run_enhanced_dedup.py

## Part 2 (15 files):
  - scripts/graph_stages/agenda_graph_builder.py
  - concatenate_scripts_broken.py
  - scripts/graph_stages/enhanced_document_linker.py
  - scripts/graph_stages/verbatim_transcript_linker.py
  - scripts/microsoft_framework/query_router.py
  - scripts/extract_all_to_markdown.py
  - scripts/microsoft_framework/graphrag_output_processor.py
  - scripts/microsoft_framework/graphrag_initializer.py
  - scripts/microsoft_framework/cosmos_synchronizer.py
  - settings.yaml
  - scripts/microsoft_framework/graphrag_pipeline.py
  - check_enhanced_results.py
  - check_ordinances.py
  - requirements.txt
  - scripts/microsoft_framework/query_graphrag.py

## Part 3 (15 files):
  - scripts/microsoft_framework/enhanced_entity_deduplicator.py
  - scripts/graph_stages/ontology_extractor.py
  - graphrag_query_ui.py
  - scripts/microsoft_framework/run_graphrag_pipeline.py
  - scripts/microsoft_framework/entity_deduplicator.py
  - scripts/graph_stages/cosmos_db_client.py
  - scripts/microsoft_framework/README.md
  - scripts/extract_all_pdfs_direct.py
  - extract_documents_for_graphrag.py
  - explore_graphrag_sources.py
  - scripts/microsoft_framework/source_tracker.py
  - config.py
  - scripts/json_to_markdown_converter.py
  - scripts/microsoft_framework/run_graphrag_direct.py
  - scripts/graph_stages/__init__.py


================================================================================


################################################################################
# File: scripts/graph_stages/agenda_graph_builder.py
################################################################################

# File: scripts/graph_stages/agenda_graph_builder.py

"""
Enhanced Agenda Graph Builder - RICH VERSION
Builds comprehensive graph representation from LLM-extracted agenda ontology.
"""
import logging
from typing import Dict, List, Optional, Any
from pathlib import Path
import hashlib
import json
import calendar
import re

from .cosmos_db_client import CosmosGraphClient

log = logging.getLogger('pipeline_debug.graph_builder')


class AgendaGraphBuilder:
    """Build comprehensive graph representation from rich agenda ontology."""
    
    def __init__(self, cosmos_client: CosmosGraphClient, upsert_mode: bool = True):
        self.cosmos = cosmos_client
        self.upsert_mode = upsert_mode
        self.entity_id_cache = {}  # Cache for entity IDs
        self.partition_value = 'demo'  # Partition value property
        
        # Track statistics
        self.stats = {
            'nodes_created': 0,
            'nodes_updated': 0,
            'edges_created': 0,
            'edges_skipped': 0
        }
    
    @staticmethod
    def normalize_item_code(code: str) -> str:
        """Normalize item codes to consistent format for matching."""
        if not code:
            return code
        
        # Log original code for debugging
        original = code
        
        # Apply normalization patterns
        patterns = [
            (r'^([A-Z])\.?-?(\d+)\.?$', r'\1-\2'),     # A.-1. -> A-1
            (r'^(\d+)\.?-?(\d+)\.?$', r'\1-\2'),       # 1.-1. -> 1-1
            (r'^([A-Z]\d+)$', r'\1'),                   # E1 -> E1 (no change)
        ]
        
        for pattern, replacement in patterns:
            match = re.match(pattern, code)
            if match:
                code = re.sub(pattern, replacement, code)
                break
        else:
            # First, extract valid code pattern if input is messy
            code_match = re.match(r'^([A-Z][-.]?\d+)', code)
            if code_match:
                code = code_match.group(1)
            
            # Remove all dots and ensure consistent format
            code = code.rstrip('.')
            code = re.sub(r'([A-Z])\.(-)', r'\1\2', code)
            code = re.sub(r'([A-Z])\.(\d)', r'\1-\2', code)
            code = re.sub(r'([A-Z])(\d)', r'\1-\2', code)
            code = code.replace('.', '')
            
            # Ensure format is always "E-9" not "E9"
            if re.match(r'^[A-Z]\d+$', code):
                code = f"{code[0]}-{code[1:]}"
        
        if original != code:
            log.debug(f"Normalized '{original}' -> '{code}'")
        
        return code
    
    @staticmethod
    def ensure_us_date_format(date_str: str) -> str:
        """Ensure date is in US format MM-DD-YYYY with dashes."""
        # Handle different input formats
        if '.' in date_str:
            # Format: 01.23.2024 -> 01-23-2024
            return date_str.replace('.', '-')
        elif '/' in date_str:
            # Format: 01/23/2024 -> 01-23-2024
            return date_str.replace('/', '-')
        elif re.match(r'^\d{4}-\d{2}-\d{2}$', date_str):
            # ISO format: 2024-01-23 -> 01-23-2024
            parts = date_str.split('-')
            return f"{parts[1]}-{parts[2]}-{parts[0]}"
        else:
            # Already in correct format or unknown
            return date_str

    async def build_graph(self, ontology_file: Path, linked_docs: Optional[Dict] = None, upsert: bool = True) -> Dict:
        """Build graph from ontology file - main entry point."""
        # Load ontology
        with open(ontology_file, 'r', encoding='utf-8') as f:
            ontology = json.load(f)
        
        return await self.build_graph_from_ontology(ontology, ontology_file, linked_docs)

    async def build_graph_from_ontology(self, ontology: Dict, source_path: Path, linked_docs: Optional[Dict] = None) -> Dict:
        """Build comprehensive graph representation from rich ontology."""
        log.info(f"ðŸ”¨ Starting enhanced graph build for {source_path.name}")
        log.info(f"ðŸ”§ Upsert mode: {'ENABLED' if self.upsert_mode else 'DISABLED'}")
        
        # Reset statistics
        self.stats = {
            'nodes_created': 0,
            'nodes_updated': 0,
            'edges_created': 0,
            'edges_skipped': 0
        }
        
        try:
            graph_data = {
                'nodes': {},
                'edges': [],
                'statistics': {}
            }
            
            # Store agenda structure for reference
            self.current_agenda_structure = ontology.get('agenda_structure', [])
            
            # Store ontology for reference
            self.current_ontology = ontology
            
            # CRITICAL: Ensure meeting date is in US format
            meeting_date_original = ontology['meeting_date']
            meeting_date_us = self.ensure_us_date_format(meeting_date_original)
            meeting_info = ontology['meeting_info']
            
            log.info(f"ðŸ“… Meeting date: {meeting_date_original} -> {meeting_date_us}")
            
            # 1. Create Meeting node as the root
            meeting_id = f"meeting-{meeting_date_us}"
            await self._create_meeting_node(meeting_date_us, meeting_info, source_path.name)
            log.info(f"âœ… Created meeting node: {meeting_id}")
            
            # 1.5 Create Date node and link to meeting
            try:
                date_id = await self._create_date_node(meeting_date_original, meeting_id)
                graph_data['nodes'][date_id] = {
                    'type': 'Date',
                    'date': meeting_date_original
                }
            except Exception as e:
                log.error(f"Failed to create date node: {e}")
            
            graph_data['nodes'][meeting_id] = {
                'type': 'Meeting',
                'date': meeting_date_us,
                'info': meeting_info
            }
            
            # 2. Create nodes for officials present  
            await self._create_official_nodes(meeting_info, meeting_id)
            
            # 3. Process sections and agenda items
            section_count = 0
            item_count = 0
            
            sections = ontology.get('sections', [])
            log.info(f"ðŸ“‘ Processing {len(sections)} sections")
            
            for section_idx, section in enumerate(sections):
                try:
                    section_count += 1
                    section_id = f"section-{meeting_date_us}-{section_idx}"
                    
                    # Create Section node
                    await self._create_section_node(section_id, section, section_idx)
                    log.info(f"âœ… Created section {section_idx}: {section.get('section_name', 'Unknown')}")
                    
                    graph_data['nodes'][section_id] = {
                        'type': 'Section',
                        'name': section['section_name'],
                        'order': section_idx
                    }
                    
                    # Link section to meeting
                    if await self.cosmos.create_edge_if_not_exists(
                        from_id=meeting_id,
                        to_id=section_id,
                        edge_type='HAS_SECTION',
                        properties={'order': section_idx}
                    ):
                        self.stats['edges_created'] += 1
                    else:
                        self.stats['edges_skipped'] += 1
                    
                    # Process items in section
                    previous_item_id = None
                    items = section.get('items', [])
                    
                    for item_idx, item in enumerate(items):
                        try:
                            if not item.get('item_code'):
                                log.warning(f"Skipping item without code in section {section['section_name']}")
                                continue
                                
                            item_count += 1
                            # Normalize the item code
                            normalized_code = self.normalize_item_code(item['item_code'])
                            # Use US date format for item ID
                            item_id = f"item-{meeting_date_us}-{normalized_code}"
                            
                            log.info(f"Creating item: {item_id} (from code: {item['item_code']})")
                            
                            # Create enhanced AgendaItem node
                            await self._create_enhanced_agenda_item_node(item_id, item, section)
                            
                            graph_data['nodes'][item_id] = {
                                'type': 'AgendaItem',
                                'code': normalized_code,
                                'original_code': item['item_code'],
                                'title': item.get('title', 'Unknown')
                            }
                            
                            # Link item to section
                            if await self.cosmos.create_edge_if_not_exists(
                                from_id=section_id,
                                to_id=item_id,
                                edge_type='CONTAINS_ITEM',
                                properties={'order': item_idx}
                            ):
                                self.stats['edges_created'] += 1
                            else:
                                self.stats['edges_skipped'] += 1
                            
                            # Create sequential relationships
                            if previous_item_id:
                                if await self.cosmos.create_edge_if_not_exists(
                                    from_id=previous_item_id,
                                    to_id=item_id,
                                    edge_type='FOLLOWS',
                                    properties={'sequence': item_idx}
                                ):
                                    self.stats['edges_created'] += 1
                                else:
                                    self.stats['edges_skipped'] += 1
                            
                            previous_item_id = item_id
                            
                            # Create rich relationships for this item
                            await self._create_item_relationships(item, item_id, meeting_date_us)
                            
                            # Create URL nodes and relationships
                            await self._create_url_relationships(item, item_id)
                                
                        except Exception as e:
                            log.error(f"Failed to process item {item.get('item_code', 'unknown')}: {e}")
                            
                except Exception as e:
                    log.error(f"Failed to process section {section.get('section_name', 'unknown')}: {e}")
            
            # 4. Create entity nodes from extracted entities
            entity_count = await self._create_entity_nodes(ontology.get('entities', []), meeting_id)
            
            # 5. Create relationships from ontology
            relationship_count = 0
            for rel in ontology.get('relationships', []):
                try:
                    await self._create_ontology_relationship(rel, meeting_date_us)
                    relationship_count += 1
                except Exception as e:
                    log.error(f"Failed to create relationship: {e}")
            
            # Initialize verbatim count
            verbatim_count = 0
            
            # 6. Process linked documents if available
            if linked_docs:
                await self.process_linked_documents(linked_docs, meeting_id, meeting_date_us)
                
                # Process verbatim transcripts if available
                if "verbatim_transcripts" in linked_docs:
                    verbatim_count = await self.process_verbatim_transcripts(
                        linked_docs["verbatim_transcripts"], 
                        meeting_id, 
                        meeting_date_us
                    )
            
            # Update statistics
            graph_data['statistics'] = {
                'sections': section_count,
                'items': item_count, 
                'entities': entity_count,
                'relationships': relationship_count,
                'meeting_date': meeting_date_us,
                'verbatim_transcripts': verbatim_count if verbatim_count else 0
            }
            
            log.info(f"ðŸŽ‰ Enhanced graph build complete for {source_path.name}")
            log.info(f"   ðŸ“Š Statistics:")
            log.info(f"      - Nodes created: {self.stats['nodes_created']}")
            log.info(f"      - Nodes updated: {self.stats['nodes_updated']}")
            log.info(f"      - Edges created: {self.stats['edges_created']}")
            log.info(f"      - Edges skipped: {self.stats['edges_skipped']}")
            log.info(f"   - Sections: {section_count}")
            log.info(f"   - Items: {item_count}")
            log.info(f"   - Entities: {entity_count}")
            log.info(f"   - Relationships: {relationship_count}")
            
            return graph_data
            
        except Exception as e:
            log.error(f"CRITICAL ERROR in build_graph_from_ontology: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    async def _create_meeting_node(self, meeting_date: str, meeting_info: Dict, source_file: str = None) -> str:
        """Create or update Meeting node with comprehensive metadata."""
        meeting_id = f"meeting-{meeting_date}"
        
        # Handle case where meeting_info might be a list (API response error)
        if isinstance(meeting_info, list):
            log.error(f"meeting_info is a list instead of dict: {meeting_info}")
            # Use default values
            meeting_info = {
                'type': 'Regular Meeting',
                'time': '5:30 PM',
                'location': 'City Commission Chambers',
                'commissioners': [],
                'officials': {}
            }
        
        # Handle location - could be string or dict
        location = meeting_info.get('location', 'City Commission Chambers')
        if isinstance(location, dict):
            location_str = f"{location.get('name', 'City Commission Chambers')}"
            if location.get('address'):
                location_str += f" - {location['address']}"
        else:
            location_str = str(location) if location else "City Commission Chambers"
        
        properties = {
            'nodeType': 'Meeting',
            'date': meeting_date,
            'type': meeting_info.get('type', 'Regular Meeting'),
            'time': meeting_info.get('time', '5:30 PM'),
            'location': location_str
        }
        
        if source_file:
            properties['source_file'] = source_file
        
        # Use upsert instead of create
        if self.upsert_mode:
            created = await self.cosmos.upsert_vertex('Meeting', meeting_id, properties)
            if created:
                self.stats['nodes_created'] += 1
                log.info(f"âœ… Created Meeting node: {meeting_id}")
            else:
                self.stats['nodes_updated'] += 1
                log.info(f"ðŸ“ Updated Meeting node: {meeting_id}")
        else:
            await self.cosmos.create_vertex('Meeting', meeting_id, properties)
            self.stats['nodes_created'] += 1
            log.info(f"âœ… Created Meeting node: {meeting_id}")
        
        return meeting_id
    
    async def _create_date_node(self, date_str: str, meeting_id: str) -> str:
        """Create a Date node and link it to the meeting."""
        from datetime import datetime
        
        # Parse date from MM.DD.YYYY format
        parts = date_str.split('.')
        if len(parts) != 3:
            log.error(f"Invalid date format: {date_str}")
            return None
            
        month, day, year = int(parts[0]), int(parts[1]), int(parts[2])
        
        # Create consistent date ID in ISO format
        date_id = f"date-{year:04d}-{month:02d}-{day:02d}"
        
        # Check if date already exists
        if await self.cosmos.vertex_exists(date_id):
            log.info(f"Date {date_id} already exists")
            # Still create the relationship
            await self.cosmos.create_edge(
                from_id=meeting_id,
                to_id=date_id,
                edge_type='OCCURRED_ON',
                properties={'primary_date': True}
            )
            return date_id
        
        # Get day of week
        date_obj = datetime(year, month, day)
        day_of_week = date_obj.strftime('%A')
        
        # Create date node
        properties = {
            'nodeType': 'Date',
            'full_date': date_str,
            'year': year,
            'month': month,
            'day': day,
            'quarter': (month - 1) // 3 + 1,
            'month_name': calendar.month_name[month],
            'day_of_week': day_of_week,
            'iso_date': f'{year:04d}-{month:02d}-{day:02d}'
        }
        
        await self.cosmos.create_vertex('Date', date_id, properties)
        log.info(f"âœ… Created Date node: {date_id}")
        
        # Create relationship: Meeting -> OCCURRED_ON -> Date
        await self.cosmos.create_edge(
            from_id=meeting_id,
            to_id=date_id,
            edge_type='OCCURRED_ON',
            properties={'primary_date': True}
        )
        
        return date_id
    
    async def _create_section_node(self, section_id: str, section: Dict, order: int) -> str:
        """Create or update Section node."""
        properties = {
            'nodeType': 'Section',
            'title': section.get('section_name', 'Unknown'),
            'type': section.get('section_type', 'OTHER'),
            'description': section.get('description', ''),
            'order': order,
            'is_empty': len(section.get('items', [])) == 0  # Mark empty sections
        }
        
        if self.upsert_mode:
            created = await self.cosmos.upsert_vertex('Section', section_id, properties)
            if created:
                self.stats['nodes_created'] += 1
            else:
                self.stats['nodes_updated'] += 1
        else:
            if await self.cosmos.vertex_exists(section_id):
                log.info(f"Section {section_id} already exists, skipping creation")
                return section_id
            await self.cosmos.create_vertex('Section', section_id, properties)
            self.stats['nodes_created'] += 1
        
        # If section is empty, create a placeholder item
        if properties['is_empty']:
            empty_item_id = f"{section_id}-none"
            await self._create_empty_item_node(empty_item_id, section_id, "None")
        
        return section_id

    async def _create_empty_item_node(self, item_id: str, section_id: str, placeholder_text: str) -> str:
        """Create a placeholder node for empty sections."""
        properties = {
            'nodeType': 'AgendaItem',
            'code': 'NONE',
            'original_code': 'NONE',
            'title': placeholder_text,
            'type': 'Empty Section',
            'section': section_id,
            'is_placeholder': True
        }
        
        if self.upsert_mode:
            created = await self.cosmos.upsert_vertex('AgendaItem', item_id, properties)
            if created:
                self.stats['nodes_created'] += 1
            else:
                self.stats['nodes_updated'] += 1
        else:
            await self.cosmos.create_vertex('AgendaItem', item_id, properties)
            self.stats['nodes_created'] += 1
        
        # Link placeholder item to section
        if await self.cosmos.create_edge_if_not_exists(
            from_id=section_id,
            to_id=item_id,
            edge_type='CONTAINS_ITEM',
            properties={'order': 0, 'is_placeholder': True}
        ):
            self.stats['edges_created'] += 1
        else:
            self.stats['edges_skipped'] += 1
        
        return item_id
    
    async def _create_enhanced_agenda_item_node(self, item_id: str, item: Dict, section: Dict) -> str:
        """Create or update AgendaItem node with rich metadata from LLM extraction."""
        # Store both original and normalized codes
        original_code = item.get('item_code', '')
        normalized_code = self.normalize_item_code(original_code)
        
        # DEBUG: Log what we're receiving
        log.info(f"DEBUG: Creating enhanced node for item {original_code}")
        log.info(f"DEBUG: Item data keys: {list(item.keys())}")
        log.info(f"DEBUG: URLs in item: {item.get('urls', 'NO URLS')}")
        
        properties = {
            'nodeType': 'AgendaItem',
            'code': normalized_code,
            'original_code': original_code,
            'title': item.get('title', 'Unknown'),
            'type': item.get('item_type', 'Item'),
            'section': section.get('section_name', 'Unknown'),
            'section_type': section.get('section_type', 'other')
        }
        
        # Add enhanced details from LLM extraction
        if item.get('description'):
            properties['description'] = item['description'][:500]  # Limit length
        
        if item.get('document_reference'):
            properties['document_reference'] = item['document_reference']
        
        # Add sponsors as JSON array
        if item.get('sponsors'):
            properties['sponsors_json'] = json.dumps(item['sponsors'])
        
        # Add departments as JSON array  
        if item.get('departments'):
            properties['departments_json'] = json.dumps(item['departments'])
        
        # Add actions as JSON array
        if item.get('actions'):
            properties['actions_json'] = json.dumps(item['actions'])
        
        # Add stakeholders as JSON array
        if item.get('stakeholders'):
            properties['stakeholders_json'] = json.dumps(item['stakeholders'])
        
        # Add URLs as JSON array
        if item.get('urls'):
            properties['urls_json'] = json.dumps(item['urls'])
            properties['has_urls'] = True
            log.info(f"DEBUG: Added URLs to properties for {original_code}")
        
        # DEBUG: Log what properties we're storing
        log.info(f"DEBUG: Properties being stored: {list(properties.keys())}")
        log.info(f"DEBUG: Has URLs: {properties.get('has_urls', False)}")
        
        if self.upsert_mode:
            created = await self.cosmos.upsert_vertex('AgendaItem', item_id, properties)
            if created:
                self.stats['nodes_created'] += 1
            else:
                self.stats['nodes_updated'] += 1
        else:
            await self.cosmos.create_vertex('AgendaItem', item_id, properties)
            self.stats['nodes_created'] += 1
        
        return item_id
    
    async def _create_item_relationships(self, item: Dict, item_id: str, meeting_date: str):
        """Create rich relationships for agenda items."""
        
        # Sponsor relationships
        for sponsor in item.get('sponsors', []):
            try:
                person_id = await self._ensure_person_node(sponsor, 'Commissioner')
                await self.cosmos.create_edge_if_not_exists(
                    from_id=person_id,
                    to_id=item_id,
                    edge_type='SPONSORS',
                    properties={'role': 'sponsor'}
                )
                self.stats['edges_created'] += 1
            except Exception as e:
                log.error(f"Failed to create sponsor relationship: {e}")
        
        # Department relationships
        for dept in item.get('departments', []):
            try:
                dept_id = await self._ensure_department_node(dept)
                await self.cosmos.create_edge_if_not_exists(
                    from_id=dept_id,
                    to_id=item_id,
                    edge_type='RESPONSIBLE_FOR',
                    properties={'role': 'responsible_department'}
                )
                self.stats['edges_created'] += 1
            except Exception as e:
                log.error(f"Failed to create department relationship: {e}")
        
        # Stakeholder relationships  
        for stakeholder in item.get('stakeholders', []):
            try:
                org_id = await self._ensure_organization_node(stakeholder, 'Stakeholder')
                await self.cosmos.create_edge_if_not_exists(
                    from_id=org_id,
                    to_id=item_id,
                    edge_type='INVOLVED_IN',
                    properties={'role': 'stakeholder'}
                )
                self.stats['edges_created'] += 1
            except Exception as e:
                log.error(f"Failed to create stakeholder relationship: {e}")
        
        # Action relationships
        for action in item.get('actions', []):
            try:
                action_id = await self._ensure_action_node(action)
                await self.cosmos.create_edge_if_not_exists(
                    from_id=item_id,
                    to_id=action_id,
                    edge_type='REQUIRES_ACTION',
                    properties={'action_type': action}
                )
                self.stats['edges_created'] += 1
            except Exception as e:
                log.error(f"Failed to create action relationship: {e}")
    
    async def _create_url_relationships(self, item: Dict, item_id: str):
        """Create URL nodes and link to agenda items."""
        for url in item.get('urls', []):
            try:
                url_id = await self._ensure_url_node(url)
                await self.cosmos.create_edge_if_not_exists(
                    from_id=item_id,
                    to_id=url_id,
                    edge_type='HAS_URL',
                    properties={'url_type': 'document_link'}
                )
                self.stats['edges_created'] += 1
            except Exception as e:
                log.error(f"Failed to create URL relationship: {e}")
    
    async def _create_ontology_relationship(self, rel: Dict, meeting_date: str):
        """Create relationship from ontology data."""
        try:
            source = rel.get('source', '')
            target = rel.get('target', '')
            relationship = rel.get('relationship', '')
            source_type = rel.get('source_type', '')
            target_type = rel.get('target_type', '')
            
            # Determine source and target IDs based on type
            if source_type == 'person':
                source_id = await self._ensure_person_node(source, 'Participant')
            elif source_type == 'department':
                source_id = await self._ensure_department_node(source)
            elif source_type == 'organization':
                source_id = await self._ensure_organization_node(source, 'Organization')
            else:
                log.warning(f"Unknown source type: {source_type}")
                return
            
            if target_type == 'agenda_item':
                # Normalize target agenda item code
                normalized_target = self.normalize_item_code(target)
                target_id = f"item-{meeting_date}-{normalized_target}"
            else:
                log.warning(f"Unknown target type: {target_type}")
                return
            
            # Create the relationship
            await self.cosmos.create_edge_if_not_exists(
                from_id=source_id,
                to_id=target_id,
                edge_type=relationship.upper(),
                properties={
                    'source_type': source_type,
                    'target_type': target_type
                }
            )
            
        except Exception as e:
            log.error(f"Failed to create ontology relationship: {e}")
    
    async def _ensure_person_node(self, name: str, role: str) -> str:
        """Create or retrieve person node with upsert support."""
        clean_name = name.strip()
        # Clean the ID by removing invalid characters
        cleaned_id_part = clean_name.lower().replace(' ', '-').replace('.', '').replace("'", '').replace('"', '').replace('/', '-')
        person_id = f"person-{cleaned_id_part}"
        
        # Check cache first
        if person_id in self.entity_id_cache:
            return person_id
        
        # Check if exists in database
        if await self.cosmos.vertex_exists(person_id):
            self.entity_id_cache[person_id] = True
            return person_id
        
        # Create new person
        properties = {
            'nodeType': 'Person',
            'name': clean_name,
            'roles': role
        }
        
        if self.upsert_mode:
            created = await self.cosmos.upsert_vertex('Person', person_id, properties)
            if created:
                self.stats['nodes_created'] += 1
            else:
                self.stats['nodes_updated'] += 1
        else:
            if not await self.cosmos.vertex_exists(person_id):
                await self.cosmos.create_vertex('Person', person_id, properties)
                self.stats['nodes_created'] += 1
        
        self.entity_id_cache[person_id] = True
        return person_id
    
    async def _ensure_organization_node(self, name: str, org_type: str) -> str:
        """Create or retrieve organization node."""
        # Clean the ID
        cleaned_org_name = name.lower().replace(' ', '-').replace('.', '').replace("'", '').replace('"', '').replace('/', '-').replace(',', '')
        org_id = f"org-{cleaned_org_name}"
        
        if org_id in self.entity_id_cache:
            return org_id
        
        if await self.cosmos.vertex_exists(org_id):
            self.entity_id_cache[org_id] = True
            return org_id
        
        properties = {
            'nodeType': 'Organization',
            'name': name,
            'type': org_type
        }
        
        await self.cosmos.create_vertex('Organization', org_id, properties)
        self.entity_id_cache[org_id] = True
        self.stats['nodes_created'] += 1
        return org_id
    
    async def _ensure_department_node(self, name: str) -> str:
        """Create or retrieve department node."""
        cleaned_dept_name = name.lower().replace(' ', '-').replace('.', '').replace("'", '').replace('"', '').replace('/', '-')
        dept_id = f"dept-{cleaned_dept_name}"
        
        if dept_id in self.entity_id_cache:
            return dept_id
        
        if await self.cosmos.vertex_exists(dept_id):
            self.entity_id_cache[dept_id] = True
            return dept_id
        
        properties = {
            'nodeType': 'Department', 
            'name': name,
            'type': 'CityDepartment'
        }
        
        await self.cosmos.create_vertex('Department', dept_id, properties)
        self.entity_id_cache[dept_id] = True
        self.stats['nodes_created'] += 1
        return dept_id
    
    async def _ensure_location_node(self, name: str, context: str = '') -> str:
        """Create or retrieve location node."""
        cleaned_loc_name = name.lower().replace(' ', '-').replace('.', '').replace("'", '').replace('"', '').replace('/', '-').replace(',', '')
        loc_id = f"location-{cleaned_loc_name}"
        
        if loc_id in self.entity_id_cache:
            return loc_id
        
        if await self.cosmos.vertex_exists(loc_id):
            self.entity_id_cache[loc_id] = True
            return loc_id
        
        properties = {
            'nodeType': 'Location',
            'name': name,
            'context': context[:200] if context else '',
            'type': 'Location'
        }
        
        await self.cosmos.create_vertex('Location', loc_id, properties)
        self.entity_id_cache[loc_id] = True
        self.stats['nodes_created'] += 1
        return loc_id
    
    async def _ensure_action_node(self, action: str) -> str:
        """Create or retrieve action node."""
        cleaned_action = action.lower().replace(' ', '-').replace('.', '')
        action_id = f"action-{cleaned_action}"
        
        if action_id in self.entity_id_cache:
            return action_id
        
        if await self.cosmos.vertex_exists(action_id):
            self.entity_id_cache[action_id] = True
            return action_id
        
        properties = {
            'nodeType': 'Action',
            'name': action,
            'type': 'RequiredAction'
        }
        
        await self.cosmos.create_vertex('Action', action_id, properties)
        self.entity_id_cache[action_id] = True
        self.stats['nodes_created'] += 1
        return action_id
    
    async def _ensure_url_node(self, url: str) -> str:
        """Create or retrieve URL node."""
        url_hash = hashlib.md5(url.encode()).hexdigest()[:12]
        url_id = f"url-{url_hash}"
        
        if url_id in self.entity_id_cache:
            return url_id
        
        if await self.cosmos.vertex_exists(url_id):
            self.entity_id_cache[url_id] = True
            return url_id
        
        properties = {
            'nodeType': 'URL',
            'url': url,
            'domain': url.split('/')[2] if '://' in url else 'unknown',
            'type': 'Hyperlink'
        }
        
        await self.cosmos.create_vertex('URL', url_id, properties)
        self.entity_id_cache[url_id] = True
        self.stats['nodes_created'] += 1
        return url_id

    async def process_linked_documents(self, linked_docs: Dict, meeting_id: str, meeting_date: str):
        """Process and create nodes for linked documents."""
        log.info("ðŸ“„ Processing linked documents...")
        
        created_count = 0
        missing_items = []
        
        for doc_type, docs in linked_docs.items():
            # Skip verbatim_transcripts as they're processed separately
            if doc_type == "verbatim_transcripts":
                log.info(f"â­ï¸  Skipping {doc_type} - processed separately")
                continue
                
            if not docs:
                continue
                
            log.info(f"\n   ðŸ“‚ Processing {len(docs)} {doc_type}")
            
            for doc in docs:
                # Validate item_code before processing
                item_code = doc.get('item_code')
                
                # Enhanced matching for documents without item codes
                if not doc.get("item_code"):
                    doc_number = doc.get("document_number", "")
                    log.info(f"ðŸ” Searching for document {doc_number} in agenda structure")
                    
                    # Search through all sections and items for matching document reference
                    found_item = None
                    for section in self.current_ontology.get("sections", []):
                        for item in section.get("items", []):
                            if item.get("document_reference") == doc_number:
                                doc["item_code"] = item.get("item_code")
                                log.info(f"âœ… Found matching agenda item by document reference: {doc['item_code']}")
                                found_item = item
                                break
                        if found_item:
                            break
                
                item_code = doc.get('item_code')
                if item_code and len(item_code) > 10:  # Suspiciously long
                    log.error(f"Invalid item code detected: {item_code[:50]}...")
                    # Try to extract a valid code
                    code_match = re.match(r'^([A-Z]-?\d+)', item_code)
                    if code_match:
                        item_code = code_match.group(1)
                        doc['item_code'] = item_code
                        log.info(f"Extracted valid code: {item_code}")
                    else:
                        log.error(f"Could not extract valid code, skipping document {doc.get('document_number')}")
                        continue
                
                # Use the singular form for logging
                doc_type_singular = doc_type[:-1] if doc_type.endswith('s') else doc_type
                
                if doc_type in ['ordinances', 'resolutions']:
                    log.info(f"\n   Processing {doc_type_singular} {doc.get('document_number', 'unknown')}")
                    log.info(f"      Item code: {doc.get('item_code', 'MISSING')}")
                    
                    # Create document node
                    doc_id = await self._create_document_node(doc, doc_type, meeting_date)
                    
                    if doc_id:
                        created_count += 1
                        log.info(f"      âœ… Created document node: {doc_id}")
                        
                        # Link to meeting
                        await self.cosmos.create_edge(
                            from_id=doc_id,
                            to_id=meeting_id,
                            edge_type='PRESENTED_AT',
                            properties={'date': meeting_date}
                        )
                        
                        # Try to link to agenda item if item_code exists
                        item_code = doc.get('item_code')
                        if item_code:
                            # Log the normalization process
                            log.debug(f"Original item code: '{item_code}'")
                            normalized_code = self.normalize_item_code(item_code)
                            log.debug(f"Normalized item code: '{normalized_code}'")
                            
                            item_id = f"item-{meeting_date}-{normalized_code}"
                            log.info(f"Looking for agenda item: {item_id}")
                            
                            # Check if agenda item exists
                            if await self.cosmos.vertex_exists(item_id):
                                log.info(f"âœ… Found agenda item: {item_id}")
                                await self.cosmos.create_edge(
                                    from_id=item_id,
                                    to_id=doc_id,
                                    edge_type='REFERENCES_DOCUMENT',
                                    properties={'document_type': doc_type_singular}
                                )
                                log.info(f"      ðŸ”— Linked to agenda item: {item_id}")
                            else:
                                # Try alternative formats
                                alt_ids = [
                                    f"item-{meeting_date}-E-9",
                                    f"item-{meeting_date}-E9",
                                    f"item-{meeting_date}-E.-9.",
                                    f"item-{meeting_date}-E.-9"
                                ]
                                
                                found = False
                                for alt_id in alt_ids:
                                    if await self.cosmos.vertex_exists(alt_id):
                                        log.info(f"âœ… Found agenda item with alternative ID: {alt_id}")
                                        item_id = alt_id
                                        found = True
                                        await self.cosmos.create_edge(
                                            from_id=item_id,
                                            to_id=doc_id,
                                            edge_type='REFERENCES_DOCUMENT',
                                            properties={'document_type': doc_type_singular}
                                        )
                                        log.info(f"      ðŸ”— Linked to agenda item: {alt_id}")
                                        break
                                
                                if not found:
                                    # Try to find by document number
                                    doc_num = doc.get('document_number', '')
                                    
                                    # Search all agenda items for matching document reference
                                    if hasattr(self, 'current_agenda_structure'):
                                        for section in self.current_agenda_structure:  # Need to store this
                                            for item in section.get('items', []):
                                                if item.get('document_reference') == doc_num:
                                                    # Found matching item by document number
                                                    item_code = self.normalize_item_code(item['item_code'])
                                                    item_id = f"item-{meeting_date}-{item_code}"
                                                    log.info(f"âœ… Found item by document reference: {item_id}")
                                                    await self.cosmos.create_edge(
                                                        from_id=item_id,
                                                        to_id=doc_id,
                                                        edge_type='REFERENCES_DOCUMENT',
                                                        properties={'document_type': doc_type_singular}
                                                    )
                                                    log.info(f"      ðŸ”— Linked to agenda item via document reference: {item_id}")
                                                    found = True
                                                    break
                                            if found:
                                                break
                                    
                                    if not found:
                                        log.warning(f"âŒ Agenda item not found: {item_id} or alternatives")
                                        missing_items.append({
                                            'document_number': doc.get('document_number'),
                                            'item_code': item_code,
                                            'normalized_code': normalized_code,
                                            'expected_item_id': item_id,
                                            'document_type': doc_type_singular
                                        })
                        else:
                            log.warning(f"      âš ï¸  No item_code found for {doc.get('document_number')}")
        
        log.info(f"ðŸ“„ Document processing complete: {created_count} documents created")
        if missing_items:
            log.warning(f"âš ï¸  {len(missing_items)} documents could not be linked to agenda items")
        
        return missing_items

    async def _create_document_node(self, doc_info: Dict, doc_type: str, meeting_date: str) -> str:
        """Create or update an Ordinance or Resolution node."""
        doc_number = doc_info.get('document_number', 'unknown')
        
        # Use the document type from doc_info if available, otherwise use the passed type
        actual_doc_type = doc_info.get('document_type', doc_type)
        
        # Ensure consistency in ID generation
        if actual_doc_type.lower() == 'resolution':
            doc_id = f"resolution-{doc_number}"
            node_type = 'Resolution'
        else:
            doc_id = f"ordinance-{doc_number}"
            node_type = 'Ordinance'
        
        # Get full title without truncation
        title = doc_info.get('title', '')
        if not title and doc_info.get('parsed_data', {}).get('title'):
            title = doc_info['parsed_data']['title']
        
        if title is None:
            title = f"Untitled {actual_doc_type.capitalize()} {doc_number}"
            log.warning(f"No title found for {actual_doc_type} {doc_number}, using default")
        
        properties = {
            'nodeType': node_type,
            'document_number': doc_number,
            'full_title': title,
            'title': title[:200] if len(title) > 200 else title,
            'document_type': actual_doc_type.capitalize(),
            'meeting_date': meeting_date
        }
        
        # Add parsed metadata
        parsed_data = doc_info.get('parsed_data', {})
        
        if parsed_data.get('date_passed'):
            properties['date_passed'] = parsed_data['date_passed']
        
        if parsed_data.get('agenda_item'):
            properties['agenda_item'] = parsed_data['agenda_item']
        
        # Add vote details as JSON
        if parsed_data.get('vote_details'):
            properties['vote_details'] = json.dumps(parsed_data['vote_details'])
        
        # Add signatories
        if parsed_data.get('signatories', {}).get('mayor'):
            properties['mayor_signature'] = parsed_data['signatories']['mayor']
        
        if self.upsert_mode:
            created = await self.cosmos.upsert_vertex(node_type, doc_id, properties)
            if created:
                self.stats['nodes_created'] += 1
                log.info(f"âœ… Created document node: {doc_id}")
            else:
                self.stats['nodes_updated'] += 1
                log.info(f"ðŸ“ Updated document node: {doc_id}")
        else:
            await self.cosmos.create_vertex(node_type, doc_id, properties)
            self.stats['nodes_created'] += 1
        
        # Create edges for sponsors
        if parsed_data.get('motion', {}).get('moved_by'):
            person_id = await self._ensure_person_node(
                parsed_data['motion']['moved_by'], 
                'Commissioner'
            )
            if await self.cosmos.create_edge_if_not_exists(person_id, doc_id, 'MOVED'):
                self.stats['edges_created'] += 1
            else:
                self.stats['edges_skipped'] += 1
        
        return doc_id

    async def process_verbatim_transcripts(self, transcripts: Dict, meeting_id: str, meeting_date: str) -> int:
        """Process and create nodes for verbatim transcript documents."""
        log.info("ðŸŽ¤ Processing verbatim transcripts...")
        
        # Handle empty results gracefully
        if not any(transcripts.values()):
            log.info("ðŸŽ¤ No verbatim transcripts found for this meeting")
            return 0
        
        # Ensure PUBLIC COMMENT section exists if we have public comment transcripts
        if transcripts.get("public_comments"):
            # Create PUBLIC COMMENT section if not already present
            section_id = f"section-{meeting_date}-public-comment"
            await self._create_section_node(section_id, {
                'section_name': 'PUBLIC COMMENT',
                'section_type': 'PUBLIC_COMMENT',
                'description': 'Public comments from the meeting',
                'items': []
            }, 999)  # High order number to place at end
            
            # Link to meeting
            await self.cosmos.create_edge_if_not_exists(
                from_id=meeting_id,
                to_id=section_id,
                edge_type='HAS_SECTION'
            )
        
        created_count = 0
        
        # Process item-specific transcripts
        for transcript in transcripts.get("item_transcripts", []):
            transcript_id = await self._create_transcript_node(transcript, meeting_date, "item")
            if transcript_id:
                created_count += 1
                
                # Link to meeting
                await self.cosmos.create_edge(
                    from_id=transcript_id,
                    to_id=meeting_id,
                    edge_type='TRANSCRIBED_AT',
                    properties={'date': meeting_date}
                )
                
                # Link to specific agenda items
                for item_code in transcript.get("item_codes", []):
                    normalized_code = self.normalize_item_code(item_code)
                    item_id = f"item-{meeting_date}-{normalized_code}"
                    
                    if await self.cosmos.vertex_exists(item_id):
                        await self.cosmos.create_edge(
                            from_id=item_id,
                            to_id=transcript_id,
                            edge_type='HAS_TRANSCRIPT',
                            properties={'transcript_type': 'verbatim'}
                        )
                        log.info(f"   ðŸ”— Linked transcript to item: {item_id}")
                    else:
                        log.warning(f"   âš ï¸  Agenda item not found: {item_id}")
        
        # Process public comment transcripts
        for transcript in transcripts.get("public_comments", []):
            transcript_id = await self._create_transcript_node(transcript, meeting_date, "public_comment")
            if transcript_id:
                created_count += 1
                
                # Link to meeting
                await self.cosmos.create_edge(
                    from_id=transcript_id,
                    to_id=meeting_id,
                    edge_type='PUBLIC_COMMENT_AT',
                    properties={'date': meeting_date}
                )
        
        # Process section transcripts
        for transcript in transcripts.get("section_transcripts", []):
            transcript_id = await self._create_transcript_node(transcript, meeting_date, "section")
            if transcript_id:
                created_count += 1
                
                # Link to meeting
                await self.cosmos.create_edge(
                    from_id=transcript_id,
                    to_id=meeting_id,
                    edge_type='TRANSCRIBED_AT',
                    properties={'date': meeting_date}
                )
        
        log.info(f"ðŸŽ¤ Transcript processing complete: {created_count} transcripts created")
        return created_count

    async def _create_transcript_node(self, transcript_info: Dict, meeting_date: str, transcript_type: str) -> str:
        """Create or update a Transcript node."""
        filename = transcript_info.get("filename", "unknown")
        
        # Create unique ID based on filename
        transcript_id = f"transcript-{meeting_date}-{filename.replace('.pdf', '').replace(' ', '-').lower()}"
        
        properties = {
            'nodeType': 'Transcript',
            'filename': filename,
            'transcript_type': transcript_type,
            'meeting_date': meeting_date,
            'page_count': transcript_info.get('page_count', 0),
            'item_info': transcript_info.get('item_info_raw', ''),
            'items_covered': json.dumps(transcript_info.get('item_codes', [])),
            'sections_covered': json.dumps(transcript_info.get('section_codes', [])),
            'text_excerpt': transcript_info.get('text_excerpt', '')[:500]
        }
        
        if self.upsert_mode:
            created = await self.cosmos.upsert_vertex('Transcript', transcript_id, properties)
            if created:
                self.stats['nodes_created'] += 1
                log.info(f"âœ… Created transcript node: {transcript_id}")
            else:
                self.stats['nodes_updated'] += 1
                log.info(f"ðŸ“ Updated transcript node: {transcript_id}")
        else:
            await self.cosmos.create_vertex('Transcript', transcript_id, properties)
            self.stats['nodes_created'] += 1
        
        return transcript_id

    async def _create_agenda_item_node(self, item: Dict[str, Any], meeting_id: str):
        """Create an agenda item node with enhanced properties including URLs."""
        item_code = item.get('item_code', 'Unknown')
        node_id = f"item_{meeting_id}_{item_code}"
        
        # DEBUG: Log what we're receiving
        log.info(f"DEBUG: Creating node for item {item_code}")
        log.info(f"DEBUG: Item data keys: {list(item.keys())}")
        log.info(f"DEBUG: URLs in item: {item.get('urls', 'NO URLS')}")
        
        # Prepare URLs for storage - convert to JSON string for graph property
        urls = item.get('urls', [])
        urls_json = json.dumps(urls) if urls else None
        
        # Prepare properties
        properties = {
            'item_code': item_code,
            'title': item.get('title', ''),
            'document_reference': item.get('document_reference', ''),
            'item_type': item.get('item_type', 'Agenda Item'),
            'section_name': item.get('section_name', ''),
            'has_items': item.get('has_items', True),
            'meeting_id': meeting_id,
            'entity_type': 'agenda_item'
        }
        
        # Add URL-related properties
        if urls_json:
            properties['urls'] = urls_json
            properties['url_count'] = len(urls)
            
            # Also store first URL separately for easy access
            if urls:
                properties['primary_url'] = urls[0].get('url', '')
                properties['primary_url_text'] = urls[0].get('text', '')
        
        # DEBUG: Log what properties we're storing
        log.info(f"DEBUG: Properties being stored: {list(properties.keys())}")
        log.info(f"DEBUG: URL count: {properties.get('url_count', 0)}")
        
        # Create or update the node
        created = await self.cosmos.upsert_vertex(
            label='AgendaItem',
            vertex_id=node_id,
            properties=properties
        )
        
        action = "Created" if created else "Updated"
        url_info = f" with {len(urls)} URLs" if urls else ""
        log.info(f"{action} agenda item node: {node_id}{url_info}")
        
        return node_id

    async def _create_ordinance_document_node(self, doc_info: Dict[str, Any], 
                                            meeting_id: str,
                                            date_str: str):
        """Create an ordinance document node with URL support."""
        doc_number = doc_info['document_number']
        node_id = f"ordinance_{doc_number}_{date_str}"
        
        # Extract URLs if available from parsed_data
        urls = []
        if 'parsed_data' in doc_info and isinstance(doc_info['parsed_data'], dict):
            parsed_urls = doc_info['parsed_data'].get('urls', [])
            if parsed_urls:
                urls = parsed_urls
        
        properties = {
            'document_number': doc_number,
            'title': doc_info.get('title', ''),
            'document_type': 'Ordinance',
            'filename': doc_info.get('filename', ''),
            'item_code': doc_info.get('item_code', ''),
            'meeting_id': meeting_id,
            'entity_type': 'ordinance_document'
        }
        
        # Add URL properties if available
        if urls:
            properties['urls'] = json.dumps(urls)
            properties['url_count'] = len(urls)
            if urls[0]:
                properties['primary_url'] = urls[0].get('url', '')
        
        # Add parsed metadata
        if 'parsed_data' in doc_info:
            parsed = doc_info['parsed_data']
            if 'date_passed' in parsed:
                properties['date_passed'] = parsed['date_passed']
            if 'vote_details' in parsed:
                properties['vote_details'] = json.dumps(parsed['vote_details'])
            if 'motion' in parsed:
                properties['motion'] = json.dumps(parsed['motion'])
            if 'signatories' in parsed:
                properties['signatories'] = json.dumps(parsed['signatories'])
        
        created = await self.cosmos.upsert_vertex(
            label='OrdinanceDocument',
            vertex_id=node_id,
            properties=properties
        )
        
        action = "Created" if created else "Updated"
        log.info(f"{action} ordinance document node: {node_id}")
        
        return node_id

    async def _create_resolution_document_node(self, doc_info: Dict[str, Any], 
                                             meeting_id: str,
                                             date_str: str):
        """Create a resolution document node with URL support."""
        doc_number = doc_info['document_number']
        node_id = f"resolution_{doc_number}_{date_str}"
        
        # Extract URLs if available
        urls = []
        if 'parsed_data' in doc_info and isinstance(doc_info['parsed_data'], dict):
            parsed_urls = doc_info['parsed_data'].get('urls', [])
            if parsed_urls:
                urls = parsed_urls
        
        properties = {
            'document_number': doc_number,
            'title': doc_info.get('title', ''),
            'document_type': 'Resolution',
            'filename': doc_info.get('filename', ''),
            'item_code': doc_info.get('item_code', ''),
            'meeting_id': meeting_id,
            'entity_type': 'resolution_document'
        }
        
        # Add URL properties if available
        if urls:
            properties['urls'] = json.dumps(urls)
            properties['url_count'] = len(urls)
            if urls[0]:
                properties['primary_url'] = urls[0].get('url', '')
        
        # Add parsed metadata
        if 'parsed_data' in doc_info:
            parsed = doc_info['parsed_data']
            if 'date_passed' in parsed:
                properties['date_passed'] = parsed['date_passed']
            if 'vote_details' in parsed:
                properties['vote_details'] = json.dumps(parsed['vote_details'])
            if 'motion' in parsed:
                properties['motion'] = json.dumps(parsed['motion'])
            if 'signatories' in parsed:
                properties['signatories'] = json.dumps(parsed['signatories'])
            if 'purpose' in parsed:
                properties['purpose'] = parsed['purpose']
        
        created = await self.cosmos.upsert_vertex(
            label='ResolutionDocument',
            vertex_id=node_id,
            properties=properties
        )
        
        action = "Created" if created else "Updated"
        log.info(f"{action} resolution document node: {node_id}")
        
        return node_id

    async def _create_verbatim_transcript_node(self, transcript_info: Dict[str, Any],
                                             meeting_id: str,
                                             date_str: str):
        """Create a verbatim transcript node with potential URL support."""
        filename = transcript_info['filename']
        # Create a unique ID based on filename
        node_id = f"transcript_{date_str}_{filename.replace('.pdf', '').replace(' ', '_')}"
        
        properties = {
            'filename': filename,
            'meeting_date': transcript_info['meeting_date'],
            'item_codes': json.dumps(transcript_info.get('item_codes', [])),
            'section_codes': json.dumps(transcript_info.get('section_codes', [])),
            'transcript_type': transcript_info['transcript_type'],
            'page_count': transcript_info.get('page_count', 0),
            'item_info_raw': transcript_info.get('item_info_raw', ''),
            'meeting_id': meeting_id,
            'entity_type': 'verbatim_transcript'
        }
        
        # Add URL properties if transcripts start including URLs
        urls = transcript_info.get('urls', [])
        if urls:
            properties['urls'] = json.dumps(urls)
            properties['url_count'] = len(urls)
        
        created = await self.cosmos.upsert_vertex(
            label='VerbatimTranscript',
            vertex_id=node_id,
            properties=properties
        )
        
        action = "Created" if created else "Updated"
        log.info(f"{action} verbatim transcript node: {node_id}")
        
        return node_id

    async def _create_official_nodes(self, meeting_info: Dict, meeting_id: str):
        """Create nodes for city officials and link to meeting."""
        officials = meeting_info.get('officials', {})
        commissioners = meeting_info.get('commissioners', [])
        
        # Create official nodes
        for role, name in officials.items():
            if name and name != 'null':
                person_id = await self._ensure_person_node(name, role.replace('_', ' ').title())
                await self.cosmos.create_edge_if_not_exists(
                    from_id=person_id,
                    to_id=meeting_id,
                    edge_type='ATTENDED',
                    properties={'role': role.replace('_', ' ').title()}
                )
        
        # Create commissioner nodes
        for idx, commissioner in enumerate(commissioners):
            if commissioner and commissioner != 'null':
                person_id = await self._ensure_person_node(commissioner, 'Commissioner')
                await self.cosmos.create_edge_if_not_exists(
                    from_id=person_id,
                    to_id=meeting_id,
                    edge_type='ATTENDED',
                    properties={'role': 'Commissioner', 'seat': idx + 1}
                )
    
    async def _create_entity_nodes(self, entities: List[Dict], meeting_id: str) -> int:
        """Create nodes for all extracted entities."""
        entity_count = 0
        
        for entity in entities:
            try:
                entity_type = entity.get('type', 'unknown')
                entity_name = entity.get('name', '')
                entity_role = entity.get('role', '')
                entity_context = entity.get('context', '')
                
                if not entity_name:
                    continue
                
                if entity_type == 'person':
                    person_id = await self._ensure_person_node(entity_name, entity_role)
                    await self.cosmos.create_edge_if_not_exists(
                        from_id=person_id,
                        to_id=meeting_id,
                        edge_type='MENTIONED_IN',
                        properties={
                            'context': entity_context[:100],
                            'role': entity_role
                        }
                    )
                    entity_count += 1
                    
                elif entity_type == 'organization':
                    org_id = await self._ensure_organization_node(entity_name, entity_role)
                    await self.cosmos.create_edge_if_not_exists(
                        from_id=org_id,
                        to_id=meeting_id,
                        edge_type='MENTIONED_IN',
                        properties={
                            'context': entity_context[:100],
                            'org_type': entity_role
                        }
                    )
                    entity_count += 1
                    
                elif entity_type == 'department':
                    dept_id = await self._ensure_department_node(entity_name)
                    await self.cosmos.create_edge_if_not_exists(
                        from_id=dept_id,
                        to_id=meeting_id,
                        edge_type='INVOLVED_IN',
                        properties={'context': entity_context[:100]}
                    )
                    entity_count += 1
                    
                elif entity_type == 'location':
                    loc_id = await self._ensure_location_node(entity_name, entity_context)
                    await self.cosmos.create_edge_if_not_exists(
                        from_id=loc_id,
                        to_id=meeting_id,
                        edge_type='REFERENCED_IN',
                        properties={'context': entity_context[:100]}
                    )
                    entity_count += 1
                    
            except Exception as e:
                log.error(f"Failed to create entity node for {entity}: {e}")
        
        return entity_count


================================================================================


################################################################################
# File: concatenate_scripts_broken.py
################################################################################

# File: concatenate_scripts_broken.py

import os
import sys
import datetime
import re
import fnmatch

# --- Configuration Constants ---

# Define allowed file extensions and specific filenames
ALLOWED_EXTENSIONS = [
    '.js', '.jsx', '.html', '.css', '.py', '.md', 
    '.json', '.toml', '.yaml', '.yml', '.gitignore'
]
ALLOWED_FILENAMES = [
    'requirements.txt',
    'setup.py',
    'pyproject.toml',
    'Dockerfile',
    'docker-compose.yml',
    'docker-compose.yaml'
]

# Variables for files and directories to exclude
OUTPUT_FILENAME_TEMPLATE = 'concatenated_scripts_part{}.txt'
# Dynamically get the name of the script file itself
SCRIPT_FILENAME = os.path.basename(sys.argv[0]) 

EXCLUDED_FILES = [
    'concatenated_scripts_part1.txt',
    'concatenated_scripts_part2.txt',
    'concatenated_scripts_part3.txt',
    SCRIPT_FILENAME, # Exclude the script file itself
    '.env', # Exclude environment variable files
    '.DS_Store', # macOS system file
    'city_clerk_graph.html', # Exclude specific HTML file
    # RAG Pipeline Files - Exclude entire RAG system
    'rag_local_web_app.py',
    'pipeline_modular_optimized.py',
    'supabase_clear_database.py',
    'test_vector_search.py',
    'find_duplicates.py',
    'topic_filter_and_title.py',
    # GraphRAG output and data files
    'city_clerk_documents.csv',
    'graphrag_run.log',
    'live_monitor.py',
    'test_query.py',
    'analyze_docs.py',
    # GraphRAG specific output files
    'indexing-engine.log',
    'entities.parquet',
    'relationships.parquet',
    'communities.parquet',
    'community_reports.parquet',
    'text_units.parquet',
    'documents.parquet',
    'create_base_extracted_entities.parquet',
    'create_base_entity_graph.parquet',
    'create_final_entities.parquet',
    'create_final_relationships.parquet',
    'create_final_communities.parquet',
    'create_final_community_reports.parquet',
    'domain_examples.txt',
    'entity_extraction.txt',
    'community_report.txt',
    'summarize_descriptions.txt',
    # Pipeline output files
    'pipeline_results.json',
    'extraction_results.json',
    'processing_log.txt',
    'pipeline_log.txt',
    'monitor_log.txt',
    'extraction_log.txt',
    'processing_summary.json',
    'extraction_summary.json',
    'pipeline_status.json',
    'run_summary.json',
    'performance_metrics.json',
    # Graph database output files
    'graph_analysis.json',
    'network_analysis.json',
    'node_analysis.json',
    'edge_analysis.json',
    'community_detection.json',
    'centrality_analysis.json',
    'graph_metrics.json',
    'graph_export.gexf',
    'graph_export.graphml',
    'graph_export.gml',
    'network_export.json',
    'adjacency_matrix.csv',
    'edge_list.csv',
    'node_list.csv',
    'graph_visualization.html',
    'network_visualization.html',
    # Token counting and analysis files
    'token_counts.json',
    'token_analysis.json',
    'content_analysis.json',
    'document_stats.json',
    'processing_stats.json',
    # Test and debug files
    'test_python_detection.py',
    'debug_output.txt',
    'test_output.json',
    'debug_log.txt',
    # JSON output files - common patterns
    'output.json',
    'results.json',
    'processed.json',
    'extracted.json',
    'data.json',
    'cache.json',
    'temp.json',
    'backup.json',
    'export.json',
    'report.json',
    'log.json',
    'response.json',
    'api-response.json',
    'processed_documents.json',
    'extracted_text.json',
    'vectorstore.json',
    'embeddings.json',
    'index.json',
    'metadata.json',
    'processed_metadata.json',
    # Library and version files
    'package-lock.json',
    'yarn.lock',
    'composer.lock',
    'Pipfile.lock',
    'poetry.lock',
    'pnpm-lock.yaml',
    'npm-shrinkwrap.json',
    'bower.json',
    'component.json',
    # Virtual environment files
    'pyvenv.cfg',
    'activate',
    'activate.bat',
    'activate.ps1',
    'activate.fish',
    'activate.csh',
    'pip-selfcheck.json',
    # IDE and editor files
    '.vscode',
    '.idea',
    'Thumbs.db',
    'Desktop.ini',
    # Coverage and test files
    '.coverage',
    '.nyc_output',
    'coverage.xml',
    '.hypothesis',
    '.pytest_cache',
    # Documentation files that are typically very long
    'CHANGELOG.md',
    'CHANGELOG.txt',
    'HISTORY.md',
    'HISTORY.txt',
    'LICENSE',
    'LICENSE.txt',
    'LICENSE.md',
    'COPYING',
    'NOTICE',
    'NOTICE.txt',
    'AUTHORS',
    'AUTHORS.txt',
    'CONTRIBUTORS',
    'CONTRIBUTORS.txt',
    'INSTALL',
    'INSTALL.txt',
    'INSTALL.md',
]

# Expanded list of exclusions for virtual environments and node modules
EXCLUDED_DIRS = [
    '__pycache__',
    '.git',
    'node_modules',       # Node modules
    'dist',               # Build output
    '.netlify',           # Netlify directory
    'venv',               # Common Python virtual env name
    '.venv',              # Another common virtual env name
    'env',                # Another common virtual env name
    'virtualenv',         # Another virtual env name
    'city_clerk_rag',     # Specific virtual env folder for this project
    'city-clerk-rag',     # Alternative naming
    'city_clerk_env',     # Potential virtual env name
    'city-clerk-env',     # Potential virtual env name
    'cache',              # Cache directories
    'artifacts',          # Generated artifacts
    'reports',            # Report files
    'logs',               # Log files
    'temp',               # Temporary files
    'tmp',                # Temporary files
    'city_clerk_documents/global',  # Source PDFs directory
    'city_clerk_documents/txt',     # Extracted text files
    'city_clerk_documents/json',    # Extracted JSON files
    'city_clerk_documents/extracted_text',     # Pipeline extracted text output
    'city_clerk_documents/extracted_markdown', # Pipeline markdown output
    'city_clerk_documents/processed',          # Any processed documents
    'city_clerk_documents/cache',              # Document processing cache
    'city_clerk_documents/graph_json',         # Processed JSON outputs from documents
    'city_clerk_documents/global copy',        # Copy of source documents directory
    'documents/',
    'debug',              # Document processing debug outputs
    'prompts',            # Generated prompts from document processing
    'scripts/graph_stages', # Graph processing stages from documents
    'scripts/microsoft_framework', # Framework processing outputs
    # GraphRAG Directories - Exclude GraphRAG processing directories  
    'graphrag_data',          # Entire GraphRAG working directory
    'graphrag_data/output',   # GraphRAG output files
    'graphrag_data/logs',     # GraphRAG processing logs
    'graphrag_data/cache',    # GraphRAG cache files
    'graphrag_data/artifacts', # GraphRAG artifacts
    'graphrag_data/prompts',  # Generated GraphRAG prompts
    'graphrag_data/input',    # GraphRAG input processing
    'graphrag_data/storage',  # GraphRAG storage
    # RAG Pipeline Directories - Exclude entire RAG system
    'stages',             # RAG pipeline stages directory
    'scripts/stages',     # Full path to RAG stages
    'pipeline_output',    # General pipeline output
    'processing_output',  # Processing output directory
    'extracted_output',   # Extraction output directory
    'vectorstore',        # Vector database storage
    'embeddings',         # Embeddings cache/storage
    'index',              # Search index files
    'search_index',       # Search index files
    'vector_index',       # Vector index files
    'chroma_db',          # ChromaDB storage
    'faiss_index',        # FAISS index storage
    'lancedb',            # LanceDB storage
    'qdrant_storage',     # Qdrant storage
    # Output directories from graph_database pipeline
    'output',             # General output directory
    'results',            # Results directory
    'processed_data',     # Processed data output
    'analysis_results',   # Analysis results
    'graph_output',       # Graph analysis output
    'network_output',     # Network analysis output
    'visualization_output', # Visualization files
    'exports',            # Export directories
    'backups',            # Backup directories
    # Library and vendor directories
    'lib',                # Library directories
    'libs',               # Library directories
    'vendor',             # Vendor/third-party code
    'vendors',            # Vendor directories
    'third-party',        # Third-party libraries
    'third_party',        # Third-party libraries
    'site-packages',      # Python site packages
    'include',            # C/C++ include directories
    'bin',                # Binary directories
    'build',              # Build directories
    'target',             # Build target directories
    '.pytest_cache',      # Pytest cache
    '.coverage',          # Coverage files
    '.mypy_cache',        # MyPy cache
    '.tox',               # Tox environments
    'htmlcov',            # Coverage HTML reports
    'coverage',           # Coverage directories
    # Documentation that's typically long
    'docs',               # Documentation
    'documentation',      # Documentation
    'examples',           # Example code (often not needed)
    'samples',            # Sample code
    'test',               # Test directories
    'tests',              # Test directories
    'testing',            # Testing directories
    '__tests__',          # Jest tests
    'spec',               # Spec files
    'specs',              # Spec files
]

# Path-based exclusions - these are specific paths we want to exclude
EXCLUDED_PATHS = [
    # Add any specific paths that should be excluded for city clerk RAG
]

# Essential documentation files that contain architectural information
ESSENTIAL_DOCS = [
    'README.md',           # Main project README if it exists
    'config.py',           # Configuration files are important
    'settings.py',         # Settings files
]

# Additional patterns to identify virtual environments
VENV_PATTERNS = [
    'venv', 'virtualenv', 'env', 'python3', 'python', 'city_clerk_rag', 'city-clerk-rag',
    '.venv', '.env', 'venv_', 'env_'  # Additional common virtual environment patterns
]

# --- Helper Functions ---

def is_venv_or_node_modules(path):
    """
    More robust check for virtual environments and node_modules.
    Returns True if the path appears to be a virtual environment or node_modules.
    """
    path_lower = path.lower()
    path_parts = os.path.normpath(path).split(os.sep)
    
    # Check for node_modules
    if 'node_modules' in path_parts:
        return True
    
    # Check if this directory itself has virtual environment indicators
    if os.path.exists(os.path.join(path, 'pyvenv.cfg')) or \
       os.path.exists(os.path.join(path, 'bin', 'activate')) or \
       os.path.exists(os.path.join(path, 'Scripts', 'activate.bat')) or \
       os.path.exists(os.path.join(path, 'lib', 'python')):
        return True
    
    # Check for common virtual environment patterns, but only if the directory name itself matches
    directory_name = os.path.basename(path).lower()
    for pattern in VENV_PATTERNS:
        if directory_name == pattern or directory_name.startswith(pattern + '_') or directory_name.startswith(pattern + '-'):
            # Additional check - does it contain typical venv structures?
            if os.path.exists(os.path.join(path, 'bin', 'activate')) or \
               os.path.exists(os.path.join(path, 'Scripts', 'activate.bat')) or \
               os.path.exists(os.path.join(path, 'lib', 'python')):
                return True
    
    return False

def is_library_or_unnecessary_file(file_path, filename):
    """
    Determines if a file is a library file or unnecessarily long content that should be excluded.
    Returns True if the file should be excluded.
    """
    # Check for common library file patterns
    library_patterns = [
        'jquery', 'bootstrap', 'lodash', 'moment', 'axios', 'react', 'vue', 'angular',
        'webpack', 'babel', 'eslint', 'prettier', 'typescript', 'd3.js', 'chart.js',
        'three.js', 'socket.io', 'express', 'mongoose', 'sequelize', 'prisma',
        'tensorflow', 'pytorch', 'numpy', 'pandas', 'scipy', 'matplotlib',
        'requests', 'flask', 'django', 'fastapi', 'sqlalchemy', 'celery'
    ]
    
    filename_lower = filename.lower()
    
    # Check if filename contains library patterns
    if any(lib in filename_lower for lib in library_patterns):
        return True
    
    # Check for version numbers in filename (suggests library files)
    version_patterns = [
        r'v\d+\.\d+',           # v1.2, v10.1
        r'_v\d+\.\d+',          # _v1.2
        r'-v\d+\.\d+',          # -v1.2
        r'\d+\.\d+\.\d+',       # 1.2.3
        r'_\d+\.\d+\.\d+',      # _1.2.3
        r'-\d+\.\d+\.\d+',      # -1.2.3
        r'\.min\.',             # minified files
        r'\.bundle\.',          # bundled files
    ]
    
    if any(re.search(pattern, filename_lower) for pattern in version_patterns):
        return True
    
    # Check for specific file types that are typically libraries or unnecessary
    unnecessary_extensions = [
        '.min.js', '.min.css', '.bundle.js', '.bundle.css',
        '.map', '.min.map', '.bundle.map'
    ]
    
    if any(filename_lower.endswith(ext) for ext in unnecessary_extensions):
        return True
    
    # Check if file is in a path that suggests it's a library
    path_parts = file_path.lower().split(os.sep)
    library_path_indicators = [
        'lib', 'libs', 'library', 'libraries', 'vendor', 'vendors',
        'third-party', 'third_party', 'external', 'dependencies',
        'modules', 'packages', 'assets', 'static', 'public',
        'dist', 'build', 'compiled', 'generated'
    ]
    
    if any(indicator in path_parts for indicator in library_path_indicators):
        return True
    
    return False

def is_file_too_long(file_path, max_lines=500, max_size_mb=1):
    """
    Check if a file is too long and likely contains generated or library content.
    Returns True if file should be excluded due to length or size.
    """
    try:
        # Check file size first (faster)
        file_size_mb = os.path.getsize(file_path) / (1024 * 1024)
        if file_size_mb > max_size_mb:
            print(f"[DEBUG] Skipping large file ({file_size_mb:.1f}MB): {file_path}")
            return True
        
        # Then check line count
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            line_count = sum(1 for _ in f)
            
        # Skip very long files that are likely generated or library code
        if line_count > max_lines:
            # Allow some exceptions for our own code
            filename = os.path.basename(file_path).lower()
            
            # Don't exclude our main project files even if long
            if any(keyword in filename for keyword in ['config', 'settings', 'main', 'app', 'index']):
                return False
                
            print(f"[DEBUG] Skipping long file ({line_count} lines): {file_path}")
            return True
            
    except Exception:
        pass  # If we can't read it, let other checks handle it
        
    return False

def is_output_json_file(file_path, filename):
    """
    Determines if a JSON file is likely an output/generated file based on path and naming patterns.
    Returns True if the JSON file should be excluded.
    """
    if not filename.lower().endswith('.json'):
        return False
    
    # Skip JSON files in output/generated directories
    path_parts = file_path.lower().split(os.sep)
    output_indicators = [
        'output', 'outputs', 'results', 'processed', 'generated', 
        'extracted', 'cache', 'temp', 'tmp', 'backup', 'export',
        'reports', 'logs', 'artifacts', 'data', 'json'
    ]
    
    # Check if file is in a directory that suggests it's output
    if any(indicator in path_parts for indicator in output_indicators):
        return True
    
    # Check filename patterns that suggest output files
    filename_lower = filename.lower()
    output_patterns = [
        '_processed.json', '_extracted.json', '_output.json', '_results.json',
        '_cache.json', '_temp.json', '_backup.json', '_export.json',
        '_response.json', '_data.json', '_metadata.json'
    ]
    
    if any(filename_lower.endswith(pattern) for pattern in output_patterns):
        return True
    
    # Check for timestamp patterns in filename (suggests generated files)
    timestamp_patterns = [
        r'\d{4}-\d{2}-\d{2}',  # YYYY-MM-DD
        r'\d{8}',              # YYYYMMDD
        r'\d{4}\d{2}\d{2}_\d{6}',  # YYYYMMDD_HHMMSS
        r'_\d{13}\.json$',     # Unix timestamp
    ]
    
    if any(re.search(pattern, filename_lower) for pattern in timestamp_patterns):
        return True
    
    return False

def get_comment_style(filename):
    """Gets the appropriate comment style based on file extension."""
    _, ext = os.path.splitext(filename)
    ext = ext.lower()
    
    # JavaScript family - uses //
    if ext in ['.js', '.jsx', '.ts', '.tsx']:
        return ('// ', '') 
        
    # CSS uses /* ... */ block comments
    elif ext in ['.css']:
        return ('/* ', ' */')
        
    # Python, Shell, YAML, etc. - uses #
    elif ext in ['.py', '.sh', '.yaml', '.yml', '.toml', '.gitignore', '.r', '.pl', '.rb']:
        return ('# ', '')
        
    # HTML family - uses <!-- ... -->
    elif ext in ['.html', '.xml', '.vue', '.svg']:
        return ('<!-- ', ' -->')
        
    # SQL - uses --
    elif ext == '.sql':
        return ('-- ', '')
        
    # Markdown - can use HTML comments
    elif ext == '.md':
        return ('<!-- ', ' -->')
        
    # Special files
    elif filename.lower() == 'requirements.txt':
        return ('# ', '')
        
    # JSON doesn't support comments
    elif ext == '.json':
        return None
        
    # Default for unknown types
    else:
        print(f"[WARN] Unknown file type '{ext}' for header comment. Using '# '.")
        return ('# ', '')

def matches_excluded_pattern(filename):
    """
    Check if filename matches any of the excluded file patterns (including wildcards).
    """
    # Files with wildcard patterns that need special handling
    wildcard_patterns = [
        '*.pyc', '*.pyo', '*.pyd', '*.so', '*.dll', '*.dylib', '*.o', '*.obj',
        '*.exe', '*.out', '*.class', '*.jar', '*.war', '*.swp', '*.swo', '*~',
        '*.tmp', '*.log', 'npm-debug.log*', 'yarn-debug.log*', 'yarn-error.log*',
        'lerna-debug.log*', '*.cover', '*.py,cover',
        # Data and output files
        '*.csv', '*.parquet', '*.db', '*.sqlite', '*.sqlite3',
        # GraphRAG specific files
        'graphrag_*.log', '*_monitor_*.log', '*.lancedb',
        # Pipeline output files with timestamps or dynamic names
        '*_extracted.json', '*_processed.json', '*_results.json',
        '*_output.json', '*_summary.json', '*_report.json',
        '*_analysis.json', '*_metrics.json', '*_stats.json',
        'pipeline_*', 'extraction_*', 'processing_*',
        'graph_*', 'network_*', 'community_*',
        # GraphRAG workflow files
        'create_*.parquet', 'final_*.parquet', 'base_*.parquet',
        # Log files from pipelines
        '*_pipeline.log', '*_extraction.log', '*_processing.log',
        '*_indexing.log', '*_graph.log', '*_monitor.log',
        # Backup and temporary files
        '*.backup', '*.bak', '*.temp', '*.cache',
        # Export files
        '*.gexf', '*.graphml', '*.gml', '*.gephi',
        # Vector database files
        '*.faiss', '*.ann', '*.hnsw', '*.ivf',
        # Archive and compressed files that are likely outputs
        '*_output.zip', '*_results.tar.gz', '*_export.zip',
        # Test and debug files
        'test_*.py', 'debug_*', '*_test.json', '*_debug.log'
    ]
    
    return any(fnmatch.fnmatch(filename.lower(), pattern) for pattern in wildcard_patterns)

def should_process_file(file_path, filename):
    """Checks if a file should be processed based on exclusions and allowed types."""
    # Check if path contains node_modules or virtual environment
    if is_venv_or_node_modules(file_path):
        print(f"[DEBUG] Skipping file in node_modules or venv: {file_path}")
        return False
    
    # Include essential documentation files regardless of other exclusions
    relative_path = os.path.relpath(file_path, os.getcwd()).replace('\\', '/')
    if any(relative_path == doc_path or relative_path.endswith(doc_path) for doc_path in ESSENTIAL_DOCS):
        return True
    
    # Check absolute exclusions first
    if filename in EXCLUDED_FILES:
        # print(f"[DEBUG] Skipping explicitly excluded file: {filename}")
        return False
    
    # Check wildcard pattern exclusions
    if matches_excluded_pattern(filename):
        print(f"[DEBUG] Skipping file matching excluded pattern: {filename}")
        return False
    
    # Check for library files and unnecessary content
    if is_library_or_unnecessary_file(file_path, filename):
        print(f"[DEBUG] Skipping library/unnecessary file: {filename}")
        return False
    
    # Check for output JSON files
    if is_output_json_file(file_path, filename):
        print(f"[DEBUG] Skipping output JSON file: {filename}")
        return False
    
    # Check if file is too long (likely generated/library content)
    if is_file_too_long(file_path):
        return False
        
    # Check if it's an allowed specific filename
    if filename in ALLOWED_FILENAMES:
        return True
        
    # Check if it has an allowed extension
    _, ext = os.path.splitext(filename)
    if ext.lower() in ALLOWED_EXTENSIONS:
        return True
        
    # print(f"[DEBUG] Skipping file with disallowed type or name: {filename}")
    return False

def create_file_header(file_path, relative_path):
    """
    Creates a properly formatted header for the file based on its type.
    Returns the header text using the appropriate comment style.
    """
    filename = os.path.basename(file_path)
    comment_style = get_comment_style(filename)
    
    if comment_style is None:  # No comments supported (e.g., JSON)
        return None
    
    comment_start, comment_end = comment_style
    header_content = f"File: {relative_path}"
    
    # For multi-line block comments (CSS, HTML, etc.)
    if comment_end:
        header = f"{comment_start}\n{header_content}\n{comment_end}"
    else:  # Line comments (JS, Python, etc.)
        header = f"{comment_start}{header_content}"
    
    return header

def check_for_existing_header(content, relative_path):
    """
    Checks if the file already has a header about its path.
    Returns the content with ALL existing headers removed.
    """
    # Common header patterns with capture groups
    header_patterns = [
        r'^\s*(//\s*File:.*?)\n',        # JavaScript style
        r'^\s*(#\s*File:.*?)\n',         # Python style
        r'^\s*(/\*\s*File:.*?\*/)',       # CSS style
        r'^\s*(<!--\s*File:.*?-->)',      # HTML style
        r'^\s*(--\s*File:.*?)\n',        # SQL style
    ]
    
    # Check for and remove any header pattern at the beginning of the file
    clean_content = content
    
    # First, try looking for headers at the very beginning
    for pattern in header_patterns:
        clean_content = re.sub(f'^{pattern}\\s*', '', clean_content, flags=re.MULTILINE|re.DOTALL)
    
    # Look for multiple header blocks with separating lines
    clean_content = re.sub(r'^#{80}\s*\n^#\s*File:.*?\n^#{80}\s*\n\s*', '', clean_content, flags=re.MULTILINE|re.DOTALL)
    
    # Check if we have the file path in a header anywhere in the first 10 lines
    first_lines = content.split('\n')[:10]
    first_block = '\n'.join(first_lines)
    
    file_path_pattern = re.escape(relative_path)
    has_header = re.search(file_path_pattern, first_block) is not None
    
    return has_header, clean_content

def prepend_header_if_needed(content, header, relative_path):
    """
    Prepends a header to the content if no suitable header exists.
    Returns the content with a header.
    """
    if header is None:
        return content
    
    # Check if content already has a header and clean up any duplicates
    has_header, clean_content = check_for_existing_header(content, relative_path)
    
    # If it already has a header, just return the cleaned content
    if has_header:
        return clean_content
    
    # Add the header to the cleaned content
    return f"{header}\n\n{clean_content}"

def generate_directory_structure(root_dir='.'):
    """Generates a comprehensive text representation of the directory structure with file details."""
    print("[DEBUG] Generating directory structure...")
    structure = ["# Directory Structure", "#" * 80]
    processed_paths = set() 
    abs_root = os.path.abspath(root_dir)
    abs_excluded_dirs = {os.path.join(abs_root, d) for d in EXCLUDED_DIRS}

    def format_file_size(size_bytes):
        """Convert bytes to human readable format."""
        if size_bytes == 0:
            return "0B"
        size_names = ["B", "KB", "MB", "GB"]
        i = 0
        while size_bytes >= 1024 and i < len(size_names) - 1:
            size_bytes /= 1024.0
            i += 1
        return f"{size_bytes:.1f}{size_names[i]}"

    def get_file_info(file_path):
        """Get file information including size and type."""
        try:
            size = os.path.getsize(file_path)
            _, ext = os.path.splitext(file_path)
            ext = ext.lower() if ext else 'no ext'
            return f" ({format_file_size(size)}, {ext})"
        except OSError:
            return " (size unknown)"

    def add_directory(path, prefix=""):
        real_path = os.path.realpath(path)
        if real_path in processed_paths:
            structure.append(f"{prefix}[WARN] Symlink loop or duplicate processing: {path}")
            return
        processed_paths.add(real_path)

        # For the root directory, don't check exclusions - we want to show everything
        is_root = (real_path == abs_root)
        
        if not is_root:
            # Additional check for node_modules and virtual environments
            if is_venv_or_node_modules(real_path):
                structure.append(f"{prefix}[EXCLUDED] Virtual environment or node_modules: {os.path.basename(real_path)}/")
                return
                
            # Check if the current directory is in an excluded path
            if is_path_excluded(real_path, abs_root):
                structure.append(f"{prefix}[EXCLUDED] Excluded path: {os.path.basename(real_path)}/")
                return
                
            # Check if the current directory itself is excluded
            if is_directory_excluded(real_path, abs_root):
                structure.append(f"{prefix}[EXCLUDED] Excluded directory: {os.path.basename(real_path)}/")
                return
        
        # Check if path is *under* an excluded dir (needed for topdown=False or initial call)
        if not is_root:
            is_under_excluded = any(real_path.startswith(excluded + os.path.sep) or real_path == excluded for excluded in abs_excluded_dirs)
            if is_under_excluded:
                return

        try:
            items = sorted(os.listdir(path))
        except OSError as e:
            print(f"[WARN] Could not list directory {path}: {e}")
            structure.append(f"{prefix}[ERROR] Cannot access directory: {e}")
            return

        entries = []
        excluded_items = []
        
        for item in items:
             item_path = os.path.join(path, item)
             item_real_path = os.path.realpath(item_path)
             
             is_dir = os.path.isdir(item_path)
             is_file = os.path.isfile(item_path)

             # Track excluded items for summary
             if is_venv_or_node_modules(item_path):
                 excluded_items.append((item, "venv/node_modules"))
                 continue

             # Check directory exclusions
             if is_dir:
                 if not is_directory_excluded(item_path, abs_root):
                     entries.append((item, True, None)) # Mark as directory
                 else:
                     excluded_items.append((item, "excluded dir"))
             # Check file exclusions
             elif is_file:
                 if item not in EXCLUDED_FILES:
                     file_info = get_file_info(item_path)
                     entries.append((item, False, file_info)) # Mark as file with info
                 else:
                     excluded_items.append((item, "excluded file"))
        
        # Add included entries
        for i, (entry_name, is_dir_entry, file_info) in enumerate(entries):
            is_last = (i == len(entries) - 1) and len(excluded_items) == 0
            connector = "â””â”€â”€ " if is_last else "â”œâ”€â”€ "
            
            if is_dir_entry:
                 structure.append(f"{prefix}{connector}{entry_name}/")
                 child_prefix = prefix + ("    " if is_last else "â”‚   ")
                 add_directory(os.path.join(path, entry_name), child_prefix)
            else:
                 structure.append(f"{prefix}{connector}{entry_name}{file_info}")
        
        # Add summary of excluded items if any
        if excluded_items:
            connector = "â””â”€â”€ " if len(entries) == 0 else "â”œâ”€â”€ "
            structure.append(f"{prefix}{connector}[EXCLUDED] {len(excluded_items)} items: {', '.join([f'{name} ({reason})' for name, reason in excluded_items[:3]])}")
            if len(excluded_items) > 3:
                structure.append(f"{prefix}    ... and {len(excluded_items) - 3} more excluded items")

    add_directory(os.path.abspath(root_dir))
    print("[DEBUG] Directory structure generation complete.")
    return "\n".join(structure)


def is_path_excluded(path, root_dir):
    """
    Checks if the given path is in an excluded path.
    """
    rel_path = os.path.relpath(path, root_dir)
    for excluded_path in EXCLUDED_PATHS:
        # Check if rel_path is or starts with the excluded path
        if rel_path == excluded_path or rel_path.startswith(excluded_path + os.sep):
            return True
    return False

def is_directory_excluded(dir_path, root_dir):
    """
    Checks if a directory should be excluded based on both simple directory names 
    and path-based exclusions.
    """
    # Get the directory name
    dir_name = os.path.basename(dir_path)
    
    # Check if the directory name itself is excluded
    if dir_name in EXCLUDED_DIRS:
        return True
    
    # Get the relative path from root
    rel_path = os.path.relpath(dir_path, root_dir).replace('\\', '/')
    
    # Check path-based exclusions
    for excluded_dir in EXCLUDED_DIRS:
        # If excluded_dir contains a path separator, treat it as a path-based exclusion
        if '/' in excluded_dir:
            if rel_path == excluded_dir or rel_path.startswith(excluded_dir + '/'):
                return True
        # Also handle Windows-style paths
        elif '\\' in excluded_dir:
            excluded_dir_normalized = excluded_dir.replace('\\', '/')
            if rel_path == excluded_dir_normalized or rel_path.startswith(excluded_dir_normalized + '/'):
                return True
    
    return False

def collect_file_contents(root_dir='.'):
    """
    Collects contents of all files to be processed, returning a list of file blocks
    where each block contains the file path and content.
    """
    print(f"[DEBUG] Starting content collection process. Root: {root_dir}")
    abs_root = os.path.abspath(root_dir)
    abs_excluded_dirs = {os.path.join(abs_root, d) for d in EXCLUDED_DIRS}
    
    file_blocks = []
    
    # --- Walk Directory and Process Files ---
    print(f"[DEBUG] Walking directory tree from: {abs_root}")
    processed_files_count = 0
    skipped_files_count = 0
    skipped_venv_count = 0
    skipped_node_modules_count = 0

    for root, dirs, files in os.walk(abs_root, topdown=True):
        # Skip this directory and its subdirectories if it's a virtual env or node_modules
        if is_venv_or_node_modules(root):
            print(f"[DEBUG] Skipping virtual environment or node_modules directory: {root}")
            if 'node_modules' in root:
                skipped_node_modules_count += 1
            else:
                skipped_venv_count += 1
            dirs[:] = []  # Skip all subdirectories
            continue
            
        # Skip if the current directory is in an excluded path
        if is_path_excluded(root, abs_root):
            print(f"[DEBUG] Skipping excluded path directory: {root}")
            dirs[:] = []  # Skip all subdirectories
            continue

        # Filter excluded directories *before* recursion
        dirs[:] = [d for d in dirs if not is_directory_excluded(os.path.join(root, d), abs_root) and not is_venv_or_node_modules(os.path.join(root, d))]
        
        files.sort()

        relative_root = os.path.relpath(root, abs_root)
        if relative_root == '.': relative_root = '' 

        # Safety check: ensure current root isn't inside an excluded dir
        is_in_excluded_dir = any(root.startswith(excluded + os.path.sep) or root == excluded for excluded in abs_excluded_dirs)
        if is_in_excluded_dir: 
            continue

        for file in files:
            file_path = os.path.join(root, file)
            relative_file_path = os.path.normpath(os.path.join(relative_root, file))

            # 1. Check if file should be processed at all (type, name, exclusion)
            if not should_process_file(file_path, file):
                skipped_files_count += 1
                continue

            # 2. Read content for concatenation
            print(f"[DEBUG] Processing file for concatenation: {relative_file_path}")
            processed_files_count += 1
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read().strip()
                
                # 3. Create and add a properly formatted header
                header = create_file_header(file_path, relative_file_path)
                content_with_header = prepend_header_if_needed(content, header, relative_file_path)
                
                # 4. Create the block for the concatenated output
                block_content = []
                block_content.append("#" * 80)
                block_content.append(f"# File: {relative_file_path}")
                block_content.append("#" * 80 + "\n")
                block_content.append(content_with_header) 
                block_content.append("\n\n" + "="*80 + "\n\n")  # Separator
                
                file_blocks.append({
                    'path': relative_file_path,
                    'content': "\n".join(block_content),
                    'size': len("\n".join(block_content))
                })

            except Exception as e:
                print(f"[WARN] Error reading {file_path} for concatenation: {e}. Skipping content.")
                # Add error note as a block
                block_content = []
                block_content.append("#" * 80)
                block_content.append(f"# File: {relative_file_path}")
                block_content.append("#" * 80 + "\n")
                block_content.append(f"[ERROR: Could not read file content due to: {e}]\n\n")
                block_content.append("="*80 + "\n\n")
                
                file_blocks.append({
                    'path': relative_file_path,
                    'content': "\n".join(block_content),
                    'size': len("\n".join(block_content))
                })

    print(f"[INFO] Successfully processed {processed_files_count} files")
    print(f"[INFO] Skipped {skipped_files_count} files (excluded types/names)")
    print(f"[INFO] Skipped {skipped_venv_count} virtual environment directories")
    print(f"[INFO] Skipped {skipped_node_modules_count} node_modules directories")
    return file_blocks, processed_files_count, skipped_files_count


def distribute_files_across_parts(file_blocks, num_parts=3):
    """
    Distributes file blocks across multiple parts ensuring roughly equal size
    and that no file is split across parts.
    """
    # Calculate total size
    total_size = sum(block['size'] for block in file_blocks)
    target_size_per_part = total_size / num_parts
    
    print(f"[DEBUG] Total content size: {total_size} bytes")
    print(f"[DEBUG] Target size per part: {target_size_per_part} bytes")
    
    # Sort files by size (largest first) to help balance distribution
    file_blocks.sort(key=lambda x: x['size'], reverse=True)
    
    # Initialize parts
    parts = [[] for _ in range(num_parts)]
    part_sizes = [0] * num_parts
    
    # Greedy algorithm to distribute files
    for block in file_blocks:
        # Find the part with the smallest current size
        smallest_part_idx = part_sizes.index(min(part_sizes))
        
        # Add the file to that part
        parts[smallest_part_idx].append(block)
        part_sizes[smallest_part_idx] += block['size']
    
    # Print the distribution results
    for i, size in enumerate(part_sizes):
        print(f"[INFO] Part {i+1} size: {size} bytes ({len(parts[i])} files)")
    
    return parts


def write_parts_to_files(parts, root_dir='.'):
    """Writes each part to a separate file without duplicating content."""
    abs_root = os.path.abspath(root_dir)
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # Generate directory structure once for part 1 only
    directory_structure = generate_directory_structure(abs_root)
    
    # Create file index showing which files are in which parts
    file_index = ["# File Index - Which Files Are in Which Parts", "#" * 80]
    for i, part in enumerate(parts, 1):
        file_index.append(f"\n## Part {i} ({len(part)} files):")
        for block in part:
            file_index.append(f"  - {block['path']}")
    file_index_content = "\n".join(file_index)
    
    for i, part in enumerate(parts, 1):
        output_file = OUTPUT_FILENAME_TEMPLATE.format(i)
        output_path = os.path.join(abs_root, output_file)
        
        # Create content
        all_content = []
        
        # Add header
        concatenated_header = (
            f"# Concatenated Project Code - Part {i} of {len(parts)}\n"
            f"# Generated: {timestamp}\n"
            f"# Root Directory: {abs_root}\n"
            f"{'='*80}\n"
        )
        all_content.append(concatenated_header)
        
        # Add directory structure only to part 1
        if i == 1:
            all_content.append(directory_structure)
            all_content.append("\n\n" + "="*80 + "\n\n")
        
        # Add file index to all parts for navigation
        all_content.append(file_index_content)
        all_content.append("\n\n" + "="*80 + "\n\n")
        
        # Add file contents for this part
        for block in part:
            all_content.append(block['content'])
        
        # Write the file
        print(f"[DEBUG] Writing part {i} to: {output_path}")
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write("\n".join(all_content))
            print(f"[INFO] Successfully created {output_path} with {len(part)} files")
        except Exception as e:
            print(f"[ERROR] Critical error writing output file {output_path}: {e}")


# --- Main Function ---
def split_concatenated_scripts(num_parts=3, root_dir='.'):
    """
    Collects file contents, splits them into multiple parts with similar sizes,
    and writes each part to a separate file.
    """
    # 1. Collect all file contents
    file_blocks, processed_count, skipped_count = collect_file_contents(root_dir)
    
    # 2. Distribute files across parts
    parts = distribute_files_across_parts(file_blocks, num_parts)
    
    # 3. Write each part to a file
    write_parts_to_files(parts, root_dir)
    
    print(f"[INFO] Successfully split {processed_count} files into {num_parts} parts")
    print(f"[INFO] Files created: {', '.join([OUTPUT_FILENAME_TEMPLATE.format(i+1) for i in range(num_parts)])}")


# --- Main Execution ---
if __name__ == '__main__':
    split_concatenated_scripts(num_parts=3, root_dir='.')


================================================================================


################################################################################
# File: scripts/graph_stages/enhanced_document_linker.py
################################################################################

# File: scripts/graph_stages/enhanced_document_linker.py

"""
Enhanced Document Linker
Links both ordinance and resolution documents to their corresponding agenda items.
Now with full OCR support for all documents.
"""

import logging
import re
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import json
from datetime import datetime
import PyPDF2
from groq import Groq
import os
from dotenv import load_dotenv
import asyncio
from asyncio import Semaphore

# Import the PDF extractor for OCR support
from .pdf_extractor import PDFExtractor

load_dotenv()

log = logging.getLogger('enhanced_document_linker')


class EnhancedDocumentLinker:
    """Links ordinance and resolution documents to agenda items."""
    
    def __init__(self,
                 openai_api_key: Optional[str] = None,
                 model: str = "gpt-4.1-mini-2025-04-14",
                 agenda_extraction_max_tokens: int = 32768):
        """Initialize the enhanced document linker."""
        self.api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY not found in environment")
        
        self.client = Groq()
        self.model = model
        self.agenda_extraction_max_tokens = agenda_extraction_max_tokens
        # Add semaphore without changing signature
        self.semaphore = Semaphore(3)  # Default value, no parameter change
        
        # Initialize PDF extractor for OCR
        self.pdf_extractor = PDFExtractor(
            pdf_dir=Path("."),  # We'll use it file by file
            output_dir=Path("city_clerk_documents/extracted_text")
        )
    
    async def link_documents_for_meeting(self, 
                                       meeting_date: str,
                                       ordinances_dir: Path,
                                       resolutions_dir: Path) -> Dict[str, List[Dict]]:
        """Process documents in parallel with rate limiting."""
        log.info(f"ðŸ”— Enhanced linking: documents for meeting date: {meeting_date}")
        log.info(f"ðŸ“ Ordinances directory: {ordinances_dir}")
        log.info(f"ðŸ“ Resolutions directory: {resolutions_dir}")
        
        # Create debug directory
        debug_dir = Path("city_clerk_documents/graph_json/debug")
        debug_dir.mkdir(exist_ok=True)
        
        # Convert date format: "01.09.2024" -> "01_09_2024"
        date_underscore = meeting_date.replace(".", "_")
        
        # Initialize results
        linked_documents = {
            "ordinances": [],
            "resolutions": []
        }
        
        # Find all matching files
        matching_files = []
        
        # Process ordinances
        if ordinances_dir.exists():
            ordinance_files = self._find_matching_files(ordinances_dir, date_underscore)
            log.info(f"ðŸ“„ Found {len(ordinance_files)} ordinance files")
            matching_files.extend([(f, "ordinance") for f in ordinance_files])
        else:
            log.warning(f"âš ï¸  Ordinances directory not found: {ordinances_dir}")
        
        # Process resolutions
        if resolutions_dir.exists():
            # Check for year subdirectory first
            year = meeting_date.split('.')[-1]  # Extract year from date
            year_dir = resolutions_dir / year
            
            if year_dir.exists():
                resolution_files = self._find_matching_files(year_dir, date_underscore)
                log.info(f"ðŸ“„ Found {len(resolution_files)} resolution files in {year} directory")
            else:
                # Fall back to main resolutions directory
                resolution_files = self._find_matching_files(resolutions_dir, date_underscore)
                log.info(f"ðŸ“„ Found {len(resolution_files)} resolution files in main directory")
            matching_files.extend([(f, "resolution") for f in resolution_files])
        else:
            log.warning(f"âš ï¸  Resolutions directory not found: {resolutions_dir}")
        
        # Process documents in parallel
        tasks = []
        for doc_path, doc_type in matching_files:
            task = self._process_document_with_semaphore(doc_path, meeting_date, doc_type)
            tasks.append(task)
        
        # Gather results
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Process results
        for result in results:
            if isinstance(result, Exception):
                log.error(f"Error processing document: {result}")
            elif result:
                doc_type = result.get('document_type', '').lower()
                if "ordinance" in doc_type:
                    linked_documents["ordinances"].append(result)
                    # Save extracted text for GraphRAG
                    self._save_extracted_text(Path(result['path']), result, "ordinance")
                else:
                    linked_documents["resolutions"].append(result)
                    # Save extracted text for GraphRAG
                    self._save_extracted_text(Path(result['path']), result, "resolution")
        
        # Save enhanced linked documents info
        with open(debug_dir / "enhanced_linked_documents.json", 'w') as f:
            json.dump(linked_documents, f, indent=2)
        
        # Log summary
        total_linked = len(linked_documents['ordinances']) + len(linked_documents['resolutions'])
        log.info(f"âœ… Enhanced linking complete:")
        log.info(f"   ðŸ“„ Ordinances: {len(linked_documents['ordinances'])}")
        log.info(f"   ðŸ“„ Resolutions: {len(linked_documents['resolutions'])}")
        log.info(f"   ðŸ“„ Total linked: {total_linked}")
        
        # Save detailed report
        self._generate_linking_report(meeting_date, linked_documents, debug_dir)
        
        return linked_documents
    
    async def _process_document_with_semaphore(self, doc_path: Path, meeting_date: str, doc_type: str):
        """Process document with semaphore for rate limiting."""
        async with self.semaphore:  # Limit concurrent LLM calls
            return await self._process_document(doc_path, meeting_date, doc_type)
    
    def _find_matching_files(self, directory: Path, date_pattern: str) -> List[Path]:
        """Find all PDF files matching the date pattern."""
        # Pattern: YYYY-## - MM_DD_YYYY.pdf
        pattern = f"*{date_pattern}.pdf"
        matching_files = list(directory.glob(pattern))
        
        # Also try without spaces in case filenames vary
        pattern2 = f"*{date_pattern}*.pdf"
        additional_files = [f for f in directory.glob(pattern2) if f not in matching_files]
        matching_files.extend(additional_files)
        
        # Also check for variations in date format
        # Some files might use dashes instead of underscores
        date_dash = date_pattern.replace("_", "-")
        pattern3 = f"*{date_dash}*.pdf"
        more_files = [f for f in directory.glob(pattern3) if f not in matching_files]
        matching_files.extend(more_files)
        
        return sorted(matching_files)
    
    async def _process_document(self, doc_path: Path, meeting_date: str, doc_type: str) -> Optional[Dict[str, Any]]:
        """Process a single document to extract agenda item reference with OCR."""
        try:
            # Extract document number from filename
            doc_match = re.match(r'^(\d{4}-\d{2,3})', doc_path.name)
            if not doc_match:
                log.warning(f"Could not parse document number from {doc_path.name}")
                return None
            
            document_number = doc_match.group(1)
            
            # Extract text using Docling OCR (replacing PyPDF2)
            log.info(f"ðŸ” Running OCR on {doc_path.name}...")
            text, pages = self.pdf_extractor.extract_text_from_pdf(doc_path)
            
            if not text:
                log.warning(f"No text extracted from {doc_path.name}")
                return None
            
            log.info(f"âœ… OCR extracted {len(text)} characters from {len(pages)} pages")
            
            # Extract agenda item code using LLM (existing logic)
            item_code = await self._extract_agenda_item_code(text, document_number, doc_type)
            
            # Extract additional metadata
            parsed_data = self._parse_document_metadata(text, doc_type)
            
            doc_info = {
                "path": str(doc_path),
                "filename": doc_path.name,
                "document_number": document_number,
                "item_code": item_code,
                "document_type": doc_type.capitalize(),
                "title": self._extract_title(text, doc_type),
                "parsed_data": parsed_data,
                "meeting_date": meeting_date,
                "full_text": text,  # Store full OCR text
                "pages": pages,     # Store page-level data
                "extraction_method": "docling_ocr"
            }
            
            log.info(f"ðŸ“„ Processed {doc_type} {doc_path.name}: Item {item_code or 'NOT_FOUND'}")
            return doc_info
            
        except Exception as e:
            log.error(f"Error processing {doc_path.name}: {e}")
            return None
    
    def _save_extracted_text(self, pdf_path: Path, doc_info: Dict, doc_type: str):
        """Save with enhanced entity hints."""
        output_dir = Path("city_clerk_documents/extracted_text")
        output_dir.mkdir(exist_ok=True)
        
        # Create filename based on document number
        doc_number = doc_info['document_number']
        meeting_date = doc_info['meeting_date'].replace('.', '_')
        output_filename = f"{doc_type}_{doc_number}_{meeting_date}_extracted.json"
        output_path = output_dir / output_filename
        
        # Add entity hints for GraphRAG
        entity_hints = {
            "explicit_entities": [],
            "relationships": []
        }
        
        # Extract all identifiers
        if doc_info.get('document_number'):
            entity_hints['explicit_entities'].append({
                'name': doc_info['document_number'],
                'type': doc_type.upper(),
                'description': f"{doc_type.title()} filing number"
            })
        
        if doc_info.get('item_code'):
            entity_hints['explicit_entities'].append({
                'name': doc_info['item_code'],
                'type': 'AGENDA_ITEM',
                'description': f"Agenda item for {doc_info.get('document_number', doc_type)}"
            })
            
            # Add relationship
            if doc_info.get('document_number'):
                entity_hints['relationships'].append({
                    'source': doc_info['document_number'],
                    'target': doc_info['item_code'],
                    'type': 'relates_to_agenda_item'
                })
        
        # Prepare data for saving
        save_data = {
            "document_type": doc_type,
            "document_number": doc_number,
            "meeting_date": doc_info['meeting_date'],
            "item_code": doc_info.get('item_code'),
            "title": doc_info.get('title'),
            "full_text": doc_info.get('full_text'),
            "pages": doc_info.get('pages', []),
            "parsed_data": doc_info.get('parsed_data', {}),
            "entity_hints": entity_hints,
            "metadata": {
                "filename": doc_info['filename'],
                "extraction_method": "docling_ocr",
                "extracted_at": datetime.now().isoformat()
            }
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, indent=2, ensure_ascii=False)
        
        log.info(f"ðŸ’¾ Saved extracted text to: {output_path}")
        
        # NEW: Also save as markdown for GraphRAG
        self._save_as_markdown(pdf_path, doc_info, doc_type, output_dir)

    def _save_as_markdown(self, doc_path: Path, doc_info: Dict[str, Any], doc_type: str, output_dir: Path):
        """Save document as markdown with enhanced metadata header."""
        markdown_dir = output_dir.parent / "extracted_markdown"
        markdown_dir.mkdir(exist_ok=True)
        
        # Build metadata header
        header = self._build_enhanced_header(doc_path, doc_info, doc_type)
        
        # Combine with full text
        full_content = header + "\n\n" + doc_info.get('full_text', '')
        
        # Save markdown file
        doc_number = doc_info['document_number']
        meeting_date = doc_info['meeting_date'].replace('.', '_')
        md_filename = f"{doc_type}_{doc_number}_{meeting_date}.md"
        md_path = markdown_dir / md_filename
        
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(full_content)
        
        log.info(f"ðŸ“ Saved markdown to: {md_path}")

    def _build_enhanced_header(self, doc_path: Path, doc_info: Dict[str, Any], doc_type: str) -> str:
        """Build enhanced metadata header for GraphRAG."""
        
        item_code = doc_info.get('item_code', 'N/A')
        doc_number = doc_info.get('document_number', 'N/A')
        
        header = f"""---
ENTITIES IN THIS DOCUMENT:
- AGENDA_ITEM: {item_code}
- {doc_type.upper()}: {doc_number}
- DOCUMENT_TYPE: {doc_type.upper()}

---

**THIS DOCUMENT CONTAINS:**
The following entities should be extracted:
- Agenda Item {item_code} (entity type: agenda_item)
- {doc_type.capitalize()} {doc_number} (entity type: {doc_type})
- Meeting Date: {doc_info.get('meeting_date', 'N/A')} (entity type: meeting)

**EXAMPLE EXTRACTION:**
From the text "relating to agenda item {item_code}", extract:
- Entity: "{item_code}", Type: "agenda_item"

From the text "{doc_type} {doc_number}", extract:
- Entity: "{doc_number}", Type: "{doc_type}"

---

{self._build_existing_header(doc_path, doc_info, doc_type)}
"""
        return header

    def _build_existing_header(self, doc_path: Path, doc_info: Dict[str, Any], doc_type: str) -> str:
        """Build the existing metadata header for backwards compatibility."""
        # Get directory structure
        parts = doc_path.parts
        path_context = '/'.join(parts[-3:-1]) if len(parts) >= 3 else ''
        
        header = f"""
DOCUMENT METADATA AND CONTEXT
=============================

**DOCUMENT IDENTIFICATION:**
- Full Path: {path_context}/{doc_path.name}
- Document Type: {doc_type.upper()}
- Filename: {doc_path.name}

**PARSED INFORMATION:**
- Document Number: {doc_info.get('document_number', 'N/A')}
- Meeting Date: {doc_info.get('meeting_date', 'N/A')}
- Related Agenda Item: {doc_info.get('item_code', 'N/A')}
- Title: {doc_info.get('title', 'N/A')}

**SEARCHABLE IDENTIFIERS:**
- DOCUMENT_NUMBER: {doc_info.get('document_number', 'N/A')}
- MEETING_DATE: {doc_info.get('meeting_date', 'N/A')}
- AGENDA_ITEM: {doc_info.get('item_code', 'N/A')}
- DOCUMENT_TYPE: {doc_type.upper()}

**NATURAL LANGUAGE DESCRIPTION:**
This is {doc_type.capitalize()} {doc_info.get('document_number', '')} from the {doc_info.get('meeting_date', '')} City Commission meeting, relating to agenda item {doc_info.get('item_code', 'unknown')}.

**QUERY HELPERS:**
- To find information about {doc_info.get('item_code', 'this item')}, search for 'Item {doc_info.get('item_code', '')}' or '{doc_info.get('item_code', '')}'
- To find this document, search for '{doc_info.get('document_number', '')}'
- This {doc_type} {self._get_doc_type_description(doc_type)}

---

## What is Item {doc_info.get('item_code', 'N/A')}?
Item {doc_info.get('item_code', 'N/A')} is implemented by this {doc_type}.
{doc_info.get('item_code', 'N/A')} refers to {doc_type} {doc_info.get('document_number', '')}.

**RELATIONSHIP**: {doc_info.get('document_number', '')} implements agenda item {doc_info.get('item_code', '')}.

---

# ORIGINAL DOCUMENT CONTENT
"""
        return header

    def _get_doc_type_description(self, doc_type: str) -> str:
        """Get description for document type."""
        descriptions = {
            'ordinance': 'modifies city code and requires multiple readings',
            'resolution': 'expresses city policy or authorizes specific actions'
        }
        return descriptions.get(doc_type, 'is an official city document')
    
    async def _extract_agenda_item_code(self, text: str, document_number: str, doc_type: str) -> Optional[str]:
        """Extract agenda item code from document text using LLM."""
        debug_dir = Path("city_clerk_documents/graph_json/debug")
        debug_dir.mkdir(exist_ok=True)
        
        # Try regex patterns first for better accuracy
        patterns = [
            r'Item\s+([A-Z]\.-?\d+\.?)',  # Item D.-1.
            r'Agenda\s+Item[:\s]+([A-Z]\.-?\d+\.?)',  # Agenda Item: D.-1.
            r'Section\s+([A-Z])[,\s]+Item\s+(\d+)',  # Section D, Item 1
            r'consent\s+agenda.*item\s+([A-Z]\.-?\d+\.?)',  # Consent Agenda ... Item D.-1.
            r'\b([A-Z]\.-\d+\.?)\s+\d{2}-\d{4}',  # D.-1. 23-6830 pattern
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                if len(match.groups()) == 2:  # Section X, Item Y format
                    code = f"{match.group(1)}-{match.group(2)}"
                else:
                    code = match.group(1)
                normalized_code = self._normalize_item_code(code)
                log.info(f"âœ… Found agenda item code via regex for {document_number}: {normalized_code}")
                return normalized_code
        
        # Customize prompt based on document type
        if doc_type == "resolution":
            doc_type_text = "resolution"
            typical_sections = "F items (e.g., F-1, F-2, F-3)"
        else:
            doc_type_text = "ordinance"
            typical_sections = "E items (e.g., E-1, E-2, E-3)"
        
        prompt = f"""You are analyzing a City of Coral Gables {doc_type_text} document (Document #{document_number}).

Your task is to find the AGENDA ITEM CODE referenced in this document.

CRITICAL INSTRUCTIONS:
1. Search the ENTIRE document for agenda item references
2. Return ONLY the code in this format: AGENDA_ITEM: [code]
3. The code should be ONLY the letter and number (e.g., E-2, F-10, H-1)
4. Do NOT include any explanations, reasoning, or additional text
5. If no agenda item is found, return: AGENDA_ITEM: NOT_FOUND

Examples of valid responses:
- AGENDA_ITEM: E-2
- AGENDA_ITEM: F-10
- AGENDA_ITEM: H-1
- AGENDA_ITEM: NOT_FOUND

DO NOT RETURN ANYTHING ELSE. NO EXPLANATIONS.

Full document text:
{text}"""
        
        try:
            response = self.client.chat.completions.create(
                model="meta-llama/llama-4-maverick-17b-128e-instruct",
                messages=[
                    {"role": "system", "content": f"You are a precise data extractor for {doc_type_text} documents. Find and extract only the agenda item code. Search the ENTIRE document thoroughly."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_completion_tokens=8192,
                top_p=1,
                stream=False,
                stop=None
            )
            
            raw_response = response.choices[0].message.content.strip()
            
            # Save raw LLM response for debugging
            with open(debug_dir / f"llm_response_{doc_type}_{document_number}_raw.txt", 'w', encoding='utf-8') as f:
                f.write(raw_response)
            
            # Parse response directly (no qwen parsing needed)
            result = raw_response
            
            # Save cleaned response
            with open(debug_dir / f"llm_response_{doc_type}_{document_number}_cleaned.txt", 'w', encoding='utf-8') as f:
                f.write(result)
            
            # Parse the response
            if "AGENDA_ITEM:" in result:
                # Extract just the code part, stopping at first space or newline after the code
                parts = result.split("AGENDA_ITEM:")[1].strip()
                
                # Extract just the code pattern (letter-number)
                code_match = re.match(r'^([A-Z]-?\d+)', parts)
                if code_match:
                    code = code_match.group(1)
                    if code != "NOT_FOUND":
                        code = self._normalize_item_code(code)
                        log.info(f"âœ… Found agenda item code for {document_number}: {code}")
                        return code
                elif parts.startswith("NOT_FOUND"):
                    log.warning(f"âŒ LLM could not find agenda item in {document_number}")
                else:
                    # Try to extract code from a messy response
                    code_pattern = r'\b([A-Z]-?\d+)\b'
                    match = re.search(code_pattern, parts)
                    if match:
                        code = self._normalize_item_code(match.group(1))
                        log.info(f"âœ… Extracted agenda item code for {document_number}: {code} (from messy response)")
                        return code
                    log.error(f"âŒ Could not parse item code from response: {parts[:100]}")
            else:
                log.error(f"âŒ Invalid LLM response format for {document_number}: {result[:100]}")
            
            return None
            
        except Exception as e:
            log.error(f"Failed to extract agenda item for {doc_type} {document_number}: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _normalize_item_code(self, code: str) -> str:
        """Normalize item code to consistent format."""
        if not code:
            return code
        
        # Remove trailing dots and spaces
        code = code.rstrip('. ')
        
        # Remove dots between letter and dash: "E.-1" -> "E-1"
        code = re.sub(r'([A-Z])\.(-)', r'\1\2', code)
        
        # Handle cases without dash: "E.1" -> "E-1"
        code = re.sub(r'([A-Z])\.(\d)', r'\1-\2', code)
        
        # Remove any remaining dots
        code = code.replace('.', '')
        
        # Ensure we have a dash between letter and number
        code = re.sub(r'([A-Z])(\d)', r'\1-\2', code)
        
        return code
    
    def _extract_title(self, text: str, doc_type: str) -> str:
        """Extract document title from text."""
        # Look for "AN ORDINANCE" or "A RESOLUTION" pattern
        if doc_type == "resolution":
            pattern = r'(A\s+RESOLUTION[^.]+\.)'
        else:
            pattern = r'(AN?\s+ORDINANCE[^.]+\.)'
            
        title_match = re.search(pattern, text[:2000], re.IGNORECASE)
        if title_match:
            return title_match.group(1).strip()
        
        # Fallback to first substantive line
        lines = text.split('\n')
        for line in lines[:20]:
            if len(line) > 20 and not line.isdigit():
                return line.strip()[:200]
        
        return f"Untitled {doc_type.capitalize()}"
    
    def _parse_document_metadata(self, text: str, doc_type: str) -> Dict[str, Any]:
        """Parse additional metadata from document."""
        metadata = {
            "document_type": doc_type
        }
        
        # Extract date passed
        date_match = re.search(r'day\s+of\s+(\w+),?\s+(\d{4})', text)
        if date_match:
            metadata["date_passed"] = date_match.group(0)
        
        # Extract vote information
        vote_match = re.search(r'PASSED\s+AND\s+ADOPTED.*?(\d+).*?(\d+)', text, re.IGNORECASE | re.DOTALL)
        if vote_match:
            metadata["vote_details"] = {
                "ayes": vote_match.group(1),
                "nays": vote_match.group(2) if len(vote_match.groups()) > 1 else "0"
            }
        
        # Extract motion information
        motion_match = re.search(r'motion\s+(?:was\s+)?made\s+by\s+([^,]+)', text, re.IGNORECASE)
        if motion_match:
            metadata["motion"] = {"moved_by": motion_match.group(1).strip()}
        
        # Extract mayor signature
        mayor_match = re.search(r'Mayor[:\s]+([^\n]+)', text[-1000:])
        if mayor_match:
            metadata["signatories"] = {"mayor": mayor_match.group(1).strip()}
        
        # Resolution-specific metadata
        if doc_type == "resolution":
            # Look for resolution-specific patterns
            purpose_match = re.search(r'(?:WHEREAS|PURPOSE)[:\s]+([^.]+)', text, re.IGNORECASE)
            if purpose_match:
                metadata["purpose"] = purpose_match.group(1).strip()
        
        return metadata
    
    def _generate_linking_report(self, meeting_date: str, linked_documents: Dict, debug_dir: Path):
        """Generate a detailed report of the linking process."""
        report = {
            "meeting_date": meeting_date,
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total_ordinances": len(linked_documents["ordinances"]),
                "total_resolutions": len(linked_documents["resolutions"]),
                "ordinances_with_items": len([d for d in linked_documents["ordinances"] if d.get("item_code")]),
                "resolutions_with_items": len([d for d in linked_documents["resolutions"] if d.get("item_code")]),
                "unlinked_ordinances": len([d for d in linked_documents["ordinances"] if not d.get("item_code")]),
                "unlinked_resolutions": len([d for d in linked_documents["resolutions"] if not d.get("item_code")])
            },
            "details": {
                "ordinances": [
                    {
                        "document_number": d["document_number"],
                        "item_code": d.get("item_code", "NOT_FOUND"),
                        "title": d.get("title", "")[:100]
                    }
                    for d in linked_documents["ordinances"]
                ],
                "resolutions": [
                    {
                        "document_number": d["document_number"],
                        "item_code": d.get("item_code", "NOT_FOUND"),
                        "title": d.get("title", "")[:100]
                    }
                    for d in linked_documents["resolutions"]
                ]
            }
        }
        
        # FIXED: Use double quotes for f-string
        report_filename = f"linking_report_{meeting_date.replace('.', '_')}.json"
        report_path = debug_dir / report_filename
        
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        log.info(f"ðŸ“Š Linking report saved to: {report_path}")
    
    # Add backward compatibility method
    async def link_documents_for_meeting_legacy(self, 
                                               meeting_date: str,
                                               documents_dir: Path) -> Dict[str, List[Dict]]:
        """Legacy method for backward compatibility - ordinances only."""
        return await self.link_documents_for_meeting(
            meeting_date,
            documents_dir,  # Ordinances directory
            Path("dummy")   # No resolutions directory
        )


================================================================================


################################################################################
# File: scripts/graph_stages/verbatim_transcript_linker.py
################################################################################

# File: scripts/graph_stages/verbatim_transcript_linker.py

"""
Verbatim Transcript Linker
Links verbatim transcript documents to their corresponding agenda items.
Now with full OCR support for all pages.
"""

import logging
import re
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Set
import json
from datetime import datetime
import PyPDF2
import os
import asyncio
import multiprocessing

# Import the PDF extractor for OCR support
from .pdf_extractor import PDFExtractor

log = logging.getLogger('verbatim_transcript_linker')


class VerbatimTranscriptLinker:
    """Links verbatim transcript documents to agenda items in the graph."""
    
    def __init__(self):
        """Initialize the verbatim transcript linker."""
        # Pattern to extract date and item info from filename
        self.filename_pattern = re.compile(
            r'(\d{2})_(\d{2})_(\d{4})\s*-\s*Verbatim Transcripts\s*-\s*(.+)\.pdf',
            re.IGNORECASE
        )
        
        # Debug directory for logging - ensure parent exists
        self.debug_dir = Path("city_clerk_documents/graph_json/debug/verbatim")
        self.debug_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize PDF extractor for OCR
        self.pdf_extractor = PDFExtractor(
            pdf_dir=Path("."),  # We'll use it file by file
            output_dir=Path("city_clerk_documents/extracted_text")
        )
    
    async def link_transcripts_for_meeting(self, 
                                         meeting_date: str,
                                         verbatim_dir: Path) -> Dict[str, List[Dict]]:
        """Find and link all verbatim transcripts for a specific meeting date."""
        log.info(f"ðŸŽ¤ Linking verbatim transcripts for meeting date: {meeting_date}")
        log.info(f"ðŸ“ Verbatim directory: {verbatim_dir}")
        
        # Debug logging for troubleshooting
        log.info(f"ðŸ” Looking for verbatim transcripts in: {verbatim_dir}")
        log.info(f"ðŸ” Directory exists: {verbatim_dir.exists()}")
        if verbatim_dir.exists():
            all_files = list(verbatim_dir.glob("*.pdf"))
            log.info(f"ðŸ” Total PDF files in directory: {len(all_files)}")
            if all_files:
                log.info(f"ðŸ” Sample files: {[f.name for f in all_files[:3]]}")
        
        # Convert meeting date format: "01.09.2024" -> "01_09_2024"
        date_underscore = meeting_date.replace(".", "_")
        
        # Initialize results
        linked_transcripts = {
            "item_transcripts": [],      # Transcripts for specific agenda items
            "public_comments": [],       # Public comment transcripts
            "section_transcripts": []    # Transcripts for entire sections
        }
        
        if not verbatim_dir.exists():
            log.warning(f"âš ï¸  Verbatim directory not found: {verbatim_dir}")
            return linked_transcripts
        
        # Find all transcript files for this date
        # Try multiple patterns to ensure we catch all files
        patterns = [
            f"{date_underscore}*Verbatim*.pdf",
            f"{date_underscore} - Verbatim*.pdf",
            f"*{date_underscore}*Verbatim*.pdf"
        ]

        transcript_files = []
        for pattern in patterns:
            files = list(verbatim_dir.glob(pattern))
            log.info(f"ðŸ” Pattern '{pattern}' found {len(files)} files")
            transcript_files.extend(files)

        # Remove duplicates
        transcript_files = list(set(transcript_files))
        
        log.info(f"ðŸ“„ Found {len(transcript_files)} transcript files")
        
        if transcript_files:
            # Process transcripts in parallel
            max_concurrent = min(multiprocessing.cpu_count(), 8)
            semaphore = asyncio.Semaphore(max_concurrent)
            
            async def process_with_semaphore(transcript_path):
                async with semaphore:
                    return await self._process_transcript(transcript_path, meeting_date)
            
            # Process all transcripts concurrently
            results = await asyncio.gather(
                *[process_with_semaphore(path) for path in transcript_files],
                return_exceptions=True
            )
            
            # Categorize results
            for transcript_info, path in zip(results, transcript_files):
                if isinstance(transcript_info, Exception):
                    log.error(f"Error processing transcript {path.name}: {transcript_info}")
                    continue
                if transcript_info:
                    # Categorize based on transcript type
                    if transcript_info['transcript_type'] == 'public_comment':
                        linked_transcripts['public_comments'].append(transcript_info)
                    elif transcript_info['transcript_type'] == 'section':
                        linked_transcripts['section_transcripts'].append(transcript_info)
                    else:
                        linked_transcripts['item_transcripts'].append(transcript_info)
                    
                    # Save extracted text (can also be done in parallel)
                    self._save_extracted_text(path, transcript_info)
        
        # Save linked transcripts info for debugging
        self._save_linking_report(meeting_date, linked_transcripts)
        
        # Log summary
        total_linked = (len(linked_transcripts['item_transcripts']) + 
                       len(linked_transcripts['public_comments']) + 
                       len(linked_transcripts['section_transcripts']))
        
        log.info(f"âœ… Verbatim transcript linking complete:")
        log.info(f"   ðŸŽ¤ Item transcripts: {len(linked_transcripts['item_transcripts'])}")
        log.info(f"   ðŸŽ¤ Public comments: {len(linked_transcripts['public_comments'])}")
        log.info(f"   ðŸŽ¤ Section transcripts: {len(linked_transcripts['section_transcripts'])}")
        log.info(f"   ðŸ“„ Total linked: {total_linked}")
        
        return linked_transcripts
    
    async def _process_transcript(self, transcript_path: Path, meeting_date: str) -> Optional[Dict[str, Any]]:
        """Process a single transcript file with full OCR."""
        try:
            # Parse filename
            match = self.filename_pattern.match(transcript_path.name)
            if not match:
                log.warning(f"Could not parse transcript filename: {transcript_path.name}")
                return None
            
            month, day, year = match.groups()[:3]
            item_info = match.group(4).strip()
            
            # Parse item codes from the item info
            parsed_items = self._parse_item_codes(item_info)
            
            # Extract text from ALL pages using Docling OCR
            log.info(f"ðŸ” Running OCR on full transcript: {transcript_path.name}...")
            full_text, pages = self.pdf_extractor.extract_text_from_pdf(transcript_path)
            
            if not full_text:
                log.warning(f"No text extracted from {transcript_path.name}")
                return None
            
            log.info(f"âœ… OCR extracted {len(full_text)} characters from {len(pages)} pages")
            
            # Determine transcript type and normalize item codes
            transcript_type = self._determine_transcript_type(item_info, parsed_items)
            
            transcript_info = {
                "path": str(transcript_path),
                "filename": transcript_path.name,
                "meeting_date": meeting_date,
                "item_info_raw": item_info,
                "item_codes": parsed_items['item_codes'],
                "section_codes": parsed_items['section_codes'],
                "transcript_type": transcript_type,
                "page_count": len(pages),
                "full_text": full_text,  # Store complete transcript text
                "pages": pages,          # Store page-level data
                "extraction_method": "docling_ocr"
            }
            
            log.info(f"ðŸ“„ Processed transcript: {transcript_path.name}")
            log.info(f"   Items: {parsed_items['item_codes']}")
            log.info(f"   Type: {transcript_type}")
            
            return transcript_info
            
        except Exception as e:
            log.error(f"Error processing transcript {transcript_path.name}: {e}")
            return None
    
    def _parse_item_codes(self, item_info: str) -> Dict[str, List[str]]:
        """Parse item codes from the filename item info section."""
        result = {
            'item_codes': [],
            'section_codes': []
        }
        
        # Check for public comment first
        if re.search(r'public\s+comment', item_info, re.IGNORECASE):
            result['section_codes'].append('PUBLIC_COMMENT')
            return result
        
        # Special case: Meeting Minutes or other general labels
        if re.search(r'meeting\s+minutes', item_info, re.IGNORECASE):
            result['item_codes'].append('MEETING_MINUTES')
            return result
        
        # Special case: Full meeting transcript
        if re.search(r'public|full\s+meeting', item_info, re.IGNORECASE) and not re.search(r'comment', item_info, re.IGNORECASE):
            result['item_codes'].append('FULL_MEETING')
            return result
        
        # Special case: Discussion Items (K section)
        if re.match(r'^K\s*$', item_info.strip()):
            result['section_codes'].append('K')
            return result
        
        # Clean the item info
        item_info = item_info.strip()
        
        # Handle multiple items with "and" or "AND"
        # Examples: "F-7 and F-10", "2-1 AND 2-2"
        if ' and ' in item_info.lower():
            parts = re.split(r'\s+and\s+', item_info, flags=re.IGNORECASE)
            for part in parts:
                codes = self._extract_single_item_codes(part.strip())
                result['item_codes'].extend(codes)
        
        # Handle space-separated items
        # Examples: "E-5 E-6 E-7 E-8 E-9 E-10"
        elif re.match(r'^([A-Z]-?\d+\s*)+$', item_info):
            # Split by spaces and extract each item
            items = item_info.split()
            for item in items:
                if re.match(r'^[A-Z]-?\d+$', item):
                    normalized = self._normalize_item_code(item)
                    if normalized:
                        result['item_codes'].append(normalized)
        
        # Handle comma-separated items
        elif ',' in item_info:
            parts = item_info.split(',')
            for part in parts:
                codes = self._extract_single_item_codes(part.strip())
                result['item_codes'].extend(codes)
        
        # Single item or other format
        else:
            codes = self._extract_single_item_codes(item_info)
            result['item_codes'].extend(codes)
        
        # Remove duplicates while preserving order
        result['item_codes'] = list(dict.fromkeys(result['item_codes']))
        result['section_codes'] = list(dict.fromkeys(result['section_codes']))
        
        return result
    
    def _extract_single_item_codes(self, text: str) -> List[str]:
        """Extract item codes from a single text segment."""
        codes = []
        
        # Pattern for item codes: letter-number, letter.number, or just number-number
        # Handles: E-1, E1, E.-1., E.1, 2-1, etc.
        patterns = [
            r'([A-Z])\.?\-?(\d+)\.?',  # Letter-based items
            r'(\d+)\-(\d+)'             # Number-only items like 2-1
        ]
        
        for pattern in patterns:
            for match in re.finditer(pattern, text):
                if pattern.startswith('(\\d'):  # Number-only pattern
                    # For number-only, just use as is
                    codes.append(f"{match.group(1)}-{match.group(2)}")
                else:
                    # For letter-number format
                    letter = match.group(1)
                    number = match.group(2)
                    codes.append(f"{letter}-{number}")
        
        return codes
    
    def _normalize_item_code(self, code: str) -> str:
        """Normalize item code to consistent format (e.g., E-1, 2-1)."""
        # Remove dots and ensure dash format
        code = code.strip('. ')
        
        # Pattern: letter followed by optional punctuation and number
        letter_match = re.match(r'^([A-Z])\.?\-?(\d+)\.?$', code)
        if letter_match:
            letter = letter_match.group(1)
            number = letter_match.group(2)
            return f"{letter}-{number}"
        
        # Pattern: number-number format
        number_match = re.match(r'^(\d+)\-(\d+)$', code)
        if number_match:
            return code  # Already in correct format
        
        return code
    
    def _determine_transcript_type(self, item_info: str, parsed_items: Dict) -> str:
        """Determine the type of transcript based on parsed information."""
        if 'PUBLIC_COMMENT' in parsed_items['item_codes']:
            return 'public_comment'
        elif parsed_items['section_codes']:
            return 'section'
        elif len(parsed_items['item_codes']) > 3:
            return 'multi_item'
        elif len(parsed_items['item_codes']) == 1:
            return 'single_item'
        else:
            return 'item_group'
    
    def _save_linking_report(self, meeting_date: str, linked_transcripts: Dict):
        """Save detailed report of transcript linking."""
        report = {
            "meeting_date": meeting_date,
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total_transcripts": sum(len(v) for v in linked_transcripts.values()),
                "item_transcripts": len(linked_transcripts["item_transcripts"]),
                "public_comments": len(linked_transcripts["public_comments"]),
                "section_transcripts": len(linked_transcripts["section_transcripts"])
            },
            "transcripts": linked_transcripts
        }
        
        report_filename = f"verbatim_linking_report_{meeting_date.replace('.', '_')}.json"
        report_path = self.debug_dir / report_filename
        
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        log.info(f"ðŸ“Š Verbatim linking report saved to: {report_path}")
    
    def _validate_meeting_date(self, meeting_date: str) -> bool:
        """Validate meeting date format MM.DD.YYYY"""
        return bool(re.match(r'^\d{2}\.\d{2}\.\d{4}$', meeting_date))

    def _save_extracted_text(self, transcript_path: Path, transcript_info: Dict[str, Any]):
        """Save extracted transcript text to JSON for GraphRAG processing."""
        output_dir = Path("city_clerk_documents/extracted_text")
        output_dir.mkdir(exist_ok=True)
        
        # Create filename based on transcript info
        meeting_date = transcript_info['meeting_date'].replace('.', '_')
        item_info_clean = re.sub(r'[^a-zA-Z0-9-]', '_', transcript_info['item_info_raw'])
        output_filename = f"verbatim_{meeting_date}_{item_info_clean}_extracted.json"
        output_path = output_dir / output_filename
        
        # Prepare data for saving
        save_data = {
            "document_type": "verbatim_transcript",
            "meeting_date": transcript_info['meeting_date'],
            "item_codes": transcript_info['item_codes'],
            "section_codes": transcript_info['section_codes'],
            "transcript_type": transcript_info['transcript_type'],
            "full_text": transcript_info['full_text'],
            "pages": transcript_info['pages'],
            "metadata": {
                "filename": transcript_info['filename'],
                "item_info_raw": transcript_info['item_info_raw'],
                "page_count": transcript_info['page_count'],
                "extraction_method": "docling_ocr",
                "extracted_at": datetime.now().isoformat()
            }
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, indent=2, ensure_ascii=False)
        
        log.info(f"ðŸ’¾ Saved transcript text to: {output_path}")
        
        # NEW: Also save as markdown
        self._save_transcript_as_markdown(transcript_path, transcript_info, output_dir)

    def _save_transcript_as_markdown(self, transcript_path: Path, transcript_info: Dict[str, Any], output_dir: Path):
        """Save transcript as markdown with metadata header."""
        markdown_dir = output_dir.parent / "extracted_markdown"
        markdown_dir.mkdir(exist_ok=True)
        
        # Build header
        items_str = ', '.join(transcript_info['item_codes']) if transcript_info['item_codes'] else 'N/A'
        
        header = f"""---
DOCUMENT METADATA AND CONTEXT
=============================

**DOCUMENT IDENTIFICATION:**
- Full Path: Verbatim Items/{transcript_info['meeting_date'].split('.')[2]}/{transcript_path.name}
- Document Type: VERBATIM_TRANSCRIPT
- Filename: {transcript_path.name}

**PARSED INFORMATION:**
- Meeting Date: {transcript_info['meeting_date']}
- Agenda Items Discussed: {items_str}
- Transcript Type: {transcript_info['transcript_type']}
- Page Count: {transcript_info['page_count']}

**SEARCHABLE IDENTIFIERS:**
- MEETING_DATE: {transcript_info['meeting_date']}
- DOCUMENT_TYPE: VERBATIM_TRANSCRIPT
{self._format_item_identifiers(transcript_info['item_codes'])}

**NATURAL LANGUAGE DESCRIPTION:**
This is the verbatim transcript from the {transcript_info['meeting_date']} City Commission meeting covering the discussion of {self._describe_items(transcript_info)}.

**QUERY HELPERS:**
{self._build_transcript_query_helpers(transcript_info)}

---

{self._build_item_questions(transcript_info['item_codes'])}

# VERBATIM TRANSCRIPT CONTENT
"""
        
        # Combine with text
        full_content = header + "\n\n" + transcript_info.get('full_text', '')
        
        # Save file
        meeting_date = transcript_info['meeting_date'].replace('.', '_')
        item_info_clean = re.sub(r'[^a-zA-Z0-9-]', '_', transcript_info['item_info_raw'])
        md_filename = f"verbatim_{meeting_date}_{item_info_clean}.md"
        md_path = markdown_dir / md_filename
        
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(full_content)
        
        log.info(f"ðŸ“ Saved transcript markdown to: {md_path}")

    def _format_item_identifiers(self, item_codes: List[str]) -> str:
        """Format agenda items as searchable identifiers."""
        lines = []
        for item in item_codes:
            lines.append(f"- AGENDA_ITEM: {item}")
        return '\n'.join(lines)

    def _describe_items(self, transcript_info: Dict) -> str:
        """Create natural language description of items."""
        if transcript_info['item_codes']:
            if len(transcript_info['item_codes']) == 1:
                return f"agenda item {transcript_info['item_codes'][0]}"
            else:
                return f"agenda items {', '.join(transcript_info['item_codes'])}"
        elif 'PUBLIC_COMMENT' in transcript_info.get('section_codes', []):
            return "public comments section"
        else:
            return "the meeting proceedings"

    def _build_transcript_query_helpers(self, transcript_info: Dict) -> str:
        """Build query helpers for transcripts."""
        helpers = []
        for item in transcript_info.get('item_codes', []):
            helpers.append(f"- To find discussion about {item}, search for 'Item {item}' or '{item} discussion'")
        helpers.append(f"- To find all discussions from this meeting, search for '{transcript_info['meeting_date']}'")
        helpers.append("- This transcript contains the exact words spoken during the meeting")
        return '\n'.join(helpers)

    def _build_item_questions(self, item_codes: List[str]) -> str:
        """Build Q&A style entries for items."""
        questions = []
        for item in item_codes:
            questions.append(f"## What was discussed about Item {item}?")
            questions.append(f"The discussion of Item {item} is transcribed in this document.\n")
        return '\n'.join(questions)


================================================================================


################################################################################
# File: scripts/microsoft_framework/query_router.py
################################################################################

# File: scripts/microsoft_framework/query_router.py

from enum import Enum
from typing import Dict, Any, Optional, List, Tuple
import re
import logging

logger = logging.getLogger(__name__)

class QueryIntent(Enum):
    ENTITY_SPECIFIC = "entity_specific"  # Use Local
    HOLISTIC = "holistic"               # Use Global  
    EXPLORATORY = "exploratory"         # Use DRIFT
    TEMPORAL = "temporal"               # Use DRIFT

class QueryFocus(Enum):
    SPECIFIC_ENTITY = "specific_entity"          # User wants info about ONE specific item
    MULTIPLE_SPECIFIC = "multiple_specific"      # User wants info about MULTIPLE specific items
    COMPARISON = "comparison"                    # User wants to compare entities
    CONTEXTUAL = "contextual"                    # User wants relationships/context
    GENERAL = "general"                          # No specific entity mentioned

class SmartQueryRouter:
    """Automatically route queries to the optimal search method with intelligent intent detection."""
    
    def __init__(self):
        # Entity extraction patterns
        self.entity_patterns = {
            'agenda_item': [
                r'(?:agenda\s+)?(?:item|items)\s+([A-Z]-?\d+)',
                r'(?:item|items)\s+([A-Z]-?\d+)',
                r'([A-Z]-\d+)(?:\s+agenda)?',
                r'\b([A-Z]-\d+)\b'  # Just the code itself
            ],
            'ordinance': [
                r'ordinance(?:\s+(?:number|no\.?|#))?\s*(\d{4}-\d+|\d+)',
                r'(?:city\s+)?ordinance\s+(\d{4}-\d+|\d+)',
                r'\b(\d{4}-\d+)\b(?=.*ordinance)',
                r'ordinance\s+(\w+)'
            ],
            'resolution': [
                r'resolution(?:\s+(?:number|no\.?|#))?\s*(\d{4}-\d+|\d+)',
                r'(?:city\s+)?resolution\s+(\d{4}-\d+|\d+)',
                r'\b(\d{4}-\d+)\b(?=.*resolution)',
                r'resolution\s+(\w+)'
            ]
        }
        
        # Intent indicators
        self.specific_indicators = {
            'singular_determiners': ['the', 'this', 'that', 'a', 'an'],
            'identity_verbs': ['is', 'are', 'was', 'were', 'means', 'mean', 'refers to', 'refer to', 'concerns', 'concern', 'about'],
            'detail_nouns': ['details', 'information', 'content', 'text', 'provision', 'provisions', 'summary', 'summaries', 'description', 'descriptions'],
            'specific_question_words': ['what', 'which', 'show', 'tell', 'explain', 'describe', 'list'],
            'limiting_adverbs': ['only', 'just', 'specifically', 'exactly', 'precisely', 'individually', 'separately']
        }
        
        self.comparison_indicators = {
            'comparison_verbs': ['compare', 'contrast', 'differ', 'differentiate', 'distinguish'],
            'comparison_words': ['versus', 'vs', 'against', 'compared to', 'difference', 'differences', 'similarity', 'similarities'],
            'comparison_phrases': ['how do', 'what is the difference', 'what are the differences']
        }
        
        self.contextual_indicators = {
            'plural_forms': ['items', 'ordinances', 'resolutions', 'documents'],
            'relationship_words': ['related', 'connected', 'associated', 'linked', 'relationship', 
                                  'connections', 'references', 'mentions', 'together', 'context',
                                  'affects', 'impacts', 'influences', 'between', 'among'],
            'exploration_verbs': ['explore', 'analyze', 'understand', 'investigate'],
            'scope_expanders': ['all', 'other', 'various', 'multiple', 'several', 'any']
        }
        
        # Holistic patterns for global search
        self.holistic_patterns = [
            r"what are the (?:main|top|key) (themes|topics|issues)",
            r"summarize (?:the|all) (.*)",
            r"overall (.*)",
            r"trends in (.*)",
            r"patterns across (.*)"
        ]
        
        # Temporal patterns for drift search
        self.temporal_patterns = [
            r"how has (.*) (?:changed|evolved)",
            r"timeline of (.*)",
            r"history of (.*)",
            r"development of (.*) over time",
            r"evolution of (.*)",
            r"changes in (.*)"
        ]
    
    def determine_query_method(self, query: str) -> Dict[str, Any]:
        """Determine query method with source tracking enabled."""
        query_lower = query.lower()
        
        # First check for holistic queries (global search)
        for pattern in self.holistic_patterns:
            if re.search(pattern, query_lower):
                result = {
                    "method": "global",
                    "intent": QueryIntent.HOLISTIC,
                    "params": {
                        "community_level": self._determine_community_level(query),
                        "response_type": "multiple paragraphs"
                    }
                }
                # Add source tracking to params
                result['params']['track_sources'] = True
                result['params']['include_source_metadata'] = True
                result['params']['citation_style'] = 'inline'
                return result
        
        # Check for temporal/exploratory queries (drift search)
        for pattern in self.temporal_patterns:
            if re.search(pattern, query_lower):
                result = {
                    "method": "drift",
                    "intent": QueryIntent.TEMPORAL,
                    "params": {
                        "initial_community_level": 2,
                        "max_follow_ups": 5
                    }
                }
                # Add source tracking to params
                result['params']['track_sources'] = True
                result['params']['include_source_metadata'] = True
                result['params']['citation_style'] = 'inline'
                return result
        
        # Extract ALL entity references
        all_entities = self._extract_all_entities(query)
        
        if not all_entities:
            # No specific entity found - use local search as default
            result = {
                "method": "local",
                "intent": QueryIntent.EXPLORATORY,
                "params": {
                    "top_k_entities": 10,
                    "include_community_context": True
                }
            }
            # Add source tracking to params
            result['params']['track_sources'] = True
            result['params']['include_source_metadata'] = True
            result['params']['citation_style'] = 'inline'
            return result
        
        # Handle single entity
        if len(all_entities) == 1:
            entity_info = all_entities[0]
            query_focus = self._determine_single_entity_focus(query_lower, entity_info)
            
            if query_focus == QueryFocus.SPECIFIC_ENTITY:
                result = {
                    "method": "local",
                    "intent": QueryIntent.ENTITY_SPECIFIC,
                    "params": {
                        "entity_filter": {
                            "type": entity_info['type'].upper(),
                            "value": entity_info['value']
                        },
                        "top_k_entities": 1,
                        "include_community_context": False,
                        "strict_entity_focus": True,
                        "disable_community": True
                    }
                }
            else:  # CONTEXTUAL
                result = {
                    "method": "local",
                    "intent": QueryIntent.ENTITY_SPECIFIC,
                    "params": {
                        "entity_filter": {
                            "type": entity_info['type'].upper(),
                            "value": entity_info['value']
                        },
                        "top_k_entities": 10,
                        "include_community_context": True,
                        "strict_entity_focus": False,
                        "disable_community": False
                    }
                }
            
            # Add source tracking to params
            result['params']['track_sources'] = True
            result['params']['include_source_metadata'] = True
            result['params']['citation_style'] = 'inline'
            return result
        
        # Handle multiple entities
        else:
            query_focus = self._determine_multi_entity_focus(query_lower, all_entities)
            
            if query_focus == QueryFocus.MULTIPLE_SPECIFIC:
                # User wants specific info about each entity separately
                result = {
                    "method": "local",
                    "intent": QueryIntent.ENTITY_SPECIFIC,
                    "params": {
                        "multiple_entities": all_entities,
                        "top_k_entities": 1,
                        "include_community_context": False,
                        "strict_entity_focus": True,
                        "disable_community": True,
                        "aggregate_results": True  # Combine results for each entity
                    }
                }
            elif query_focus == QueryFocus.COMPARISON:
                # User wants to compare entities
                result = {
                    "method": "local",
                    "intent": QueryIntent.ENTITY_SPECIFIC,
                    "params": {
                        "multiple_entities": all_entities,
                        "top_k_entities": 5,
                        "include_community_context": True,  # Need context for comparison
                        "strict_entity_focus": False,
                        "disable_community": False,
                        "comparison_mode": True
                    }
                }
            else:  # CONTEXTUAL
                # User wants relationships between entities
                result = {
                    "method": "local",
                    "intent": QueryIntent.ENTITY_SPECIFIC,
                    "params": {
                        "multiple_entities": all_entities,
                        "top_k_entities": 10,
                        "include_community_context": True,
                        "strict_entity_focus": False,
                        "disable_community": False,
                        "focus_on_relationships": True
                    }
                }
            
            # Add source tracking to params
            result['params']['track_sources'] = True
            result['params']['include_source_metadata'] = True
            result['params']['citation_style'] = 'inline'
            return result
    
    def _extract_all_entities(self, query: str) -> List[Dict[str, str]]:
        """Extract ALL entity references from query."""
        query_lower = query.lower()
        entities = []
        found_positions = {}  # Track positions to avoid duplicates
        
        for entity_type, patterns in self.entity_patterns.items():
            for pattern in patterns:
                for match in re.finditer(pattern, query_lower, re.IGNORECASE):
                    value = match.group(1)
                    position = match.start()
                    
                    # Normalize value
                    if entity_type == 'agenda_item':
                        value = value.upper()
                        if not '-' in value and len(value) > 1:
                            value = f"{value[0]}-{value[1:]}"
                    
                    # Check if we already found an entity at this position
                    if position not in found_positions:
                        found_positions[position] = True
                        entities.append({
                            'type': entity_type,
                            'value': value,
                            'position': position
                        })
        
        # Sort by position and remove position info
        entities = sorted(entities, key=lambda x: x['position'])
        for entity in entities:
            del entity['position']
        
        # Remove duplicates while preserving order
        seen = set()
        unique_entities = []
        for entity in entities:
            key = f"{entity['type']}:{entity['value']}"
            if key not in seen:
                seen.add(key)
                unique_entities.append(entity)
        
        return unique_entities
    
    def _determine_single_entity_focus(self, query_lower: str, entity_info: Dict) -> QueryFocus:
        """Determine focus for single entity queries."""
        specific_score = 0
        contextual_score = 0
        
        tokens = query_lower.split()
        
        # Check for limiting words
        for word in self.specific_indicators['limiting_adverbs']:
            if word in tokens:
                specific_score += 3
        
        # Check for relationship words
        for word in self.contextual_indicators['relationship_words']:
            if word in tokens:
                contextual_score += 3
        
        # Simple "what is X" patterns
        if re.match(r'^(what|whats|what\'s)\s+(is|are)\s+', query_lower):
            specific_score += 2
        
        # Very short queries tend to be specific
        if len(tokens) <= 3:
            specific_score += 2
        
        # Check for detail-seeking patterns
        for noun in self.specific_indicators['detail_nouns']:
            if noun in query_lower:
                specific_score += 1
        
        logger.info(f"Single entity focus scores - Specific: {specific_score}, Contextual: {contextual_score}")
        
        return QueryFocus.SPECIFIC_ENTITY if specific_score > contextual_score else QueryFocus.CONTEXTUAL
    
    def _determine_multi_entity_focus(self, query_lower: str, entities: List[Dict]) -> QueryFocus:
        """Determine focus for multi-entity queries."""
        tokens = query_lower.split()
        
        # Check for comparison indicators
        comparison_score = 0
        for verb in self.comparison_indicators['comparison_verbs']:
            if verb in tokens:
                comparison_score += 3
        
        for word in self.comparison_indicators['comparison_words']:
            if word in query_lower:
                comparison_score += 2
        
        for phrase in self.comparison_indicators['comparison_phrases']:
            if phrase in query_lower:
                comparison_score += 2
        
        # Check for specific information indicators
        specific_score = 0
        
        # "What are E-1 and E-2?" suggests wanting specific info
        if re.match(r'^(what|whats|what\'s)\s+(are|is)\s+', query_lower):
            specific_score += 2
        
        # Check for "separately" or "individually"
        if any(word in tokens for word in ['separately', 'individually', 'each']):
            specific_score += 3
        
        # Check for detail nouns with plural entities
        for noun in self.specific_indicators['detail_nouns']:
            if noun in query_lower:
                specific_score += 1
        
        # Check for contextual/relationship indicators
        contextual_score = 0
        for word in self.contextual_indicators['relationship_words']:
            if word in tokens:
                contextual_score += 2
        
        # Check for "and" patterns that suggest relationships
        # e.g., "relationship between E-1 and E-2"
        if re.search(r'between.*and', query_lower):
            contextual_score += 3
        
        logger.info(f"Multi-entity focus scores - Comparison: {comparison_score}, Specific: {specific_score}, Contextual: {contextual_score}")
        
        # Determine focus based on highest score
        if comparison_score >= specific_score and comparison_score >= contextual_score:
            return QueryFocus.COMPARISON
        elif specific_score > contextual_score:
            return QueryFocus.MULTIPLE_SPECIFIC
        else:
            return QueryFocus.CONTEXTUAL
    
    def _determine_community_level(self, query: str) -> int:
        """Determine optimal community level based on query scope."""
        if any(word in query.lower() for word in ["entire", "all", "overall", "whole"]):
            return 0  # Highest level
        elif any(word in query.lower() for word in ["department", "district", "area"]):
            return 1  # Mid level
        else:
            return 2  # Lower level for more specific summaries


================================================================================


################################################################################
# File: scripts/extract_all_to_markdown.py
################################################################################

# File: scripts/extract_all_to_markdown.py

#!/usr/bin/env python3
"""
Extract all PDFs to markdown format for GraphRAG processing.
"""

import asyncio
from pathlib import Path
import logging
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import os

from graph_stages.pdf_extractor import PDFExtractor
from graph_stages.agenda_pdf_extractor import AgendaPDFExtractor
from graph_stages.enhanced_document_linker import EnhancedDocumentLinker
from graph_stages.verbatim_transcript_linker import VerbatimTranscriptLinker

logging.basicConfig(level=logging.INFO)
log = logging.getLogger(__name__)

async def extract_all_documents():
    """Extract all city clerk documents with parallel processing."""
    
    base_dir = Path("city_clerk_documents/global/City Comissions 2024")
    markdown_dir = Path("city_clerk_documents/extracted_markdown")
    markdown_dir.mkdir(exist_ok=True)
    
    if not base_dir.exists():
        log.error(f"âŒ Base directory not found: {base_dir}")
        log.error(f"   Current working directory: {Path.cwd()}")
        return
    
    log.info(f"ðŸ“ Base directory found: {base_dir}")
    
    # Increase max workers based on system capabilities
    max_workers = min(os.cpu_count() * 2, 16)  # Increased from min(cpu_count, 8)
    
    stats = {
        'agendas': 0,
        'ordinances': 0,
        'resolutions': 0,
        'transcripts': 0,
        'errors': 0
    }
    
    # Process Agendas in parallel
    log.info(f"ðŸ“‹ Extracting Agendas with {max_workers} workers...")
    agenda_dir = base_dir / "Agendas"
    if agenda_dir.exists():
        log.info(f"   Found agenda directory: {agenda_dir}")
        extractor = AgendaPDFExtractor()
        agenda_pdfs = list(agenda_dir.glob("*.pdf"))
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_pdf = {
                executor.submit(process_agenda_pdf, extractor, pdf): pdf 
                for pdf in agenda_pdfs
            }
            
            for future in as_completed(future_to_pdf):
                pdf = future_to_pdf[future]
                try:
                    success = future.result()
                    if success:
                        stats['agendas'] += 1
                except Exception as e:
                    log.error(f"Failed to extract {pdf}: {e}")
                    stats['errors'] += 1
    else:
        log.warning(f"âš ï¸  Agenda directory not found: {agenda_dir}")
    
    # Process Ordinances and Resolutions in parallel
    log.info("ðŸ“œ Extracting Ordinances and Resolutions in parallel...")
    ord_dir = base_dir / "Ordinances"
    res_dir = base_dir / "Resolutions"
    
    if ord_dir.exists() and res_dir.exists():
        linker = EnhancedDocumentLinker()
        
        # Combine ordinances and resolutions for parallel processing
        all_docs = []
        for pdf in ord_dir.rglob("*.pdf"):
            all_docs.append(('ordinance', pdf))
        for pdf in res_dir.rglob("*.pdf"):
            all_docs.append(('resolution', pdf))
        
        # Process in parallel with asyncio
        async def process_documents_batch(docs_batch):
            tasks = []
            for doc_type, pdf_path in docs_batch:
                meeting_date = extract_meeting_date_from_filename(pdf_path.name)
                if meeting_date:
                    task = process_document_async(linker, pdf_path, meeting_date, doc_type)
                    tasks.append(task)
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            return results
        
        # Process in batches to avoid overwhelming the system
        batch_size = max_workers * 2
        for i in range(0, len(all_docs), batch_size):
            batch = all_docs[i:i + batch_size]
            results = await process_documents_batch(batch)
            
            for result, (doc_type, _) in zip(results, batch):
                if isinstance(result, Exception):
                    stats['errors'] += 1
                    log.error(f"Error: {result}")
                elif result:
                    stats['ordinances' if doc_type == 'ordinance' else 'resolutions'] += 1
    else:
        log.warning(f"âš ï¸  Ordinances or Resolutions directory not found: {ord_dir}, {res_dir}")
    
    log.info("ðŸŽ¤ Extracting Verbatim Transcripts...")
    verbatim_dirs = [
        base_dir / "Verbatim Items",
        base_dir / "Verbating Items"
    ]
    
    verbatim_dir = None
    for vdir in verbatim_dirs:
        if vdir.exists():
            verbatim_dir = vdir
            break
    
    if verbatim_dir:
        log.info(f"   Found verbatim directory: {verbatim_dir}")
        transcript_linker = VerbatimTranscriptLinker()
        
        all_verb_pdfs = list(verbatim_dir.rglob("*.pdf"))
        log.info(f"   Found {len(all_verb_pdfs)} verbatim PDFs")
        
        # Process verbatim transcripts in parallel batches
        batch_size = max_workers
        for i in range(0, len(all_verb_pdfs), batch_size):
            batch = all_verb_pdfs[i:i + batch_size]
            
            tasks = []
            for pdf_path in batch:
                meeting_date = extract_meeting_date_from_verbatim(pdf_path.name)
                if meeting_date:
                    task = process_verbatim_async(transcript_linker, pdf_path, meeting_date)
                    tasks.append(task)
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in results:
                if isinstance(result, Exception):
                    stats['errors'] += 1
                    log.error(f"Error: {result}")
                elif result:
                    stats['transcripts'] += 1
    else:
        log.warning(f"âš ï¸  Verbatim directory not found. Tried: {verbatim_dirs}")
    
    log.info("\nðŸ“Š Extraction Summary:")
    log.info(f"   Agendas: {stats['agendas']}")
    log.info(f"   Ordinances: {stats['ordinances']}")
    log.info(f"   Resolutions: {stats['resolutions']}")
    log.info(f"   Transcripts: {stats['transcripts']}")
    log.info(f"   Errors: {stats['errors']}")
    log.info(f"   Total: {sum(stats.values()) - stats['errors']}")
    
    log.info(f"\nâœ… All documents extracted to:")
    log.info(f"   JSON: city_clerk_documents/extracted_text/")
    log.info(f"   Markdown: {markdown_dir}")

def process_agenda_pdf(extractor, pdf):
    """Process single agenda PDF (for thread pool)."""
    try:
        log.info(f"   Processing: {pdf.name}")
        agenda_data = extractor.extract_agenda(pdf)
        output_path = Path("city_clerk_documents/extracted_text") / f"{pdf.stem}_extracted.json"
        extractor.save_extracted_agenda(agenda_data, output_path)
        return True
    except Exception as e:
        log.error(f"Failed to extract {pdf}: {e}")
        return False

async def process_document_async(linker, pdf_path, meeting_date, doc_type):
    """Process document asynchronously."""
    try:
        doc_info = await linker._process_document(pdf_path, meeting_date, doc_type)
        if doc_info:
            linker._save_extracted_text(pdf_path, doc_info, doc_type)
            return True
        return False
    except Exception as e:
        raise e

# Add async helper
async def process_verbatim_async(transcript_linker, pdf_path, meeting_date):
    """Process verbatim transcript asynchronously."""
    try:
        transcript_info = await transcript_linker._process_transcript(pdf_path, meeting_date)
        if transcript_info:
            transcript_linker._save_extracted_text(pdf_path, transcript_info)
            return True
        return False
    except Exception as e:
        raise e

def extract_meeting_date_from_filename(filename: str) -> str:
    """Extract meeting date from ordinance/resolution filename."""
    import re
    
    match = re.search(r'(\d{2})_(\d{2})_(\d{4})', filename)
    if match:
        month, day, year = match.groups()
        return f"{month}.{day}.{year}"
    
    return None

def extract_meeting_date_from_verbatim(filename: str) -> str:
    """Extract meeting date from verbatim transcript filename."""
    import re
    
    match = re.match(r'(\d{2})_(\d{2})_(\d{4})', filename)
    if match:
        month, day, year = match.groups()
        return f"{month}.{day}.{year}"
    
    return None

if __name__ == "__main__":
    asyncio.run(extract_all_documents())


================================================================================


################################################################################
# File: scripts/microsoft_framework/graphrag_output_processor.py
################################################################################

# File: scripts/microsoft_framework/graphrag_output_processor.py

from pathlib import Path
import pandas as pd
import json
from typing import Dict, List, Any
import re
import ast
import logging

log = logging.getLogger(__name__)

class GraphRAGOutputProcessor:
    """Process and load GraphRAG output artifacts."""
    
    def __init__(self, output_dir: Path):
        self.output_dir = Path(output_dir)
        
    def parse_config_string(self, config_str: str) -> Dict[str, Any]:
        """Parse configuration strings from GraphRAG output."""
        try:
            # Handle strings like: "default_vector_store": { "type": "lancedb", ... }
            # First try to parse as JSON-like string
            if '"default_vector_store"' in config_str or "'default_vector_store'" in config_str:
                # Extract the dictionary part
                match = re.search(r'"default_vector_store"\s*:\s*({[^}]+})', config_str)
                if not match:
                    match = re.search(r"'default_vector_store'\s*:\s*({[^}]+})", config_str)
                
                if match:
                    dict_str = match.group(1)
                    # Replace single quotes with double quotes for JSON parsing
                    dict_str = dict_str.replace("'", '"')
                    # Handle None/null values
                    dict_str = dict_str.replace('None', 'null')
                    dict_str = dict_str.replace('True', 'true')
                    dict_str = dict_str.replace('False', 'false')
                    
                    return json.loads(dict_str)
            
            # Try to parse as Python literal
            return ast.literal_eval(config_str)
        except Exception as e:
            log.error(f"Failed to parse config string: {e}")
            return {}
    
    def extract_vector_store_config(self, artifacts: Dict[str, Any]) -> Dict[str, Any]:
        """Extract vector store configuration from artifacts."""
        vector_configs = {}
        
        # Check entities for embedded config
        if 'entities' in artifacts:
            entities_df = artifacts['entities']
            # Look for configuration in entity descriptions or metadata
            for _, entity in entities_df.iterrows():
                if 'description' in entity and 'default_vector_store' in str(entity['description']):
                    config = self.parse_config_string(str(entity['description']))
                    if config:
                        vector_configs['from_entities'] = config
        
        # Check community reports
        if 'community_reports' in artifacts:
            reports_df = artifacts['community_reports']
            for _, report in reports_df.iterrows():
                if 'summary' in report and 'default_vector_store' in str(report['summary']):
                    config = self.parse_config_string(str(report['summary']))
                    if config:
                        vector_configs['from_reports'] = config
        
        return vector_configs
    
    def load_artifacts(self) -> Dict[str, Any]:
        """Load all GraphRAG artifacts."""
        artifacts = {}
        
        # Load entities
        entities_path = self.output_dir / "entities.parquet"
        if entities_path.exists():
            try:
                import pandas as pd
                artifacts['entities'] = pd.read_parquet(entities_path)
            except Exception as e:
                log.error(f"Failed to load entities: {e}")
        
        # Load relationships
        relationships_path = self.output_dir / "relationships.parquet"
        if relationships_path.exists():
            try:
                import pandas as pd
                artifacts['relationships'] = pd.read_parquet(relationships_path)
            except Exception as e:
                log.error(f"Failed to load relationships: {e}")
        
        # Load community reports
        reports_path = self.output_dir / "community_reports.parquet"
        if reports_path.exists():
            try:
                import pandas as pd
                artifacts['community_reports'] = pd.read_parquet(reports_path)
            except Exception as e:
                log.error(f"Failed to load community reports: {e}")
        
        return artifacts
    
    def load_graphrag_artifacts(self) -> Dict[str, Any]:
        """Load all GraphRAG output artifacts."""
        artifacts = {}
        
        # Load entities
        entities_path = self.output_dir / "entities.parquet"
        if entities_path.exists():
            artifacts['entities'] = pd.read_parquet(entities_path)
        
        # Load relationships
        relationships_path = self.output_dir / "relationships.parquet"
        if relationships_path.exists():
            artifacts['relationships'] = pd.read_parquet(relationships_path)
        
        # Load communities
        communities_path = self.output_dir / "communities.parquet"
        if communities_path.exists():
            artifacts['communities'] = pd.read_parquet(communities_path)
        
        # Load community reports
        reports_path = self.output_dir / "community_reports.parquet"
        if reports_path.exists():
            artifacts['community_reports'] = pd.read_parquet(reports_path)
        
        return artifacts
    
    def get_entity_summary(self) -> Dict[str, Any]:
        """Get summary of entities."""
        entities_path = self.output_dir / "entities.parquet"
        if not entities_path.exists():
            return {}
        
        try:
            entities_df = pd.read_parquet(entities_path)
            
            summary = {
                'total_entities': len(entities_df),
                'entity_types': entities_df['type'].value_counts().to_dict() if 'type' in entities_df.columns else {}
            }
            return summary
        except Exception as e:
            log.error(f"Failed to load entities: {e}")
            return {}
    
    def get_relationship_summary(self) -> Dict[str, Any]:
        """Get summary of relationships."""
        relationships_path = self.output_dir / "relationships.parquet"
        if not relationships_path.exists():
            return {}
        
        try:
            relationships_df = pd.read_parquet(relationships_path)
            
            summary = {
                'total_relationships': len(relationships_df),
                'relationship_types': relationships_df['type'].value_counts().to_dict() if 'type' in relationships_df.columns else {}
            }
            return summary
        except Exception as e:
            log.error(f"Failed to load relationships: {e}")
            return {}
    
    def get_community_summary(self) -> Dict[str, Any]:
        """Get summary statistics of extracted communities."""
        communities_path = self.output_dir / "communities.parquet"
        if communities_path.exists():
            communities_df = pd.read_parquet(communities_path)
            
            summary = {
                'total_communities': len(communities_df),
                'community_types': communities_df['type'].value_counts().to_dict()
            }
            return summary
        else:
            return {}
    
    def parse_vector_store_config(self, text: str) -> dict:
        """Parse vector store configuration from text."""
        import re
        import json
        
        pattern = r'"default_vector_store"\s*:\s*(\{[^}]+\})'
        match = re.search(pattern, text)
        
        if match:
            try:
                config_str = match.group(1)
                config_str = config_str.replace("null", "null")
                config_str = config_str.replace("true", "true")
                config_str = config_str.replace("false", "false")
                return json.loads(config_str)
            except:
                pass
        return {}


================================================================================


################################################################################
# File: scripts/microsoft_framework/graphrag_initializer.py
################################################################################

# File: scripts/microsoft_framework/graphrag_initializer.py

import os
from pathlib import Path
import yaml
import subprocess
import sys
from typing import Dict, Any

class GraphRAGInitializer:
    """Initialize and configure Microsoft GraphRAG for city clerk documents."""
    
    def __init__(self, project_root: Path):
        self.project_root = Path(project_root)
        self.graphrag_root = self.project_root / "graphrag_data"
        
    def setup_environment(self):
        """Setup GraphRAG environment and configuration."""
        # Get the correct Python executable
        if hasattr(sys, 'real_prefix') or (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix):
            # We're in a virtualenv
            python_exe = sys.executable
        else:
            # Try to find venv Python
            venv_python = os.path.join(os.path.dirname(sys.executable), '..', 'venv', 'bin', 'python3')
            if os.path.exists(venv_python):
                python_exe = venv_python
            else:
                python_exe = sys.executable
        
        print(f"ðŸ Using Python: {python_exe}")
        
        # Create directory structure
        self.graphrag_root.mkdir(exist_ok=True)
        
        # Run GraphRAG init
        subprocess.run([
            python_exe,  # Use the correct Python
            "-m", "graphrag", "init", 
            "--root", str(self.graphrag_root),
            "--force"
        ])
        
        # Configure settings
        self._configure_settings()
        self._configure_prompts()
        
    def _configure_settings(self):
        """Configure with enhanced extraction."""
        settings = {
            "encoding_model": "cl100k_base",
            "skip_workflows": [],
            "models": {
                "default_chat_model": {
                    "api_key": "${OPENAI_API_KEY}",
                    "type": "openai_chat",
                    "model": "gpt-4.1-mini-2025-04-14",
                    "encoding_model": "cl100k_base",
                    "max_tokens": 32768,
                    "temperature": 0,
                    "api_type": "openai"
                },
                "default_embedding_model": {
                    "api_key": "${OPENAI_API_KEY}",
                    "type": "openai_embedding",
                    "model": "text-embedding-3-small",
                    "encoding_model": "cl100k_base",
                    "batch_size": 16,
                    "batch_max_tokens": 2048
                }
            },
            "input": {
                "type": "file",
                "file_type": "csv",
                "base_dir": ".",
                "source_column": "text",
                "text_column": "text",
                "title_column": "title"
            },
            "chunks": {
                "group_by_columns": [
                    "document_type",
                    "meeting_date",
                    "item_code"
                ],
                "overlap": 200,
                "size": 1200
            },
            "extract_graph": {
                "model_id": "default_chat_model",
                "prompt": "prompts/entity_extraction.txt",
                "entity_types": [
                    "agenda_item",
                    "ordinance", 
                    "resolution",
                    "document_number",
                    "cross_reference",
                    "person",
                    "organization",
                    "meeting",
                    "money",
                    "project"
                ],
                "max_gleanings": 3,
                "pattern_examples": {
                    "agenda_item": ["E-1", "F-10", "Item E-1", "(Agenda Item: E-1)"],
                    "ordinance": ["2024-01", "Ordinance 3576", "Ordinance No. 3576"],
                    "document_number": ["2024-01", "3576", "Resolution 2024-123"]
                }
            },
            "entity_extraction": {
                "entity_types": [
                    "person",
                    "organization",
                    "location",
                    "document",
                    "meeting",
                    "money",
                    "project",
                    "agenda_item",
                    "ordinance",
                    "resolution",
                    "contract"
                ],
                "max_gleanings": 2
            },
            "community_reports": {
                "max_input_length": 32768,
                "max_length": 2000
            },
            "claim_extraction": {
                "description": "Extract voting records, motions, and decisions",
                "enabled": True
            },
            "cluster_graph": {
                "max_cluster_size": 10
            },
            "storage": {
                "base_dir": "./output/artifacts",
                "type": "file"
            }
        }
        
        settings_path = self.graphrag_root / "settings.yaml"
        with open(settings_path, 'w') as f:
            yaml.dump(settings, f, sort_keys=False)
    
    def _configure_prompts(self):
        """Setup prompt configuration placeholders."""
        # This method will be called after auto-tuning
        pass


================================================================================


################################################################################
# File: scripts/microsoft_framework/cosmos_synchronizer.py
################################################################################

# File: scripts/microsoft_framework/cosmos_synchronizer.py

from pathlib import Path
import pandas as pd
import json
from typing import Dict, List, Any
import asyncio
from scripts.graph_stages.cosmos_db_client import CosmosGraphClient

class GraphRAGCosmosSync:
    """Synchronize GraphRAG output with Cosmos DB."""
    
    def __init__(self, graphrag_output_dir: Path):
        self.output_dir = Path(graphrag_output_dir)
        self.cosmos_client = CosmosGraphClient()
        
    async def sync_to_cosmos(self):
        """Sync GraphRAG data to Cosmos DB."""
        await self.cosmos_client.connect()
        
        try:
            # Load GraphRAG artifacts
            entities_df = pd.read_parquet(self.output_dir / "entities.parquet")
            relationships_df = pd.read_parquet(self.output_dir / "relationships.parquet")
            communities_df = pd.read_parquet(self.output_dir / "communities.parquet")
            
            # Sync entities
            print(f"ðŸ“¤ Syncing {len(entities_df)} entities to Cosmos DB...")
            for _, entity in entities_df.iterrows():
                await self._sync_entity(entity)
            
            # Sync relationships
            print(f"ðŸ”— Syncing {len(relationships_df)} relationships...")
            for _, rel in relationships_df.iterrows():
                await self._sync_relationship(rel)
            
            # Sync communities as properties
            print(f"ðŸ˜ï¸ Syncing {len(communities_df)} communities...")
            for _, community in communities_df.iterrows():
                await self._sync_community(community)
                
        finally:
            await self.cosmos_client.close()
    
    async def _sync_entity(self, entity: pd.Series):
        """Sync a GraphRAG entity to Cosmos DB."""
        # Map GraphRAG entity to Cosmos vertex
        vertex_id = f"graphrag_entity_{entity['id']}"
        
        properties = {
            'name': entity['name'],
            'type': entity['type'],
            'description': entity['description'],
            'graphrag_id': entity['id'],
            'community_ids': json.dumps(entity.get('community_ids', [])),
            'has_graphrag': True
        }
        
        # Map to appropriate label based on type
        label_map = {
            'person': 'Person',
            'organization': 'Organization',
            'location': 'Location',
            'document': 'Document',
            'meeting': 'Meeting',
            'agenda_item': 'AgendaItem',
            'project': 'Project'
        }
        
        label = label_map.get(entity['type'].lower(), 'Entity')
        
        await self.cosmos_client.upsert_vertex(
            label=label,
            vertex_id=vertex_id,
            properties=properties
        )
    
    async def _sync_relationship(self, rel: pd.Series):
        """Sync a GraphRAG relationship to Cosmos DB."""
        from_id = f"graphrag_entity_{rel['source']}"
        to_id = f"graphrag_entity_{rel['target']}"
        
        properties = {
            'description': rel['description'],
            'weight': rel.get('weight', 1.0),
            'graphrag_rel_id': rel['id']
        }
        
        await self.cosmos_client.create_edge_if_not_exists(
            from_id=from_id,
            to_id=to_id,
            edge_type=rel['type'].upper(),
            properties=properties
        )
    
    async def _sync_community(self, community: pd.Series):
        """Sync a GraphRAG community as metadata to relevant entities."""
        # Communities can be stored as properties on entities
        # or as separate vertices depending on your schema preference
        pass


================================================================================


################################################################################
# File: settings.yaml
################################################################################

# File: settings.yaml

### GraphRAG Configuration for City Clerk Documents

### LLM settings ###
models:
  default_chat_model:
    type: openai_chat
    auth_type: api_key
    api_key: ${OPENAI_API_KEY}
    model: gpt-4.1-mini-2025-04-14
    encoding_model: cl100k_base
    max_tokens: 16384
    temperature: 0

  default_embedding_model:
    type: openai_embedding
    auth_type: api_key
    api_key: ${OPENAI_API_KEY}
    model: text-embedding-3-small
    batch_size: 16
    batch_max_tokens: 2048

### Input settings ###
input:
  type: file
  file_type: csv
  base_dir: "."
  file_pattern: "city_clerk_documents.csv"

### Chunking settings ###
chunks:
  size: 800
  overlap: 300

### Output/storage settings ###
storage:
  type: file
  base_dir: "output"

### Community detection settings ###
cluster_graph:
  max_cluster_size: 10

### Entity extraction ###
entity_extraction:
  entity_types:
    - person
    - organization 
    - location
    - document
    - meeting
    - money
    - project
    - agenda_item: "pattern: [A-Z]-?\\d+"
    - ordinance
    - resolution
    - contract
    - document_number: "pattern: \\d{4}-\\d+"
  max_gleanings: 3

claim_extraction:
  description: Extract voting records, motions, and decisions
  enabled: true
  prompt: prompts/city_clerk_claims.txt

community_reports:
  max_input_length: 16384
  max_length: 2000
  prompt: prompts/city_clerk_community_report.txt

# Legacy LLM config for backward compatibility
llm:
  api_key: ${OPENAI_API_KEY}
  api_type: openai
  max_tokens: 16384
  model: gpt-4.1-mini-2025-04-14
  temperature: 0

# Legacy embeddings config for backward compatibility  
embeddings:
  api_key: ${OPENAI_API_KEY}
  batch_max_tokens: 2048
  batch_size: 16
  model: text-embedding-3-small

query:
  drift_search:
    follow_up_depth: 5
    follow_up_expansion: 3
    include_global_context: true
    initial_community_level: 2
    max_iterations: 5
    max_tokens: 16384
    primer_queries: 3
    relevance_threshold: 0.7
    similarity_threshold: 0.8
    temperature: 0.0
    termination_strategy: convergence
  global_search:
    community_level: 2
    max_tokens: 16384
    n: 1
    rate_relevancy_model: gpt-4.1-mini-2025-04-14
    relevance_score_threshold: 0.7
    temperature: 0.0
    top_p: 1.0
    use_dynamic_community_selection: true
  local_search:
    community_prop: 0.1
    conversation_history_max_turns: 5
    max_tokens: 16384
    temperature: 0.0
    text_unit_prop: 0.5
    top_k_entities: 10
    top_k_relationships: 10

### Enhanced Entity Deduplication Settings ###
entity_deduplication:
  enabled: true
  
  # Matching strategies
  enable_partial_name_matching: true     # "Vince Lago" matches "Lago"
  enable_token_matching: true            # Token-based overlap
  enable_semantic_matching: true         # TF-IDF similarity
  enable_graph_structure_matching: true  # Neighbor overlap
  enable_abbreviation_matching: true     # "V. Lago" matches "Vince Lago"
  enable_role_based_matching: true       # "Mayor" matches "Mayor Lago"
  
  # Thresholds
  exact_match_threshold: 0.95
  high_similarity_threshold: 0.85
  partial_match_threshold: 0.7
  clustering_tolerance: 0.15
  min_combined_score: 0.7
  
  # Score weights
  weights:
    string_similarity: 0.2
    token_overlap: 0.4
    graph_structure: 0.2
    semantic_similarity: 0.2
  
  # Configuration presets
  presets:
    aggressive:
      min_combined_score: 0.65
    conservative:
      min_combined_score: 0.85
    name_focused:
      min_combined_score: 0.7


================================================================================


################################################################################
# File: scripts/microsoft_framework/graphrag_pipeline.py
################################################################################

# File: scripts/microsoft_framework/graphrag_pipeline.py

import asyncio
import subprocess
from pathlib import Path
import pandas as pd
import json
from typing import Dict, List, Any

# Import other components
from .graphrag_initializer import GraphRAGInitializer
from .document_adapter import CityClerkDocumentAdapter
from .prompt_tuner import CityClerkPromptTuner
from .graphrag_output_processor import GraphRAGOutputProcessor

class CityClerkGraphRAGPipeline:
    """Main pipeline for processing city clerk documents with GraphRAG."""
    
    def __init__(self, project_root: Path):
        self.project_root = Path(project_root)
        self.graphrag_root = self.project_root / "graphrag_data"
        self.output_dir = self.graphrag_root / "output"
        
    async def run_full_pipeline(self, force_reindex: bool = False):
        """Run the complete GraphRAG indexing pipeline."""
        
        # Step 1: Initialize GraphRAG
        print("ðŸ”§ Initializing GraphRAG...")
        initializer = GraphRAGInitializer(self.project_root)
        initializer.setup_environment()
        
        # Step 2: Prepare documents
        print("ðŸ“„ Preparing documents...")
        adapter = CityClerkDocumentAdapter(
            self.project_root / "city_clerk_documents/extracted_text"
        )
        df = adapter.prepare_documents_for_graphrag(self.graphrag_root)
        
        # Step 3: Run prompt tuning
        print("ðŸŽ¯ Tuning prompts for city clerk domain...")
        tuner = CityClerkPromptTuner(self.graphrag_root)
        tuner.run_auto_tuning()
        tuner.customize_prompts()
        
        # Step 4: Run GraphRAG indexing
        print("ðŸ—ï¸ Running GraphRAG indexing...")
        subprocess.run([
            "graphrag", "index",
            "--root", str(self.graphrag_root),
            "--verbose"
        ])
        
        # Step 5: Process outputs
        print("ðŸ“Š Processing GraphRAG outputs...")
        processor = GraphRAGOutputProcessor(self.output_dir)
        graph_data = processor.load_graphrag_artifacts()
        
        return graph_data


================================================================================


################################################################################
# File: check_enhanced_results.py
################################################################################

# File: check_enhanced_results.py

#!/usr/bin/env python3
import pandas as pd
import sys
from pathlib import Path

# Add project root to path
sys.path.append('.')

def check_results():
    print('ðŸ“Š Enhanced Deduplication Results:')
    
    # Load original and deduplicated data
    orig_entities = pd.read_parquet('graphrag_data/output/entities.parquet')
    dedup_entities = pd.read_parquet('graphrag_data/output/deduplicated/entities.parquet')
    
    print(f'   Original entities: {len(orig_entities)}')
    print(f'   Deduplicated entities: {len(dedup_entities)}')
    print(f'   Entities merged: {len(orig_entities) - len(dedup_entities)}')
    print(f'   Reduction: {((len(orig_entities) - len(dedup_entities))/len(orig_entities)*100):.1f}%')
    
    # Check for aliases
    if 'aliases' in dedup_entities.columns:
        entities_with_aliases = dedup_entities[dedup_entities['aliases'].notna() & (dedup_entities['aliases'] != '')]
        print(f'   Entities with aliases: {len(entities_with_aliases)}')
        
        if len(entities_with_aliases) > 0:
            print('\nðŸ“ Example merged entities:')
            for idx, entity in entities_with_aliases.head(5).iterrows():
                print(f"   - '{entity['title']}'")
                print(f"     Aliases: {entity['aliases']}")
    else:
        print('   No aliases column found')
    
    # Check for enhanced reports
    output_dir = Path('graphrag_data/output')
    json_report = output_dir / 'enhanced_deduplication_report.json'
    text_report = output_dir / 'enhanced_deduplication_report.txt'
    
    if json_report.exists():
        print(f'\nðŸ“‹ Enhanced report available: {json_report}')
    if text_report.exists():
        print(f'ðŸ“‹ Text report available: {text_report}')
    
    if not json_report.exists() and not text_report.exists():
        print('\nâš ï¸  Enhanced deduplication reports not found')

if __name__ == '__main__':
    check_results()


================================================================================


################################################################################
# File: check_ordinances.py
################################################################################

# File: check_ordinances.py

import pandas as pd

# Check what ordinances were extracted
entities = pd.read_parquet('graphrag_data/output/entities.parquet')

# First check the columns
print("Columns in entities:", entities.columns.tolist())
print("Sample row:")
print(entities.head(1))

# Look for ordinances
ordinances = entities[entities['type'] == 'ORDINANCE']
print(f"\nOrdinances extracted: {len(ordinances)}")
print(ordinances[['title', 'description']].head(10))

# Also check for 2024-01 in any entity
ord_2024_01 = entities[entities['title'].str.contains('2024-01', case=False, na=False) | 
                       entities['description'].str.contains('2024-01', case=False, na=False)]
print(f"\n2024-01 mentions: {len(ord_2024_01)}")
if len(ord_2024_01) > 0:
    print(ord_2024_01[['title', 'type', 'description']])

# Check if "ORDINANCE 3576" was extracted (that's the Cocoplum ordinance)
ord_3576 = entities[entities['title'].str.contains('3576', na=False)]
print(f"\nOrdinance 3576 found: {len(ord_3576)}")
if len(ord_3576) > 0:
    print(ord_3576[['title', 'type', 'description']].iloc[0])
    print(f"\nFull description of first 3576 ordinance:")
    print(ord_3576.iloc[0]['description'])

# Check if Ordinance 3576 is the E-1 Cocoplum ordinance
if len(ord_3576) > 0:
    desc = ord_3576.iloc[0]['description']
    if 'Cocoplum' in desc or 'E-1' in desc:
        print("\nâœ… Ordinance 3576 IS the Cocoplum/E-1 ordinance!")
        print(f"Description: {desc}")
    else:
        print("\nâŒ Ordinance 3576 does not appear to be the Cocoplum/E-1 ordinance")
        print(f"Description: {desc}")


================================================================================


################################################################################
# File: requirements.txt
################################################################################

################################################################################
################################################################################
# Python dependencies for Misophonia Research RAG System

# Core dependencies
openai>=1.82.0
groq>=0.4.1
gremlinpython>=3.7.3
aiofiles>=24.1.0
python-dotenv>=1.0.0

# Graph Visualization dependencies
dash>=2.14.0
plotly>=5.17.0
networkx>=3.2.0
dash-table>=5.0.0
dash-cytoscape>=0.3.0
dash-bootstrap-components>=1.0.0

# Database and API dependencies
supabase>=2.0.0

# GraphRAG dependencies
graphrag==2.3.0
pyyaml>=6.0.0
pyarrow>=14.0.0
scipy>=1.11.0

# Optional for async progress bars
tqdm

# Data processing
pandas>=2.0.0
numpy

# PDF processing
PyPDF2>=3.0.0
unstructured[pdf]==0.10.30
pytesseract>=0.3.10
pdf2image>=1.16.3
docling
pdfminer.six>=20221105

# PDF processing with hyperlink support
PyMuPDF>=1.23.0

# Utilities
colorama>=0.4.6

# For concurrent processing
concurrent-log-handler>=0.9.20

# Async dependencies for optimized pipeline
aiohttp>=3.8.0

# Token counting for OpenAI rate limiting
tiktoken>=0.5.0

# Enhanced deduplication dependencies
python-Levenshtein>=0.20.0
scikit-learn>=1.3.0


================================================================================


################################################################################
# File: scripts/microsoft_framework/query_graphrag.py
################################################################################

# File: scripts/microsoft_framework/query_graphrag.py

#!/usr/bin/env python3
import subprocess
import sys
from pathlib import Path

# Use venv Python
venv_python = Path("venv/bin/python3")
if not venv_python.exists():
    print("Error: venv not found!")
    sys.exit(1)

# Get query from command line
query = " ".join(sys.argv[1:]) if len(sys.argv) > 1 else "Who is Mayor Lago?"

# Run query
cmd = [
    str(venv_python),
    "-m", "graphrag", "query",
    "--root", "graphrag_data",
    "--method", "local",
    "--query", query
]

result = subprocess.run(cmd, capture_output=True, text=True)
print(result.stdout)


================================================================================

