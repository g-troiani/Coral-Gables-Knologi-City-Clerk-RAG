# Concatenated Project Code - Part 3 of 3
# Generated: 2025-06-05 10:54:45
# Root Directory: /Users/gianmariatroiani/Documents/knologiÃä/graph_database
================================================================================

# File Index - Which Files Are in Which Parts
################################################################################

## Part 1 (6 files):
  - scripts/graph_stages/enhanced_document_linker.py
  - scripts/graph_stages/pdf_extractor.py
  - scripts/test_ontology_urls.py
  - scripts/minimal_url_test.py
  - requirements.txt
  - scripts/test_e1_urls.py

## Part 2 (6 files):
  - scripts/graph_stages/verbatim_transcript_linker.py
  - test_query_system.py
  - scripts/test_url_extraction.py
  - scripts/test_graph_urls.py
  - graph_clear_database.py
  - scripts/check_extracted_urls.py

## Part 3 (6 files):
  - scripts/graph_stages/document_linker.py
  - scripts/graph_stages/cosmos_db_client.py
  - scripts/debug_visualizer.py
  - scripts/check_graph_empty.py
  - config.py
  - scripts/graph_stages/__init__.py


================================================================================


################################################################################
# File: scripts/graph_stages/document_linker.py
################################################################################

# File: scripts/graph_stages/document_linker.py

"""
Document Linker
Links ordinance documents to their corresponding agenda items.
"""

import logging
import re
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import json
from datetime import datetime
import PyPDF2
from openai import OpenAI
import os
from dotenv import load_dotenv

load_dotenv()

log = logging.getLogger('document_linker')


class DocumentLinker:
    """Links ordinance and resolution documents to agenda items."""
    
    def __init__(self,
                 openai_api_key: Optional[str] = None,
                 model: str = "gpt-4.1-mini-2025-04-14",
                 agenda_extraction_max_tokens: int = 32768):
        """Initialize the document linker."""
        self.api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY not found in environment")
        
        self.client = OpenAI(api_key=self.api_key)
        self.model = model
        self.agenda_extraction_max_tokens = agenda_extraction_max_tokens
    
    async def link_documents_for_meeting(self, 
                                       meeting_date: str,
                                       documents_dir: Path) -> Dict[str, List[Dict]]:
        """Find and link all documents for a specific meeting date."""
        log.info(f"üîó Linking documents for meeting date: {meeting_date}")
        log.info(f"üìÅ Looking for ordinances in: {documents_dir}")
        
        # Create debug directory
        debug_dir = Path("city_clerk_documents/graph_json/debug")
        debug_dir.mkdir(exist_ok=True)
        
        # Convert date format: "01.09.2024" -> "01_09_2024"
        date_underscore = meeting_date.replace(".", "_")
        
        # Log what we're searching for
        with open(debug_dir / "document_search_info.txt", 'w') as f:
            f.write(f"Meeting Date: {meeting_date}\n")
            f.write(f"Date with underscores: {date_underscore}\n")
            f.write(f"Search directory: {documents_dir}\n")
            f.write(f"Search pattern: *{date_underscore}.pdf\n")
        
        # Find all matching ordinance/resolution files
        # Pattern: YYYY-## - MM_DD_YYYY.pdf
        pattern = f"*{date_underscore}.pdf"
        matching_files = list(documents_dir.glob(pattern))
        
        # Also try without spaces in case filenames vary
        pattern2 = f"*{date_underscore}*.pdf"
        additional_files = [f for f in documents_dir.glob(pattern2) if f not in matching_files]
        matching_files.extend(additional_files)
        
        # List all files in directory for debugging
        all_files = list(documents_dir.glob("*.pdf"))
        with open(debug_dir / "all_ordinance_files.txt", 'w') as f:
            f.write(f"Total files in {documents_dir}: {len(all_files)}\n")
            for file in sorted(all_files):
                f.write(f"  - {file.name}\n")
        
        log.info(f"üìÑ Found {len(matching_files)} documents for date {meeting_date}")
        
        if matching_files:
            log.info("üìÑ Documents found:")
            for f in matching_files[:5]:
                log.info(f"   - {f.name}")
            if len(matching_files) > 5:
                log.info(f"   ... and {len(matching_files) - 5} more")
        
        # Save matched files list
        with open(debug_dir / "matched_documents.txt", 'w') as f:
            f.write(f"Documents matching date {meeting_date}:\n")
            for file in matching_files:
                f.write(f"  - {file.name}\n")
        
        # Process each document
        linked_documents = {
            "ordinances": [],
            "resolutions": []
        }
        
        for doc_path in matching_files:
            # Extract document info
            doc_info = await self._process_document(doc_path, meeting_date)
            
            if doc_info:
                # Categorize by type
                if "ordinance" in doc_info.get("title", "").lower():
                    linked_documents["ordinances"].append(doc_info)
                else:
                    linked_documents["resolutions"].append(doc_info)
        
        # Save linked documents info
        with open(debug_dir / "linked_documents.json", 'w') as f:
            json.dump(linked_documents, f, indent=2)
        
        log.info(f"‚úÖ Linked {len(linked_documents['ordinances'])} ordinances, {len(linked_documents['resolutions'])} resolutions")
        return linked_documents
    
    async def _process_document(self, doc_path: Path, meeting_date: str) -> Optional[Dict[str, Any]]:
        """Process a single document to extract agenda item reference."""
        try:
            # Extract document number from filename (e.g., "2024-04" from "2024-04 - 01_23_2024.pdf")
            doc_match = re.match(r'^(\d{4}-\d{2})', doc_path.name)
            if not doc_match:
                log.warning(f"Could not parse document number from {doc_path.name}")
                return None
            
            document_number = doc_match.group(1)
            
            # Extract text from PDF
            text = self._extract_pdf_text(doc_path)
            if not text:
                log.warning(f"No text extracted from {doc_path.name}")
                return None
            
            # Extract agenda item code using LLM
            item_code = await self._extract_agenda_item_code(text, document_number)
            
            # Extract additional metadata
            parsed_data = self._parse_document_metadata(text)
            
            doc_info = {
                "path": str(doc_path),
                "filename": doc_path.name,
                "document_number": document_number,
                "item_code": item_code,
                "document_type": "Ordinance" if "ordinance" in text[:500].lower() else "Resolution",
                "title": self._extract_title(text),
                "parsed_data": parsed_data
            }
            
            log.info(f"üìÑ Processed {doc_path.name}: Item {item_code or 'NOT_FOUND'}")
            return doc_info
            
        except Exception as e:
            log.error(f"Error processing {doc_path.name}: {e}")
            return None
    
    def _extract_pdf_text(self, pdf_path: Path) -> str:
        """Extract text from PDF file."""
        try:
            with open(pdf_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                text_parts = []
                
                # Extract text from all pages
                for page in reader.pages:
                    text = page.extract_text()
                    if text:
                        text_parts.append(text)
                
                return "\n".join(text_parts)
        except Exception as e:
            log.error(f"Failed to extract text from {pdf_path.name}: {e}")
            return ""
    
    async def _extract_agenda_item_code(self, text: str, document_number: str) -> Optional[str]:
        """Extract agenda item code from document text using LLM."""
        # Create debug directory if it doesn't exist
        debug_dir = Path("city_clerk_documents/graph_json/debug")
        debug_dir.mkdir(exist_ok=True)
        
        # Send the entire document to qwen-32b
        text_excerpt = text
        
        # Save the full text being sent to LLM for debugging
        with open(debug_dir / f"llm_input_{document_number}.txt", 'w', encoding='utf-8') as f:
            f.write(f"Document: {document_number}\n")
            f.write(f"Text length: {len(text)} characters\n")
            f.write(f"Using model: {self.model}\n")
            f.write(f"Max tokens: {self.agenda_extraction_max_tokens}\n")
            f.write("\n--- FULL DOCUMENT SENT TO LLM ---\n")
            f.write(text_excerpt)
        
        log.info(f"üìÑ Sending full document to LLM for {document_number}: {len(text)} characters")
        
        prompt = f"""You are analyzing a City of Coral Gables ordinance document (Document #{document_number}).

Your task is to find the AGENDA ITEM CODE referenced in this document.

IMPORTANT: The agenda item can appear ANYWHERE in the document - on page 3, at the end, or anywhere else. Search the ENTIRE document carefully.

The agenda item typically appears in formats like:
- (Agenda Item: E-1)
- Agenda Item: E-3)
- (Agenda Item E-1)
- Item H-3
- H.-3. (with periods and dots)
- E.-2. (with dots)
- E-2 (without dots)
- Item E-2

Full document text:
{text_excerpt}

Respond in this EXACT format:
AGENDA_ITEM: [code] or AGENDA_ITEM: NOT_FOUND

Important: Return the code as it appears (e.g., E-2, not E.-2.)

Examples:
- If you find "(Agenda Item: E-2)" ‚Üí respond: AGENDA_ITEM: E-2
- If you find "Item E-2" ‚Üí respond: AGENDA_ITEM: E-2
- If you find "H.-3." ‚Üí respond: AGENDA_ITEM: H-3
- If no agenda item found ‚Üí respond: AGENDA_ITEM: NOT_FOUND"""
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a precise data extractor. Find and extract only the agenda item code. Search the ENTIRE document thoroughly."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_tokens=self.agenda_extraction_max_tokens  # Use 100,000 for qwen
            )
            
            raw_response = response.choices[0].message.content.strip()
            
            # Save raw LLM response for debugging
            with open(debug_dir / f"llm_response_{document_number}_raw.txt", 'w', encoding='utf-8') as f:
                f.write(raw_response)
            
            # Parse response directly (no qwen parsing needed)
            result = raw_response
            
            # Log the response for debugging
            log.info(f"LLM response for {document_number} (first 200 chars): {result[:200]}")
            
            # Parse the response
            if "AGENDA_ITEM:" in result:
                code = result.split("AGENDA_ITEM:")[1].strip()
                if code != "NOT_FOUND":
                    # Normalize the code (remove dots, ensure format)
                    code = self._normalize_item_code(code)
                    log.info(f"‚úÖ Found agenda item code for {document_number}: {code}")
                    return code
                else:
                    log.warning(f"‚ùå LLM could not find agenda item in {document_number}")
            else:
                log.error(f"‚ùå Invalid LLM response format for {document_number}: {result[:100]}")
            
            return None
            
        except Exception as e:
            log.error(f"Failed to extract agenda item for {document_number}: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _normalize_item_code(self, code: str) -> str:
        """Normalize item code to consistent format."""
        if not code:
            return code
        
        # Remove trailing dots and spaces
        code = code.rstrip('. ')
        
        # Remove dots between letter and dash: "E.-1" -> "E-1"
        code = re.sub(r'([A-Z])\.(-)', r'\1\2', code)
        
        # Handle cases without dash: "E.1" -> "E-1"
        code = re.sub(r'([A-Z])\.(\d)', r'\1-\2', code)
        
        # Remove any remaining dots
        code = code.replace('.', '')
        
        return code
    
    def _extract_title(self, text: str) -> str:
        """Extract document title from text."""
        # Look for "AN ORDINANCE" or "A RESOLUTION" pattern
        title_match = re.search(r'(AN?\s+(ORDINANCE|RESOLUTION)[^.]+\.)', text[:2000], re.IGNORECASE)
        if title_match:
            return title_match.group(1).strip()
        
        # Fallback to first substantive line
        lines = text.split('\n')
        for line in lines[:20]:
            if len(line) > 20 and not line.isdigit():
                return line.strip()[:200]
        
        return "Untitled Document"
    
    def _parse_document_metadata(self, text: str) -> Dict[str, Any]:
        """Parse additional metadata from document."""
        metadata = {}
        
        # Extract date passed
        date_match = re.search(r'day\s+of\s+(\w+),?\s+(\d{4})', text)
        if date_match:
            metadata["date_passed"] = date_match.group(0)
        
        # Extract vote information
        vote_match = re.search(r'PASSED\s+AND\s+ADOPTED.*?(\d+).*?(\d+)', text, re.IGNORECASE | re.DOTALL)
        if vote_match:
            metadata["vote_details"] = {
                "ayes": vote_match.group(1),
                "nays": vote_match.group(2) if len(vote_match.groups()) > 1 else "0"
            }
        
        # Extract motion information
        motion_match = re.search(r'motion\s+(?:was\s+)?made\s+by\s+([^,]+)', text, re.IGNORECASE)
        if motion_match:
            metadata["motion"] = {"moved_by": motion_match.group(1).strip()}
        
        # Extract mayor signature
        mayor_match = re.search(r'Mayor[:\s]+([^\n]+)', text[-1000:])
        if mayor_match:
            metadata["signatories"] = {"mayor": mayor_match.group(1).strip()}
        
        return metadata


================================================================================


################################################################################
# File: scripts/graph_stages/cosmos_db_client.py
################################################################################

# File: scripts/graph_stages/cosmos_db_client.py

"""
Azure Cosmos DB Gremlin client for city clerk graph database.
Provides async operations for graph manipulation.
"""

from __future__ import annotations
import asyncio
import logging
from typing import Any, Dict, List, Optional, Union
import os
from gremlin_python.driver import client, serializer
from gremlin_python.driver.driver_remote_connection import DriverRemoteConnection
from gremlin_python.structure.graph import Graph
from dotenv import load_dotenv
import json

load_dotenv()

log = logging.getLogger('cosmos_graph_client')


class CosmosGraphClient:
    """Async client for Azure Cosmos DB Gremlin API."""
    
    def __init__(self, 
                 endpoint: Optional[str] = None,
                 key: Optional[str] = None,
                 database: Optional[str] = None,
                 container: Optional[str] = None,
                 partition_value: str = "demo"):
        """Initialize Cosmos DB client."""
        self.endpoint = endpoint or os.getenv("COSMOS_ENDPOINT")
        self.key = key or os.getenv("COSMOS_KEY")
        self.database = database or os.getenv("COSMOS_DATABASE", "cgGraph")
        self.container = container or os.getenv("COSMOS_CONTAINER", "cityClerk")
        self.partition_value = partition_value
        
        if not all([self.endpoint, self.key, self.database, self.container]):
            raise ValueError("Missing required Cosmos DB configuration")
        
        self._client = None
        self._loop = None  # Don't get the loop in __init__
    
    async def connect(self) -> None:
        """Establish connection to Cosmos DB."""
        try:
            # Get the current running loop
            self._loop = asyncio.get_running_loop()
            
            self._client = client.Client(
                f"{self.endpoint}/gremlin",
                "g",
                username=f"/dbs/{self.database}/colls/{self.container}",
                password=self.key,
                message_serializer=serializer.GraphSONSerializersV2d0()
            )
            log.info(f"‚úÖ Connected to Cosmos DB: {self.database}/{self.container}")
        except Exception as e:
            log.error(f"‚ùå Failed to connect to Cosmos DB: {e}")
            raise
    
    async def _execute_query(self, query: str, bindings: Optional[Dict] = None) -> List[Any]:
        """Execute a Gremlin query asynchronously."""
        if not self._client:
            await self.connect()
        
        try:
            # Get the current event loop
            loop = asyncio.get_running_loop()
            
            # Run synchronous operation in thread pool
            future = loop.run_in_executor(
                None,
                lambda: self._client.submit(query, bindings or {})
            )
            callback = await future
            
            # Collect results
            results = []
            for result in callback:
                results.extend(result)
            
            return results
        except Exception as e:
            log.error(f"Query execution failed: {query[:100]}... Error: {e}")
            raise
    
    async def clear_graph(self) -> None:
        """Clear all vertices and edges from the graph."""
        log.warning("üóëÔ∏è  Clearing entire graph...")
        try:
            # Drop all vertices (edges are automatically removed)
            await self._execute_query("g.V().drop()")
            log.info("‚úÖ Graph cleared successfully")
        except Exception as e:
            log.error(f"Failed to clear graph: {e}")
            raise
    
    async def create_vertex(self, 
                          label: str,
                          vertex_id: str,
                          properties: Dict[str, Any],
                          update_if_exists: bool = True) -> None:
        """Create a vertex with properties, optionally updating if exists."""
        
        # Check if vertex already exists
        if await self.vertex_exists(vertex_id):
            if update_if_exists:
                # Update existing vertex
                await self.update_vertex(vertex_id, properties)
                log.info(f"Updated existing vertex: {vertex_id}")
            else:
                log.info(f"Vertex already exists, skipping: {vertex_id}")
            return
        
        # Build property chain
        prop_chain = ""
        for key, value in properties.items():
            if value is not None:
                # Handle different value types
                if isinstance(value, bool):
                    prop_chain += f".property('{key}', {str(value).lower()})"
                elif isinstance(value, (int, float)):
                    prop_chain += f".property('{key}', {value})"
                elif isinstance(value, list):
                    # Convert list to JSON string
                    json_val = json.dumps(value).replace("'", "\\'")
                    prop_chain += f".property('{key}', '{json_val}')"
                else:
                    # Escape string values
                    escaped_val = str(value).replace("'", "\\'").replace('"', '\\"')
                    prop_chain += f".property('{key}', '{escaped_val}')"
        
        # Always add partition key
        prop_chain += f".property('partitionKey', '{self.partition_value}')"
        
        query = f"g.addV('{label}').property('id', '{vertex_id}'){prop_chain}"
        
        await self._execute_query(query)

    async def update_vertex(self, vertex_id: str, properties: Dict[str, Any]) -> None:
        """Update properties of an existing vertex."""
        # Build property update chain
        prop_chain = ""
        for key, value in properties.items():
            if value is not None:
                if isinstance(value, bool):
                    prop_chain += f".property('{key}', {str(value).lower()})"
                elif isinstance(value, (int, float)):
                    prop_chain += f".property('{key}', {value})"
                elif isinstance(value, list):
                    json_val = json.dumps(value).replace("'", "\\'")
                    prop_chain += f".property('{key}', '{json_val}')"
                else:
                    escaped_val = str(value).replace("'", "\\'").replace('"', '\\"')
                    prop_chain += f".property('{key}', '{escaped_val}')"
        
        query = f"g.V('{vertex_id}'){prop_chain}"
        
        try:
            await self._execute_query(query)
            log.info(f"Updated vertex {vertex_id}")
        except Exception as e:
            log.error(f"Failed to update vertex {vertex_id}: {e}")
            raise

    async def upsert_vertex(self, 
                           label: str,
                           vertex_id: str,
                           properties: Dict[str, Any]) -> bool:
        """Create or update a vertex. Returns True if created, False if updated."""
        # Check if vertex exists
        if await self.vertex_exists(vertex_id):
            await self.update_vertex(vertex_id, properties)
            return False  # Updated
        else:
            await self.create_vertex(label, vertex_id, properties)
            return True  # Created

    async def create_edge(self,
                         from_id: str,
                         to_id: str,
                         edge_type: str,
                         properties: Optional[Dict[str, Any]] = None) -> None:
        """Create an edge between two vertices."""
        # Build property chain for edge
        prop_chain = ""
        if properties:
            for key, value in properties.items():
                if value is not None:
                    if isinstance(value, bool):
                        prop_chain += f".property('{key}', {str(value).lower()})"
                    elif isinstance(value, (int, float)):
                        prop_chain += f".property('{key}', {value})"
                    else:
                        escaped_val = str(value).replace("'", "\\'")
                        prop_chain += f".property('{key}', '{escaped_val}')"
        
        query = f"g.V('{from_id}').addE('{edge_type}').to(g.V('{to_id}')){prop_chain}"
        
        try:
            await self._execute_query(query)
        except Exception as e:
            log.error(f"Failed to create edge {from_id} -> {to_id}: {e}")
            raise
    
    async def create_edge_if_not_exists(self,
                                       from_id: str,
                                       to_id: str,
                                       edge_type: str,
                                       properties: Optional[Dict[str, Any]] = None) -> bool:
        """Create an edge if it doesn't already exist. Returns True if created."""
        # Check if edge already exists
        check_query = f"g.V('{from_id}').outE('{edge_type}').where(inV().hasId('{to_id}')).count()"
        
        try:
            result = await self._execute_query(check_query)
            exists = result[0] > 0 if result else False
            
            if not exists:
                await self.create_edge(from_id, to_id, edge_type, properties)
                return True
            else:
                log.debug(f"Edge already exists: {from_id} -[{edge_type}]-> {to_id}")
                return False
        except Exception as e:
            log.error(f"Failed to check/create edge: {e}")
            raise
    
    async def vertex_exists(self, vertex_id: str) -> bool:
        """Check if a vertex exists."""
        result = await self._execute_query(f"g.V('{vertex_id}').count()")
        return result[0] > 0 if result else False
    
    async def get_vertex(self, vertex_id: str) -> Optional[Dict]:
        """Get a vertex by ID."""
        result = await self._execute_query(f"g.V('{vertex_id}').valueMap(true)")
        return result[0] if result else None
    
    async def close(self) -> None:
        """Close the connection properly."""
        if self._client:
            try:
                # Close the client synchronously since it's not async
                self._client.close()
                self._client = None
                log.info("Connection closed")
            except Exception as e:
                log.warning(f"Error during client close: {e}")
    
    async def __aenter__(self):
        await self.connect()
        return self
    
    async def __aexit__(self, *args):
        await self.close()


================================================================================


################################################################################
# File: scripts/debug_visualizer.py
################################################################################

# File: scripts/debug_visualizer.py

#!/usr/bin/env python3
"""
Debug script to check visualizer data loading
"""

import sys
import asyncio
import json
sys.path.append('scripts')

from graph_stages.cosmos_db_client import CosmosGraphClient

async def debug_visualizer_data():
    """Debug what data the visualizer should be loading."""
    
    client = CosmosGraphClient()
    
    try:
        print("üîç Debugging visualizer data loading...")
        
        # Check what node labels exist
        print("\n1. Checking node labels in database:")
        labels_result = await client._execute_query("g.V().label().dedup()")
        print(f"   Node labels found: {labels_result}")
        
        # Check specific counts by label
        for label in labels_result:
            count_result = await client._execute_query(f"g.V().hasLabel('{label}').count()")
            count = count_result[0] if count_result else 0
            print(f"   {label}: {count} nodes")
        
        # Test the exact query the visualizer uses
        print("\n2. Testing visualizer query:")
        visualizer_query = "g.V().valueMap(true)"
        vertices = await client._execute_query(visualizer_query)
        print(f"   Query returned {len(vertices)} vertices")
        
        # Show sample vertex data
        if vertices:
            print("\n3. Sample vertex data (first 3):")
            for i, vertex in enumerate(vertices[:3]):
                print(f"   Vertex {i+1}:")
                print(f"     ID: {vertex.get('id')}")
                print(f"     Label: {vertex.get('label')}")
                print(f"     Properties: {list(vertex.keys())}")
                
                # Show specific properties for agenda items
                if vertex.get('label') == 'AgendaItem':
                    code = vertex.get('code', ['Unknown'])[0] if isinstance(vertex.get('code'), list) else vertex.get('code', 'Unknown')
                    title = vertex.get('title', ['Unknown'])[0] if isinstance(vertex.get('title'), list) else vertex.get('title', 'Unknown')
                    print(f"     Code: {code}")
                    print(f"     Title: {title[:50]}...")
        
        # Check edges
        print("\n4. Testing edge query:")
        edge_query = """
            g.E().project('source','target','label')
            .by(outV().id())
            .by(inV().id())
            .by(label())
        """
        edges = await client._execute_query(edge_query)
        print(f"   Query returned {len(edges)} edges")
        
        if edges:
            print("   Sample edges (first 3):")
            for i, edge in enumerate(edges[:3]):
                print(f"     Edge {i+1}: {edge['source']} -[{edge['label']}]-> {edge['target']}")
        
        # Check if there are Meeting nodes specifically
        print("\n5. Checking for Meeting nodes:")
        meeting_result = await client._execute_query("g.V().hasLabel('Meeting').valueMap(true)")
        print(f"   Found {len(meeting_result)} Meeting nodes")
        if meeting_result:
            for i, meeting in enumerate(meeting_result):
                date = meeting.get('date', ['Unknown'])[0] if isinstance(meeting.get('date'), list) else meeting.get('date', 'Unknown')
                print(f"     Meeting {i+1}: {date}")
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
    finally:
        await client.close()

if __name__ == "__main__":
    asyncio.run(debug_visualizer_data())


================================================================================


################################################################################
# File: scripts/check_graph_empty.py
################################################################################

# File: scripts/check_graph_empty.py

#!/usr/bin/env python3
"""
Check if the graph database is empty
"""

import sys
import asyncio
sys.path.append('scripts')

from graph_stages.cosmos_db_client import CosmosGraphClient

async def check_graph_empty():
    """Check if the graph database is empty."""
    
    # Connect to graph
    client = CosmosGraphClient()
    
    try:
        print("üîç Checking graph database status...")
        
        # Count all vertices
        vertex_result = await client._execute_query("g.V().count()")
        vertex_count = vertex_result[0] if vertex_result else 0
        
        # Count all edges
        edge_result = await client._execute_query("g.E().count()")
        edge_count = edge_result[0] if edge_result else 0
        
        print(f"üìä Graph Database Status:")
        print(f"   üîµ Vertices: {vertex_count}")
        print(f"   üîó Edges: {edge_count}")
        
        if vertex_count == 0 and edge_count == 0:
            print("‚úÖ Graph database is completely empty!")
        else:
            print("‚ö†Ô∏è  Graph database still contains data:")
            
            if vertex_count > 0:
                # Show some sample vertices
                samples = await client._execute_query("g.V().limit(5).valueMap()")
                print(f"   Sample vertices: {len(samples)} found")
                for i, sample in enumerate(samples[:3]):
                    print(f"     {i+1}. {sample}")
                
            if edge_count > 0:
                # Show some sample edges
                edge_samples = await client._execute_query("g.E().limit(5).valueMap()")
                print(f"   Sample edges: {len(edge_samples)} found")
        
    except Exception as e:
        print(f"‚ùå Error checking graph: {e}")
    finally:
        await client.close()

if __name__ == "__main__":
    asyncio.run(check_graph_empty())


================================================================================


################################################################################
# File: config.py
################################################################################

# File: config.py

#!/usr/bin/env python3
"""
Configuration file for graph database visualization
Manages database credentials and connection settings
"""

import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Azure Cosmos DB Gremlin Configuration
COSMOS_ENDPOINT = os.getenv("COSMOS_ENDPOINT", "wss://aida-graph-db.gremlin.cosmos.azure.com:443")
COSMOS_KEY = os.getenv("COSMOS_KEY", "")  # This will be set from .env file
DATABASE = os.getenv("COSMOS_DATABASE", "cgGraph")
CONTAINER = os.getenv("COSMOS_CONTAINER", "cityClerk")
PARTITION_KEY = os.getenv("COSMOS_PARTITION_KEY", "partitionKey")
PARTITION_VALUE = os.getenv("COSMOS_PARTITION_VALUE", "demo")

def validate_config():
    """Validate that all required configuration is available"""
    required_vars = {
        "COSMOS_KEY": COSMOS_KEY,
        "COSMOS_ENDPOINT": COSMOS_ENDPOINT,
        "DATABASE": DATABASE,
        "CONTAINER": CONTAINER
    }
    
    missing_vars = [var for var, value in required_vars.items() if not value]
    
    if missing_vars:
        print("‚ùå Missing required configuration:")
        for var in missing_vars:
            print(f"   - {var}")
        print("\nüîß Please create a .env file with your credentials:")
        print("   COSMOS_KEY=your_actual_cosmos_key_here")
        print("   COSMOS_ENDPOINT=wss://aida-graph-db.gremlin.cosmos.azure.com:443")
        print("   COSMOS_DATABASE=cgGraph")
        print("   COSMOS_CONTAINER=cityClerk")
        return False
    
    return True

if __name__ == "__main__":
    print("üîß Configuration Check:")
    if validate_config():
        print("‚úÖ All configuration variables are set!")
    else:
        print("‚ùå Configuration incomplete!")


================================================================================


################################################################################
# File: scripts/graph_stages/__init__.py
################################################################################

# File: scripts/graph_stages/__init__.py

"""
Graph Pipeline Stages
=====================
Components for building city clerk document knowledge graph.
"""

from .cosmos_db_client import CosmosGraphClient
from .agenda_pdf_extractor import AgendaPDFExtractor
from .agenda_ontology_extractor import CityClerkOntologyExtractor
from .enhanced_document_linker import EnhancedDocumentLinker
from .agenda_graph_builder import AgendaGraphBuilder
from .verbatim_transcript_linker import VerbatimTranscriptLinker

__all__ = [
    'CosmosGraphClient',
    'AgendaPDFExtractor',
    'CityClerkOntologyExtractor',
    'EnhancedDocumentLinker',
    'AgendaGraphBuilder',
    'VerbatimTranscriptLinker'
]


================================================================================

