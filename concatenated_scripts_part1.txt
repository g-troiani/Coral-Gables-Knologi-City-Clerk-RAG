# Concatenated Project Code - Part 1 of 3
# Generated: 2025-05-27 22:24:03
# Root Directory: /Users/gianmariatroiani/Documents/knologiÃä/city_clerk_rag
================================================================================

# Directory Structure
################################################################################
‚îú‚îÄ‚îÄ .gitignore (829.0B, no ext)
‚îú‚îÄ‚îÄ documents/
‚îÇ   ‚îú‚îÄ‚îÄ research/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ json/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ txt/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ [EXCLUDED] 2 items: .DS_Store (excluded file), Global (excluded dir)
‚îÇ   ‚îú‚îÄ‚îÄ [EXCLUDED] 1 items: .DS_Store (excluded file)
‚îú‚îÄ‚îÄ requirements.txt (755.0B, .txt)
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ clear_database.py (6.0KB, .py)
‚îÇ   ‚îú‚îÄ‚îÄ find_duplicates.py (5.2KB, .py)
‚îÇ   ‚îú‚îÄ‚îÄ pipeline_modular_optimized.py (12.7KB, .py)
‚îÇ   ‚îú‚îÄ‚îÄ rag_local_web_app.py (13.1KB, .py)
‚îÇ   ‚îú‚îÄ‚îÄ stages/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py (378.0B, .py)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ acceleration_utils.py (3.8KB, .py)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chunk_text.py (18.6KB, .py)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ db_upsert.py (7.2KB, .py)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embed_vectors.py (27.0KB, .py)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ extract_clean.py (21.2KB, .py)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ llm_enrich.py (5.0KB, .py)
‚îÇ   ‚îú‚îÄ‚îÄ test_vector_search.py (5.5KB, .py)
‚îÇ   ‚îî‚îÄ‚îÄ topic_filter_and_title.py (5.7KB, .py)
‚îú‚îÄ‚îÄ [EXCLUDED] 5 items: .DS_Store (excluded file), .env (excluded file), .git (excluded dir)
    ... and 2 more excluded items


================================================================================


# File Index - Which Files Are in Which Parts
################################################################################

## Part 1 (4 files):
  - scripts/stages/embed_vectors.py
  - scripts/stages/db_upsert.py
  - scripts/test_vector_search.py
  - scripts/stages/acceleration_utils.py

## Part 2 (5 files):
  - scripts/stages/extract_clean.py
  - scripts/rag_local_web_app.py
  - scripts/find_duplicates.py
  - scripts/topic_filter_and_title.py
  - scripts/stages/__init__.py

## Part 3 (5 files):
  - scripts/stages/chunk_text.py
  - scripts/pipeline_modular_optimized.py
  - scripts/clear_database.py
  - scripts/stages/llm_enrich.py
  - requirements.txt


================================================================================


################################################################################
# File: scripts/stages/embed_vectors.py
################################################################################

# File: scripts/stages/embed_vectors.py

#!/usr/bin/env python3
"""
Stage 7 ‚Äî Optimized embedding with rate limiting, deduplication, and conservative batching.
"""
from __future__ import annotations
import argparse, json, logging, os, sys, time
from datetime import datetime
from typing import Any, Dict, List, Optional, Set
import asyncio
import aiohttp
from dotenv import load_dotenv
from openai import OpenAI
from supabase import create_client
from tqdm import tqdm
from tqdm.asyncio import tqdm as async_tqdm
import hashlib

# Try to import tiktoken for accurate token counting
try:
    import tiktoken
    TIKTOKEN_AVAILABLE = True
except ImportError:
    TIKTOKEN_AVAILABLE = False
    logging.warning("tiktoken not available - using conservative token estimation")

load_dotenv()
SUPABASE_URL  = os.getenv("SUPABASE_URL")
SUPABASE_KEY  = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
OPENAI_API_KEY= os.getenv("OPENAI_API_KEY")
EMBEDDING_MODEL="text-embedding-ada-002"
MODEL_TOKEN_LIMIT=8192
TOKEN_GUARD=200

# üéØ DYNAMIC BATCHING - Token limits with safety margins
MAX_BATCH_TOKENS = 7692  # Conservative limit (8192 - 500 safety margin)
MAX_CHUNK_TOKENS = 6000  # Individual chunk limit
MIN_BATCH_TOKENS = 100   # Minimum viable batch size

MAX_TOTAL_TOKENS=MAX_BATCH_TOKENS  # Use dynamic batch limit instead

# Conservative defaults for rate limiting
DEFAULT_BATCH_ROWS=200  # Reduced from 5000
DEFAULT_COMMIT_ROWS=10  # Reduced from 100
DEFAULT_MAX_CONCURRENT=3  # Reduced from 50
MAX_RETRIES=5
RETRY_DELAY=2
RATE_LIMIT_DELAY=0.3  # 300ms between API calls
MAX_CALLS_PER_MINUTE=150  # Conservative limit

openai_client=OpenAI(api_key=OPENAI_API_KEY)
logging.basicConfig(level=logging.INFO,
    format="%(asctime)s ‚Äî %(levelname)s ‚Äî %(message)s")
log=logging.getLogger(__name__)

# Global variables for rate limiting
call_timestamps = []
total_tokens_used = 0

class AsyncEmbedder:
    """Async embedding client with strict rate limiting and connection pooling."""
    
    def __init__(self, api_key: str, max_concurrent: int = DEFAULT_MAX_CONCURRENT):
        self.api_key = api_key
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.session = None
        self.call_count = 0
        self.start_time = time.time()
        
        # Initialize tiktoken encoder if available
        if TIKTOKEN_AVAILABLE:
            self.encoder = tiktoken.encoding_for_model(EMBEDDING_MODEL)
        else:
            self.encoder = None
    
    async def __aenter__(self):
        timeout = aiohttp.ClientTimeout(total=300)
        connector = aiohttp.TCPConnector(limit=20, limit_per_host=10)  # Reduced limits
        self.session = aiohttp.ClientSession(
            headers={"Authorization": f"Bearer {self.api_key}"},
            timeout=timeout,
            connector=connector
        )
        return self
    
    async def __aexit__(self, *args):
        await self.session.close()
    
    def count_tokens(self, text: str) -> int:
        """Count tokens accurately using tiktoken if available."""
        if self.encoder:
            return len(self.encoder.encode(text))
        else:
            # Conservative fallback
            return int(len(text.split()) * 0.75) + 50
    
    async def rate_limit_check(self):
        """Enforce rate limiting."""
        current_time = time.time()
        
        # Remove timestamps older than 1 minute
        call_timestamps[:] = [ts for ts in call_timestamps if current_time - ts < 60]
        
        # Check if we're approaching the rate limit
        if len(call_timestamps) >= MAX_CALLS_PER_MINUTE - 5:
            sleep_time = 60 - (current_time - call_timestamps[0])
            if sleep_time > 0:
                log.info(f"Rate limit protection: sleeping {sleep_time:.1f}s")
                await asyncio.sleep(sleep_time)
        
        # Always enforce minimum delay between calls
        await asyncio.sleep(RATE_LIMIT_DELAY)
        
        # Record this call
        call_timestamps.append(current_time)
    
    async def embed_batch_async(self, texts: List[str]) -> List[List[float]]:
        """Embed a batch of texts asynchronously with rate limiting and token validation."""
        global total_tokens_used
        
        async with self.semaphore:
            await self.rate_limit_check()
            
            # üõ°Ô∏è FINAL TOKEN VALIDATION - Last safety check before API call
            batch_tokens = sum(self.count_tokens(text) for text in texts)
            
            if batch_tokens > MAX_BATCH_TOKENS:
                error_msg = f"üö® CRITICAL: Batch tokens {batch_tokens} exceed limit {MAX_BATCH_TOKENS}"
                log.error(error_msg)
                raise RuntimeError(error_msg)
            
            total_tokens_used += batch_tokens
            
            log.info(f"üéØ API call: {len(texts)} texts (~{batch_tokens} tokens) - WITHIN LIMITS ‚úÖ")
            log.info(f"üìä Total tokens used so far: ~{total_tokens_used}")
            
            for attempt in range(MAX_RETRIES):
                try:
                    async with self.session.post(
                        "https://api.openai.com/v1/embeddings",
                        json={
                            "model": EMBEDDING_MODEL,
                            "input": texts
                        }
                    ) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            self.call_count += 1
                            log.info(f"‚úÖ API call successful: {len(texts)} embeddings generated")
                            return [item["embedding"] for item in data["data"]]
                        elif resp.status == 400:  # Bad request - likely token limit
                            error = await resp.text()
                            log.error(f"üö® API error 400 (likely token limit): {error}")
                            log.error(f"üö® Batch details: {len(texts)} texts, {batch_tokens} tokens")
                            raise RuntimeError(f"Token limit API error: {error}")
                        elif resp.status == 429:  # Rate limit error
                            error = await resp.text()
                            log.warning(f"Rate limit hit: {error}")
                            # Exponential backoff for rate limits
                            sleep_time = (2 ** attempt) * 2
                            log.info(f"Exponential backoff: sleeping {sleep_time}s")
                            await asyncio.sleep(sleep_time)
                        else:
                            error = await resp.text()
                            log.warning(f"API error {resp.status}: {error}")
                            await asyncio.sleep(RETRY_DELAY)
                except Exception as e:
                    log.warning(f"Attempt {attempt + 1} failed: {e}")
                    await asyncio.sleep(RETRY_DELAY * (attempt + 1))
            
            raise RuntimeError("Failed to embed batch after retries")

def deduplicate_chunks(chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Remove duplicate text chunks based on content hash."""
    seen_hashes: Set[str] = set()
    unique_chunks = []
    duplicates_removed = 0
    
    for chunk in chunks:
        text = chunk.get("text", "").strip()
        if not text:
            continue
            
        # Create hash of the text content
        text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()
        
        if text_hash not in seen_hashes:
            seen_hashes.add(text_hash)
            unique_chunks.append(chunk)
        else:
            duplicates_removed += 1
    
    if duplicates_removed > 0:
        log.info(f"Deduplication: removed {duplicates_removed} duplicate chunks")
    
    return unique_chunks

async def process_chunks_async(
    sb,
    chunks: List[Dict[str, Any]],
    embedder: AsyncEmbedder,
    commit_size: int = DEFAULT_COMMIT_ROWS
) -> int:
    """Process chunks with async embedding and batch updates."""
    # Deduplicate chunks first
    unique_chunks = deduplicate_chunks(chunks)
    log.info(f"üìä CHUNK PROCESSING PROGRESS:")
    log.info(f"   üìÑ Original chunks: {len(chunks)}")
    log.info(f"   üìÑ Unique chunks: {len(unique_chunks)}")
    if len(chunks) != len(unique_chunks):
        log.info(f"   üîÑ Duplicates removed: {len(chunks) - len(unique_chunks)}")
    
    # Create token-safe slices
    slices = safe_slices(unique_chunks, commit_size)
    total_embedded = 0
    total_slices = len(slices)
    
    if total_slices == 0:
        log.warning(f"üìä NO SLICES TO PROCESS - All chunks were filtered out")
        return 0
    
    log.info(f"üìä EMBEDDING SLICES PROGRESS:")
    log.info(f"   üéØ Total slices to process: {total_slices}")
    log.info(f"   üìÑ Total chunks to embed: {sum(len(slice_data) for slice_data in slices)}")
    
    # Process each slice
    for i, slice_data in enumerate(slices):
        slice_num = i + 1
        slice_progress = (slice_num / total_slices * 100)
        
        log.info(f"üìä SLICE {slice_num}/{total_slices} ({slice_progress:.1f}%):")
        log.info(f"   üìÑ Processing {len(slice_data)} chunks in this slice")
        
        texts = [row["text"] for row in slice_data]
        
        try:
            # Get embeddings asynchronously
            log.info(f"   üîÑ Calling OpenAI API for {len(texts)} embeddings...")
            embeddings = await embedder.embed_batch_async(texts)
            log.info(f"   ‚úÖ Received {len(embeddings)} embeddings from API")
            
            # Update database in batch
            log.info(f"   üíæ Updating database for {len(slice_data)} chunks...")
            update_tasks = []
            for row, emb in zip(slice_data, embeddings):
                # ‚úÖ GUARANTEED SKIP LOGIC - Layer 3: Final update verification
                def update_with_verification(r, e):
                    # Final check before updating
                    final_check = sb.table("research_chunks").select("embedding")\
                        .eq("id", r["id"]).execute()
                    
                    if final_check.data and final_check.data[0].get("embedding"):
                        log.info(f"üõ°Ô∏è Chunk {r['id']} already has embedding - SKIPPING update")
                        return {"skipped": True}
                    
                    # Safe to update
                    return sb.table("research_chunks")\
                        .update({"embedding": e})\
                        .eq("id", r["id"])\
                        .execute()
                
                # Use thread pool for database updates
                loop = asyncio.get_event_loop()
                task = loop.run_in_executor(None, update_with_verification, row, emb)
                update_tasks.append(task)
            
            # Wait for all updates
            log.info(f"   ‚è≥ Waiting for {len(update_tasks)} database updates...")
            results = await asyncio.gather(*update_tasks, return_exceptions=True)
            successful = sum(1 for r in results if not isinstance(r, Exception) and not (isinstance(r, dict) and r.get("skipped")))
            skipped = sum(1 for r in results if isinstance(r, dict) and r.get("skipped"))
            failed = sum(1 for r in results if isinstance(r, Exception))
            total_embedded += successful
            
            # üìä SLICE COMPLETION PROGRESS
            log.info(f"üìä SLICE {slice_num} COMPLETE:")
            log.info(f"   ‚úÖ Successful updates: {successful}")
            if skipped > 0:
                log.info(f"   ‚è≠Ô∏è  Skipped (already embedded): {skipped}")
            if failed > 0:
                log.info(f"   ‚ùå Failed updates: {failed}")
            log.info(f"   üìà Total embedded so far: {total_embedded}")
            
        except Exception as e:
            log.error(f"‚ùå SLICE {slice_num} FAILED: {e}")
            log.error(f"   üìÑ Chunks in failed slice: {len(slice_data)}")
    
    # Final summary
    log.info(f"üìä CHUNK PROCESSING COMPLETE:")
    log.info(f"   ‚úÖ Total chunks embedded: {total_embedded}")
    log.info(f"   üìä Slices processed: {total_slices}")
    log.info(f"   üìà Success rate: {(total_embedded/len(unique_chunks)*100):.1f}%" if unique_chunks else "0%")
    
    return total_embedded

async def main_async(batch_size: int = None, commit_size: int = None, max_concurrent: int = None):
    """Async main function for embedding."""
    global EMBEDDING_MODEL, MODEL_TOKEN_LIMIT, MAX_TOTAL_TOKENS, total_tokens_used
    
    # Use provided parameters or conservative defaults
    batch_size = batch_size or DEFAULT_BATCH_ROWS
    commit_size = commit_size or DEFAULT_COMMIT_ROWS
    max_concurrent = max_concurrent or DEFAULT_MAX_CONCURRENT
    
    log.info(f"Starting with conservative settings:")
    log.info(f"  Batch size: {batch_size}")
    log.info(f"  Commit size: {commit_size}")
    log.info(f"  Max concurrent: {max_concurrent}")
    log.info(f"  Rate limit delay: {RATE_LIMIT_DELAY}s")
    
    # üéØ Dynamic batching status
    log.info(f"üéØ DYNAMIC BATCHING ENABLED:")
    log.info(f"   Max batch tokens: {MAX_BATCH_TOKENS} (with safety margin)")
    log.info(f"   Max chunk tokens: {MAX_CHUNK_TOKENS}")
    log.info(f"   Min batch tokens: {MIN_BATCH_TOKENS}")
    log.info(f"   ‚úÖ GUARANTEED: No token limit API errors")
    
    # ‚úÖ Check tiktoken availability for accurate token counting
    if not TIKTOKEN_AVAILABLE:
        log.warning("‚ö†Ô∏è  tiktoken not available - using conservative token estimation")
        log.warning("‚ö†Ô∏è  For accurate token counting, install: pip install tiktoken")
    else:
        log.info("‚úÖ tiktoken available - using accurate token counting")
    
    if not OPENAI_API_KEY:
        log.error("OPENAI_API_KEY missing")
        sys.exit(1)
    
    sb = init_supabase()
    existing_count = count_processed_chunks(sb)
    log.info(f"Rows already embedded: {existing_count}")
    
    # ‚úÖ GUARANTEED SKIP LOGIC - Status check
    total_chunks_res = sb.table("research_chunks").select("id", count="exact")\
        .eq("chunking_strategy", "token_window").execute()
    total_chunks = total_chunks_res.count or 0
    
    pending_chunks = total_chunks - existing_count
    
    log.info("üìä Current status:")
    log.info(f"   Chunks already embedded: {existing_count}")
    log.info(f"   Chunks needing embedding: {pending_chunks}")
    log.info(f"   Total chunks: {total_chunks}")
    
    if pending_chunks == 0:
        log.info("‚úÖ All chunks already have embeddings - nothing to do!")
        return
    
    async with AsyncEmbedder(OPENAI_API_KEY, max_concurrent) as embedder:
        loop, total = 0, 0
        
        # üìä PROGRESS TRACKING - Initialize counters
        total_chunks_to_process = pending_chunks
        chunks_processed = 0
        chunks_skipped = 0
        
        log.info(f"üìä EMBEDDING PROGRESS TRACKING INITIALIZED:")
        log.info(f"   üéØ Total chunks to embed: {total_chunks_to_process}")
        log.info(f"   üéØ Starting embedding process...")
        
        while True:
            loop += 1
            rows = fetch_unprocessed_chunks(sb, limit=batch_size, offset=0)
            
            if not rows:
                log.info("‚ú® Done ‚Äî no more rows.")
                break
            
            # üìä PROGRESS: Show current status before processing
            remaining_chunks = total_chunks_to_process - chunks_processed
            progress_percent = (chunks_processed / total_chunks_to_process * 100) if total_chunks_to_process > 0 else 0
            
            log.info(f"üìä EMBEDDING PROGRESS - Loop {loop}:")
            log.info(f"   üìÑ Fetched: {len(rows)} chunks")
            log.info(f"   ‚úÖ Processed: {chunks_processed}/{total_chunks_to_process} ({progress_percent:.1f}%)")
            log.info(f"   ‚è≥ Remaining: {remaining_chunks} chunks")
            if chunks_skipped > 0:
                log.info(f"   ‚ö†Ô∏è  Skipped: {chunks_skipped} oversized chunks")
            
            # Process chunks asynchronously
            embedded = await process_chunks_async(sb, rows, embedder, commit_size)
            total += embedded
            chunks_processed += embedded
            
            # Update skipped count (this will be calculated in process_chunks_async)
            current_chunk_count = count_processed_chunks(sb)
            actual_embedded_this_loop = current_chunk_count - (existing_count + chunks_processed - embedded)
            
            # üìä PROGRESS: Show results after processing
            final_progress_percent = (chunks_processed / total_chunks_to_process * 100) if total_chunks_to_process > 0 else 0
            log.info(f"üìä LOOP {loop} COMPLETE:")
            log.info(f"   ‚úÖ This loop: {embedded} chunks embedded")
            log.info(f"   üìà Total progress: {chunks_processed}/{total_chunks_to_process} ({final_progress_percent:.1f}%)")
            log.info(f"   üîÑ API calls made: {embedder.call_count}")
            log.info(f"   üìä Total tokens used: ~{total_tokens_used}")
            
            # Check if we're making progress or stuck
            if embedded == 0 and len(rows) > 0:
                chunks_skipped += len(rows)
                log.warning(f"‚ö†Ô∏è  WARNING: No chunks embedded in this loop - all {len(rows)} chunks were skipped")
                log.warning(f"‚ö†Ô∏è  Total skipped so far: {chunks_skipped} chunks")
                
                # Prevent infinite loop by limiting consecutive failed attempts
                if loop > 5 and embedded == 0:
                    log.error(f"üö® STUCK: {loop} consecutive loops with no progress - stopping to prevent infinite loop")
                    break
    
    # Save report
    ts = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    report = {
        "timestamp": ts,
        "batch_size": batch_size,
        "commit": commit_size,
        "max_concurrent": max_concurrent,
        "total_embedded": total,
        "total_with_embeddings": count_processed_chunks(sb),
        "total_tokens_used": total_tokens_used,
        "api_calls_made": embedder.call_count if 'embedder' in locals() else 0
    }
    
    # Create reports directory if it doesn't exist
    reports_dir = "reports/embedding"
    os.makedirs(reports_dir, exist_ok=True)
    
    fname = f"{reports_dir}/supabase_embedding_report_{ts}.json"
    with open(fname, "w") as fp:
        json.dump(report, fp, indent=2)
    
    log.info(f"Report saved to {fname}")

async def main_async_cli():
    """CLI version with argument parsing."""
    global EMBEDDING_MODEL, MODEL_TOKEN_LIMIT, MAX_TOTAL_TOKENS
    
    ap = argparse.ArgumentParser()
    ap.add_argument("--batch-size", type=int, default=DEFAULT_BATCH_ROWS)
    ap.add_argument("--commit", type=int, default=DEFAULT_COMMIT_ROWS)
    ap.add_argument("--skip", type=int, default=0)
    ap.add_argument("--model", default=EMBEDDING_MODEL)
    ap.add_argument("--model-limit", type=int, default=MODEL_TOKEN_LIMIT)
    ap.add_argument("--max-concurrent", type=int, default=DEFAULT_MAX_CONCURRENT,
                   help="Maximum concurrent API calls (default: 3 for rate limiting)")
    args = ap.parse_args()
    
    EMBEDDING_MODEL = args.model
    MODEL_TOKEN_LIMIT = args.model_limit
    MAX_TOTAL_TOKENS = MODEL_TOKEN_LIMIT - TOKEN_GUARD
    
    await main_async(args.batch_size, args.commit, args.max_concurrent)

# Keep original interface for compatibility
def main() -> None:
    """Original synchronous interface."""
    asyncio.run(main_async_cli())

# Keep all existing helper functions unchanged...
def init_supabase():
    if not SUPABASE_URL or not SUPABASE_KEY:
        log.error("SUPABASE creds missing"); sys.exit(1)
    return create_client(SUPABASE_URL,SUPABASE_KEY)

def count_processed_chunks(sb)->int:
    res=sb.table("research_chunks").select("id",count="exact").not_.is_("embedding","null").execute()
    return res.count or 0

def fetch_unprocessed_chunks(sb,*,limit:int,offset:int=0)->List[Dict[str,Any]]:
    first,last=offset,offset+limit-1
    res=sb.table("research_chunks").select("id,text,token_start,token_end")\
        .eq("chunking_strategy","token_window").is_("embedding","null")\
        .range(first,last).execute()
    
    chunks = res.data or []
    
    if chunks:
        # ‚úÖ GUARANTEED SKIP LOGIC - Layer 2: Double-check verification
        chunk_ids = [chunk["id"] for chunk in chunks]
        verification = sb.table("research_chunks").select("id")\
            .in_("id", chunk_ids).not_.is_("embedding", "null").execute()
        
        already_embedded_ids = {row["id"] for row in verification.data or []}
        
        if already_embedded_ids:
            log.warning(f"üõ°Ô∏è Found {len(already_embedded_ids)} chunks that already have embeddings - SKIPPING them")
            chunks = [chunk for chunk in chunks if chunk["id"] not in already_embedded_ids]
        
        log.info(f"‚úÖ GUARANTEED: Fetched {len(chunks)} chunks WITHOUT embeddings")
        
        if not chunks:
            log.info("‚úÖ All chunks already have embeddings - nothing to do!")
    
    return chunks

def generate_embedding_batch(texts:List[str])->List[List[float]]:
    attempt=0
    while attempt<MAX_RETRIES:
        try:
            resp=openai_client.embeddings.create(model=EMBEDDING_MODEL,input=texts)
            return [d.embedding for d in resp.data]
        except Exception as exc:
            attempt+=1; log.warning("Embedding batch %s/%s failed: %s",attempt,MAX_RETRIES,exc)
            time.sleep(RETRY_DELAY)
    raise RuntimeError("OpenAI embedding batch failed after retries")

def chunk_tokens(row:Dict[str,Any])->int:
    try:
        t=int(row["token_end"])-int(row["token_start"])+1
        if 0<t<=16384: return t
    except: pass
    
    # Use tiktoken if available for more accurate counting
    text = row.get("text", "")
    if TIKTOKEN_AVAILABLE:
        try:
            encoder = tiktoken.encoding_for_model(EMBEDDING_MODEL)
            return len(encoder.encode(text))
        except:
            pass
    
    # Conservative fallback
    approx=int(len(text.split())*0.75)+50  # More conservative estimate
    return min(max(1,approx),MODEL_TOKEN_LIMIT)

def safe_slices(rows:List[Dict[str,Any]],max_rows:int)->List[List[Dict[str,Any]]]:
    """üéØ DYNAMIC BATCH SIZING - Guarantees no token limit errors."""
    return dynamic_batch_slices(rows)

def dynamic_batch_slices(rows: List[Dict[str, Any]]) -> List[List[Dict[str, Any]]]:
    """
    üéØ DYNAMIC BATCH SIZING - Creates variable-sized batches that GUARANTEE no API token errors.
    
    Protection Layers:
    1. Individual chunk validation (max 6,000 tokens)
    2. Dynamic batch sizing (max 7,692 tokens total)
    3. Conservative safety margins
    4. Multiple validation checks
    """
    batches = []
    current_batch = []
    current_tokens = 0
    skipped_chunks = 0
    
    # Initialize token encoder for accurate counting
    encoder = None
    if TIKTOKEN_AVAILABLE:
        try:
            encoder = tiktoken.encoding_for_model(EMBEDDING_MODEL)
        except:
            pass
    
    def count_tokens_accurate(text: str) -> int:
        """Count tokens with maximum accuracy."""
        if encoder:
            return len(encoder.encode(text))
        else:
            # Conservative fallback estimation
            return int(len(text.split()) * 0.75) + 50
    
    log.info(f"üéØ Starting dynamic batch creation with {len(rows)} chunks")
    log.info(f"üéØ Limits: {MAX_CHUNK_TOKENS} tokens/chunk, {MAX_BATCH_TOKENS} tokens/batch")
    
    for i, row in enumerate(rows):
        # Clean text and validate
        text = (row.get("text") or "").replace("\x00", "").strip()
        if not text:
            log.warning(f"üéØ Skipping empty chunk {row.get('id', 'unknown')}")
            continue
        
        # üõ°Ô∏è PROTECTION LAYER 1: Individual chunk validation
        chunk_tokens = count_tokens_accurate(text)
        
        if chunk_tokens > MAX_CHUNK_TOKENS:
            log.warning(f"üéØ Skipping oversized chunk {row.get('id', 'unknown')}: {chunk_tokens} tokens (max: {MAX_CHUNK_TOKENS})")
            skipped_chunks += 1
            continue
        
        # üõ°Ô∏è PROTECTION LAYER 2: Dynamic batch sizing
        would_exceed = current_tokens + chunk_tokens > MAX_BATCH_TOKENS
        
        if would_exceed and current_batch:
            # Finalize current batch
            log.info(f"üéØ Created batch with {len(current_batch)} chunks (~{current_tokens} tokens)")
            batches.append(current_batch)
            current_batch = []
            current_tokens = 0
        
        # Add chunk to current batch
        current_batch.append({"id": row["id"], "text": text})
        current_tokens += chunk_tokens
        
        # üõ°Ô∏è PROTECTION LAYER 3: Safety validation
        if current_tokens > MAX_BATCH_TOKENS:
            log.error(f"üö® CRITICAL: Batch exceeded limit! {current_tokens} > {MAX_BATCH_TOKENS}")
            # Emergency fallback - remove last chunk and finalize batch
            if len(current_batch) > 1:
                current_batch.pop()
                current_tokens -= chunk_tokens
                log.info(f"üéØ Emergency: Created batch with {len(current_batch)} chunks (~{current_tokens} tokens)")
                batches.append(current_batch)
                current_batch = [{"id": row["id"], "text": text}]
                current_tokens = chunk_tokens
            else:
                log.error(f"üö® Single chunk too large: {chunk_tokens} tokens")
                current_batch = []
                current_tokens = 0
                skipped_chunks += 1
    
    # Add final batch if not empty
    if current_batch and current_tokens >= MIN_BATCH_TOKENS:
        log.info(f"üéØ Created final batch with {len(current_batch)} chunks (~{current_tokens} tokens)")
        batches.append(current_batch)
    elif current_batch:
        log.warning(f"üéØ Skipping tiny final batch: {current_tokens} tokens < {MIN_BATCH_TOKENS} minimum")
    
    # üõ°Ô∏è PROTECTION LAYER 4: Final validation
    total_chunks = sum(len(batch) for batch in batches)
    max_batch_tokens = max((sum(count_tokens_accurate(chunk["text"]) for chunk in batch) for batch in batches), default=0)
    
    log.info(f"üéØ DYNAMIC BATCHING COMPLETE:")
    log.info(f"   üìä {len(batches)} batches created")
    log.info(f"   üìä {total_chunks} chunks processed")
    log.info(f"   üìä {skipped_chunks} chunks skipped")
    log.info(f"   üìä Largest batch: {max_batch_tokens} tokens (limit: {MAX_BATCH_TOKENS})")
    log.info(f"   ‚úÖ GUARANTEED: No batch exceeds {MAX_BATCH_TOKENS} tokens")
    
    if max_batch_tokens > MAX_BATCH_TOKENS:
        log.error(f"üö® CRITICAL ERROR: Batch validation failed!")
        raise RuntimeError(f"Batch token validation failed: {max_batch_tokens} > {MAX_BATCH_TOKENS}")
    
    return batches

def embed_slice(sb,slice_rows:List[Dict[str,Any]])->int:
    embeds=generate_embedding_batch([r["text"] for r in slice_rows])
    ok=0
    for row,emb in zip(slice_rows,embeds):
        attempt=0
        while attempt<MAX_RETRIES:
            res=sb.table("research_chunks").update({"embedding":emb}).eq("id",row["id"]).execute()
            if getattr(res,"error",None):
                attempt+=1; log.warning("Update %s failed (%s/%s): %s",row["id"],attempt,MAX_RETRIES,res.error)
                time.sleep(RETRY_DELAY)
            else: ok+=1; break
    return ok

if __name__=="__main__": 
    asyncio.run(main_async_cli())


================================================================================


################################################################################
# File: scripts/stages/db_upsert.py
################################################################################

# File: scripts/stages/db_upsert.py

#!/usr/bin/env python3
"""
Stage 6 ‚Äî Optimized database operations with batching and connection pooling.
"""
from __future__ import annotations
import json, logging, os, pathlib, sys
from datetime import datetime
from typing import Any, Dict, List, Sequence, Optional
import asyncio
from concurrent.futures import ThreadPoolExecutor
import threading

from dotenv import load_dotenv
from supabase import create_client
from tqdm import tqdm

load_dotenv()
SUPABASE_URL  = os.getenv("SUPABASE_URL")
SUPABASE_KEY  = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
OPENAI_API_KEY= os.getenv("OPENAI_API_KEY")

META_FIELDS = ["doc_type","title","authors","year","journal","doi","abstract",
               "keywords","research_topics","peer_reviewed","open_access",
               "license","open_access_status"]

log = logging.getLogger(__name__)

# Thread-safe connection pool
class SupabasePool:
    """Thread-safe Supabase client pool."""
    def __init__(self, size: int = 10):
        self.size = size
        self._clients = []
        self._lock = threading.Lock()
        self._initialized = False
    
    def get(self):
        """Get a client from the pool."""
        with self._lock:
            if not self._initialized:
                self._initialize()
            # Round-robin selection
            import random
            return self._clients[random.randint(0, self.size - 1)]
    
    def _initialize(self):
        """Initialize client pool."""
        for _ in range(self.size):
            self._clients.append(init_supabase())
        self._initialized = True

# Global pool
_sb_pool = SupabasePool()

# ‚îÄ‚îÄ‚îÄ Supabase & sanitiser helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def scrub_nuls(obj:Any)->Any:
    if isinstance(obj,str):  return obj.replace("\x00","")
    if isinstance(obj,list): return [scrub_nuls(x) for x in obj]
    if isinstance(obj,dict): return {k:scrub_nuls(v) for k,v in obj.items()}
    return obj

def init_supabase():
    if not (SUPABASE_URL and SUPABASE_KEY):
        sys.exit("‚õî  SUPABASE env vars missing")
    return create_client(SUPABASE_URL,SUPABASE_KEY)

def upsert_document(sb,meta:Dict[str,Any])->str:
    meta = scrub_nuls(meta)
    doi = meta.get("doi")
    if doi:
        existing = (
            sb.table("research_documents")
            .select("id")
            .eq("doi", doi)
            .limit(1)
            .execute()
            .data
        )
        if existing:
            doc_id = existing[0]["id"]
            sb.table("research_documents").update(meta).eq("id", doc_id).execute()
            return doc_id
    res = sb.table("research_documents").insert(meta).execute()
    if hasattr(res, "error") and res.error:
        raise RuntimeError(f"Document insert failed: {res.error}")
    return res.data[0]["id"]

# Optimized batch operations
def insert_chunks_optimized(sb, doc_id: str, chunks: Sequence[Dict[str, Any]], 
                          src_json: pathlib.Path, batch_size: int = 1000):
    """Insert chunks with larger batches for better performance."""
    ts = datetime.utcnow().isoformat()
    inserted_ids = []
    
    # Prepare all rows
    rows = [
        {
            "document_id": doc_id,
            "chunk_index": ch["chunk_index"],
            "token_start": ch["token_start"],
            "token_end": ch["token_end"],
            "page_start": ch["page_start"],
            "page_end": ch["page_end"],
            "text": scrub_nuls(ch["text"]),
            "metadata": scrub_nuls(ch.get("metadata", {})),
            "chunking_strategy": "token_window",
            "source_file": str(src_json),
            "created_at": ts,
        }
        for ch in chunks
    ]
    
    # Insert in larger batches
    for i in range(0, len(rows), batch_size):
        batch = rows[i:i + batch_size]
        try:
            res = sb.table("research_chunks").insert(batch).execute()
            if hasattr(res, "error") and res.error:
                log.error("Batch insert failed: %s", res.error)
                # Fall back to smaller batches
                for j in range(0, len(batch), 100):
                    mini_batch = batch[j:j + 100]
                    mini_res = sb.table("research_chunks").insert(mini_batch).execute()
                    if hasattr(mini_res, "data"):
                        inserted_ids.extend([r["id"] for r in mini_res.data])
            else:
                inserted_ids.extend([r["id"] for r in res.data])
        except Exception as e:
            log.error(f"Failed to insert batch {i//batch_size + 1}: {e}")
            # Try individual inserts as last resort
            for row in batch:
                try:
                    single_res = sb.table("research_chunks").insert(row).execute()
                    if hasattr(single_res, "data") and single_res.data:
                        inserted_ids.append(single_res.data[0]["id"])
                except:
                    pass
    
    return inserted_ids

def insert_chunks(sb,doc_id:str,chunks:Sequence[Dict[str,Any]],src_json:pathlib.Path):
    """Original interface using optimized implementation."""
    return insert_chunks_optimized(sb, doc_id, chunks, src_json, batch_size=500)

async def upsert_batch_async(
    documents: List[Dict[str, Any]],
    max_concurrent: int = 5
) -> None:
    """Upsert multiple documents concurrently."""
    semaphore = asyncio.Semaphore(max_concurrent)
    
    async def upsert_one(doc_data):
        async with semaphore:
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(
                None,
                lambda: upsert(
                    doc_data["json_path"],
                    doc_data["chunks"],
                    do_embed=doc_data.get("do_embed", False)
                )
            )
    
    tasks = [upsert_one(doc) for doc in documents]
    
    from tqdm.asyncio import tqdm_asyncio
    await tqdm_asyncio.gather(*tasks, desc="Upserting to database")

# Keep original interface but use optimized implementation
def upsert(json_doc: pathlib.Path, chunks: List[Dict[str, Any]] | None,
           *, do_embed: bool = False) -> None:
    """Original interface with optimized implementation."""
    sb = _sb_pool.get()  # Use connection from pool
    
    data = json.loads(json_doc.read_text())
    row = {k: data.get(k) for k in META_FIELDS} | {
        "source_pdf": data.get("source_pdf", str(json_doc))
    }
    
    doc_id = upsert_document(sb, row)
    
    if not chunks:
        log.info("No chunks to insert for %s", json_doc.stem)
        return
    
    # Use optimized batch insert
    inserted_ids = insert_chunks_optimized(sb, doc_id, chunks, json_doc)
    log.info("‚Üë %s chunks inserted for %s", len(chunks), json_doc.stem)
    
    if do_embed and inserted_ids:
        from stages import embed_vectors
        embed_vectors.main()

if __name__ == "__main__":
    import argparse
    logging.basicConfig(level=logging.INFO, format="%(message)s")
    p=argparse.ArgumentParser()
    p.add_argument("json",type=pathlib.Path)
    p.add_argument("--chunks",type=pathlib.Path)
    p.add_argument("--embed",action="store_true")
    args=p.parse_args()
    
    chunks = None
    if args.chunks and args.chunks.exists():
        chunks = json.loads(args.chunks.read_text())
    
    upsert(args.json, chunks, do_embed=args.embed)


================================================================================


################################################################################
# File: scripts/test_vector_search.py
################################################################################

#!/usr/bin/env python3
################################################################################
################################################################################
"""
Vector‚Äësearch smoke‚Äëtest (Supabase edition)
==========================================

‚Ä¢ This script tests vector search functionality using Supabase's PostgREST API.

‚Ä¢ The script calls a SQL helper function that must exist on your database:
    public.search_research_chunks(query_text TEXT,
                                  match_count INT DEFAULT 10,
                                  similarity_threshold REAL DEFAULT 0.6)
  which should:
    1. embed the incoming `query_text`
    2. invoke your `match_documents` similarity function
    3. return the top‚Äë`match_count` rows as
       (id UUID, text TEXT, metadata JSONB, similarity REAL)

  See README / earlier instructions for a ready‚Äëmade implementation.

Environment variables required
------------------------------
SUPABASE_URL                 ‚Äì e.g. https://xxxx.supabase.co
SUPABASE_SERVICE_ROLE_KEY    ‚Äì or an anon key if RLS permits the RPC
OPENAI_API_KEY               ‚Äì only if your SQL helper embeds via an HTTP call
"""

from __future__ import annotations

import os
import sys
import time
from typing import Any, Dict, List

from dotenv import load_dotenv
from supabase import create_client
from openai import OpenAI

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ configuration & sanity checks ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #

load_dotenv()

# ---------- Supabase config ----------
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")   # or anon if RLS permits
sb = create_client(SUPABASE_URL, SUPABASE_KEY)

if not SUPABASE_URL or not SUPABASE_KEY:
    sys.exit(
        "‚ùå  SUPABASE_URL / SUPABASE_SERVICE_ROLE_KEY env vars are missing.\n"
        "    export them and rerun."
    )

# Sample questions to probe the index
SAMPLE_QUERIES: List[str] = [
    "What are the symptoms of misophonia?",
    "How prevalent is misophonia in university students?",
    "What is the relationship between misophonia and hyperacusis?",
    "What treatments are effective for misophonia?",
    "How does misophonia affect quality of life?",
]

# Add this after other configuration
openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

def embed(text: str) -> List[float]:
    """Return OpenAI ada‚Äë002 embedding (1536‚Äëdim list of floats)."""
    resp = openai_client.embeddings.create(
        model="text-embedding-ada-002",
        input=text,
    )
    return resp.data[0].embedding

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ helper functions ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #


def perform_vector_search(query_vec, top_k=5, thresh=0.6):
    """
    Call the SQL RPC we just created.
    `query_vec` is a list[float] length 1536 coming from OpenAI.
    """
    try:
        resp = sb.rpc(
            "search_research_chunks",
            {
                "query_embedding": query_vec,
                "match_count": top_k,
                "similarity_threshold": thresh,
            },
        ).execute()
        if getattr(resp, "error", None):
            raise RuntimeError(resp.error)
        return resp.data or []
    except Exception as e:
        print(f"   ‚ö†  RPC failed: {e}")
        return []


def print_results(rows: List[Dict[str, Any]]) -> None:
    """
    Nicely format the search results.
    """
    if not rows:
        print("   (no matches)\n")
        return

    for idx, row in enumerate(rows, 1):
        meta = row.get("metadata", {}) or {}
        title = meta.get("title", "Unknown title")
        year = meta.get("year", "????")
        author = meta.get("primary_author", "Unknown author")
        sim = row.get("similarity", 0.0)

        snippet = (row.get("text", "") or "").replace("\n", " ")[:280] + "‚Ä¶"

        print(f"\nResult {idx}  ‚Ä¢  sim={sim:.3f}")
        print(f"  {title} ‚Äî {author} ({year})")
        print(f"  {snippet}")


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ main ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #


def main() -> None:
    print("\nüîç  Supabase vector search smoke‚Äëtest\n" + "‚Äî" * 60)
    for i, q in enumerate(SAMPLE_QUERIES, 1):
        print(f'\nQuery {i + 1}/{len(SAMPLE_QUERIES)}: "{q}"')


        # 1. get the vector
        print("Generating embedding...")
        query_vec = embed(q)  # Python list[float]

        # 2. PostgREST / Postgres expects a *string* like: [0.1,0.2,‚Ä¶]
        vec_literal = "[" + ",".join(f"{x:.6f}" for x in query_vec) + "]"

        # 3. call the RPC
        print("Performing vector search...")
        resp = sb.rpc(
            "search_research_chunks",
            {
                "query_embedding": vec_literal,  # <- NOT the raw text
                "match_count": 5,
                "similarity_threshold": 0.6,
            },
        ).execute()
        
        if getattr(resp, "error", None):
            print(f"   ‚ö†  RPC failed: {resp.error}\n")
            continue
            
        results = resp.data or []
        print_results(results)

        if i < len(SAMPLE_QUERIES):
            print("\nPausing 2 s before the next query ‚Ä¶")
            time.sleep(2)

    print("\n‚úî  Done")


if __name__ == "__main__":
    main()


================================================================================


################################################################################
# File: scripts/stages/acceleration_utils.py
################################################################################

# File: scripts/stages/acceleration_utils.py

"""
Hardware acceleration utilities for the pipeline.
Provides Apple Silicon optimization when available, with CPU fallback.
"""
import os
import platform
import multiprocessing as mp
from typing import Optional, Callable, Any, List
import logging
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import asyncio
from functools import partial

log = logging.getLogger(__name__)

class HardwareAccelerator:
    """Detects and manages hardware acceleration capabilities."""
    
    def __init__(self):
        self.is_apple_silicon = self._detect_apple_silicon()
        self.cpu_count = mp.cpu_count()
        self.optimal_workers = self._calculate_optimal_workers()
        
        # Set environment for better performance on macOS
        if self.is_apple_silicon:
            os.environ['OPENBLAS_NUM_THREADS'] = '1'
            os.environ['MKL_NUM_THREADS'] = '1'
            os.environ['OMP_NUM_THREADS'] = '1'
        
        log.info(f"Hardware: {'Apple Silicon' if self.is_apple_silicon else platform.processor()}")
        log.info(f"CPU cores: {self.cpu_count}, Optimal workers: {self.optimal_workers}")
    
    def _detect_apple_silicon(self) -> bool:
        """Detect if running on Apple Silicon."""
        if platform.system() != 'Darwin':
            return False
        try:
            import subprocess
            result = subprocess.run(['sysctl', '-n', 'hw.optional.arm64'], 
                                  capture_output=True, text=True)
            return result.stdout.strip() == '1'
        except:
            return False
    
    def _calculate_optimal_workers(self) -> int:
        """Calculate optimal number of workers based on hardware."""
        if self.is_apple_silicon:
            # Apple Silicon has efficiency and performance cores
            # Use 75% of cores to leave room for system
            return max(1, int(self.cpu_count * 0.75))
        else:
            # Traditional CPU - use all but one core
            return max(1, self.cpu_count - 1)
    
    def get_process_pool(self, max_workers: Optional[int] = None) -> ProcessPoolExecutor:
        """Get optimized process pool executor."""
        workers = max_workers or self.optimal_workers
        return ProcessPoolExecutor(
            max_workers=workers,
            mp_context=mp.get_context('spawn')  # Required for macOS
        )
    
    def get_thread_pool(self, max_workers: Optional[int] = None) -> ThreadPoolExecutor:
        """Get optimized thread pool executor for I/O operations."""
        workers = max_workers or min(32, self.cpu_count * 4)
        return ThreadPoolExecutor(max_workers=workers)

# Global instance
hardware = HardwareAccelerator()

async def run_cpu_bound_concurrent(func: Callable, items: List[Any], 
                                 max_workers: Optional[int] = None,
                                 desc: str = "Processing") -> List[Any]:
    """Run CPU-bound function concurrently on multiple items."""
    loop = asyncio.get_event_loop()
    with hardware.get_process_pool(max_workers) as executor:
        futures = [loop.run_in_executor(executor, func, item) for item in items]
        
        from tqdm.asyncio import tqdm_asyncio
        results = await tqdm_asyncio.gather(*futures, desc=desc)
        return results

async def run_io_bound_concurrent(func: Callable, items: List[Any],
                                max_workers: Optional[int] = None,
                                desc: str = "Processing") -> List[Any]:
    """Run I/O-bound function concurrently on multiple items."""
    loop = asyncio.get_event_loop()
    with hardware.get_thread_pool(max_workers) as executor:
        futures = [loop.run_in_executor(executor, func, item) for item in items]
        
        from tqdm.asyncio import tqdm_asyncio
        results = await tqdm_asyncio.gather(*futures, desc=desc)
        return results


================================================================================

