# Concatenated Project Code - Part 1 of 3
# Generated: 2025-06-04 18:42:53
# Root Directory: /Users/gianmariatroiani/Documents/knologi̊/graph_database
================================================================================

# Directory Structure
################################################################################
├── .gitignore (939.0B, no ext)
├── city_clerk_documents/
│   ├── global copy/
│   │   ├── City Comissions 2024/
│   │   │   ├── Agendas/
│   │   │   │   ├── Agenda 01.23.2024.pdf (155.1KB, .pdf)
│   │   │   │   ├── Agenda 01.9.2024.pdf (151.1KB, .pdf)
│   │   │   │   ├── Agenda 02.13.2024.pdf (155.0KB, .pdf)
│   │   │   │   ├── Agenda 02.27.2024.pdf (152.9KB, .pdf)
│   │   │   │   ├── Agenda 03.12.2024.pdf (166.6KB, .pdf)
│   │   │   │   ├── Agenda 05.07.2024.pdf (167.6KB, .pdf)
│   │   │   │   ├── Agenda 05.21.2024.pdf (172.2KB, .pdf)
│   │   │   │   └── Agenda 06.11.2024.pdf (160.2KB, .pdf)
│   │   │   ├── ExportedFolderContents.zip (62.4MB, .zip)
│   │   │   ├── Ordinances/
│   │   │   │   ├── 2024/
│   │   │   │   │   ├── 2024-01 - 01_09_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-02 - 01_09_2024.pdf (15.1MB, .pdf)
│   │   │   │   │   ├── 2024-03 - 01_09_2024.pdf (11.1MB, .pdf)
│   │   │   │   │   ├── 2024-04 - 01_23_2024.pdf (2.3MB, .pdf)
│   │   │   │   │   ├── 2024-05 - 02_13_2024.pdf (2.0MB, .pdf)
│   │   │   │   │   ├── 2024-06 - 02_13_2024.pdf (1.9MB, .pdf)
│   │   │   │   │   ├── 2024-07 - 02_13_2024.pdf (6.5MB, .pdf)
│   │   │   │   │   ├── 2024-08 - 02_27_2024.pdf (2.5MB, .pdf)
│   │   │   │   │   ├── 2024-09 - 02_27_2024.pdf (1.6MB, .pdf)
│   │   │   │   │   ├── 2024-10 - 03_12_2024.pdf (3.7MB, .pdf)
│   │   │   │   │   ├── 2024-11 - 03_12_2024 - (As Amended).pdf (5.6MB, .pdf)
│   │   │   │   │   ├── 2024-11 - 03_12_2024.pdf (5.5MB, .pdf)
│   │   │   │   │   ├── 2024-12 - 03_12_2024.pdf (1.8MB, .pdf)
│   │   │   │   │   ├── 2024-13 - 03_12_2024.pdf (3.2MB, .pdf)
│   │   │   │   │   ├── 2024-14 - 05_07_2024.pdf (2.8MB, .pdf)
│   │   │   │   │   ├── 2024-15 - 05_07_2024.pdf (3.9MB, .pdf)
│   │   │   │   │   ├── 2024-16 - 05_07_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-17 - 05_07_2024.pdf (4.3MB, .pdf)
│   │   │   │   │   ├── 2024-18 - 05_21_2024.pdf (2.4MB, .pdf)
│   │   │   │   │   ├── 2024-19 - 05_21_2024.pdf (1.7MB, .pdf)
│   │   │   │   │   ├── 2024-20 - 05_21_2024.pdf (1.6MB, .pdf)
│   │   │   │   │   ├── 2024-21 - 06_11_2024.pdf (1.7MB, .pdf)
│   │   │   │   │   ├── 2024-22 - 06_11_2024.pdf (2.0MB, .pdf)
│   │   │   │   │   ├── 2024-23 - 06_11_2024.pdf (1.9MB, .pdf)
│   │   │   │   │   ├── 2024-24 - 06_11_2024.pdf (2.1MB, .pdf)
│   │   │   │   │   └── 2024-25 - 06_11_2024.pdf (1.3MB, .pdf)
│   │   │   │   ├── [EXCLUDED] 1 items: .DS_Store (excluded file)
│   │   │   ├── Resolutions/
│   │   │   │   ├── 2024/
│   │   │   │   │   ├── 2024-01 - 01_09_2024.pdf (448.9KB, .pdf)
│   │   │   │   │   ├── 2024-02 - 01_09_2024.pdf (451.9KB, .pdf)
│   │   │   │   │   ├── 2024-03 - 01_09_2024.pdf (867.3KB, .pdf)
│   │   │   │   │   ├── 2024-04 - 01_09_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-05 - 01_09_2024.pdf (936.9KB, .pdf)
│   │   │   │   │   ├── 2024-06 - 01_09_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-07 - 01_09_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-08 - 01_23_2024.pdf (457.9KB, .pdf)
│   │   │   │   │   ├── 2024-09 - 01_23_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-10 - 01_23_2024.pdf (454.8KB, .pdf)
│   │   │   │   │   ├── 2024-10 - 03_12_2024.pdf (3.7MB, .pdf)
│   │   │   │   │   ├── 2024-100 - 05_21_2024.pdf (3.0MB, .pdf)
│   │   │   │   │   ├── 2024-101 - 05_21_2024.pdf (947.4KB, .pdf)
│   │   │   │   │   ├── 2024-102 - 05_21_2024.pdf (466.0KB, .pdf)
│   │   │   │   │   ├── 2024-103 - 05_21_2024.pdf (991.8KB, .pdf)
│   │   │   │   │   ├── 2024-104 - 05_21_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-105 - 05_21_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-106 - 05_21_2024.pdf (2.4MB, .pdf)
│   │   │   │   │   ├── 2024-107 - 05_21_2024.pdf (6.0MB, .pdf)
│   │   │   │   │   ├── 2024-108 - 05_21_2024.pdf (2.1MB, .pdf)
│   │   │   │   │   ├── 2024-109 - 05_21_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-11 - 01_23_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-110 - 05_21_2024.pdf (921.5KB, .pdf)
│   │   │   │   │   ├── 2024-111 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-112 - 05_21_2024.pdf (6.2MB, .pdf)
│   │   │   │   │   ├── 2024-113 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-114 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-115 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-116 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-117 - 05_21_2024.pdf (6.4MB, .pdf)
│   │   │   │   │   ├── 2024-118 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-119 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-12 - 01_23_2024.pdf (588.3KB, .pdf)
│   │   │   │   │   ├── 2024-120 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-121 - 05_21_2024.pdf (6.4MB, .pdf)
│   │   │   │   │   ├── 2024-122 - 05_21_2024.pdf (6.8MB, .pdf)
│   │   │   │   │   ├── 2024-123 - 05_21_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-124 - 05_21_2024.pdf (11.2MB, .pdf)
│   │   │   │   │   ├── 2024-125 - 05_21_2024.pdf (776.0KB, .pdf)
│   │   │   │   │   ├── 2024-126 - 05_21_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-127 - 05_21_2024.pdf (8.1MB, .pdf)
│   │   │   │   │   ├── 2024-129 - 06_11_2024.pdf (1.3MB, .pdf)
│   │   │   │   │   ├── 2024-13 - 01_23_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-130 - 06_11_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-131 - 06_11_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-132 - 06_11_2024.pdf (458.0KB, .pdf)
│   │   │   │   │   ├── 2024-133 - 06_11_2024 -As Amended.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-134 - 06_11_2024.pdf (884.8KB, .pdf)
│   │   │   │   │   ├── 2024-135 - 06_11_2024.pdf (1022.1KB, .pdf)
│   │   │   │   │   ├── 2024-136 - 06_11_2024.pdf (537.4KB, .pdf)
│   │   │   │   │   ├── 2024-137 - 06_11_2024.pdf (1.7MB, .pdf)
│   │   │   │   │   ├── 2024-138 - 06_11_2024.pdf (1.7MB, .pdf)
│   │   │   │   │   ├── 2024-139 - 06_11_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-14 - 01_23_2024.pdf (996.6KB, .pdf)
│   │   │   │   │   ├── 2024-140 - 06_11_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-141 - 06_11_2024.pdf (13.2MB, .pdf)
│   │   │   │   │   ├── 2024-142 - 06_11_2024.pdf (790.5KB, .pdf)
│   │   │   │   │   ├── 2024-143 -06_11_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-15 - 01_23_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-16 - 01_23_2024.pdf (1.8MB, .pdf)
│   │   │   │   │   ├── 2024-17 - 01_23_2024.pdf (488.0KB, .pdf)
│   │   │   │   │   ├── 2024-18 - 01_23_2024.pdf (849.1KB, .pdf)
│   │   │   │   │   ├── 2024-19 - 02_19_2024.pdf (1019.5KB, .pdf)
│   │   │   │   │   ├── 2024-20 - 02_13_2024.pdf (824.5KB, .pdf)
│   │   │   │   │   ├── 2024-21 - 02_13_2024.pdf (502.6KB, .pdf)
│   │   │   │   │   ├── 2024-22 - 02_13_2024.pdf (450.6KB, .pdf)
│   │   │   │   │   ├── 2024-23 - 02_13_2024.pdf (447.7KB, .pdf)
│   │   │   │   │   ├── 2024-24 - 02_13_2024.pdf (464.1KB, .pdf)
│   │   │   │   │   ├── 2024-25 - 02_13_2024.pdf (458.3KB, .pdf)
│   │   │   │   │   ├── 2024-26 - 02_13_2024.pdf (465.7KB, .pdf)
│   │   │   │   │   ├── 2024-27 - 02_13_2024.pdf (754.2KB, .pdf)
│   │   │   │   │   ├── 2024-28 - 02_13_2024.pdf (761.7KB, .pdf)
│   │   │   │   │   ├── 2024-29 - 02_13_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-30 - 02_13_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-31 - 02_13_2024.pdf (936.6KB, .pdf)
│   │   │   │   │   ├── 2024-32 - 02_13_2024.pdf (1.9MB, .pdf)
│   │   │   │   │   ├── 2024-33 - 02_13_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-34 - 02_13_2024.pdf (1.9MB, .pdf)
│   │   │   │   │   ├── 2024-35 - 02_27_2024.pdf (480.7KB, .pdf)
│   │   │   │   │   ├── 2024-36 - 02_27_2024.pdf (1.3MB, .pdf)
│   │   │   │   │   ├── 2024-37 - 02_27_2024.pdf (940.3KB, .pdf)
│   │   │   │   │   ├── 2024-38 - 02_27_2024.pdf (829.6KB, .pdf)
│   │   │   │   │   ├── 2024-39 - 02_27_2024.pdf (816.5KB, .pdf)
│   │   │   │   │   ├── 2024-41 - 02_27_2024.pdf (833.1KB, .pdf)
│   │   │   │   │   ├── 2024-42 - 02_27_2024.pdf (473.4KB, .pdf)
│   │   │   │   │   ├── 2024-43 - 02_27_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-44 - 02_27_2024.pdf (1.8MB, .pdf)
│   │   │   │   │   ├── 2024-45 - 03_12_2024.pdf (1002.1KB, .pdf)
│   │   │   │   │   ├── 2024-46 - 03_12_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-47 - 03_12_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-48 - 03_12_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-49 - 03_12_2024.pdf (899.6KB, .pdf)
│   │   │   │   │   ├── 2024-50 - 03_12_2024.pdf (462.1KB, .pdf)
│   │   │   │   │   ├── 2024-51 - 03_12_2024.pdf (538.1KB, .pdf)
│   │   │   │   │   ├── 2024-52 - 03_12_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-53 - 03_12_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-54 - 03_12_2024.pdf (2.0MB, .pdf)
│   │   │   │   │   ├── 2024-55 - 03_12_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-56 - 03_12_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-57 - 03_12_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-58 - 03_12_2024.pdf (775.8KB, .pdf)
│   │   │   │   │   ├── 2024-60 - 03_12_2024.pdf (2.3MB, .pdf)
│   │   │   │   │   ├── 2024-61 - 04_16_2024.pdf (6.4MB, .pdf)
│   │   │   │   │   ├── 2024-62 - 04_16_2024.pdf (2.7MB, .pdf)
│   │   │   │   │   ├── 2024-63 -  04_16_2024.pdf (832.3KB, .pdf)
│   │   │   │   │   ├── 2024-64 - 04_16_2024.pdf (452.6KB, .pdf)
│   │   │   │   │   ├── 2024-65 - 04_16_2024.pdf (894.5KB, .pdf)
│   │   │   │   │   ├── 2024-66 - 04_16_2024.pdf (446.6KB, .pdf)
│   │   │   │   │   ├── 2024-67 - 04_16_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-68 - 04_16_2024.pdf (5.6MB, .pdf)
│   │   │   │   │   ├── 2024-69- -04_16_2024.pdf (443.1KB, .pdf)
│   │   │   │   │   ├── 2024-70 - 04_16_2024.pdf (878.9KB, .pdf)
│   │   │   │   │   ├── 2024-71 - 04_16_2024.pdf (951.8KB, .pdf)
│   │   │   │   │   ├── 2024-72 - 04_16_2024.pdf (821.7KB, .pdf)
│   │   │   │   │   ├── 2024-73 - 04_16_2024.pdf (810.8KB, .pdf)
│   │   │   │   │   ├── 2024-74 - 04_16_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-75 - 04_16_2024.pdf (1.6MB, .pdf)
│   │   │   │   │   ├── 2024-76 - 04_16_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-77 - 04_16_2024.pdf (1013.9KB, .pdf)
│   │   │   │   │   ├── 2024-78 - 04_16_2024.pdf (1007.5KB, .pdf)
│   │   │   │   │   ├── 2024-79 - 04_16_2024.pdf (997.7KB, .pdf)
│   │   │   │   │   ├── 2024-80 - 04_16_2024.pdf (525.7KB, .pdf)
│   │   │   │   │   ├── 2024-81 - 04_16_2024.pdf (923.2KB, .pdf)
│   │   │   │   │   ├── 2024-82 - 04_16_2024.pdf (473.6KB, .pdf)
│   │   │   │   │   ├── 2024-83 - 04_16_2024.pdf (915.3KB, .pdf)
│   │   │   │   │   ├── 2024-84 - 05_07_2024.pdf (992.7KB, .pdf)
│   │   │   │   │   ├── 2024-85 - 05_07_2024.pdf (2.0MB, .pdf)
│   │   │   │   │   ├── 2024-86 - 05_07_2024.pdf (503.1KB, .pdf)
│   │   │   │   │   ├── 2024-87 - 05_07_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-88 - 05_07_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-89 - 05_07_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-90 - 05_07_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-91 - 05_07_2024.pdf (1.8MB, .pdf)
│   │   │   │   │   ├── 2024-92 - 05_07_2024.pdf (452.4KB, .pdf)
│   │   │   │   │   ├── 2024-93 - 05_07_2024.pdf (1014.3KB, .pdf)
│   │   │   │   │   ├── 2024-94 - 05_05_2024.pdf (1.3MB, .pdf)
│   │   │   │   │   ├── 2024-95 - 05_07_2024.pdf (974.5KB, .pdf)
│   │   │   │   │   ├── 2024-96 - 05_07_2024.pdf (1.5MB, .pdf)
│   │   │   │   │   ├── 2024-97 - 05_07_2024.pdf (1023.3KB, .pdf)
│   │   │   │   │   ├── 2024-98 - 05_07_2024.pdf (2.2MB, .pdf)
│   │   │   │   │   └── 2024-99 - 05_07_2024.pdf (792.7KB, .pdf)
│   │   │   │   ├── [EXCLUDED] 1 items: .DS_Store (excluded file)
│   │   │   ├── Verbating Items/
│   │   │   │   ├── 2024/
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - E-4.pdf (2.4MB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - E-5.pdf (735.6KB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - E-7.pdf (306.9KB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - E-8.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - F-10.pdf (925.3KB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - F-2.pdf (288.2KB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - F-5.pdf (360.6KB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - F-6.pdf (212.8KB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - Public Comment.pdf (762.7KB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - C- Public Comment.pdf (1.3MB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - E-3 and E-4.pdf (2.4MB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - E-5.pdf (290.7KB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - F-1.pdf (855.5KB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - F-5.pdf (3.7MB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - F-7.pdf (768.6KB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - H-1.pdf (1.5MB, .pdf)
│   │   │   │   │   ├── 02_13_2024 - Meeting Minutes - Public.pdf (363.5KB, .pdf)
│   │   │   │   │   ├── 02_13_2024 - Verbatim Transcripts - F-12.pdf (7.2MB, .pdf)
│   │   │   │   │   ├── 02_13_2024 - Verbatim Transcripts - H-1.pdf (570.8KB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - E-1.pdf (4.2MB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - E-2.pdf (281.3KB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - E-3.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - F-10.pdf (2.5MB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - F-12.pdf (448.2KB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - H-1.pdf (794.2KB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - Public Comment.pdf (439.4KB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - E-1.pdf (124.3KB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - E-2.pdf (487.9KB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - E-4 and E-11.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - F-1.pdf (416.0KB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - F-2.pdf (1.5MB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - F-4 and F-5 and F-11 and F-12.pdf (1.4MB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - H-1.pdf (121.5KB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - K - Discussion Items.pdf (285.0KB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - Public Comment.pdf (412.4KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - 2-1 AND 2-2.pdf (652.3KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - E-11.pdf (365.8KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - E-2.pdf (369.8KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - E-4.pdf (410.2KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - E-5 E-6 E-7 E-8 E-9 E-10.pdf (3.6MB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-1.pdf (751.9KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-10.pdf (1.3MB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-11.pdf (2.4MB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-12.pdf (2.8MB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-15.pdf (255.0KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-5.pdf (205.0KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-8.pdf (961.1KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-9.pdf (330.1KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - Public Comment.pdf (1.5MB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - 2-1.pdf (1.7MB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - E-1.pdf (4.6MB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - E-10.pdf (128.5KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - E-11.pdf (559.6KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - E-12.pdf (156.1KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - E-8.pdf (288.5KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - E-9.pdf (263.4KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - F-1.pdf (561.1KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - F-13.pdf (151.9KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - F-16.pdf (782.8KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - F-3.pdf (352.6KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - F-9.pdf (736.0KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - H-1.pdf (455.3KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - Public Comment.pdf (1.6MB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - D-3.pdf (539.3KB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - E-10.pdf (629.7KB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - E-3.pdf (350.4KB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - F-2.pdf (1.7MB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - F-3.pdf (2.4MB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - F-4.pdf (3.4MB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - F-5.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - F-7 and F-10.pdf (1.6MB, .pdf)
│   │   │   │   │   └── 06_11_2024 - Verbatim Transcripts - Public Comment.pdf (513.4KB, .pdf)
│   │   │   │   ├── [EXCLUDED] 1 items: .DS_Store (excluded file)
│   │   │   ├── [EXCLUDED] 1 items: .DS_Store (excluded file)
│   │   ├── [EXCLUDED] 1 items: .DS_Store (excluded file)
│   ├── graph_json/
│   │   ├── Agenda 01.9.2024_docling_extracted.json (38.2KB, .json)
│   │   ├── Agenda 01.9.2024_extracted.json (38.2KB, .json)
│   │   ├── Agenda 01.9.2024_full_text.txt (16.1KB, .txt)
│   │   ├── Agenda 01.9.2024_ontology.json (46.5KB, .json)
│   │   ├── debug/
│   │   │   ├── enhanced_linked_documents.json (3.7KB, .json)
│   │   │   ├── linking_report_01_09_2024.json (1.5KB, .json)
│   │   │   ├── llm_response_ordinance_2024-01_cleaned.txt (16.0B, .txt)
│   │   │   ├── llm_response_ordinance_2024-01_raw.txt (16.0B, .txt)
│   │   │   ├── llm_response_ordinance_2024-02_cleaned.txt (16.0B, .txt)
│   │   │   ├── llm_response_ordinance_2024-02_raw.txt (16.0B, .txt)
│   │   │   ├── llm_response_ordinance_2024-03_cleaned.txt (16.0B, .txt)
│   │   │   ├── llm_response_ordinance_2024-03_raw.txt (16.0B, .txt)
│   │   │   ├── llm_response_resolution_2024-01_cleaned.txt (16.0B, .txt)
│   │   │   ├── llm_response_resolution_2024-01_raw.txt (16.0B, .txt)
│   │   │   ├── llm_response_resolution_2024-02_cleaned.txt (16.0B, .txt)
│   │   │   ├── llm_response_resolution_2024-02_raw.txt (16.0B, .txt)
│   │   │   ├── llm_response_resolution_2024-03_cleaned.txt (17.0B, .txt)
│   │   │   ├── llm_response_resolution_2024-03_raw.txt (17.0B, .txt)
│   │   │   ├── llm_response_resolution_2024-04_cleaned.txt (16.0B, .txt)
│   │   │   ├── llm_response_resolution_2024-04_raw.txt (16.0B, .txt)
│   │   │   ├── llm_response_resolution_2024-05_cleaned.txt (17.0B, .txt)
│   │   │   ├── llm_response_resolution_2024-05_raw.txt (17.0B, .txt)
│   │   │   ├── llm_response_resolution_2024-06_cleaned.txt (16.0B, .txt)
│   │   │   ├── llm_response_resolution_2024-06_raw.txt (16.0B, .txt)
│   │   │   ├── llm_response_resolution_2024-07_cleaned.txt (16.0B, .txt)
│   │   │   ├── llm_response_resolution_2024-07_raw.txt (16.0B, .txt)
│   │   │   └── verbatim/
│   │   │       └── verbatim_linking_report_01_09_2024.json (9.1KB, .json)
│   │   ├── pipeline_report_20250603_170358.json (1.2KB, .json)
│   │   ├── pipeline_report_20250603_171806.json (5.3KB, .json)
│   │   ├── pipeline_report_20250603_174911.json (5.3KB, .json)
│   │   ├── pipeline_report_20250603_181819.json (1.8KB, .json)
│   │   ├── pipeline_report_20250603_183222.json (1.2KB, .json)
│   │   ├── pipeline_report_20250603_185650.json (1.2KB, .json)
│   │   ├── pipeline_report_20250603_192623.json (6.8KB, .json)
│   │   └── pipeline_report_20250603_200758.json (10.3KB, .json)
│   ├── [EXCLUDED] 2 items: .DS_Store (excluded file), global (excluded dir)
├── config.py (1.7KB, .py)
├── debug/
│   ├── entities_response_chunk_0.txt (5.5KB, .txt)
│   ├── entities_response_chunk_1.txt (4.7KB, .txt)
│   ├── extracted_items.json (15.5KB, .json)
│   ├── meeting_info_parsed.json (471.0B, .json)
│   └── meeting_info_response.txt (467.0B, .txt)
├── graph_clear_database.py (963.0B, .py)
├── graph_visualizer.py (23.6KB, .py)
├── ontology_model.txt (6.6KB, .txt)
├── ontology_modelv2.txt (10.2KB, .txt)
├── repository_directory_structure.txt (5.7KB, .txt)
├── requirements.txt (1018.0B, .txt)
├── run_city_clerk_pipeline.sh (440.0B, .sh)
├── run_graph_pipeline.sh (1.3KB, .sh)
├── run_graph_visualizer.sh (3.6KB, .sh)
├── scripts/
│   ├── graph_pipeline.py (21.8KB, .py)
│   ├── graph_stages/
│   │   ├── __init__.py (647.0B, .py)
│   │   ├── agenda_graph_builder.py (54.5KB, .py)
│   │   ├── agenda_ontology_extractor.py (25.7KB, .py)
│   │   ├── agenda_pdf_extractor.py (20.6KB, .py)
│   │   ├── cosmos_db_client.py (10.2KB, .py)
│   │   ├── document_linker.py (12.9KB, .py)
│   │   ├── enhanced_document_linker.py (18.3KB, .py)
│   │   ├── ontology_extractor.py (41.3KB, .py)
│   │   ├── pdf_extractor.py (6.5KB, .py)
│   │   └── verbatim_transcript_linker.py (13.9KB, .py)
│   ├── [EXCLUDED] 4 items: pipeline_modular_optimized.py (excluded file), rag_local_web_app.py (excluded file), stages (excluded dir)
│       ... and 1 more excluded items
├── [EXCLUDED] 6 items: .DS_Store (excluded file), .env (excluded file), .git (excluded dir)
    ... and 3 more excluded items


================================================================================


# File Index - Which Files Are in Which Parts
################################################################################

## Part 1 (5 files):
  - scripts/graph_stages/agenda_pdf_extractor.py
  - scripts/graph_stages/cosmos_db_client.py
  - scripts/graph_stages/pdf_extractor.py
  - graph_clear_database.py
  - requirements.txt

## Part 2 (6 files):
  - scripts/graph_stages/enhanced_document_linker.py
  - scripts/graph_stages/document_linker.py
  - city_clerk_documents/graph_json/debug/enhanced_linked_documents.json
  - config.py
  - city_clerk_documents/graph_json/debug/linking_report_01_09_2024.json
  - debug/meeting_info_parsed.json

## Part 3 (4 files):
  - debug/extracted_items.json
  - scripts/graph_stages/verbatim_transcript_linker.py
  - city_clerk_documents/graph_json/debug/verbatim/verbatim_linking_report_01_09_2024.json
  - scripts/graph_stages/__init__.py


================================================================================


################################################################################
# File: scripts/graph_stages/agenda_pdf_extractor.py
################################################################################

# scripts/graph_stages/agenda_pdf_extractor.py
"""
PDF Extractor for City Clerk Agenda Documents
Extracts text, structure, and hyperlinks from agenda PDFs.
"""

import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import json
import re
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import PdfPipelineOptions
from openai import OpenAI
import os

log = logging.getLogger(__name__)


class AgendaPDFExtractor:
    """Extract structured content from agenda PDFs using Docling and LLM."""
    
    def __init__(self, output_dir: Optional[Path] = None):
        """Initialize the agenda PDF extractor."""
        self.output_dir = output_dir or Path("city_clerk_documents/extracted_text")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize Docling converter with OCR enabled
        pipeline_options = PdfPipelineOptions()
        pipeline_options.do_ocr = True  # Enable OCR for better text extraction
        pipeline_options.do_table_structure = True  # Better table extraction
        
        self.converter = DocumentConverter(
            format_options={
                InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
            }
        )
        
        # Initialize OpenAI client for LLM extraction
        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.model = "gpt-4.1-mini-2025-04-14"
    
    def extract_agenda(self, pdf_path: Path) -> Dict[str, any]:
        """Extract agenda content from PDF using Docling + LLM."""
        log.info(f"📄 Extracting agenda from {pdf_path.name}")
        
        # Convert with Docling - pass path directly
        result = self.converter.convert(str(pdf_path))
        
        # Get the document
        doc = result.document
        
        # Get full text and markdown
        full_text = doc.export_to_markdown() or ""
        
        # Use LLM to extract structured agenda items
        log.info("🧠 Using LLM to extract agenda structure...")
        extracted_items = self._extract_agenda_items_with_llm(full_text)
        
        # Build sections from extracted items
        sections = self._build_sections_from_items(extracted_items, full_text)
        
        # Extract hyperlinks if available
        hyperlinks = self._extract_hyperlinks(doc)
        
        # Create agenda data structure with both raw and structured data
        agenda_data = {
            'source_file': pdf_path.name,
            'full_text': full_text,
            'sections': sections,
            'agenda_items': extracted_items,  # Add structured items
            'hyperlinks': hyperlinks,
            'metadata': {
                'extraction_method': 'docling+llm',
                'num_sections': len(sections),
                'num_items': self._count_items(extracted_items),
                'num_hyperlinks': len(hyperlinks)
            }
        }
        
        # IMPORTANT: Save the extracted data with the filename expected by ontology extractor
        # The ontology extractor looks for "{pdf_stem}_extracted.json"
        output_file = self.output_dir / f"{pdf_path.stem}_extracted.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(agenda_data, f, indent=2, ensure_ascii=False)
        
        # Also save debug output
        debug_file = self.output_dir / f"{pdf_path.stem}_docling_extracted.json"
        with open(debug_file, 'w', encoding='utf-8') as f:
            json.dump(agenda_data, f, indent=2, ensure_ascii=False)
        
        # Also save just the full text for debugging
        text_file = self.output_dir / f"{pdf_path.stem}_full_text.txt"
        with open(text_file, 'w', encoding='utf-8') as f:
            f.write(full_text)
        
        log.info(f"✅ Extraction complete: {len(sections)} sections, {self._count_items(extracted_items)} items, {len(hyperlinks)} hyperlinks")
        log.info(f"✅ Saved extracted data to: {output_file}")
        
        return agenda_data
    
    def _extract_agenda_items_with_llm(self, text: str) -> List[Dict[str, any]]:
        """Use LLM to extract agenda items from the text."""
        # Split text into chunks if too long
        max_chars = 30000
        chunks = []
        
        if len(text) > max_chars:
            # Split by lines to avoid breaking mid-sentence
            lines = text.split('\n')
            current_chunk = []
            current_length = 0
            
            for line in lines:
                if current_length + len(line) > max_chars and current_chunk:
                    chunks.append('\n'.join(current_chunk))
                    current_chunk = [line]
                    current_length = len(line)
                else:
                    current_chunk.append(line)
                    current_length += len(line) + 1
            
            if current_chunk:
                chunks.append('\n'.join(current_chunk))
        else:
            chunks = [text]
        
        all_items = []
        
        for i, chunk in enumerate(chunks):
            log.info(f"Processing chunk {i+1}/{len(chunks)}")
            
            prompt = """Extract ALL agenda items from this city council agenda document. Look for ALL these formats:

- Letter.-Number. Reference (e.g., H.-1. 23-6819)
- Letter-Number Reference (e.g., H-1 23-6819)
- Empty sections marked as "None"

IMPORTANT: 
1. Extract EVERY section even if it says "None"
2. Look for ALL item formats including H.-1., H.-2., etc.
3. Include items without explicit ordinance/resolution text

For EACH section/item found, extract:
1. section_name: The section name (e.g., "CITY MANAGER ITEMS")
2. item_code: The item code (e.g., "H-1") - normalize to Letter-Number format
3. document_reference: The reference number (e.g., "23-6819")
4. title: The full description
5. has_items: true if section has items, false if "None"

Return a JSON array including both sections and items.

Document text:
""" + chunk
            
            try:
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "You are an expert at extracting structured data from city government agenda documents. Return only valid JSON."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.1,
                    max_tokens=32768
                )
                
                response_text = response.choices[0].message.content.strip()
                
                # Clean up response to ensure valid JSON
                if response_text.startswith('```json'):
                    response_text = response_text.replace('```json', '').replace('```', '')
                elif response_text.startswith('```'):
                    response_text = response_text.replace('```', '')
                
                response_text = response_text.strip()
                
                # Try to parse JSON
                try:
                    data = json.loads(response_text)
                    if isinstance(data, dict) and 'items' in data:
                        items = data['items']
                    elif isinstance(data, list):
                        items = data
                    else:
                        log.warning(f"Unexpected LLM response format: {type(data)}")
                        items = []
                        
                    all_items.extend(items)
                    log.info(f"Extracted {len(items)} items from chunk {i+1}")
                    
                except json.JSONDecodeError as e:
                    log.error(f"Failed to parse JSON from chunk {i+1}: {e}")
                    log.error(f"Raw response: {response_text[:200]}...")
                    # Try manual extraction as fallback
                    manual_sections = self._manual_extract_items(chunk)
                    # Flatten sections to items for consistency
                    manual_items = []
                    for section in manual_sections:
                        manual_items.extend(section.get('items', []))
                    all_items.extend(manual_items)
                    log.info(f"Manual fallback extracted {len(manual_items)} items from {len(manual_sections)} sections")
                    
            except Exception as e:
                log.error(f"LLM extraction failed for chunk {i+1}: {e}")
                # Fallback to manual extraction
                manual_sections = self._manual_extract_items(chunk)
                # Flatten sections to items for consistency
                manual_items = []
                for section in manual_sections:
                    manual_items.extend(section.get('items', []))
                all_items.extend(manual_items)
                log.info(f"Manual fallback extracted {len(manual_items)} items from {len(manual_sections)} sections")
        
        # Deduplicate items by item_code
        seen_codes = set()
        unique_items = []
        for item in all_items:
            if item.get('item_code') and item['item_code'] not in seen_codes:
                seen_codes.add(item['item_code'])
                unique_items.append(item)
        
        log.info(f"Total unique items extracted: {len(unique_items)}")
        return unique_items
    
    def _manual_extract_items(self, text: str) -> List[Dict[str, any]]:
        """Manually extract agenda items using regex patterns."""
        sections = []
        current_section = None
        
        # Updated section patterns to catch all sections
        section_patterns = [
            (r'^([A-Z])\.\s+(.+)$', 'SECTION'),  # Letter. Section Name
            (r'^(CITY MANAGER ITEMS?)$', 'CITY_MANAGER'),
            (r'^(CITY ATTORNEY ITEMS?)$', 'CITY_ATTORNEY'),
            (r'^(BOARDS?/COMMITTEES? ITEMS?)$', 'BOARDS_COMMITTEES'),
            (r'^(PRESENTATIONS AND PROTOCOL DOCUMENTS)', 'PRESENTATIONS'),
            (r'^(APPROVAL OF MINUTES)', 'MINUTES'),
            (r'^(PUBLIC COMMENTS)', 'PUBLIC_COMMENTS'),
            (r'^(CONSENT AGENDA)', 'CONSENT'),
            (r'^(PUBLIC HEARINGS)', 'PUBLIC_HEARINGS'),
            (r'^(RESOLUTIONS)', 'RESOLUTIONS'),
            (r'^(ORDINANCES.*)', 'ORDINANCES'),
            (r'^(DISCUSSION ITEMS)', 'DISCUSSION'),
            (r'^(BOARDS AND COMMITTEES)', 'BOARDS'),
        ]
        
        # Track if we're in a section that might have "None" as content
        in_section = False
        section_content_lines = []
        
        lines = text.split('\n')
        for i, line in enumerate(lines):
            line_stripped = line.strip()
            
            # Skip empty lines
            if not line_stripped:
                continue
            
            # Check for section headers
            section_found = False
            for pattern, section_type in section_patterns:
                if re.match(pattern, line_stripped, re.IGNORECASE):
                    # Process previous section if it exists
                    if current_section:
                        # Check if section only contains "None"
                        content = ' '.join(section_content_lines).strip()
                        if content.lower() == 'none' or not current_section['items']:
                            current_section['has_items'] = False
                        sections.append(current_section)
                    
                    # Start new section
                    current_section = {
                        'section_name': line_stripped,
                        'section_type': section_type,
                        'items': [],
                        'has_items': True
                    }
                    section_content_lines = []
                    in_section = True
                    section_found = True
                    break
            
            if section_found:
                continue
            
            # Collect section content
            if in_section and current_section:
                section_content_lines.append(line_stripped)
                
            # Updated item patterns to handle multiline items
            if current_section and re.match(r'^[A-Z]\.-\d+\.?\s*$', line_stripped):
                # Item code on its own line
                item_code = line_stripped.strip()
                # Look ahead for document reference
                if i + 1 < len(lines):
                    next_line = lines[i + 1].strip()
                    doc_ref_match = re.match(r'^(\d{2}-\d{4,5})', next_line)
                    if doc_ref_match:
                        doc_ref = doc_ref_match.group(1)
                        # Get title from remaining text or next lines
                        title_start = i + 1
                        title_lines = []
                        for j in range(title_start, min(i + 5, len(lines))):
                            title_line = lines[j].strip()
                            if title_line and not re.match(r'^[A-Z]\.-\d+\.?', title_line):
                                title_lines.append(title_line)
                        
                        title = ' '.join(title_lines)
                        # Remove the document reference from title
                        title = title.replace(doc_ref, '').strip()
                        
                        current_section['items'].append({
                            'item_code': item_code.rstrip('.'),
                            'document_reference': doc_ref,
                            'title': title,
                            'item_type': self._determine_item_type(title, current_section['section_type'])
                        })
                        log.info(f"Extracted multiline item: {item_code.rstrip('.')} - {doc_ref}")
                        continue
            
            # Original item patterns for single-line items
            item_patterns = [
                r'^([A-Z]\.-\d+\.?)\s+(\d{2}-\d{4,5})\s+(.+)$',  # A.-1. 23-6764
                r'^(\d+\.-\d+\.?)\s+(\d{2}-\d{4,5})\s+(.+)$',    # 1.-1. 23-6797
                r'^([A-Z]-\d+)\s+(\d{2}-\d{4,5})\s+(.+)$',       # E-1 23-6784
            ]
            
            for pattern in item_patterns:
                match = re.match(pattern, line_stripped)
                if match and current_section:
                    item_code = match.group(1)
                    doc_ref = match.group(2)
                    title = match.group(3).strip()
                    
                    # Determine item type based on title or section
                    item_type = self._determine_item_type(title, current_section.get("section_type", ""))
                    
                    item = {
                        "item_code": item_code.rstrip('.'),
                        "document_reference": doc_ref,
                        "title": title[:300],
                        "item_type": item_type
                    }
                    
                    current_section["items"].append(item)
                    log.info(f"Extracted single-line {item_type}: {item_code} - {doc_ref}")
                    break
        
        # Don't forget the last section
        if current_section:
            # Check if section only contains "None"
            content = ' '.join(section_content_lines).strip()
            if content.lower() == 'none' or not current_section['items']:
                current_section['has_items'] = False
            sections.append(current_section)
        
        total_items = sum(len(s['items']) for s in sections)
        log.info(f"Manual extraction complete: {total_items} items in {len(sections)} sections")
        
        return sections
    
    def _determine_item_type(self, title: str, section_type: str) -> str:
        """Determine item type from title and section."""
        title_lower = title.lower()
        
        # Check title first for explicit type
        if 'an ordinance' in title_lower:
            return 'Ordinance'
        elif 'a resolution' in title_lower:
            return 'Resolution'
        elif 'proclamation' in title_lower:
            return 'Proclamation'
        elif 'recognition' in title_lower:
            return 'Recognition'
        elif 'congratulations' in title_lower:
            return 'Recognition'
        elif 'presentation' in title_lower:
            return 'Presentation'
        elif section_type == 'PRESENTATIONS':
            return 'Presentation'
        elif section_type == 'MINUTES':
            return 'Minutes Approval'
        elif section_type == 'CITY_MANAGER':
            return 'City Manager Item'
        elif section_type == 'CITY_ATTORNEY':
            return 'City Attorney Item'
        elif section_type == 'BOARDS_COMMITTEES':
            return 'Board/Committee Item'
        else:
            return 'Agenda Item'  # Generic fallback

    def _build_sections_from_items(self, extracted_data: List[Dict], full_text: str) -> List[Dict[str, str]]:
        """Build sections structure from extracted items or sections."""
        if not extracted_data:
            # If no items found, return the full document as one section
            return [{
                'title': 'Full Document',
                'text': full_text
            }]
        
        # Check if we have sections (from manual extraction) or items (from LLM)
        if extracted_data and isinstance(extracted_data[0], dict) and 'section_name' in extracted_data[0]:
            # We have sections from manual extraction
            sections = []
            for section_data in extracted_data:
                section_text_parts = []
                section_text_parts.append(f"=== {section_data['section_name']} ===\n")
                
                if section_data.get('has_items', True) and section_data.get('items'):
                    for item in section_data['items']:
                        item_text = f"{item['item_code']} - {item['document_reference']}\n{item['title']}\n"
                        section_text_parts.append(item_text)
                else:
                    section_text_parts.append("None\n")
                
                sections.append({
                    'title': section_data['section_name'],
                    'text': '\n'.join(section_text_parts)
                })
            
            return sections
        else:
            # We have items from LLM extraction - group them
            sections = []
            
            # Create agenda items section
            agenda_section_text = []
            for item in extracted_data:
                item_text = f"{item.get('item_code', 'Unknown')} - {item.get('document_reference', 'Unknown')}\n{item.get('title', 'Unknown')}\n"
                agenda_section_text.append(item_text)
            
            sections.append({
                'title': 'AGENDA ITEMS',
                'text': '\n'.join(agenda_section_text)
            })
            
            return sections
    
    def _extract_hyperlinks(self, doc) -> Dict[str, Dict[str, any]]:
        """Extract hyperlinks from the document."""
        hyperlinks = {}
        
        # Try to extract links from document structure
        if hasattr(doc, 'links'):
            for link in doc.links:
                if hasattr(link, 'text') and hasattr(link, 'url'):
                    hyperlinks[link.text] = {
                        'url': link.url,
                        'page': getattr(link, 'page', 0)
                    }
        
        # Try to extract from markdown if links are preserved there
        if hasattr(doc, 'export_to_markdown'):
            markdown = doc.export_to_markdown()
            # Extract markdown links pattern [text](url)
            link_pattern = r'\[([^\]]+)\]\(([^)]+)\)'
            for match in re.finditer(link_pattern, markdown):
                text, url = match.groups()
                if text and url:
                    hyperlinks[text] = {
                        'url': url,
                        'page': 0  # We don't have page info from markdown
                    }
        
        return hyperlinks 

    def _count_items(self, extracted_data: List[Dict]) -> int:
        """Count the number of items in extracted data (items or sections with items)."""
        if not extracted_data:
            return 0
        
        # Check if we have sections or items
        if extracted_data and isinstance(extracted_data[0], dict) and 'section_name' in extracted_data[0]:
            # We have sections - count items within them
            total_items = 0
            for section in extracted_data:
                total_items += len(section.get('items', []))
            return total_items
        else:
            # We have items directly
            return len(extracted_data)


================================================================================


################################################################################
# File: scripts/graph_stages/cosmos_db_client.py
################################################################################

# File: scripts/graph_stages/cosmos_db_client.py

"""
Azure Cosmos DB Gremlin client for city clerk graph database.
Provides async operations for graph manipulation.
"""

from __future__ import annotations
import asyncio
import logging
from typing import Any, Dict, List, Optional, Union
import os
from gremlin_python.driver import client, serializer
from gremlin_python.driver.driver_remote_connection import DriverRemoteConnection
from gremlin_python.structure.graph import Graph
from dotenv import load_dotenv
import json

load_dotenv()

log = logging.getLogger('cosmos_graph_client')


class CosmosGraphClient:
    """Async client for Azure Cosmos DB Gremlin API."""
    
    def __init__(self, 
                 endpoint: Optional[str] = None,
                 key: Optional[str] = None,
                 database: Optional[str] = None,
                 container: Optional[str] = None,
                 partition_value: str = "demo"):
        """Initialize Cosmos DB client."""
        self.endpoint = endpoint or os.getenv("COSMOS_ENDPOINT")
        self.key = key or os.getenv("COSMOS_KEY")
        self.database = database or os.getenv("COSMOS_DATABASE", "cgGraph")
        self.container = container or os.getenv("COSMOS_CONTAINER", "cityClerk")
        self.partition_value = partition_value
        
        if not all([self.endpoint, self.key, self.database, self.container]):
            raise ValueError("Missing required Cosmos DB configuration")
        
        self._client = None
        self._loop = None  # Don't get the loop in __init__
    
    async def connect(self) -> None:
        """Establish connection to Cosmos DB."""
        try:
            # Get the current running loop
            self._loop = asyncio.get_running_loop()
            
            self._client = client.Client(
                f"{self.endpoint}/gremlin",
                "g",
                username=f"/dbs/{self.database}/colls/{self.container}",
                password=self.key,
                message_serializer=serializer.GraphSONSerializersV2d0()
            )
            log.info(f"✅ Connected to Cosmos DB: {self.database}/{self.container}")
        except Exception as e:
            log.error(f"❌ Failed to connect to Cosmos DB: {e}")
            raise
    
    async def _execute_query(self, query: str, bindings: Optional[Dict] = None) -> List[Any]:
        """Execute a Gremlin query asynchronously."""
        if not self._client:
            await self.connect()
        
        try:
            # Get the current event loop
            loop = asyncio.get_running_loop()
            
            # Run synchronous operation in thread pool
            future = loop.run_in_executor(
                None,
                lambda: self._client.submit(query, bindings or {})
            )
            callback = await future
            
            # Collect results
            results = []
            for result in callback:
                results.extend(result)
            
            return results
        except Exception as e:
            log.error(f"Query execution failed: {query[:100]}... Error: {e}")
            raise
    
    async def clear_graph(self) -> None:
        """Clear all vertices and edges from the graph."""
        log.warning("🗑️  Clearing entire graph...")
        try:
            # Drop all vertices (edges are automatically removed)
            await self._execute_query("g.V().drop()")
            log.info("✅ Graph cleared successfully")
        except Exception as e:
            log.error(f"Failed to clear graph: {e}")
            raise
    
    async def create_vertex(self, 
                          label: str,
                          vertex_id: str,
                          properties: Dict[str, Any],
                          update_if_exists: bool = True) -> None:
        """Create a vertex with properties, optionally updating if exists."""
        
        # Check if vertex already exists
        if await self.vertex_exists(vertex_id):
            if update_if_exists:
                # Update existing vertex
                await self.update_vertex(vertex_id, properties)
                log.info(f"Updated existing vertex: {vertex_id}")
            else:
                log.info(f"Vertex already exists, skipping: {vertex_id}")
            return
        
        # Build property chain
        prop_chain = ""
        for key, value in properties.items():
            if value is not None:
                # Handle different value types
                if isinstance(value, bool):
                    prop_chain += f".property('{key}', {str(value).lower()})"
                elif isinstance(value, (int, float)):
                    prop_chain += f".property('{key}', {value})"
                elif isinstance(value, list):
                    # Convert list to JSON string
                    json_val = json.dumps(value).replace("'", "\\'")
                    prop_chain += f".property('{key}', '{json_val}')"
                else:
                    # Escape string values
                    escaped_val = str(value).replace("'", "\\'").replace('"', '\\"')
                    prop_chain += f".property('{key}', '{escaped_val}')"
        
        # Always add partition key
        prop_chain += f".property('partitionKey', '{self.partition_value}')"
        
        query = f"g.addV('{label}').property('id', '{vertex_id}'){prop_chain}"
        
        await self._execute_query(query)

    async def update_vertex(self, vertex_id: str, properties: Dict[str, Any]) -> None:
        """Update properties of an existing vertex."""
        # Build property update chain
        prop_chain = ""
        for key, value in properties.items():
            if value is not None:
                if isinstance(value, bool):
                    prop_chain += f".property('{key}', {str(value).lower()})"
                elif isinstance(value, (int, float)):
                    prop_chain += f".property('{key}', {value})"
                elif isinstance(value, list):
                    json_val = json.dumps(value).replace("'", "\\'")
                    prop_chain += f".property('{key}', '{json_val}')"
                else:
                    escaped_val = str(value).replace("'", "\\'").replace('"', '\\"')
                    prop_chain += f".property('{key}', '{escaped_val}')"
        
        query = f"g.V('{vertex_id}'){prop_chain}"
        
        try:
            await self._execute_query(query)
            log.info(f"Updated vertex {vertex_id}")
        except Exception as e:
            log.error(f"Failed to update vertex {vertex_id}: {e}")
            raise

    async def upsert_vertex(self, 
                           label: str,
                           vertex_id: str,
                           properties: Dict[str, Any]) -> bool:
        """Create or update a vertex. Returns True if created, False if updated."""
        # Check if vertex exists
        if await self.vertex_exists(vertex_id):
            await self.update_vertex(vertex_id, properties)
            return False  # Updated
        else:
            await self.create_vertex(label, vertex_id, properties)
            return True  # Created

    async def create_edge(self,
                         from_id: str,
                         to_id: str,
                         edge_type: str,
                         properties: Optional[Dict[str, Any]] = None) -> None:
        """Create an edge between two vertices."""
        # Build property chain for edge
        prop_chain = ""
        if properties:
            for key, value in properties.items():
                if value is not None:
                    if isinstance(value, bool):
                        prop_chain += f".property('{key}', {str(value).lower()})"
                    elif isinstance(value, (int, float)):
                        prop_chain += f".property('{key}', {value})"
                    else:
                        escaped_val = str(value).replace("'", "\\'")
                        prop_chain += f".property('{key}', '{escaped_val}')"
        
        query = f"g.V('{from_id}').addE('{edge_type}').to(g.V('{to_id}')){prop_chain}"
        
        try:
            await self._execute_query(query)
        except Exception as e:
            log.error(f"Failed to create edge {from_id} -> {to_id}: {e}")
            raise
    
    async def create_edge_if_not_exists(self,
                                       from_id: str,
                                       to_id: str,
                                       edge_type: str,
                                       properties: Optional[Dict[str, Any]] = None) -> bool:
        """Create an edge if it doesn't already exist. Returns True if created."""
        # Check if edge already exists
        check_query = f"g.V('{from_id}').outE('{edge_type}').where(inV().hasId('{to_id}')).count()"
        
        try:
            result = await self._execute_query(check_query)
            exists = result[0] > 0 if result else False
            
            if not exists:
                await self.create_edge(from_id, to_id, edge_type, properties)
                return True
            else:
                log.debug(f"Edge already exists: {from_id} -[{edge_type}]-> {to_id}")
                return False
        except Exception as e:
            log.error(f"Failed to check/create edge: {e}")
            raise
    
    async def vertex_exists(self, vertex_id: str) -> bool:
        """Check if a vertex exists."""
        result = await self._execute_query(f"g.V('{vertex_id}').count()")
        return result[0] > 0 if result else False
    
    async def get_vertex(self, vertex_id: str) -> Optional[Dict]:
        """Get a vertex by ID."""
        result = await self._execute_query(f"g.V('{vertex_id}').valueMap(true)")
        return result[0] if result else None
    
    async def close(self) -> None:
        """Close the connection properly."""
        if self._client:
            try:
                # Close the client synchronously since it's not async
                self._client.close()
                self._client = None
                log.info("Connection closed")
            except Exception as e:
                log.warning(f"Error during client close: {e}")
    
    async def __aenter__(self):
        await self.connect()
        return self
    
    async def __aexit__(self, *args):
        await self.close()


================================================================================


################################################################################
# File: scripts/graph_stages/pdf_extractor.py
################################################################################

# scripts/graph_stages/pdf_extractor.py

import os
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import logging
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.datamodel.base_models import InputFormat, DocumentStream
from docling.datamodel.pipeline_options import PdfPipelineOptions
import json
from io import BytesIO

# Set up logging
logging.basicConfig(level=logging.INFO)
log = logging.getLogger(__name__)

class PDFExtractor:
    """Extract text from PDFs using Docling for accurate OCR and text extraction."""
    
    def __init__(self, pdf_dir: Path, output_dir: Optional[Path] = None):
        """Initialize the PDF extractor with Docling."""
        self.pdf_dir = Path(pdf_dir)
        self.output_dir = output_dir or Path("city_clerk_documents/extracted_text")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Create debug directory
        self.debug_dir = self.output_dir / "debug"
        self.debug_dir.mkdir(exist_ok=True)
        
        # Initialize Docling converter with OCR enabled
        # Configure for better OCR and structure detection
        pipeline_options = PdfPipelineOptions()
        pipeline_options.do_ocr = True  # Enable OCR for better text extraction
        pipeline_options.do_table_structure = True  # Better table extraction
        
        self.converter = DocumentConverter(
            format_options={
                InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
            }
        )
    
    def extract_text_from_pdf(self, pdf_path: Path) -> Tuple[str, List[Dict[str, str]]]:
        """
        Extract text from PDF using Docling with OCR support.
        
        Returns:
            Tuple of (full_text, pages) where pages is a list of dicts with 'text' and 'page_num'
        """
        log.info(f"📄 Extracting text from: {pdf_path.name}")
        
        # Convert PDF with Docling - just pass the path directly
        result = self.converter.convert(str(pdf_path))
        
        # Get the document
        doc = result.document
        
        # Get the markdown representation which preserves structure better
        full_markdown = doc.export_to_markdown()
        
        # Extract pages if available
        pages = []
        
        # Try to extract page-level content from the document structure
        if hasattr(doc, 'pages') and doc.pages:
            for page_num, page in enumerate(doc.pages, 1):
                # Get page text
                page_text = ""
                if hasattr(page, 'text'):
                    page_text = page.text
                elif hasattr(page, 'get_text'):
                    page_text = page.get_text()
                else:
                    # Try to extract from elements
                    page_elements = []
                    if hasattr(page, 'elements'):
                        for element in page.elements:
                            if hasattr(element, 'text'):
                                page_elements.append(element.text)
                    page_text = "\n".join(page_elements)
                
                if page_text:
                    pages.append({
                        'text': page_text,
                        'page_num': page_num
                    })
        
        # If we couldn't extract pages, create one page with all content
        if not pages and full_markdown:
            pages = [{
                'text': full_markdown,
                'page_num': 1
            }]
        
        # Use markdown as the full text (better structure preservation)
        full_text = full_markdown if full_markdown else ""
        
        # Save debug information
        debug_info = {
            'file': pdf_path.name,
            'total_pages': len(pages),
            'total_characters': len(full_text),
            'extraction_method': 'docling',
            'has_markdown': bool(full_markdown),
            'doc_attributes': list(dir(doc)) if doc else []
        }
        
        debug_file = self.debug_dir / f"{pdf_path.stem}_extraction_debug.json"
        with open(debug_file, 'w', encoding='utf-8') as f:
            json.dump(debug_info, f, indent=2)
        
        # Save the full extracted text for inspection
        text_file = self.debug_dir / f"{pdf_path.stem}_full_text.txt"
        with open(text_file, 'w', encoding='utf-8') as f:
            f.write(full_text)
        
        # Save markdown version separately for debugging
        if full_markdown:
            markdown_file = self.debug_dir / f"{pdf_path.stem}_markdown.md"
            with open(markdown_file, 'w', encoding='utf-8') as f:
                f.write(full_markdown)
        
        log.info(f"✅ Extracted {len(pages)} pages, {len(full_text)} characters")
        
        return full_text, pages
    
    def extract_all_pdfs(self) -> Dict[str, Dict[str, any]]:
        """Extract text from all PDFs in the directory."""
        pdf_files = list(self.pdf_dir.glob("*.pdf"))
        log.info(f"📚 Found {len(pdf_files)} PDF files to process")
        
        extracted_data = {}
        
        for pdf_path in pdf_files:
            try:
                full_text, pages = self.extract_text_from_pdf(pdf_path)
                
                extracted_data[pdf_path.stem] = {
                    'full_text': full_text,
                    'pages': pages,
                    'metadata': {
                        'filename': pdf_path.name,
                        'num_pages': len(pages),
                        'total_chars': len(full_text),
                        'extraction_method': 'docling'
                    }
                }
                
                # Save extracted text
                output_file = self.output_dir / f"{pdf_path.stem}_extracted.json"
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(extracted_data[pdf_path.stem], f, indent=2, ensure_ascii=False)
                
                log.info(f"✅ Saved extracted text to: {output_file}")
                
            except Exception as e:
                log.error(f"❌ Failed to process {pdf_path.name}: {e}")
                import traceback
                traceback.print_exc()
                # No fallback - if Docling fails, we fail
                raise
        
        return extracted_data

# Add this to requirements.txt:
# unstructured[pdf]==0.10.30
# pytesseract>=0.3.10
# pdf2image>=1.16.3
# poppler-utils (system dependency - install with: brew install poppler)


================================================================================


################################################################################
# File: graph_clear_database.py
################################################################################

# File: graph_clear_database.py

#!/usr/bin/env python3
"""
Clear Cosmos DB Graph Database
This script will clear all vertices and edges from the graph database.
"""

import asyncio
import sys
import os
from pathlib import Path

# Add scripts directory to path
script_dir = Path(__file__).parent / 'scripts'
sys.path.append(str(script_dir))

from graph_stages.cosmos_db_client import CosmosGraphClient

async def clear_database():
    """Clear the entire graph database."""
    print('🗑️  Clearing Cosmos DB graph database...')
    print('⚠️  This will delete ALL vertices and edges!')
    
    try:
        async with CosmosGraphClient() as client:
            await client.clear_graph()
        print('✅ Graph database cleared successfully!')
        return True
        
    except Exception as e:
        print(f'❌ Error clearing database: {e}')
        return False

if __name__ == "__main__":
    success = asyncio.run(clear_database())
    if not success:
        sys.exit(1)


================================================================================


################################################################################
# File: requirements.txt
################################################################################

################################################################################
################################################################################
# Python dependencies for Misophonia Research RAG System

# Core dependencies
openai>=1.82.0
groq>=0.4.1
gremlinpython>=3.7.3
aiofiles>=24.1.0
python-dotenv>=1.0.0

# Graph Visualization dependencies
dash>=2.14.0
plotly>=5.17.0
networkx>=3.2.0
dash-table>=5.0.0
dash-cytoscape>=0.3.0

# Database and API dependencies
supabase>=2.0.0

# Optional for async progress bars
tqdm

# Data processing
pandas>=2.0.0
numpy

# PDF processing
PyPDF2>=3.0.0
unstructured[pdf]==0.10.30
pytesseract>=0.3.10
pdf2image>=1.16.3
docling
pdfminer.six>=20221105

# PDF processing with hyperlink support
PyMuPDF>=1.23.0

# Utilities
colorama>=0.4.6

# For concurrent processing
concurrent-log-handler>=0.9.20

# Async dependencies for optimized pipeline
aiohttp>=3.8.0

# Token counting for OpenAI rate limiting
tiktoken>=0.5.0


================================================================================

