# Concatenated Project Code - Part 2 of 3
# Generated: 2025-06-12 12:11:13
# Root Directory: /Users/gianmariatroiani/Documents/knologi/graph_database
================================================================================

# File Index - Which Files Are in Which Parts
################################################################################

## Part 1 (10 files):
  - scripts/microsoft_framework/query_engine.py
  - scripts/graph_stages/agenda_ontology_extractor.py
  - graphrag_query_ui.py
  - scripts/microsoft_framework/query_router.py
  - scripts/graph_stages/document_linker.py
  - scripts/microsoft_framework/README.md
  - scripts/microsoft_framework/cosmos_synchronizer.py
  - extract_documents_for_graphrag.py
  - scripts/microsoft_framework/city_clerk_settings_template.yaml
  - config.py

## Part 2 (11 files):
  - scripts/graph_stages/agenda_graph_builder.py
  - scripts/graph_stages/agenda_pdf_extractor.py
  - scripts/graph_stages/enhanced_document_linker.py
  - scripts/microsoft_framework/run_graphrag_pipeline.py
  - scripts/graph_stages/cosmos_db_client.py
  - scripts/microsoft_framework/graphrag_output_processor.py
  - scripts/microsoft_framework/graphrag_initializer.py
  - settings.yaml
  - scripts/microsoft_framework/graphrag_pipeline.py
  - scripts/microsoft_framework/__init__.py
  - scripts/graph_stages/__init__.py

## Part 3 (10 files):
  - scripts/microsoft_framework/enhanced_entity_deduplicator.py
  - scripts/graph_stages/ontology_extractor.py
  - scripts/microsoft_framework/document_adapter.py
  - scripts/graph_stages/verbatim_transcript_linker.py
  - scripts/extract_all_to_markdown.py
  - scripts/microsoft_framework/prompt_tuner.py
  - scripts/graph_stages/pdf_extractor.py
  - scripts/microsoft_framework/source_tracker.py
  - scripts/microsoft_framework/incremental_processor.py
  - requirements.txt


================================================================================


################################################################################
# File: scripts/graph_stages/agenda_graph_builder.py
################################################################################

# File: scripts/graph_stages/agenda_graph_builder.py

"""
Enhanced Agenda Graph Builder - RICH VERSION
Builds comprehensive graph representation from LLM-extracted agenda ontology.
"""
import logging
from typing import Dict, List, Optional, Any
from pathlib import Path
import hashlib
import json
import calendar
import re

from .cosmos_db_client import CosmosGraphClient

log = logging.getLogger('pipeline_debug.graph_builder')


class AgendaGraphBuilder:
    """Build comprehensive graph representation from rich agenda ontology."""
    
    def __init__(self, cosmos_client: CosmosGraphClient, upsert_mode: bool = True):
        self.cosmos = cosmos_client
        self.upsert_mode = upsert_mode
        self.entity_id_cache = {}  # Cache for entity IDs
        self.partition_value = 'demo'  # Partition value property
        
        # Track statistics
        self.stats = {
            'nodes_created': 0,
            'nodes_updated': 0,
            'edges_created': 0,
            'edges_skipped': 0
        }
    
    @staticmethod
    def normalize_item_code(code: str) -> str:
        """Normalize item codes to consistent format for matching."""
        if not code:
            return code
        
        # Log original code for debugging
        original = code
        
        # Apply normalization patterns
        patterns = [
            (r'^([A-Z])\.?-?(\d+)\.?$', r'\1-\2'),     # A.-1. -> A-1
            (r'^(\d+)\.?-?(\d+)\.?$', r'\1-\2'),       # 1.-1. -> 1-1
            (r'^([A-Z]\d+)$', r'\1'),                   # E1 -> E1 (no change)
        ]
        
        for pattern, replacement in patterns:
            match = re.match(pattern, code)
            if match:
                code = re.sub(pattern, replacement, code)
                break
        else:
            # First, extract valid code pattern if input is messy
            code_match = re.match(r'^([A-Z][-.]?\d+)', code)
            if code_match:
                code = code_match.group(1)
            
            # Remove all dots and ensure consistent format
            code = code.rstrip('.')
            code = re.sub(r'([A-Z])\.(-)', r'\1\2', code)
            code = re.sub(r'([A-Z])\.(\d)', r'\1-\2', code)
            code = re.sub(r'([A-Z])(\d)', r'\1-\2', code)
            code = code.replace('.', '')
            
            # Ensure format is always "E-9" not "E9"
            if re.match(r'^[A-Z]\d+$', code):
                code = f"{code[0]}-{code[1:]}"
        
        if original != code:
            log.debug(f"Normalized '{original}' -> '{code}'")
        
        return code
    
    @staticmethod
    def ensure_us_date_format(date_str: str) -> str:
        """Ensure date is in US format MM-DD-YYYY with dashes."""
        # Handle different input formats
        if '.' in date_str:
            # Format: 01.23.2024 -> 01-23-2024
            return date_str.replace('.', '-')
        elif '/' in date_str:
            # Format: 01/23/2024 -> 01-23-2024
            return date_str.replace('/', '-')
        elif re.match(r'^\d{4}-\d{2}-\d{2}$', date_str):
            # ISO format: 2024-01-23 -> 01-23-2024
            parts = date_str.split('-')
            return f"{parts[1]}-{parts[2]}-{parts[0]}"
        else:
            # Already in correct format or unknown
            return date_str

    async def build_graph(self, ontology_file: Path, linked_docs: Optional[Dict] = None, upsert: bool = True) -> Dict:
        """Build graph from ontology file - main entry point."""
        # Load ontology
        with open(ontology_file, 'r', encoding='utf-8') as f:
            ontology = json.load(f)
        
        return await self.build_graph_from_ontology(ontology, ontology_file, linked_docs)

    async def build_graph_from_ontology(self, ontology: Dict, source_path: Path, linked_docs: Optional[Dict] = None) -> Dict:
        """Build comprehensive graph representation from rich ontology."""
        log.info(f"ðŸ”¨ Starting enhanced graph build for {source_path.name}")
        log.info(f"ðŸ”§ Upsert mode: {'ENABLED' if self.upsert_mode else 'DISABLED'}")
        
        # Reset statistics
        self.stats = {
            'nodes_created': 0,
            'nodes_updated': 0,
            'edges_created': 0,
            'edges_skipped': 0
        }
        
        try:
            graph_data = {
                'nodes': {},
                'edges': [],
                'statistics': {}
            }
            
            # Store agenda structure for reference
            self.current_agenda_structure = ontology.get('agenda_structure', [])
            
            # Store ontology for reference
            self.current_ontology = ontology
            
            # CRITICAL: Ensure meeting date is in US format
            meeting_date_original = ontology['meeting_date']
            meeting_date_us = self.ensure_us_date_format(meeting_date_original)
            meeting_info = ontology['meeting_info']
            
            log.info(f"ðŸ“… Meeting date: {meeting_date_original} -> {meeting_date_us}")
            
            # 1. Create Meeting node as the root
            meeting_id = f"meeting-{meeting_date_us}"
            await self._create_meeting_node(meeting_date_us, meeting_info, source_path.name)
            log.info(f"âœ… Created meeting node: {meeting_id}")
            
            # 1.5 Create Date node and link to meeting
            try:
                date_id = await self._create_date_node(meeting_date_original, meeting_id)
                graph_data['nodes'][date_id] = {
                    'type': 'Date',
                    'date': meeting_date_original
                }
            except Exception as e:
                log.error(f"Failed to create date node: {e}")
            
            graph_data['nodes'][meeting_id] = {
                'type': 'Meeting',
                'date': meeting_date_us,
                'info': meeting_info
            }
            
            # 2. Create nodes for officials present  
            await self._create_official_nodes(meeting_info, meeting_id)
            
            # 3. Process sections and agenda items
            section_count = 0
            item_count = 0
            
            sections = ontology.get('sections', [])
            log.info(f"ðŸ“‘ Processing {len(sections)} sections")
            
            for section_idx, section in enumerate(sections):
                try:
                    section_count += 1
                    section_id = f"section-{meeting_date_us}-{section_idx}"
                    
                    # Create Section node
                    await self._create_section_node(section_id, section, section_idx)
                    log.info(f"âœ… Created section {section_idx}: {section.get('section_name', 'Unknown')}")
                    
                    graph_data['nodes'][section_id] = {
                        'type': 'Section',
                        'name': section['section_name'],
                        'order': section_idx
                    }
                    
                    # Link section to meeting
                    if await self.cosmos.create_edge_if_not_exists(
                        from_id=meeting_id,
                        to_id=section_id,
                        edge_type='HAS_SECTION',
                        properties={'order': section_idx}
                    ):
                        self.stats['edges_created'] += 1
                    else:
                        self.stats['edges_skipped'] += 1
                    
                    # Process items in section
                    previous_item_id = None
                    items = section.get('items', [])
                    
                    for item_idx, item in enumerate(items):
                        try:
                            if not item.get('item_code'):
                                log.warning(f"Skipping item without code in section {section['section_name']}")
                                continue
                                
                            item_count += 1
                            # Normalize the item code
                            normalized_code = self.normalize_item_code(item['item_code'])
                            # Use US date format for item ID
                            item_id = f"item-{meeting_date_us}-{normalized_code}"
                            
                            log.info(f"Creating item: {item_id} (from code: {item['item_code']})")
                            
                            # Create enhanced AgendaItem node
                            await self._create_enhanced_agenda_item_node(item_id, item, section)
                            
                            graph_data['nodes'][item_id] = {
                                'type': 'AgendaItem',
                                'code': normalized_code,
                                'original_code': item['item_code'],
                                'title': item.get('title', 'Unknown')
                            }
                            
                            # Link item to section
                            if await self.cosmos.create_edge_if_not_exists(
                                from_id=section_id,
                                to_id=item_id,
                                edge_type='CONTAINS_ITEM',
                                properties={'order': item_idx}
                            ):
                                self.stats['edges_created'] += 1
                            else:
                                self.stats['edges_skipped'] += 1
                            
                            # Create sequential relationships
                            if previous_item_id:
                                if await self.cosmos.create_edge_if_not_exists(
                                    from_id=previous_item_id,
                                    to_id=item_id,
                                    edge_type='FOLLOWS',
                                    properties={'sequence': item_idx}
                                ):
                                    self.stats['edges_created'] += 1
                                else:
                                    self.stats['edges_skipped'] += 1
                            
                            previous_item_id = item_id
                            
                            # Create rich relationships for this item
                            await self._create_item_relationships(item, item_id, meeting_date_us)
                            
                            # Create URL nodes and relationships
                            await self._create_url_relationships(item, item_id)
                                
                        except Exception as e:
                            log.error(f"Failed to process item {item.get('item_code', 'unknown')}: {e}")
                            
                except Exception as e:
                    log.error(f"Failed to process section {section.get('section_name', 'unknown')}: {e}")
            
            # 4. Create entity nodes from extracted entities
            entity_count = await self._create_entity_nodes(ontology.get('entities', []), meeting_id)
            
            # 5. Create relationships from ontology
            relationship_count = 0
            for rel in ontology.get('relationships', []):
                try:
                    await self._create_ontology_relationship(rel, meeting_date_us)
                    relationship_count += 1
                except Exception as e:
                    log.error(f"Failed to create relationship: {e}")
            
            # Initialize verbatim count
            verbatim_count = 0
            
            # 6. Process linked documents if available
            if linked_docs:
                await self.process_linked_documents(linked_docs, meeting_id, meeting_date_us)
                
                # Process verbatim transcripts if available
                if "verbatim_transcripts" in linked_docs:
                    verbatim_count = await self.process_verbatim_transcripts(
                        linked_docs["verbatim_transcripts"], 
                        meeting_id, 
                        meeting_date_us
                    )
            
            # Update statistics
            graph_data['statistics'] = {
                'sections': section_count,
                'items': item_count, 
                'entities': entity_count,
                'relationships': relationship_count,
                'meeting_date': meeting_date_us,
                'verbatim_transcripts': verbatim_count if verbatim_count else 0
            }
            
            log.info(f"ðŸŽ‰ Enhanced graph build complete for {source_path.name}")
            log.info(f"   ðŸ“Š Statistics:")
            log.info(f"      - Nodes created: {self.stats['nodes_created']}")
            log.info(f"      - Nodes updated: {self.stats['nodes_updated']}")
            log.info(f"      - Edges created: {self.stats['edges_created']}")
            log.info(f"      - Edges skipped: {self.stats['edges_skipped']}")
            log.info(f"   - Sections: {section_count}")
            log.info(f"   - Items: {item_count}")
            log.info(f"   - Entities: {entity_count}")
            log.info(f"   - Relationships: {relationship_count}")
            
            return graph_data
            
        except Exception as e:
            log.error(f"CRITICAL ERROR in build_graph_from_ontology: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    async def _create_meeting_node(self, meeting_date: str, meeting_info: Dict, source_file: str = None) -> str:
        """Create or update Meeting node with comprehensive metadata."""
        meeting_id = f"meeting-{meeting_date}"
        
        # Handle case where meeting_info might be a list (API response error)
        if isinstance(meeting_info, list):
            log.error(f"meeting_info is a list instead of dict: {meeting_info}")
            # Use default values
            meeting_info = {
                'type': 'Regular Meeting',
                'time': '5:30 PM',
                'location': 'City Commission Chambers',
                'commissioners': [],
                'officials': {}
            }
        
        # Handle location - could be string or dict
        location = meeting_info.get('location', 'City Commission Chambers')
        if isinstance(location, dict):
            location_str = f"{location.get('name', 'City Commission Chambers')}"
            if location.get('address'):
                location_str += f" - {location['address']}"
        else:
            location_str = str(location) if location else "City Commission Chambers"
        
        properties = {
            'nodeType': 'Meeting',
            'date': meeting_date,
            'type': meeting_info.get('type', 'Regular Meeting'),
            'time': meeting_info.get('time', '5:30 PM'),
            'location': location_str
        }
        
        if source_file:
            properties['source_file'] = source_file
        
        # Use upsert instead of create
        if self.upsert_mode:
            created = await self.cosmos.upsert_vertex('Meeting', meeting_id, properties)
            if created:
                self.stats['nodes_created'] += 1
                log.info(f"âœ… Created Meeting node: {meeting_id}")
            else:
                self.stats['nodes_updated'] += 1
                log.info(f"ðŸ“ Updated Meeting node: {meeting_id}")
        else:
            await self.cosmos.create_vertex('Meeting', meeting_id, properties)
            self.stats['nodes_created'] += 1
            log.info(f"âœ… Created Meeting node: {meeting_id}")
        
        return meeting_id
    
    async def _create_date_node(self, date_str: str, meeting_id: str) -> str:
        """Create a Date node and link it to the meeting."""
        from datetime import datetime
        
        # Parse date from MM.DD.YYYY format
        parts = date_str.split('.')
        if len(parts) != 3:
            log.error(f"Invalid date format: {date_str}")
            return None
            
        month, day, year = int(parts[0]), int(parts[1]), int(parts[2])
        
        # Create consistent date ID in ISO format
        date_id = f"date-{year:04d}-{month:02d}-{day:02d}"
        
        # Check if date already exists
        if await self.cosmos.vertex_exists(date_id):
            log.info(f"Date {date_id} already exists")
            # Still create the relationship
            await self.cosmos.create_edge(
                from_id=meeting_id,
                to_id=date_id,
                edge_type='OCCURRED_ON',
                properties={'primary_date': True}
            )
            return date_id
        
        # Get day of week
        date_obj = datetime(year, month, day)
        day_of_week = date_obj.strftime('%A')
        
        # Create date node
        properties = {
            'nodeType': 'Date',
            'full_date': date_str,
            'year': year,
            'month': month,
            'day': day,
            'quarter': (month - 1) // 3 + 1,
            'month_name': calendar.month_name[month],
            'day_of_week': day_of_week,
            'iso_date': f'{year:04d}-{month:02d}-{day:02d}'
        }
        
        await self.cosmos.create_vertex('Date', date_id, properties)
        log.info(f"âœ… Created Date node: {date_id}")
        
        # Create relationship: Meeting -> OCCURRED_ON -> Date
        await self.cosmos.create_edge(
            from_id=meeting_id,
            to_id=date_id,
            edge_type='OCCURRED_ON',
            properties={'primary_date': True}
        )
        
        return date_id
    
    async def _create_section_node(self, section_id: str, section: Dict, order: int) -> str:
        """Create or update Section node."""
        properties = {
            'nodeType': 'Section',
            'title': section.get('section_name', 'Unknown'),
            'type': section.get('section_type', 'OTHER'),
            'description': section.get('description', ''),
            'order': order,
            'is_empty': len(section.get('items', [])) == 0  # Mark empty sections
        }
        
        if self.upsert_mode:
            created = await self.cosmos.upsert_vertex('Section', section_id, properties)
            if created:
                self.stats['nodes_created'] += 1
            else:
                self.stats['nodes_updated'] += 1
        else:
            if await self.cosmos.vertex_exists(section_id):
                log.info(f"Section {section_id} already exists, skipping creation")
                return section_id
            await self.cosmos.create_vertex('Section', section_id, properties)
            self.stats['nodes_created'] += 1
        
        # If section is empty, create a placeholder item
        if properties['is_empty']:
            empty_item_id = f"{section_id}-none"
            await self._create_empty_item_node(empty_item_id, section_id, "None")
        
        return section_id

    async def _create_empty_item_node(self, item_id: str, section_id: str, placeholder_text: str) -> str:
        """Create a placeholder node for empty sections."""
        properties = {
            'nodeType': 'AgendaItem',
            'code': 'NONE',
            'original_code': 'NONE',
            'title': placeholder_text,
            'type': 'Empty Section',
            'section': section_id,
            'is_placeholder': True
        }
        
        if self.upsert_mode:
            created = await self.cosmos.upsert_vertex('AgendaItem', item_id, properties)
            if created:
                self.stats['nodes_created'] += 1
            else:
                self.stats['nodes_updated'] += 1
        else:
            await self.cosmos.create_vertex('AgendaItem', item_id, properties)
            self.stats['nodes_created'] += 1
        
        # Link placeholder item to section
        if await self.cosmos.create_edge_if_not_exists(
            from_id=section_id,
            to_id=item_id,
            edge_type='CONTAINS_ITEM',
            properties={'order': 0, 'is_placeholder': True}
        ):
            self.stats['edges_created'] += 1
        else:
            self.stats['edges_skipped'] += 1
        
        return item_id
    
    async def _create_enhanced_agenda_item_node(self, item_id: str, item: Dict, section: Dict) -> str:
        """Create or update AgendaItem node with rich metadata from LLM extraction."""
        # Store both original and normalized codes
        original_code = item.get('item_code', '')
        normalized_code = self.normalize_item_code(original_code)
        
        # DEBUG: Log what we're receiving
        log.info(f"DEBUG: Creating enhanced node for item {original_code}")
        log.info(f"DEBUG: Item data keys: {list(item.keys())}")
        log.info(f"DEBUG: URLs in item: {item.get('urls', 'NO URLS')}")
        
        properties = {
            'nodeType': 'AgendaItem',
            'code': normalized_code,
            'original_code': original_code,
            'title': item.get('title', 'Unknown'),
            'type': item.get('item_type', 'Item'),
            'section': section.get('section_name', 'Unknown'),
            'section_type': section.get('section_type', 'other')
        }
        
        # Add enhanced details from LLM extraction
        if item.get('description'):
            properties['description'] = item['description'][:500]  # Limit length
        
        if item.get('document_reference'):
            properties['document_reference'] = item['document_reference']
        
        # Add sponsors as JSON array
        if item.get('sponsors'):
            properties['sponsors_json'] = json.dumps(item['sponsors'])
        
        # Add departments as JSON array  
        if item.get('departments'):
            properties['departments_json'] = json.dumps(item['departments'])
        
        # Add actions as JSON array
        if item.get('actions'):
            properties['actions_json'] = json.dumps(item['actions'])
        
        # Add stakeholders as JSON array
        if item.get('stakeholders'):
            properties['stakeholders_json'] = json.dumps(item['stakeholders'])
        
        # Add URLs as JSON array
        if item.get('urls'):
            properties['urls_json'] = json.dumps(item['urls'])
            properties['has_urls'] = True
            log.info(f"DEBUG: Added URLs to properties for {original_code}")
        
        # DEBUG: Log what properties we're storing
        log.info(f"DEBUG: Properties being stored: {list(properties.keys())}")
        log.info(f"DEBUG: Has URLs: {properties.get('has_urls', False)}")
        
        if self.upsert_mode:
            created = await self.cosmos.upsert_vertex('AgendaItem', item_id, properties)
            if created:
                self.stats['nodes_created'] += 1
            else:
                self.stats['nodes_updated'] += 1
        else:
            await self.cosmos.create_vertex('AgendaItem', item_id, properties)
            self.stats['nodes_created'] += 1
        
        return item_id
    
    async def _create_item_relationships(self, item: Dict, item_id: str, meeting_date: str):
        """Create rich relationships for agenda items."""
        
        # Sponsor relationships
        for sponsor in item.get('sponsors', []):
            try:
                person_id = await self._ensure_person_node(sponsor, 'Commissioner')
                await self.cosmos.create_edge_if_not_exists(
                    from_id=person_id,
                    to_id=item_id,
                    edge_type='SPONSORS',
                    properties={'role': 'sponsor'}
                )
                self.stats['edges_created'] += 1
            except Exception as e:
                log.error(f"Failed to create sponsor relationship: {e}")
        
        # Department relationships
        for dept in item.get('departments', []):
            try:
                dept_id = await self._ensure_department_node(dept)
                await self.cosmos.create_edge_if_not_exists(
                    from_id=dept_id,
                    to_id=item_id,
                    edge_type='RESPONSIBLE_FOR',
                    properties={'role': 'responsible_department'}
                )
                self.stats['edges_created'] += 1
            except Exception as e:
                log.error(f"Failed to create department relationship: {e}")
        
        # Stakeholder relationships  
        for stakeholder in item.get('stakeholders', []):
            try:
                org_id = await self._ensure_organization_node(stakeholder, 'Stakeholder')
                await self.cosmos.create_edge_if_not_exists(
                    from_id=org_id,
                    to_id=item_id,
                    edge_type='INVOLVED_IN',
                    properties={'role': 'stakeholder'}
                )
                self.stats['edges_created'] += 1
            except Exception as e:
                log.error(f"Failed to create stakeholder relationship: {e}")
        
        # Action relationships
        for action in item.get('actions', []):
            try:
                action_id = await self._ensure_action_node(action)
                await self.cosmos.create_edge_if_not_exists(
                    from_id=item_id,
                    to_id=action_id,
                    edge_type='REQUIRES_ACTION',
                    properties={'action_type': action}
                )
                self.stats['edges_created'] += 1
            except Exception as e:
                log.error(f"Failed to create action relationship: {e}")
    
    async def _create_url_relationships(self, item: Dict, item_id: str):
        """Create URL nodes and link to agenda items."""
        for url in item.get('urls', []):
            try:
                url_id = await self._ensure_url_node(url)
                await self.cosmos.create_edge_if_not_exists(
                    from_id=item_id,
                    to_id=url_id,
                    edge_type='HAS_URL',
                    properties={'url_type': 'document_link'}
                )
                self.stats['edges_created'] += 1
            except Exception as e:
                log.error(f"Failed to create URL relationship: {e}")
    
    async def _create_ontology_relationship(self, rel: Dict, meeting_date: str):
        """Create relationship from ontology data."""
        try:
            source = rel.get('source', '')
            target = rel.get('target', '')
            relationship = rel.get('relationship', '')
            source_type = rel.get('source_type', '')
            target_type = rel.get('target_type', '')
            
            # Determine source and target IDs based on type
            if source_type == 'person':
                source_id = await self._ensure_person_node(source, 'Participant')
            elif source_type == 'department':
                source_id = await self._ensure_department_node(source)
            elif source_type == 'organization':
                source_id = await self._ensure_organization_node(source, 'Organization')
            else:
                log.warning(f"Unknown source type: {source_type}")
                return
            
            if target_type == 'agenda_item':
                # Normalize target agenda item code
                normalized_target = self.normalize_item_code(target)
                target_id = f"item-{meeting_date}-{normalized_target}"
            else:
                log.warning(f"Unknown target type: {target_type}")
                return
            
            # Create the relationship
            await self.cosmos.create_edge_if_not_exists(
                from_id=source_id,
                to_id=target_id,
                edge_type=relationship.upper(),
                properties={
                    'source_type': source_type,
                    'target_type': target_type
                }
            )
            
        except Exception as e:
            log.error(f"Failed to create ontology relationship: {e}")
    
    async def _ensure_person_node(self, name: str, role: str) -> str:
        """Create or retrieve person node with upsert support."""
        clean_name = name.strip()
        # Clean the ID by removing invalid characters
        cleaned_id_part = clean_name.lower().replace(' ', '-').replace('.', '').replace("'", '').replace('"', '').replace('/', '-')
        person_id = f"person-{cleaned_id_part}"
        
        # Check cache first
        if person_id in self.entity_id_cache:
            return person_id
        
        # Check if exists in database
        if await self.cosmos.vertex_exists(person_id):
            self.entity_id_cache[person_id] = True
            return person_id
        
        # Create new person
        properties = {
            'nodeType': 'Person',
            'name': clean_name,
            'roles': role
        }
        
        if self.upsert_mode:
            created = await self.cosmos.upsert_vertex('Person', person_id, properties)
            if created:
                self.stats['nodes_created'] += 1
            else:
                self.stats['nodes_updated'] += 1
        else:
            if not await self.cosmos.vertex_exists(person_id):
                await self.cosmos.create_vertex('Person', person_id, properties)
                self.stats['nodes_created'] += 1
        
        self.entity_id_cache[person_id] = True
        return person_id
    
    async def _ensure_organization_node(self, name: str, org_type: str) -> str:
        """Create or retrieve organization node."""
        # Clean the ID
        cleaned_org_name = name.lower().replace(' ', '-').replace('.', '').replace("'", '').replace('"', '').replace('/', '-').replace(',', '')
        org_id = f"org-{cleaned_org_name}"
        
        if org_id in self.entity_id_cache:
            return org_id
        
        if await self.cosmos.vertex_exists(org_id):
            self.entity_id_cache[org_id] = True
            return org_id
        
        properties = {
            'nodeType': 'Organization',
            'name': name,
            'type': org_type
        }
        
        await self.cosmos.create_vertex('Organization', org_id, properties)
        self.entity_id_cache[org_id] = True
        self.stats['nodes_created'] += 1
        return org_id
    
    async def _ensure_department_node(self, name: str) -> str:
        """Create or retrieve department node."""
        cleaned_dept_name = name.lower().replace(' ', '-').replace('.', '').replace("'", '').replace('"', '').replace('/', '-')
        dept_id = f"dept-{cleaned_dept_name}"
        
        if dept_id in self.entity_id_cache:
            return dept_id
        
        if await self.cosmos.vertex_exists(dept_id):
            self.entity_id_cache[dept_id] = True
            return dept_id
        
        properties = {
            'nodeType': 'Department', 
            'name': name,
            'type': 'CityDepartment'
        }
        
        await self.cosmos.create_vertex('Department', dept_id, properties)
        self.entity_id_cache[dept_id] = True
        self.stats['nodes_created'] += 1
        return dept_id
    
    async def _ensure_location_node(self, name: str, context: str = '') -> str:
        """Create or retrieve location node."""
        cleaned_loc_name = name.lower().replace(' ', '-').replace('.', '').replace("'", '').replace('"', '').replace('/', '-').replace(',', '')
        loc_id = f"location-{cleaned_loc_name}"
        
        if loc_id in self.entity_id_cache:
            return loc_id
        
        if await self.cosmos.vertex_exists(loc_id):
            self.entity_id_cache[loc_id] = True
            return loc_id
        
        properties = {
            'nodeType': 'Location',
            'name': name,
            'context': context[:200] if context else '',
            'type': 'Location'
        }
        
        await self.cosmos.create_vertex('Location', loc_id, properties)
        self.entity_id_cache[loc_id] = True
        self.stats['nodes_created'] += 1
        return loc_id
    
    async def _ensure_action_node(self, action: str) -> str:
        """Create or retrieve action node."""
        cleaned_action = action.lower().replace(' ', '-').replace('.', '')
        action_id = f"action-{cleaned_action}"
        
        if action_id in self.entity_id_cache:
            return action_id
        
        if await self.cosmos.vertex_exists(action_id):
            self.entity_id_cache[action_id] = True
            return action_id
        
        properties = {
            'nodeType': 'Action',
            'name': action,
            'type': 'RequiredAction'
        }
        
        await self.cosmos.create_vertex('Action', action_id, properties)
        self.entity_id_cache[action_id] = True
        self.stats['nodes_created'] += 1
        return action_id
    
    async def _ensure_url_node(self, url: str) -> str:
        """Create or retrieve URL node."""
        url_hash = hashlib.md5(url.encode()).hexdigest()[:12]
        url_id = f"url-{url_hash}"
        
        if url_id in self.entity_id_cache:
            return url_id
        
        if await self.cosmos.vertex_exists(url_id):
            self.entity_id_cache[url_id] = True
            return url_id
        
        properties = {
            'nodeType': 'URL',
            'url': url,
            'domain': url.split('/')[2] if '://' in url else 'unknown',
            'type': 'Hyperlink'
        }
        
        await self.cosmos.create_vertex('URL', url_id, properties)
        self.entity_id_cache[url_id] = True
        self.stats['nodes_created'] += 1
        return url_id

    async def process_linked_documents(self, linked_docs: Dict, meeting_id: str, meeting_date: str):
        """Process and create nodes for linked documents."""
        log.info("ðŸ“„ Processing linked documents...")
        
        created_count = 0
        missing_items = []
        
        for doc_type, docs in linked_docs.items():
            # Skip verbatim_transcripts as they're processed separately
            if doc_type == "verbatim_transcripts":
                log.info(f"â­ï¸  Skipping {doc_type} - processed separately")
                continue
                
            if not docs:
                continue
                
            log.info(f"\n   ðŸ“‚ Processing {len(docs)} {doc_type}")
            
            for doc in docs:
                # Validate item_code before processing
                item_code = doc.get('item_code')
                
                # Enhanced matching for documents without item codes
                if not doc.get("item_code"):
                    doc_number = doc.get("document_number", "")
                    log.info(f"ðŸ” Searching for document {doc_number} in agenda structure")
                    
                    # Search through all sections and items for matching document reference
                    found_item = None
                    for section in self.current_ontology.get("sections", []):
                        for item in section.get("items", []):
                            if item.get("document_reference") == doc_number:
                                doc["item_code"] = item.get("item_code")
                                log.info(f"âœ… Found matching agenda item by document reference: {doc['item_code']}")
                                found_item = item
                                break
                        if found_item:
                            break
                
                item_code = doc.get('item_code')
                if item_code and len(item_code) > 10:  # Suspiciously long
                    log.error(f"Invalid item code detected: {item_code[:50]}...")
                    # Try to extract a valid code
                    code_match = re.match(r'^([A-Z]-?\d+)', item_code)
                    if code_match:
                        item_code = code_match.group(1)
                        doc['item_code'] = item_code
                        log.info(f"Extracted valid code: {item_code}")
                    else:
                        log.error(f"Could not extract valid code, skipping document {doc.get('document_number')}")
                        continue
                
                # Use the singular form for logging
                doc_type_singular = doc_type[:-1] if doc_type.endswith('s') else doc_type
                
                if doc_type in ['ordinances', 'resolutions']:
                    log.info(f"\n   Processing {doc_type_singular} {doc.get('document_number', 'unknown')}")
                    log.info(f"      Item code: {doc.get('item_code', 'MISSING')}")
                    
                    # Create document node
                    doc_id = await self._create_document_node(doc, doc_type, meeting_date)
                    
                    if doc_id:
                        created_count += 1
                        log.info(f"      âœ… Created document node: {doc_id}")
                        
                        # Link to meeting
                        await self.cosmos.create_edge(
                            from_id=doc_id,
                            to_id=meeting_id,
                            edge_type='PRESENTED_AT',
                            properties={'date': meeting_date}
                        )
                        
                        # Try to link to agenda item if item_code exists
                        item_code = doc.get('item_code')
                        if item_code:
                            # Log the normalization process
                            log.debug(f"Original item code: '{item_code}'")
                            normalized_code = self.normalize_item_code(item_code)
                            log.debug(f"Normalized item code: '{normalized_code}'")
                            
                            item_id = f"item-{meeting_date}-{normalized_code}"
                            log.info(f"Looking for agenda item: {item_id}")
                            
                            # Check if agenda item exists
                            if await self.cosmos.vertex_exists(item_id):
                                log.info(f"âœ… Found agenda item: {item_id}")
                                await self.cosmos.create_edge(
                                    from_id=item_id,
                                    to_id=doc_id,
                                    edge_type='REFERENCES_DOCUMENT',
                                    properties={'document_type': doc_type_singular}
                                )
                                log.info(f"      ðŸ”— Linked to agenda item: {item_id}")
                            else:
                                # Try alternative formats
                                alt_ids = [
                                    f"item-{meeting_date}-E-9",
                                    f"item-{meeting_date}-E9",
                                    f"item-{meeting_date}-E.-9.",
                                    f"item-{meeting_date}-E.-9"
                                ]
                                
                                found = False
                                for alt_id in alt_ids:
                                    if await self.cosmos.vertex_exists(alt_id):
                                        log.info(f"âœ… Found agenda item with alternative ID: {alt_id}")
                                        item_id = alt_id
                                        found = True
                                        await self.cosmos.create_edge(
                                            from_id=item_id,
                                            to_id=doc_id,
                                            edge_type='REFERENCES_DOCUMENT',
                                            properties={'document_type': doc_type_singular}
                                        )
                                        log.info(f"      ðŸ”— Linked to agenda item: {alt_id}")
                                        break
                                
                                if not found:
                                    # Try to find by document number
                                    doc_num = doc.get('document_number', '')
                                    
                                    # Search all agenda items for matching document reference
                                    if hasattr(self, 'current_agenda_structure'):
                                        for section in self.current_agenda_structure:  # Need to store this
                                            for item in section.get('items', []):
                                                if item.get('document_reference') == doc_num:
                                                    # Found matching item by document number
                                                    item_code = self.normalize_item_code(item['item_code'])
                                                    item_id = f"item-{meeting_date}-{item_code}"
                                                    log.info(f"âœ… Found item by document reference: {item_id}")
                                                    await self.cosmos.create_edge(
                                                        from_id=item_id,
                                                        to_id=doc_id,
                                                        edge_type='REFERENCES_DOCUMENT',
                                                        properties={'document_type': doc_type_singular}
                                                    )
                                                    log.info(f"      ðŸ”— Linked to agenda item via document reference: {item_id}")
                                                    found = True
                                                    break
                                            if found:
                                                break
                                    
                                    if not found:
                                        log.warning(f"âŒ Agenda item not found: {item_id} or alternatives")
                                        missing_items.append({
                                            'document_number': doc.get('document_number'),
                                            'item_code': item_code,
                                            'normalized_code': normalized_code,
                                            'expected_item_id': item_id,
                                            'document_type': doc_type_singular
                                        })
                        else:
                            log.warning(f"      âš ï¸  No item_code found for {doc.get('document_number')}")
        
        log.info(f"ðŸ“„ Document processing complete: {created_count} documents created")
        if missing_items:
            log.warning(f"âš ï¸  {len(missing_items)} documents could not be linked to agenda items")
        
        return missing_items

    async def _create_document_node(self, doc_info: Dict, doc_type: str, meeting_date: str) -> str:
        """Create or update an Ordinance or Resolution node."""
        doc_number = doc_info.get('document_number', 'unknown')
        
        # Use the document type from doc_info if available, otherwise use the passed type
        actual_doc_type = doc_info.get('document_type', doc_type)
        
        # Ensure consistency in ID generation
        if actual_doc_type.lower() == 'resolution':
            doc_id = f"resolution-{doc_number}"
            node_type = 'Resolution'
        else:
            doc_id = f"ordinance-{doc_number}"
            node_type = 'Ordinance'
        
        # Get full title without truncation
        title = doc_info.get('title', '')
        if not title and doc_info.get('parsed_data', {}).get('title'):
            title = doc_info['parsed_data']['title']
        
        if title is None:
            title = f"Untitled {actual_doc_type.capitalize()} {doc_number}"
            log.warning(f"No title found for {actual_doc_type} {doc_number}, using default")
        
        properties = {
            'nodeType': node_type,
            'document_number': doc_number,
            'full_title': title,
            'title': title[:200] if len(title) > 200 else title,
            'document_type': actual_doc_type.capitalize(),
            'meeting_date': meeting_date
        }
        
        # Add parsed metadata
        parsed_data = doc_info.get('parsed_data', {})
        
        if parsed_data.get('date_passed'):
            properties['date_passed'] = parsed_data['date_passed']
        
        if parsed_data.get('agenda_item'):
            properties['agenda_item'] = parsed_data['agenda_item']
        
        # Add vote details as JSON
        if parsed_data.get('vote_details'):
            properties['vote_details'] = json.dumps(parsed_data['vote_details'])
        
        # Add signatories
        if parsed_data.get('signatories', {}).get('mayor'):
            properties['mayor_signature'] = parsed_data['signatories']['mayor']
        
        if self.upsert_mode:
            created = await self.cosmos.upsert_vertex(node_type, doc_id, properties)
            if created:
                self.stats['nodes_created'] += 1
                log.info(f"âœ… Created document node: {doc_id}")
            else:
                self.stats['nodes_updated'] += 1
                log.info(f"ðŸ“ Updated document node: {doc_id}")
        else:
            await self.cosmos.create_vertex(node_type, doc_id, properties)
            self.stats['nodes_created'] += 1
        
        # Create edges for sponsors
        if parsed_data.get('motion', {}).get('moved_by'):
            person_id = await self._ensure_person_node(
                parsed_data['motion']['moved_by'], 
                'Commissioner'
            )
            if await self.cosmos.create_edge_if_not_exists(person_id, doc_id, 'MOVED'):
                self.stats['edges_created'] += 1
            else:
                self.stats['edges_skipped'] += 1
        
        return doc_id

    async def process_verbatim_transcripts(self, transcripts: Dict, meeting_id: str, meeting_date: str) -> int:
        """Process and create nodes for verbatim transcript documents."""
        log.info("ðŸŽ¤ Processing verbatim transcripts...")
        
        # Handle empty results gracefully
        if not any(transcripts.values()):
            log.info("ðŸŽ¤ No verbatim transcripts found for this meeting")
            return 0
        
        # Ensure PUBLIC COMMENT section exists if we have public comment transcripts
        if transcripts.get("public_comments"):
            # Create PUBLIC COMMENT section if not already present
            section_id = f"section-{meeting_date}-public-comment"
            await self._create_section_node(section_id, {
                'section_name': 'PUBLIC COMMENT',
                'section_type': 'PUBLIC_COMMENT',
                'description': 'Public comments from the meeting',
                'items': []
            }, 999)  # High order number to place at end
            
            # Link to meeting
            await self.cosmos.create_edge_if_not_exists(
                from_id=meeting_id,
                to_id=section_id,
                edge_type='HAS_SECTION'
            )
        
        created_count = 0
        
        # Process item-specific transcripts
        for transcript in transcripts.get("item_transcripts", []):
            transcript_id = await self._create_transcript_node(transcript, meeting_date, "item")
            if transcript_id:
                created_count += 1
                
                # Link to meeting
                await self.cosmos.create_edge(
                    from_id=transcript_id,
                    to_id=meeting_id,
                    edge_type='TRANSCRIBED_AT',
                    properties={'date': meeting_date}
                )
                
                # Link to specific agenda items
                for item_code in transcript.get("item_codes", []):
                    normalized_code = self.normalize_item_code(item_code)
                    item_id = f"item-{meeting_date}-{normalized_code}"
                    
                    if await self.cosmos.vertex_exists(item_id):
                        await self.cosmos.create_edge(
                            from_id=item_id,
                            to_id=transcript_id,
                            edge_type='HAS_TRANSCRIPT',
                            properties={'transcript_type': 'verbatim'}
                        )
                        log.info(f"   ðŸ”— Linked transcript to item: {item_id}")
                    else:
                        log.warning(f"   âš ï¸  Agenda item not found: {item_id}")
        
        # Process public comment transcripts
        for transcript in transcripts.get("public_comments", []):
            transcript_id = await self._create_transcript_node(transcript, meeting_date, "public_comment")
            if transcript_id:
                created_count += 1
                
                # Link to meeting
                await self.cosmos.create_edge(
                    from_id=transcript_id,
                    to_id=meeting_id,
                    edge_type='PUBLIC_COMMENT_AT',
                    properties={'date': meeting_date}
                )
        
        # Process section transcripts
        for transcript in transcripts.get("section_transcripts", []):
            transcript_id = await self._create_transcript_node(transcript, meeting_date, "section")
            if transcript_id:
                created_count += 1
                
                # Link to meeting
                await self.cosmos.create_edge(
                    from_id=transcript_id,
                    to_id=meeting_id,
                    edge_type='TRANSCRIBED_AT',
                    properties={'date': meeting_date}
                )
        
        log.info(f"ðŸŽ¤ Transcript processing complete: {created_count} transcripts created")
        return created_count

    async def _create_transcript_node(self, transcript_info: Dict, meeting_date: str, transcript_type: str) -> str:
        """Create or update a Transcript node."""
        filename = transcript_info.get("filename", "unknown")
        
        # Create unique ID based on filename
        transcript_id = f"transcript-{meeting_date}-{filename.replace('.pdf', '').replace(' ', '-').lower()}"
        
        properties = {
            'nodeType': 'Transcript',
            'filename': filename,
            'transcript_type': transcript_type,
            'meeting_date': meeting_date,
            'page_count': transcript_info.get('page_count', 0),
            'item_info': transcript_info.get('item_info_raw', ''),
            'items_covered': json.dumps(transcript_info.get('item_codes', [])),
            'sections_covered': json.dumps(transcript_info.get('section_codes', [])),
            'text_excerpt': transcript_info.get('text_excerpt', '')[:500]
        }
        
        if self.upsert_mode:
            created = await self.cosmos.upsert_vertex('Transcript', transcript_id, properties)
            if created:
                self.stats['nodes_created'] += 1
                log.info(f"âœ… Created transcript node: {transcript_id}")
            else:
                self.stats['nodes_updated'] += 1
                log.info(f"ðŸ“ Updated transcript node: {transcript_id}")
        else:
            await self.cosmos.create_vertex('Transcript', transcript_id, properties)
            self.stats['nodes_created'] += 1
        
        return transcript_id

    async def _create_agenda_item_node(self, item: Dict[str, Any], meeting_id: str):
        """Create an agenda item node with enhanced properties including URLs."""
        item_code = item.get('item_code', 'Unknown')
        node_id = f"item_{meeting_id}_{item_code}"
        
        # DEBUG: Log what we're receiving
        log.info(f"DEBUG: Creating node for item {item_code}")
        log.info(f"DEBUG: Item data keys: {list(item.keys())}")
        log.info(f"DEBUG: URLs in item: {item.get('urls', 'NO URLS')}")
        
        # Prepare URLs for storage - convert to JSON string for graph property
        urls = item.get('urls', [])
        urls_json = json.dumps(urls) if urls else None
        
        # Prepare properties
        properties = {
            'item_code': item_code,
            'title': item.get('title', ''),
            'document_reference': item.get('document_reference', ''),
            'item_type': item.get('item_type', 'Agenda Item'),
            'section_name': item.get('section_name', ''),
            'has_items': item.get('has_items', True),
            'meeting_id': meeting_id,
            'entity_type': 'agenda_item'
        }
        
        # Add URL-related properties
        if urls_json:
            properties['urls'] = urls_json
            properties['url_count'] = len(urls)
            
            # Also store first URL separately for easy access
            if urls:
                properties['primary_url'] = urls[0].get('url', '')
                properties['primary_url_text'] = urls[0].get('text', '')
        
        # DEBUG: Log what properties we're storing
        log.info(f"DEBUG: Properties being stored: {list(properties.keys())}")
        log.info(f"DEBUG: URL count: {properties.get('url_count', 0)}")
        
        # Create or update the node
        created = await self.cosmos.upsert_vertex(
            label='AgendaItem',
            vertex_id=node_id,
            properties=properties
        )
        
        action = "Created" if created else "Updated"
        url_info = f" with {len(urls)} URLs" if urls else ""
        log.info(f"{action} agenda item node: {node_id}{url_info}")
        
        return node_id

    async def _create_ordinance_document_node(self, doc_info: Dict[str, Any], 
                                            meeting_id: str,
                                            date_str: str):
        """Create an ordinance document node with URL support."""
        doc_number = doc_info['document_number']
        node_id = f"ordinance_{doc_number}_{date_str}"
        
        # Extract URLs if available from parsed_data
        urls = []
        if 'parsed_data' in doc_info and isinstance(doc_info['parsed_data'], dict):
            parsed_urls = doc_info['parsed_data'].get('urls', [])
            if parsed_urls:
                urls = parsed_urls
        
        properties = {
            'document_number': doc_number,
            'title': doc_info.get('title', ''),
            'document_type': 'Ordinance',
            'filename': doc_info.get('filename', ''),
            'item_code': doc_info.get('item_code', ''),
            'meeting_id': meeting_id,
            'entity_type': 'ordinance_document'
        }
        
        # Add URL properties if available
        if urls:
            properties['urls'] = json.dumps(urls)
            properties['url_count'] = len(urls)
            if urls[0]:
                properties['primary_url'] = urls[0].get('url', '')
        
        # Add parsed metadata
        if 'parsed_data' in doc_info:
            parsed = doc_info['parsed_data']
            if 'date_passed' in parsed:
                properties['date_passed'] = parsed['date_passed']
            if 'vote_details' in parsed:
                properties['vote_details'] = json.dumps(parsed['vote_details'])
            if 'motion' in parsed:
                properties['motion'] = json.dumps(parsed['motion'])
            if 'signatories' in parsed:
                properties['signatories'] = json.dumps(parsed['signatories'])
        
        created = await self.cosmos.upsert_vertex(
            label='OrdinanceDocument',
            vertex_id=node_id,
            properties=properties
        )
        
        action = "Created" if created else "Updated"
        log.info(f"{action} ordinance document node: {node_id}")
        
        return node_id

    async def _create_resolution_document_node(self, doc_info: Dict[str, Any], 
                                             meeting_id: str,
                                             date_str: str):
        """Create a resolution document node with URL support."""
        doc_number = doc_info['document_number']
        node_id = f"resolution_{doc_number}_{date_str}"
        
        # Extract URLs if available
        urls = []
        if 'parsed_data' in doc_info and isinstance(doc_info['parsed_data'], dict):
            parsed_urls = doc_info['parsed_data'].get('urls', [])
            if parsed_urls:
                urls = parsed_urls
        
        properties = {
            'document_number': doc_number,
            'title': doc_info.get('title', ''),
            'document_type': 'Resolution',
            'filename': doc_info.get('filename', ''),
            'item_code': doc_info.get('item_code', ''),
            'meeting_id': meeting_id,
            'entity_type': 'resolution_document'
        }
        
        # Add URL properties if available
        if urls:
            properties['urls'] = json.dumps(urls)
            properties['url_count'] = len(urls)
            if urls[0]:
                properties['primary_url'] = urls[0].get('url', '')
        
        # Add parsed metadata
        if 'parsed_data' in doc_info:
            parsed = doc_info['parsed_data']
            if 'date_passed' in parsed:
                properties['date_passed'] = parsed['date_passed']
            if 'vote_details' in parsed:
                properties['vote_details'] = json.dumps(parsed['vote_details'])
            if 'motion' in parsed:
                properties['motion'] = json.dumps(parsed['motion'])
            if 'signatories' in parsed:
                properties['signatories'] = json.dumps(parsed['signatories'])
            if 'purpose' in parsed:
                properties['purpose'] = parsed['purpose']
        
        created = await self.cosmos.upsert_vertex(
            label='ResolutionDocument',
            vertex_id=node_id,
            properties=properties
        )
        
        action = "Created" if created else "Updated"
        log.info(f"{action} resolution document node: {node_id}")
        
        return node_id

    async def _create_verbatim_transcript_node(self, transcript_info: Dict[str, Any],
                                             meeting_id: str,
                                             date_str: str):
        """Create a verbatim transcript node with potential URL support."""
        filename = transcript_info['filename']
        # Create a unique ID based on filename
        node_id = f"transcript_{date_str}_{filename.replace('.pdf', '').replace(' ', '_')}"
        
        properties = {
            'filename': filename,
            'meeting_date': transcript_info['meeting_date'],
            'item_codes': json.dumps(transcript_info.get('item_codes', [])),
            'section_codes': json.dumps(transcript_info.get('section_codes', [])),
            'transcript_type': transcript_info['transcript_type'],
            'page_count': transcript_info.get('page_count', 0),
            'item_info_raw': transcript_info.get('item_info_raw', ''),
            'meeting_id': meeting_id,
            'entity_type': 'verbatim_transcript'
        }
        
        # Add URL properties if transcripts start including URLs
        urls = transcript_info.get('urls', [])
        if urls:
            properties['urls'] = json.dumps(urls)
            properties['url_count'] = len(urls)
        
        created = await self.cosmos.upsert_vertex(
            label='VerbatimTranscript',
            vertex_id=node_id,
            properties=properties
        )
        
        action = "Created" if created else "Updated"
        log.info(f"{action} verbatim transcript node: {node_id}")
        
        return node_id

    async def _create_official_nodes(self, meeting_info: Dict, meeting_id: str):
        """Create nodes for city officials and link to meeting."""
        officials = meeting_info.get('officials', {})
        commissioners = meeting_info.get('commissioners', [])
        
        # Create official nodes
        for role, name in officials.items():
            if name and name != 'null':
                person_id = await self._ensure_person_node(name, role.replace('_', ' ').title())
                await self.cosmos.create_edge_if_not_exists(
                    from_id=person_id,
                    to_id=meeting_id,
                    edge_type='ATTENDED',
                    properties={'role': role.replace('_', ' ').title()}
                )
        
        # Create commissioner nodes
        for idx, commissioner in enumerate(commissioners):
            if commissioner and commissioner != 'null':
                person_id = await self._ensure_person_node(commissioner, 'Commissioner')
                await self.cosmos.create_edge_if_not_exists(
                    from_id=person_id,
                    to_id=meeting_id,
                    edge_type='ATTENDED',
                    properties={'role': 'Commissioner', 'seat': idx + 1}
                )
    
    async def _create_entity_nodes(self, entities: List[Dict], meeting_id: str) -> int:
        """Create nodes for all extracted entities."""
        entity_count = 0
        
        for entity in entities:
            try:
                entity_type = entity.get('type', 'unknown')
                entity_name = entity.get('name', '')
                entity_role = entity.get('role', '')
                entity_context = entity.get('context', '')
                
                if not entity_name:
                    continue
                
                if entity_type == 'person':
                    person_id = await self._ensure_person_node(entity_name, entity_role)
                    await self.cosmos.create_edge_if_not_exists(
                        from_id=person_id,
                        to_id=meeting_id,
                        edge_type='MENTIONED_IN',
                        properties={
                            'context': entity_context[:100],
                            'role': entity_role
                        }
                    )
                    entity_count += 1
                    
                elif entity_type == 'organization':
                    org_id = await self._ensure_organization_node(entity_name, entity_role)
                    await self.cosmos.create_edge_if_not_exists(
                        from_id=org_id,
                        to_id=meeting_id,
                        edge_type='MENTIONED_IN',
                        properties={
                            'context': entity_context[:100],
                            'org_type': entity_role
                        }
                    )
                    entity_count += 1
                    
                elif entity_type == 'department':
                    dept_id = await self._ensure_department_node(entity_name)
                    await self.cosmos.create_edge_if_not_exists(
                        from_id=dept_id,
                        to_id=meeting_id,
                        edge_type='INVOLVED_IN',
                        properties={'context': entity_context[:100]}
                    )
                    entity_count += 1
                    
                elif entity_type == 'location':
                    loc_id = await self._ensure_location_node(entity_name, entity_context)
                    await self.cosmos.create_edge_if_not_exists(
                        from_id=loc_id,
                        to_id=meeting_id,
                        edge_type='REFERENCED_IN',
                        properties={'context': entity_context[:100]}
                    )
                    entity_count += 1
                    
            except Exception as e:
                log.error(f"Failed to create entity node for {entity}: {e}")
        
        return entity_count


================================================================================


################################################################################
# File: scripts/graph_stages/agenda_pdf_extractor.py
################################################################################

# scripts/graph_stages/agenda_pdf_extractor.py
"""
PDF Extractor for City Clerk Agenda Documents
Extracts text, structure, and hyperlinks from agenda PDFs.
"""

import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import json
import re
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import PdfPipelineOptions
from groq import Groq
import os
import fitz  # PyMuPDF for hyperlink extraction
from functools import lru_cache
import hashlib
import asyncio

log = logging.getLogger(__name__)


class AgendaPDFExtractor:
    """Extract structured content from agenda PDFs using Docling and LLM."""
    
    def __init__(self, output_dir: Optional[Path] = None):
        """Initialize the agenda PDF extractor."""
        self.output_dir = output_dir or Path("city_clerk_documents/extracted_text")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize Docling converter with OCR enabled
        pipeline_options = PdfPipelineOptions()
        pipeline_options.do_ocr = True  # Enable OCR for better text extraction
        pipeline_options.do_table_structure = True  # Better table extraction
        
        self.converter = DocumentConverter(
            format_options={
                InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
            }
        )
        
        # Initialize OpenAI client for LLM extraction
        self.client = Groq()
        self.model = "gpt-4.1-mini-2025-04-14"
        
        # Initialize extraction cache
        self._extraction_cache = {}
    
    async def extract_agenda(self, pdf_path: Path) -> Dict[str, any]:
        """Extract agenda with caching."""
        log.info(f"ðŸ“„ Extracting agenda from {pdf_path.name}")
        
        # Check cache first
        file_hash = self._get_file_hash(pdf_path)
        if file_hash in self._extraction_cache:
            log.info(f"ðŸ“‹ Using cached extraction for {pdf_path.name}")
            return self._extraction_cache[file_hash]
        
        # Convert with Docling - pass path directly
        result = self.converter.convert(str(pdf_path))
        
        # Get the document
        doc = result.document
        
        # Get full text and markdown
        full_text = doc.export_to_markdown() or ""
        
        # Use LLM to extract structured agenda items
        log.info("ðŸ§  Using LLM to extract agenda structure...")
        extracted_items = self._extract_agenda_items_with_llm(full_text)
        
        # Build sections from extracted items
        sections = self._build_sections_from_items(extracted_items, full_text)
        
        # Extract hyperlinks using PyMuPDF
        hyperlinks = self._extract_hyperlinks_pymupdf(pdf_path)
        
        # Parallelize item extraction
        agenda_items_with_urls = extracted_items
        if extracted_items:
            # Process items in parallel batches
            batch_size = 5
            all_enhanced_items = []
            
            for i in range(0, len(extracted_items), batch_size):
                batch = extracted_items[i:i + batch_size]
                
                # Create tasks for parallel execution
                tasks = [
                    self._extract_item_details_async(item, sections)
                    for item in batch
                ]
                
                # Execute batch in parallel
                batch_results = await asyncio.gather(*tasks, return_exceptions=True)
                
                for result, original_item in zip(batch_results, batch):
                    if isinstance(result, Exception):
                        log.error(f"Error extracting item {original_item.get('item_code')}: {result}")
                        all_enhanced_items.append(original_item)
                    else:
                        all_enhanced_items.append(result)
            
            extracted_items = all_enhanced_items
        
        # Associate hyperlinks with agenda items
        agenda_items_with_urls = self._associate_urls_with_items(extracted_items, hyperlinks, full_text)
        
        # Create agenda data structure with both raw and structured data
        agenda_data = {
            'source_file': pdf_path.name,
            'full_text': full_text,
            'sections': sections,
            'agenda_items': agenda_items_with_urls,  # Updated with URLs
            'hyperlinks': hyperlinks,
            'meeting_info': self._extract_meeting_info(pdf_path, full_text),
            'metadata': {
                'extraction_method': 'docling+llm+pymupdf',
                'num_sections': len(sections),
                'num_items': self._count_items(agenda_items_with_urls),
                'num_hyperlinks': len(hyperlinks)
            }
        }
        
        # Cache result
        self._extraction_cache[file_hash] = agenda_data
        
        # Save using the new save method
        output_file = self.output_dir / f"{pdf_path.stem}_extracted.json"
        self.save_extracted_agenda(agenda_data, output_file)
        
        log.info(f"âœ… Extraction complete: {len(sections)} sections, {self._count_items(agenda_items_with_urls)} items, {len(hyperlinks)} hyperlinks")
        log.info(f"âœ… Saved extracted data to: {output_file}")
        
        return agenda_data
    
    # Add async version of item extraction
    async def _extract_item_details_async(self, item: Dict, pages_dict: Dict[int, str]) -> Dict:
        """Async version of item detail extraction."""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            None, 
            self._extract_item_details, 
            item, 
            pages_dict
        )
    
    def _extract_item_details(self, item: Dict, pages_dict: Dict[int, str]) -> Dict:
        """Extract detailed information for an agenda item."""
        # This is a placeholder implementation - enhance with actual detail extraction logic
        # For now, just return the item as-is
        return item
    
    def _get_file_hash(self, file_path: Path) -> str:
        """Get hash of file for caching."""
        with open(file_path, 'rb') as f:
            return hashlib.md5(f.read()).hexdigest()

    def save_extracted_agenda(self, agenda_data: dict, output_path: Path):
        """Save extracted agenda data to JSON file."""
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(agenda_data, f, indent=2, ensure_ascii=False)
        
        log.info(f"âœ… Saved extracted agenda to: {output_path}")
        
        # NEW: Also save as markdown
        self._save_agenda_as_markdown(agenda_data, output_path)

    def _save_agenda_as_markdown(self, agenda_data: dict, json_path: Path):
        """Save agenda as enhanced markdown for GraphRAG."""
        markdown_dir = json_path.parent.parent / "extracted_markdown"
        markdown_dir.mkdir(exist_ok=True)
        
        meeting_info = agenda_data.get('meeting_info', {})
        meeting_date = meeting_info.get('date', None)
        
        # If date not found in meeting_info, extract from filename
        if not meeting_date or meeting_date == 'N/A':
            import re
            # Try to extract from the JSON filename first
            # Pattern: "Agenda 01.9.2024_extracted.json"
            filename_match = re.search(r'Agenda[_ ](\d{1,2})\.(\d{1,2})\.(\d{4})', json_path.name)
            if filename_match:
                month = filename_match.group(1).zfill(2)
                day = filename_match.group(2).zfill(2)
                year = filename_match.group(3)
                meeting_date = f"{month}.{day}.{year}"
                log.info(f"ðŸ“… Extracted date from filename: {meeting_date}")
            else:
                # Last resort: check the original PDF name in agenda_data
                source_file = agenda_data.get('source_file', '')
                pdf_match = re.search(r'(\d{1,2})\.(\d{1,2})\.(\d{4})', source_file)
                if pdf_match:
                    month = pdf_match.group(1).zfill(2)
                    day = pdf_match.group(2).zfill(2)
                    year = pdf_match.group(3)
                    meeting_date = f"{month}.{day}.{year}"
                    log.info(f"ðŸ“… Extracted date from source file: {meeting_date}")
                else:
                    meeting_date = 'unknown'
                    log.warning("âš ï¸ Could not extract meeting date from any source")
        
        # Convert to underscore format for filename
        if meeting_date != 'unknown':
            meeting_date_filename = meeting_date.replace('.', '_')
        else:
            meeting_date_filename = 'unknown'
        
        # Build comprehensive header
        header = self._build_agenda_header(agenda_data)
        
        # Add detailed agenda items section
        items_section = self._build_agenda_items_section(agenda_data)
        
        # Combine with full text
        full_content = header + items_section + "\n\n# FULL AGENDA TEXT\n\n" + agenda_data.get('full_text', '')
        
        # Save markdown
        md_filename = f"agenda_{meeting_date_filename}.md"
        md_path = markdown_dir / md_filename
        
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(full_content)
        
        log.info(f"ðŸ“ Saved agenda markdown to: {md_path}")

    def _build_agenda_header(self, agenda_data: dict) -> str:
        """Build comprehensive agenda header."""
        meeting_info = agenda_data.get('meeting_info', {})
        agenda_items = agenda_data.get('agenda_items', [])
        
        all_item_codes = [item.get('item_code', '') for item in agenda_items if item.get('item_code')]
        
        header = f"""---
DOCUMENT METADATA AND CONTEXT
=============================

**DOCUMENT IDENTIFICATION:**
- Document Type: AGENDA
- Meeting Date: {meeting_info.get('date', 'N/A')}

**ENTITIES IN THIS DOCUMENT:**
{self._format_agenda_entities(all_item_codes)}

**SEARCHABLE IDENTIFIERS:**
- DOCUMENT_TYPE: AGENDA
{self._format_item_identifiers(all_item_codes)}

---

"""
        return header

    def _format_agenda_entities(self, item_codes: list) -> str:
        """Format agenda item entities."""
        lines = []
        for code in item_codes[:10]:
            lines.append(f"- AGENDA_ITEM: {code}")
        if len(item_codes) > 10:
            lines.append(f"- ... and {len(item_codes) - 10} more items")
        return '\n'.join(lines)

    def _format_item_identifiers(self, item_codes: list) -> str:
        """Format item identifiers."""
        lines = []
        for code in item_codes:
            lines.append(f"- AGENDA_ITEM: {code}")
        return '\n'.join(lines)

    def _build_agenda_items_section(self, agenda_data: dict) -> str:
        """Build agenda items section."""
        lines = ["## AGENDA ITEMS QUICK REFERENCE\n"]
        
        for item in agenda_data.get('agenda_items', []):
            item_code = item.get('item_code', 'UNKNOWN')
            lines.append(f"### Agenda Item {item_code}")
            lines.append(f"**Title:** {item.get('title', 'N/A')}")
            lines.append(f"\n**What is Item {item_code}?**")
            lines.append(f"Item {item_code} is '{item.get('title', 'N/A')}'")
            lines.append("")
        
        return '\n'.join(lines)

    def _extract_meeting_info(self, pdf_path: Path, full_text: str) -> dict:
        """Extract meeting information from the agenda."""
        meeting_info = {
            'date': 'N/A',
            'time': 'N/A',
            'location': 'N/A'
        }
        
        # Try to extract date from filename first
        import re
        date_match = re.search(r'(\d{2})\.(\d{2})\.(\d{4})', pdf_path.name)
        if date_match:
            month, day, year = date_match.groups()
            meeting_info['date'] = f"{month}.{day}.{year}"
        
        # Try to extract time and location from text
        lines = full_text.split('\n')[:50]  # Check first 50 lines
        for line in lines:
            line = line.strip()
            # Look for time patterns
            time_match = re.search(r'(\d{1,2}:\d{2}\s*[AP]M)', line, re.IGNORECASE)
            if time_match and meeting_info['time'] == 'N/A':
                meeting_info['time'] = time_match.group(1)
            
            # Look for location
            if 'city hall' in line.lower() or 'commission chamber' in line.lower():
                meeting_info['location'] = line[:100]  # Limit length
        
        return meeting_info
    
    def _extract_hyperlinks_pymupdf(self, pdf_path: Path) -> List[Dict[str, any]]:
        """Extract hyperlinks from PDF using PyMuPDF."""
        hyperlinks = []
        
        try:
            # Open PDF with PyMuPDF
            pdf_document = fitz.open(str(pdf_path))
            
            for page_num in range(len(pdf_document)):
                page = pdf_document[page_num]
                
                # Get all links on the page
                links = page.get_links()
                
                for link in links:
                    if link.get('uri'):  # External URL
                        # Get the link text by extracting text from the link rectangle
                        rect = fitz.Rect(link['from'])
                        link_text = page.get_text(clip=rect).strip()
                        
                        # Clean up the link text
                        link_text = ' '.join(link_text.split())
                        
                        hyperlinks.append({
                            'url': link['uri'],
                            'text': link_text or 'Click here',
                            'page': page_num + 1,
                            'rect': {
                                'x0': link['from'].x0,
                                'y0': link['from'].y0,
                                'x1': link['from'].x1,
                                'y1': link['from'].y1
                            }
                        })
            
            pdf_document.close()
            
            log.info(f"ðŸ”— Extracted {len(hyperlinks)} hyperlinks from PDF")
            
        except Exception as e:
            log.error(f"Failed to extract hyperlinks with PyMuPDF: {e}")
        
        return hyperlinks
    
    def _associate_urls_with_items(self, items: List[Dict], hyperlinks: List[Dict], full_text: str) -> List[Dict]:
        """Associate extracted URLs with their corresponding agenda items."""
        # Create a mapping of items by their document reference
        items_by_ref = {}
        for item in items:
            if item.get('document_reference'):
                items_by_ref[item['document_reference']] = item
                # Initialize URLs list for each item
                item['urls'] = []
        
        # Try to associate URLs with items based on proximity and context
        for link in hyperlinks:
            # Strategy 1: Check if the link text contains a document reference
            for ref, item in items_by_ref.items():
                if ref in link.get('text', ''):
                    item['urls'].append({
                        'url': link['url'],
                        'text': link['text'],
                        'page': link['page']
                    })
                    log.info(f"ðŸ”— Associated URL with item {item.get('item_code', 'Unknown')}: {link['url'][:50]}...")
                    break
            else:
                # Strategy 2: Check for item codes in the link text
                link_text = link.get('text', '').upper()
                for item in items:
                    item_code = item.get('item_code', '')
                    if item_code and item_code in link_text:
                        item['urls'].append({
                            'url': link['url'],
                            'text': link['text'],
                            'page': link['page']
                        })
                        log.info(f"ðŸ”— Associated URL with item {item_code}: {link['url'][:50]}...")
                        break
        
        # Log summary of URL associations
        items_with_urls = sum(1 for item in items if item.get('urls'))
        total_urls_associated = sum(len(item.get('urls', [])) for item in items)
        log.info(f"ðŸ“Š Associated {total_urls_associated} URLs with {items_with_urls} agenda items")
        
        return items
    
    def _extract_agenda_items_with_llm(self, text: str) -> List[Dict[str, any]]:
        """Use LLM to extract agenda items from the text."""
        # Split text into chunks if too long
        max_chars = 30000
        chunks = []
        
        if len(text) > max_chars:
            # Split by lines to avoid breaking mid-sentence
            lines = text.split('\n')
            current_chunk = []
            current_length = 0
            
            for line in lines:
                if current_length + len(line) > max_chars and current_chunk:
                    chunks.append('\n'.join(current_chunk))
                    current_chunk = [line]
                    current_length = len(line)
                else:
                    current_chunk.append(line)
                    current_length += len(line) + 1
            
            if current_chunk:
                chunks.append('\n'.join(current_chunk))
        else:
            chunks = [text]
        
        all_items = []
        
        for i, chunk in enumerate(chunks):
            log.info(f"Processing chunk {i+1}/{len(chunks)}")
            
            prompt = """Extract ALL agenda items from this city council agenda document. Look for ALL these formats:

- Letter.-Number. Reference (e.g., H.-1. 23-6819)
- Letter-Number Reference (e.g., H-1 23-6819)
- Empty sections marked as "None"

IMPORTANT: 
1. Extract EVERY section even if it says "None"
2. Look for ALL item formats including H.-1., H.-2., etc.
3. Include items without explicit ordinance/resolution text

For EACH section/item found, extract:
1. section_name: The section name (e.g., "CITY MANAGER ITEMS")
2. item_code: The item code (e.g., "H-1") - normalize to Letter-Number format
3. document_reference: The reference number (e.g., "23-6819")
4. title: The full description
5. has_items: true if section has items, false if "None"

Return a JSON array including both sections and items.

Document text:
""" + chunk
            
            try:
                response = self.client.chat.completions.create(
                    model="meta-llama/llama-4-maverick-17b-128e-instruct",
                    messages=[
                        {"role": "system", "content": "You are an expert at extracting structured data from city government agenda documents. Return only valid JSON."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0,
                    max_completion_tokens=8192,
                    top_p=1,
                    stream=False,
                    stop=None
                )
                
                response_text = response.choices[0].message.content.strip()
                
                # Clean up response to ensure valid JSON
                if response_text.startswith('```json'):
                    response_text = response_text.replace('```json', '').replace('```', '')
                elif response_text.startswith('```'):
                    response_text = response_text.replace('```', '')
                
                response_text = response_text.strip()
                
                # Try to parse JSON
                try:
                    data = json.loads(response_text)
                    if isinstance(data, dict) and 'items' in data:
                        items = data['items']
                    elif isinstance(data, list):
                        items = data
                    else:
                        log.warning(f"Unexpected LLM response format: {type(data)}")
                        items = []
                        
                    all_items.extend(items)
                    log.info(f"Extracted {len(items)} items from chunk {i+1}")
                    
                except json.JSONDecodeError as e:
                    log.error(f"Failed to parse JSON from chunk {i+1}: {e}")
                    log.error(f"Raw response: {response_text[:200]}...")
                    # Try manual extraction as fallback
                    manual_sections = self._manual_extract_items(chunk)
                    # Flatten sections to items for consistency
                    manual_items = []
                    for section in manual_sections:
                        manual_items.extend(section.get('items', []))
                    all_items.extend(manual_items)
                    log.info(f"Manual fallback extracted {len(manual_items)} items from {len(manual_sections)} sections")
                    
            except Exception as e:
                log.error(f"LLM extraction failed for chunk {i+1}: {e}")
                # Fallback to manual extraction
                manual_sections = self._manual_extract_items(chunk)
                # Flatten sections to items for consistency
                manual_items = []
                for section in manual_sections:
                    manual_items.extend(section.get('items', []))
                all_items.extend(manual_items)
                log.info(f"Manual fallback extracted {len(manual_items)} items from {len(manual_sections)} sections")
        
        # Deduplicate items by item_code
        seen_codes = set()
        unique_items = []
        for item in all_items:
            if item.get('item_code') and item['item_code'] not in seen_codes:
                seen_codes.add(item['item_code'])
                unique_items.append(item)
        
        log.info(f"Total unique items extracted: {len(unique_items)}")
        return unique_items
    
    def _manual_extract_items(self, text: str) -> List[Dict[str, any]]:
        """Manually extract agenda items using regex patterns."""
        sections = []
        current_section = None
        
        # Updated section patterns to catch all sections
        section_patterns = [
            (r'^([A-Z])\.\s+(.+)$', 'SECTION'),  # Letter. Section Name
            (r'^(CITY MANAGER ITEMS?)$', 'CITY_MANAGER'),
            (r'^(CITY ATTORNEY ITEMS?)$', 'CITY_ATTORNEY'),
            (r'^(BOARDS?/COMMITTEES? ITEMS?)$', 'BOARDS_COMMITTEES'),
            (r'^(PRESENTATIONS AND PROTOCOL DOCUMENTS)', 'PRESENTATIONS'),
            (r'^(APPROVAL OF MINUTES)', 'MINUTES'),
            (r'^(PUBLIC COMMENTS)', 'PUBLIC_COMMENTS'),
            (r'^(CONSENT AGENDA)', 'CONSENT'),
            (r'^(PUBLIC HEARINGS)', 'PUBLIC_HEARINGS'),
            (r'^(RESOLUTIONS)', 'RESOLUTIONS'),
            (r'^(ORDINANCES.*)', 'ORDINANCES'),
            (r'^(DISCUSSION ITEMS)', 'DISCUSSION'),
            (r'^(BOARDS AND COMMITTEES)', 'BOARDS'),
        ]
        
        # Track if we're in a section that might have "None" as content
        in_section = False
        section_content_lines = []
        
        lines = text.split('\n')
        for i, line in enumerate(lines):
            line_stripped = line.strip()
            
            # Skip empty lines
            if not line_stripped:
                continue
            
            # Check for section headers
            section_found = False
            for pattern, section_type in section_patterns:
                if re.match(pattern, line_stripped, re.IGNORECASE):
                    # Process previous section if it exists
                    if current_section:
                        # Check if section only contains "None"
                        content = ' '.join(section_content_lines).strip()
                        if content.lower() == 'none' or not current_section['items']:
                            current_section['has_items'] = False
                        sections.append(current_section)
                    
                    # Start new section
                    current_section = {
                        'section_name': line_stripped,
                        'section_type': section_type,
                        'items': [],
                        'has_items': True
                    }
                    section_content_lines = []
                    in_section = True
                    section_found = True
                    break
            
            if section_found:
                continue
            
            # Collect section content
            if in_section and current_section:
                section_content_lines.append(line_stripped)
                
            # Updated item patterns to handle multiline items
            if current_section and re.match(r'^[A-Z]\.-\d+\.?\s*$', line_stripped):
                # Item code on its own line
                item_code = line_stripped.strip()
                # Look ahead for document reference
                if i + 1 < len(lines):
                    next_line = lines[i + 1].strip()
                    doc_ref_match = re.match(r'^(\d{2}-\d{4,5})', next_line)
                    if doc_ref_match:
                        doc_ref = doc_ref_match.group(1)
                        # Get title from remaining text or next lines
                        title_start = i + 1
                        title_lines = []
                        for j in range(title_start, min(i + 5, len(lines))):
                            title_line = lines[j].strip()
                            if title_line and not re.match(r'^[A-Z]\.-\d+\.?', title_line):
                                title_lines.append(title_line)
                        
                        title = ' '.join(title_lines)
                        # Remove the document reference from title
                        title = title.replace(doc_ref, '').strip()
                        
                        current_section['items'].append({
                            'item_code': item_code.rstrip('.'),
                            'document_reference': doc_ref,
                            'title': title,
                            'item_type': self._determine_item_type(title, current_section['section_type'])
                        })
                        log.info(f"Extracted multiline item: {item_code.rstrip('.')} - {doc_ref}")
                        continue
            
            # Original item patterns for single-line items
            item_patterns = [
                r'^([A-Z]\.-\d+\.?)\s+(\d{2}-\d{4,5})\s+(.+)$',  # A.-1. 23-6764
                r'^(\d+\.-\d+\.?)\s+(\d{2}-\d{4,5})\s+(.+)$',    # 1.-1. 23-6797
                r'^([A-Z]-\d+)\s+(\d{2}-\d{4,5})\s+(.+)$',       # E-1 23-6784
            ]
            
            for pattern in item_patterns:
                match = re.match(pattern, line_stripped)
                if match and current_section:
                    item_code = match.group(1)
                    doc_ref = match.group(2)
                    title = match.group(3).strip()
                    
                    # Determine item type based on title or section
                    item_type = self._determine_item_type(title, current_section.get("section_type", ""))
                    
                    item = {
                        "item_code": item_code.rstrip('.'),
                        "document_reference": doc_ref,
                        "title": title[:300],
                        "item_type": item_type
                    }
                    
                    current_section["items"].append(item)
                    log.info(f"Extracted single-line {item_type}: {item_code} - {doc_ref}")
                    break
        
        # Don't forget the last section
        if current_section:
            # Check if section only contains "None"
            content = ' '.join(section_content_lines).strip()
            if content.lower() == 'none' or not current_section['items']:
                current_section['has_items'] = False
            sections.append(current_section)
        
        total_items = sum(len(s['items']) for s in sections)
        log.info(f"Manual extraction complete: {total_items} items in {len(sections)} sections")
        
        return sections
    
    def _determine_item_type(self, title: str, section_type: str) -> str:
        """Determine item type from title and section."""
        title_lower = title.lower()
        
        # Check title first for explicit type
        if 'an ordinance' in title_lower:
            return 'Ordinance'
        elif 'a resolution' in title_lower:
            return 'Resolution'
        elif 'proclamation' in title_lower:
            return 'Proclamation'
        elif 'recognition' in title_lower:
            return 'Recognition'
        elif 'congratulations' in title_lower:
            return 'Recognition'
        elif 'presentation' in title_lower:
            return 'Presentation'
        elif section_type == 'PRESENTATIONS':
            return 'Presentation'
        elif section_type == 'MINUTES':
            return 'Minutes Approval'
        elif section_type == 'CITY_MANAGER':
            return 'City Manager Item'
        elif section_type == 'CITY_ATTORNEY':
            return 'City Attorney Item'
        elif section_type == 'BOARDS_COMMITTEES':
            return 'Board/Committee Item'
        else:
            return 'Agenda Item'  # Generic fallback

    def _build_sections_from_items(self, extracted_data: List[Dict], full_text: str) -> List[Dict[str, str]]:
        """Build sections structure from extracted items or sections."""
        if not extracted_data:
            # If no items found, return the full document as one section
            return [{
                'title': 'Full Document',
                'text': full_text
            }]
        
        # Check if we have sections (from manual extraction) or items (from LLM)
        if extracted_data and isinstance(extracted_data[0], dict) and 'section_name' in extracted_data[0]:
            # We have sections from manual extraction
            sections = []
            for section_data in extracted_data:
                section_text_parts = []
                section_text_parts.append(f"=== {section_data['section_name']} ===\n")
                
                if section_data.get('has_items', True) and section_data.get('items'):
                    for item in section_data['items']:
                        item_text = f"{item['item_code']} - {item['document_reference']}\n{item['title']}\n"
                        section_text_parts.append(item_text)
                else:
                    section_text_parts.append("None\n")
                
                sections.append({
                    'title': section_data['section_name'],
                    'text': '\n'.join(section_text_parts)
                })
            
            return sections
        else:
            # We have items from LLM extraction - group them
            sections = []
            
            # Create agenda items section
            agenda_section_text = []
            for item in extracted_data:
                item_text = f"{item.get('item_code', 'Unknown')} - {item.get('document_reference', 'Unknown')}\n{item.get('title', 'Unknown')}\n"
                agenda_section_text.append(item_text)
            
            sections.append({
                'title': 'AGENDA ITEMS',
                'text': '\n'.join(agenda_section_text)
            })
            
            return sections
    
    def _extract_hyperlinks(self, doc) -> Dict[str, Dict[str, any]]:
        """Extract hyperlinks from the document."""
        hyperlinks = {}
        
        # Try to extract links from document structure
        if hasattr(doc, 'links'):
            for link in doc.links:
                if hasattr(link, 'text') and hasattr(link, 'url'):
                    hyperlinks[link.text] = {
                        'url': link.url,
                        'page': getattr(link, 'page', 0)
                    }
        
        # Try to extract from markdown if links are preserved there
        if hasattr(doc, 'export_to_markdown'):
            markdown = doc.export_to_markdown()
            # Extract markdown links pattern [text](url)
            link_pattern = r'\[([^\]]+)\]\(([^)]+)\)'
            for match in re.finditer(link_pattern, markdown):
                text, url = match.groups()
                if text and url:
                    hyperlinks[text] = {
                        'url': url,
                        'page': 0  # We don't have page info from markdown
                    }
        
        return hyperlinks 

    def _count_items(self, extracted_data: List[Dict]) -> int:
        """Count the number of items in extracted data (items or sections with items)."""
        if not extracted_data:
            return 0
        
        # Check if we have sections or items
        if extracted_data and isinstance(extracted_data[0], dict) and 'section_name' in extracted_data[0]:
            # We have sections - count items within them
            total_items = 0
            for section in extracted_data:
                total_items += len(section.get('items', []))
            return total_items
        else:
            # We have items directly
            return len(extracted_data)


================================================================================


################################################################################
# File: scripts/graph_stages/enhanced_document_linker.py
################################################################################

# File: scripts/graph_stages/enhanced_document_linker.py

"""
Enhanced Document Linker
Links both ordinance and resolution documents to their corresponding agenda items.
Now with full OCR support for all documents.
"""

import logging
import re
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import json
from datetime import datetime
import PyPDF2
from groq import Groq
import os
from dotenv import load_dotenv
import asyncio
from asyncio import Semaphore

# Import the PDF extractor for OCR support
from .pdf_extractor import PDFExtractor

load_dotenv()

log = logging.getLogger('enhanced_document_linker')


class EnhancedDocumentLinker:
    """Links ordinance and resolution documents to agenda items."""
    
    def __init__(self,
                 openai_api_key: Optional[str] = None,
                 model: str = "gpt-4.1-mini-2025-04-14",
                 agenda_extraction_max_tokens: int = 32768):
        """Initialize the enhanced document linker."""
        self.api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY not found in environment")
        
        self.client = Groq()
        self.model = model
        self.agenda_extraction_max_tokens = agenda_extraction_max_tokens
        # Add semaphore without changing signature
        self.semaphore = Semaphore(3)  # Default value, no parameter change
        
        # Initialize PDF extractor for OCR
        self.pdf_extractor = PDFExtractor(
            pdf_dir=Path("."),  # We'll use it file by file
            output_dir=Path("city_clerk_documents/extracted_text")
        )
    
    async def link_documents_for_meeting(self, 
                                       meeting_date: str,
                                       ordinances_dir: Path,
                                       resolutions_dir: Path) -> Dict[str, List[Dict]]:
        """Process documents in parallel with rate limiting."""
        log.info(f"ðŸ”— Enhanced linking: documents for meeting date: {meeting_date}")
        log.info(f"ðŸ“ Ordinances directory: {ordinances_dir}")
        log.info(f"ðŸ“ Resolutions directory: {resolutions_dir}")
        
        # Create debug directory
        debug_dir = Path("city_clerk_documents/graph_json/debug")
        debug_dir.mkdir(exist_ok=True)
        
        # Convert date format: "01.09.2024" -> "01_09_2024"
        date_underscore = meeting_date.replace(".", "_")
        
        # Initialize results
        linked_documents = {
            "ordinances": [],
            "resolutions": []
        }
        
        # Find all matching files
        matching_files = []
        
        # Process ordinances
        if ordinances_dir.exists():
            ordinance_files = self._find_matching_files(ordinances_dir, date_underscore)
            log.info(f"ðŸ“„ Found {len(ordinance_files)} ordinance files")
            matching_files.extend([(f, "ordinance") for f in ordinance_files])
        else:
            log.warning(f"âš ï¸  Ordinances directory not found: {ordinances_dir}")
        
        # Process resolutions
        if resolutions_dir.exists():
            # Check for year subdirectory first
            year = meeting_date.split('.')[-1]  # Extract year from date
            year_dir = resolutions_dir / year
            
            if year_dir.exists():
                resolution_files = self._find_matching_files(year_dir, date_underscore)
                log.info(f"ðŸ“„ Found {len(resolution_files)} resolution files in {year} directory")
            else:
                # Fall back to main resolutions directory
                resolution_files = self._find_matching_files(resolutions_dir, date_underscore)
                log.info(f"ðŸ“„ Found {len(resolution_files)} resolution files in main directory")
            matching_files.extend([(f, "resolution") for f in resolution_files])
        else:
            log.warning(f"âš ï¸  Resolutions directory not found: {resolutions_dir}")
        
        # Process documents in parallel
        tasks = []
        for doc_path, doc_type in matching_files:
            task = self._process_document_with_semaphore(doc_path, meeting_date, doc_type)
            tasks.append(task)
        
        # Gather results
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Process results
        for result in results:
            if isinstance(result, Exception):
                log.error(f"Error processing document: {result}")
            elif result:
                doc_type = result.get('document_type', '').lower()
                if "ordinance" in doc_type:
                    linked_documents["ordinances"].append(result)
                    # Save extracted text for GraphRAG
                    self._save_extracted_text(Path(result['path']), result, "ordinance")
                else:
                    linked_documents["resolutions"].append(result)
                    # Save extracted text for GraphRAG
                    self._save_extracted_text(Path(result['path']), result, "resolution")
        
        # Save enhanced linked documents info
        with open(debug_dir / "enhanced_linked_documents.json", 'w') as f:
            json.dump(linked_documents, f, indent=2)
        
        # Log summary
        total_linked = len(linked_documents['ordinances']) + len(linked_documents['resolutions'])
        log.info(f"âœ… Enhanced linking complete:")
        log.info(f"   ðŸ“„ Ordinances: {len(linked_documents['ordinances'])}")
        log.info(f"   ðŸ“„ Resolutions: {len(linked_documents['resolutions'])}")
        log.info(f"   ðŸ“„ Total linked: {total_linked}")
        
        # Save detailed report
        self._generate_linking_report(meeting_date, linked_documents, debug_dir)
        
        return linked_documents
    
    async def _process_document_with_semaphore(self, doc_path: Path, meeting_date: str, doc_type: str):
        """Process document with semaphore for rate limiting."""
        async with self.semaphore:  # Limit concurrent LLM calls
            return await self._process_document(doc_path, meeting_date, doc_type)
    
    def _find_matching_files(self, directory: Path, date_pattern: str) -> List[Path]:
        """Find all PDF files matching the date pattern."""
        # Pattern: YYYY-## - MM_DD_YYYY.pdf
        pattern = f"*{date_pattern}.pdf"
        matching_files = list(directory.glob(pattern))
        
        # Also try without spaces in case filenames vary
        pattern2 = f"*{date_pattern}*.pdf"
        additional_files = [f for f in directory.glob(pattern2) if f not in matching_files]
        matching_files.extend(additional_files)
        
        # Also check for variations in date format
        # Some files might use dashes instead of underscores
        date_dash = date_pattern.replace("_", "-")
        pattern3 = f"*{date_dash}*.pdf"
        more_files = [f for f in directory.glob(pattern3) if f not in matching_files]
        matching_files.extend(more_files)
        
        return sorted(matching_files)
    
    async def _process_document(self, doc_path: Path, meeting_date: str, doc_type: str) -> Optional[Dict[str, Any]]:
        """Process a single document to extract agenda item reference with OCR."""
        try:
            # Extract document number from filename
            doc_match = re.match(r'^(\d{4}-\d{2,3})', doc_path.name)
            if not doc_match:
                log.warning(f"Could not parse document number from {doc_path.name}")
                return None
            
            document_number = doc_match.group(1)
            
            # Extract text using Docling OCR (replacing PyPDF2)
            log.info(f"ðŸ” Running OCR on {doc_path.name}...")
            text, pages = self.pdf_extractor.extract_text_from_pdf(doc_path)
            
            if not text:
                log.warning(f"No text extracted from {doc_path.name}")
                return None
            
            log.info(f"âœ… OCR extracted {len(text)} characters from {len(pages)} pages")
            
            # Extract agenda item code using LLM (existing logic)
            item_code = await self._extract_agenda_item_code(text, document_number, doc_type)
            
            # Extract additional metadata
            parsed_data = self._parse_document_metadata(text, doc_type)
            
            doc_info = {
                "path": str(doc_path),
                "filename": doc_path.name,
                "document_number": document_number,
                "item_code": item_code,
                "document_type": doc_type.capitalize(),
                "title": self._extract_title(text, doc_type),
                "parsed_data": parsed_data,
                "meeting_date": meeting_date,
                "full_text": text,  # Store full OCR text
                "pages": pages,     # Store page-level data
                "extraction_method": "docling_ocr"
            }
            
            log.info(f"ðŸ“„ Processed {doc_type} {doc_path.name}: Item {item_code or 'NOT_FOUND'}")
            return doc_info
            
        except Exception as e:
            log.error(f"Error processing {doc_path.name}: {e}")
            return None
    
    def _save_extracted_text(self, pdf_path: Path, doc_info: Dict, doc_type: str):
        """Save with enhanced entity hints."""
        output_dir = Path("city_clerk_documents/extracted_text")
        output_dir.mkdir(exist_ok=True)
        
        # Create filename based on document number
        doc_number = doc_info['document_number']
        meeting_date = doc_info['meeting_date'].replace('.', '_')
        output_filename = f"{doc_type}_{doc_number}_{meeting_date}_extracted.json"
        output_path = output_dir / output_filename
        
        # Add entity hints for GraphRAG
        entity_hints = {
            "explicit_entities": [],
            "relationships": []
        }
        
        # Extract all identifiers
        if doc_info.get('document_number'):
            entity_hints['explicit_entities'].append({
                'name': doc_info['document_number'],
                'type': doc_type.upper(),
                'description': f"{doc_type.title()} filing number"
            })
        
        if doc_info.get('item_code'):
            entity_hints['explicit_entities'].append({
                'name': doc_info['item_code'],
                'type': 'AGENDA_ITEM',
                'description': f"Agenda item for {doc_info.get('document_number', doc_type)}"
            })
            
            # Add relationship
            if doc_info.get('document_number'):
                entity_hints['relationships'].append({
                    'source': doc_info['document_number'],
                    'target': doc_info['item_code'],
                    'type': 'relates_to_agenda_item'
                })
        
        # Prepare data for saving
        save_data = {
            "document_type": doc_type,
            "document_number": doc_number,
            "meeting_date": doc_info['meeting_date'],
            "item_code": doc_info.get('item_code'),
            "title": doc_info.get('title'),
            "full_text": doc_info.get('full_text'),
            "pages": doc_info.get('pages', []),
            "parsed_data": doc_info.get('parsed_data', {}),
            "entity_hints": entity_hints,
            "metadata": {
                "filename": doc_info['filename'],
                "extraction_method": "docling_ocr",
                "extracted_at": datetime.now().isoformat()
            }
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, indent=2, ensure_ascii=False)
        
        log.info(f"ðŸ’¾ Saved extracted text to: {output_path}")
        
        # NEW: Also save as markdown for GraphRAG
        self._save_as_markdown(pdf_path, doc_info, doc_type, output_dir)

    def _save_as_markdown(self, doc_path: Path, doc_info: Dict[str, Any], doc_type: str, output_dir: Path):
        """Save document as markdown with enhanced metadata header."""
        markdown_dir = output_dir.parent / "extracted_markdown"
        markdown_dir.mkdir(exist_ok=True)
        
        # Build metadata header
        header = self._build_enhanced_header(doc_path, doc_info, doc_type)
        
        # Combine with full text
        full_content = header + "\n\n" + doc_info.get('full_text', '')
        
        # Save markdown file
        doc_number = doc_info['document_number']
        meeting_date = doc_info['meeting_date'].replace('.', '_')
        md_filename = f"{doc_type}_{doc_number}_{meeting_date}.md"
        md_path = markdown_dir / md_filename
        
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(full_content)
        
        log.info(f"ðŸ“ Saved markdown to: {md_path}")

    def _build_enhanced_header(self, doc_path: Path, doc_info: Dict[str, Any], doc_type: str) -> str:
        """Build enhanced metadata header for GraphRAG."""
        
        item_code = doc_info.get('item_code', 'N/A')
        doc_number = doc_info.get('document_number', 'N/A')
        
        header = f"""---
ENTITIES IN THIS DOCUMENT:
- AGENDA_ITEM: {item_code}
- {doc_type.upper()}: {doc_number}
- DOCUMENT_TYPE: {doc_type.upper()}

---

**THIS DOCUMENT CONTAINS:**
The following entities should be extracted:
- Agenda Item {item_code} (entity type: agenda_item)
- {doc_type.capitalize()} {doc_number} (entity type: {doc_type})
- Meeting Date: {doc_info.get('meeting_date', 'N/A')} (entity type: meeting)

**EXAMPLE EXTRACTION:**
From the text "relating to agenda item {item_code}", extract:
- Entity: "{item_code}", Type: "agenda_item"

From the text "{doc_type} {doc_number}", extract:
- Entity: "{doc_number}", Type: "{doc_type}"

---

{self._build_existing_header(doc_path, doc_info, doc_type)}
"""
        return header

    def _build_existing_header(self, doc_path: Path, doc_info: Dict[str, Any], doc_type: str) -> str:
        """Build the existing metadata header for backwards compatibility."""
        # Get directory structure
        parts = doc_path.parts
        path_context = '/'.join(parts[-3:-1]) if len(parts) >= 3 else ''
        
        header = f"""
DOCUMENT METADATA AND CONTEXT
=============================

**DOCUMENT IDENTIFICATION:**
- Full Path: {path_context}/{doc_path.name}
- Document Type: {doc_type.upper()}
- Filename: {doc_path.name}

**PARSED INFORMATION:**
- Document Number: {doc_info.get('document_number', 'N/A')}
- Meeting Date: {doc_info.get('meeting_date', 'N/A')}
- Related Agenda Item: {doc_info.get('item_code', 'N/A')}
- Title: {doc_info.get('title', 'N/A')}

**SEARCHABLE IDENTIFIERS:**
- DOCUMENT_NUMBER: {doc_info.get('document_number', 'N/A')}
- MEETING_DATE: {doc_info.get('meeting_date', 'N/A')}
- AGENDA_ITEM: {doc_info.get('item_code', 'N/A')}
- DOCUMENT_TYPE: {doc_type.upper()}

**NATURAL LANGUAGE DESCRIPTION:**
This is {doc_type.capitalize()} {doc_info.get('document_number', '')} from the {doc_info.get('meeting_date', '')} City Commission meeting, relating to agenda item {doc_info.get('item_code', 'unknown')}.

**QUERY HELPERS:**
- To find information about {doc_info.get('item_code', 'this item')}, search for 'Item {doc_info.get('item_code', '')}' or '{doc_info.get('item_code', '')}'
- To find this document, search for '{doc_info.get('document_number', '')}'
- This {doc_type} {self._get_doc_type_description(doc_type)}

---

## What is Item {doc_info.get('item_code', 'N/A')}?
Item {doc_info.get('item_code', 'N/A')} is implemented by this {doc_type}.
{doc_info.get('item_code', 'N/A')} refers to {doc_type} {doc_info.get('document_number', '')}.

**RELATIONSHIP**: {doc_info.get('document_number', '')} implements agenda item {doc_info.get('item_code', '')}.

---

# ORIGINAL DOCUMENT CONTENT
"""
        return header

    def _get_doc_type_description(self, doc_type: str) -> str:
        """Get description for document type."""
        descriptions = {
            'ordinance': 'modifies city code and requires multiple readings',
            'resolution': 'expresses city policy or authorizes specific actions'
        }
        return descriptions.get(doc_type, 'is an official city document')
    
    async def _extract_agenda_item_code(self, text: str, document_number: str, doc_type: str) -> Optional[str]:
        """Extract agenda item code from document text using LLM."""
        debug_dir = Path("city_clerk_documents/graph_json/debug")
        debug_dir.mkdir(exist_ok=True)
        
        # Try regex patterns first for better accuracy
        patterns = [
            r'Item\s+([A-Z]\.-?\d+\.?)',  # Item D.-1.
            r'Agenda\s+Item[:\s]+([A-Z]\.-?\d+\.?)',  # Agenda Item: D.-1.
            r'Section\s+([A-Z])[,\s]+Item\s+(\d+)',  # Section D, Item 1
            r'consent\s+agenda.*item\s+([A-Z]\.-?\d+\.?)',  # Consent Agenda ... Item D.-1.
            r'\b([A-Z]\.-\d+\.?)\s+\d{2}-\d{4}',  # D.-1. 23-6830 pattern
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                if len(match.groups()) == 2:  # Section X, Item Y format
                    code = f"{match.group(1)}-{match.group(2)}"
                else:
                    code = match.group(1)
                normalized_code = self._normalize_item_code(code)
                log.info(f"âœ… Found agenda item code via regex for {document_number}: {normalized_code}")
                return normalized_code
        
        # Customize prompt based on document type
        if doc_type == "resolution":
            doc_type_text = "resolution"
            typical_sections = "F items (e.g., F-1, F-2, F-3)"
        else:
            doc_type_text = "ordinance"
            typical_sections = "E items (e.g., E-1, E-2, E-3)"
        
        prompt = f"""You are analyzing a City of Coral Gables {doc_type_text} document (Document #{document_number}).

Your task is to find the AGENDA ITEM CODE referenced in this document.

CRITICAL INSTRUCTIONS:
1. Search the ENTIRE document for agenda item references
2. Return ONLY the code in this format: AGENDA_ITEM: [code]
3. The code should be ONLY the letter and number (e.g., E-2, F-10, H-1)
4. Do NOT include any explanations, reasoning, or additional text
5. If no agenda item is found, return: AGENDA_ITEM: NOT_FOUND

Examples of valid responses:
- AGENDA_ITEM: E-2
- AGENDA_ITEM: F-10
- AGENDA_ITEM: H-1
- AGENDA_ITEM: NOT_FOUND

DO NOT RETURN ANYTHING ELSE. NO EXPLANATIONS.

Full document text:
{text}"""
        
        try:
            response = self.client.chat.completions.create(
                model="meta-llama/llama-4-maverick-17b-128e-instruct",
                messages=[
                    {"role": "system", "content": f"You are a precise data extractor for {doc_type_text} documents. Find and extract only the agenda item code. Search the ENTIRE document thoroughly."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_completion_tokens=8192,
                top_p=1,
                stream=False,
                stop=None
            )
            
            raw_response = response.choices[0].message.content.strip()
            
            # Save raw LLM response for debugging
            with open(debug_dir / f"llm_response_{doc_type}_{document_number}_raw.txt", 'w', encoding='utf-8') as f:
                f.write(raw_response)
            
            # Parse response directly (no qwen parsing needed)
            result = raw_response
            
            # Save cleaned response
            with open(debug_dir / f"llm_response_{doc_type}_{document_number}_cleaned.txt", 'w', encoding='utf-8') as f:
                f.write(result)
            
            # Parse the response
            if "AGENDA_ITEM:" in result:
                # Extract just the code part, stopping at first space or newline after the code
                parts = result.split("AGENDA_ITEM:")[1].strip()
                
                # Extract just the code pattern (letter-number)
                code_match = re.match(r'^([A-Z]-?\d+)', parts)
                if code_match:
                    code = code_match.group(1)
                    if code != "NOT_FOUND":
                        code = self._normalize_item_code(code)
                        log.info(f"âœ… Found agenda item code for {document_number}: {code}")
                        return code
                elif parts.startswith("NOT_FOUND"):
                    log.warning(f"âŒ LLM could not find agenda item in {document_number}")
                else:
                    # Try to extract code from a messy response
                    code_pattern = r'\b([A-Z]-?\d+)\b'
                    match = re.search(code_pattern, parts)
                    if match:
                        code = self._normalize_item_code(match.group(1))
                        log.info(f"âœ… Extracted agenda item code for {document_number}: {code} (from messy response)")
                        return code
                    log.error(f"âŒ Could not parse item code from response: {parts[:100]}")
            else:
                log.error(f"âŒ Invalid LLM response format for {document_number}: {result[:100]}")
            
            return None
            
        except Exception as e:
            log.error(f"Failed to extract agenda item for {doc_type} {document_number}: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _normalize_item_code(self, code: str) -> str:
        """Normalize item code to consistent format."""
        if not code:
            return code
        
        # Remove trailing dots and spaces
        code = code.rstrip('. ')
        
        # Remove dots between letter and dash: "E.-1" -> "E-1"
        code = re.sub(r'([A-Z])\.(-)', r'\1\2', code)
        
        # Handle cases without dash: "E.1" -> "E-1"
        code = re.sub(r'([A-Z])\.(\d)', r'\1-\2', code)
        
        # Remove any remaining dots
        code = code.replace('.', '')
        
        # Ensure we have a dash between letter and number
        code = re.sub(r'([A-Z])(\d)', r'\1-\2', code)
        
        return code
    
    def _extract_title(self, text: str, doc_type: str) -> str:
        """Extract document title from text."""
        # Look for "AN ORDINANCE" or "A RESOLUTION" pattern
        if doc_type == "resolution":
            pattern = r'(A\s+RESOLUTION[^.]+\.)'
        else:
            pattern = r'(AN?\s+ORDINANCE[^.]+\.)'
            
        title_match = re.search(pattern, text[:2000], re.IGNORECASE)
        if title_match:
            return title_match.group(1).strip()
        
        # Fallback to first substantive line
        lines = text.split('\n')
        for line in lines[:20]:
            if len(line) > 20 and not line.isdigit():
                return line.strip()[:200]
        
        return f"Untitled {doc_type.capitalize()}"
    
    def _parse_document_metadata(self, text: str, doc_type: str) -> Dict[str, Any]:
        """Parse additional metadata from document."""
        metadata = {
            "document_type": doc_type
        }
        
        # Extract date passed
        date_match = re.search(r'day\s+of\s+(\w+),?\s+(\d{4})', text)
        if date_match:
            metadata["date_passed"] = date_match.group(0)
        
        # Extract vote information
        vote_match = re.search(r'PASSED\s+AND\s+ADOPTED.*?(\d+).*?(\d+)', text, re.IGNORECASE | re.DOTALL)
        if vote_match:
            metadata["vote_details"] = {
                "ayes": vote_match.group(1),
                "nays": vote_match.group(2) if len(vote_match.groups()) > 1 else "0"
            }
        
        # Extract motion information
        motion_match = re.search(r'motion\s+(?:was\s+)?made\s+by\s+([^,]+)', text, re.IGNORECASE)
        if motion_match:
            metadata["motion"] = {"moved_by": motion_match.group(1).strip()}
        
        # Extract mayor signature
        mayor_match = re.search(r'Mayor[:\s]+([^\n]+)', text[-1000:])
        if mayor_match:
            metadata["signatories"] = {"mayor": mayor_match.group(1).strip()}
        
        # Resolution-specific metadata
        if doc_type == "resolution":
            # Look for resolution-specific patterns
            purpose_match = re.search(r'(?:WHEREAS|PURPOSE)[:\s]+([^.]+)', text, re.IGNORECASE)
            if purpose_match:
                metadata["purpose"] = purpose_match.group(1).strip()
        
        return metadata
    
    def _generate_linking_report(self, meeting_date: str, linked_documents: Dict, debug_dir: Path):
        """Generate a detailed report of the linking process."""
        report = {
            "meeting_date": meeting_date,
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total_ordinances": len(linked_documents["ordinances"]),
                "total_resolutions": len(linked_documents["resolutions"]),
                "ordinances_with_items": len([d for d in linked_documents["ordinances"] if d.get("item_code")]),
                "resolutions_with_items": len([d for d in linked_documents["resolutions"] if d.get("item_code")]),
                "unlinked_ordinances": len([d for d in linked_documents["ordinances"] if not d.get("item_code")]),
                "unlinked_resolutions": len([d for d in linked_documents["resolutions"] if not d.get("item_code")])
            },
            "details": {
                "ordinances": [
                    {
                        "document_number": d["document_number"],
                        "item_code": d.get("item_code", "NOT_FOUND"),
                        "title": d.get("title", "")[:100]
                    }
                    for d in linked_documents["ordinances"]
                ],
                "resolutions": [
                    {
                        "document_number": d["document_number"],
                        "item_code": d.get("item_code", "NOT_FOUND"),
                        "title": d.get("title", "")[:100]
                    }
                    for d in linked_documents["resolutions"]
                ]
            }
        }
        
        # FIXED: Use double quotes for f-string
        report_filename = f"linking_report_{meeting_date.replace('.', '_')}.json"
        report_path = debug_dir / report_filename
        
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        log.info(f"ðŸ“Š Linking report saved to: {report_path}")
    
    # Add backward compatibility method
    async def link_documents_for_meeting_legacy(self, 
                                               meeting_date: str,
                                               documents_dir: Path) -> Dict[str, List[Dict]]:
        """Legacy method for backward compatibility - ordinances only."""
        return await self.link_documents_for_meeting(
            meeting_date,
            documents_dir,  # Ordinances directory
            Path("dummy")   # No resolutions directory
        )


================================================================================


################################################################################
# File: scripts/microsoft_framework/run_graphrag_pipeline.py
################################################################################

# File: scripts/microsoft_framework/run_graphrag_pipeline.py

#!/usr/bin/env python3
"""
Runner script for the City Clerk GraphRAG Pipeline.

This script demonstrates how to run the complete GraphRAG pipeline and view results.
"""

import asyncio
import os
import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

# Detect virtual environment
def get_venv_python():
    """Get the correct Python executable from virtual environment."""
    # Check if we're in a venv
    if sys.prefix != sys.base_prefix:
        return sys.executable
    
    # Try common venv locations
    venv_paths = [
        'venv/bin/python3',
        'venv/bin/python',
        '.venv/bin/python3',
        '.venv/bin/python',
        'city_clerk_rag/bin/python3',
        'city_clerk_rag/bin/python'
    ]
    
    for venv_path in venv_paths:
        full_path = os.path.join(os.getcwd(), venv_path)
        if os.path.exists(full_path):
            return full_path
    
    # Fallback
    return sys.executable

# Use this in all subprocess calls
PYTHON_EXE = get_venv_python()
print(f"ðŸ Using Python: {PYTHON_EXE}")

from scripts.microsoft_framework import (
    CityClerkGraphRAGPipeline,
    CityClerkQueryEngine,
    SmartQueryRouter,
    GraphRAGCosmosSync,
    handle_user_query,
    GraphRAGInitializer,
    CityClerkDocumentAdapter,
    CityClerkPromptTuner,
    GraphRAGOutputProcessor
)
from scripts.microsoft_framework.enhanced_entity_deduplicator import EnhancedEntityDeduplicator, DEDUP_CONFIGS

# ============================================================================
# PIPELINE CONTROL FLAGS - Set these to control which modules run
# ============================================================================

# Core Pipeline Steps
RUN_INITIALIZATION = True      # Initialize GraphRAG environment and settings
RUN_DOCUMENT_PREP = True       # Convert extracted JSONs to GraphRAG CSV format
RUN_PROMPT_TUNING = True       # Auto-tune prompts for city clerk domain
RUN_GRAPHRAG_INDEX = True      # Run the actual GraphRAG indexing process

# Post-Processing Steps  
DISPLAY_RESULTS = True         # Show summary of extracted entities/relationships
TEST_QUERIES = True            # Run example queries to test the system
SYNC_TO_COSMOS = False         # Sync GraphRAG results to Cosmos DB

# Advanced Options
FORCE_REINDEX = False          # Force re-indexing even if output exists
VERBOSE_MODE = True            # Show detailed progress information
SKIP_CONFIRMATION = False      # Skip confirmation prompts

# Enhanced Deduplication Control
RUN_DEDUPLICATION = True       
DEDUP_CONFIG = 'conservative'   # CHANGED from 'name_focused' to 'conservative'
DEDUP_CUSTOM_CONFIG = {
    'min_combined_score': 0.8,  # INCREASED from 0.7
    'enable_partial_name_matching': True,
    'enable_abbreviation_matching': True,
    'weights': {
        'string_similarity': 0.3,
        'token_overlap': 0.2,
        'graph_structure': 0.4,  # INCREASED weight on graph structure
        'semantic_similarity': 0.1
    }
}

# ============================================================================

async def main():
    """Main pipeline execution with modular control."""
    
    # Check environment variables
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ Error: OPENAI_API_KEY environment variable not set")
        print("Please set it with: export OPENAI_API_KEY='your-api-key'")
        return
    
    print("ðŸš€ City Clerk GraphRAG Pipeline")
    print("=" * 50)
    print("ðŸ“‹ Module Configuration:")
    print(f"   Initialize Environment: {'âœ…' if RUN_INITIALIZATION else 'â­ï¸'}")
    print(f"   Prepare Documents:      {'âœ…' if RUN_DOCUMENT_PREP else 'â­ï¸'}")
    print(f"   Tune Prompts:          {'âœ…' if RUN_PROMPT_TUNING else 'â­ï¸'}")
    print(f"   Run GraphRAG Index:    {'âœ…' if RUN_GRAPHRAG_INDEX else 'â­ï¸'}")
    print(f"   Display Results:       {'âœ…' if DISPLAY_RESULTS else 'â­ï¸'}")
    print(f"   Test Queries:          {'âœ…' if TEST_QUERIES else 'â­ï¸'}")
    print(f"   Sync to Cosmos:        {'âœ…' if SYNC_TO_COSMOS else 'â­ï¸'}")
    print("=" * 50)
    
    if not SKIP_CONFIRMATION:
        confirm = input("\nProceed with this configuration? (y/N): ")
        if confirm.lower() not in ['y', 'yes']:
            print("âŒ Pipeline cancelled")
            return
    
    try:
        graphrag_root = project_root / "graphrag_data"
        
        # Step 1: Initialize GraphRAG Environment
        if RUN_INITIALIZATION:
            print("\nðŸ“‹ Step 1: Initializing GraphRAG Environment")
            print("-" * 30)
            
            initializer = GraphRAGInitializer(project_root)
            initializer.setup_environment()
            print("âœ… GraphRAG environment initialized")
        else:
            print("\nâ­ï¸  Skipping GraphRAG initialization")
            if not graphrag_root.exists():
                print("âŒ GraphRAG root doesn't exist! Enable RUN_INITIALIZATION")
                return
        
        # Step 2: Prepare Documents
        if RUN_DOCUMENT_PREP:
            print("\nðŸ“‹ Step 2: Preparing Documents for GraphRAG")
            print("-" * 30)
            
            adapter = CityClerkDocumentAdapter(
                project_root / "city_clerk_documents/extracted_text"
            )
            
            # Use JSON files directly for better structure preservation
            df = adapter.prepare_documents_for_graphrag(graphrag_root)
            print(f"âœ… Prepared {len(df)} isolated documents for GraphRAG")
            print("   Each agenda item is now a completely separate entity")
        else:
            print("\nâ­ï¸  Skipping document preparation")
            csv_path = graphrag_root / "city_clerk_documents.csv"
            if not csv_path.exists():
                print("âŒ No prepared documents found! Enable RUN_DOCUMENT_PREP")
                return
        
        # Step 3: Prompt Tuning
        if RUN_PROMPT_TUNING:
            print("\nðŸ“‹ Step 3: Tuning Prompts for City Clerk Domain")
            print("-" * 30)
            
            tuner = CityClerkPromptTuner(graphrag_root)
            prompts_dir = graphrag_root / "prompts"

            # If we are forcing a re-index or skipping confirmation, we should always regenerate prompts
            # to ensure the latest versions from the scripts are used.
            if FORCE_REINDEX or SKIP_CONFIRMATION:
                print("ðŸ“ Forcing prompt regeneration to apply new rules...")
                if prompts_dir.exists():
                    import shutil
                    shutil.rmtree(prompts_dir)
                tuner.create_manual_prompts()
                print("âœ… Prompts regenerated successfully.")
            
            # Original interactive logic for manual runs
            else:
                if prompts_dir.exists() and list(prompts_dir.glob("*.txt")):
                    print("ðŸ“ Existing prompts found")
                    reuse = input("Use existing prompts? (Y/n): ")
                    if reuse.lower() != 'n':
                        print("ðŸ”„ Re-creating manual prompts...")
                        tuner.create_manual_prompts()
                        print("âœ… Prompts created manually")
                    else:
                        print("âœ… Using existing prompts")
                else:
                    print("ðŸ“ Creating prompts manually...")
                    tuner.create_manual_prompts()
                    print("âœ… Prompts created")
        else:
            print("\nâ­ï¸  Skipping prompt tuning")
        
        # Step 4: Run GraphRAG Indexing
        if RUN_GRAPHRAG_INDEX:
            print("\nðŸ“‹ Step 4: Running GraphRAG Indexing")
            print("-" * 30)
            
            # Check if output already exists
            output_dir = graphrag_root / "output"
            if output_dir.exists() and list(output_dir.glob("*.parquet")):
                print("ðŸ“ Existing GraphRAG output found")
                if not FORCE_REINDEX:
                    reindex = input("Re-run indexing? This may take time (y/N): ")
                    if reindex.lower() != 'y':
                        print("âœ… Using existing index")
                    else:
                        print("ðŸ—ï¸ Re-indexing documents...")
                        await run_graphrag_indexing(graphrag_root, VERBOSE_MODE)
                else:
                    print("ðŸ—ï¸ Force re-indexing documents...")
                    await run_graphrag_indexing(graphrag_root, VERBOSE_MODE)
            else:
                print("ðŸ—ï¸ Running GraphRAG indexing (this may take several minutes)...")
                await run_graphrag_indexing(graphrag_root, VERBOSE_MODE)
        else:
            print("\nâ­ï¸  Skipping GraphRAG indexing")
        
        # Step 4.5: Enhanced Entity Deduplication
        if RUN_DEDUPLICATION and RUN_GRAPHRAG_INDEX:
            print("\nðŸ“‹ Step 4.5: Enhanced Entity Deduplication")
            print("-" * 30)
            
            output_dir = graphrag_root / "output"
            if output_dir.exists() and list(output_dir.glob("*.parquet")):
                print(f"ðŸ” Running enhanced deduplication (config: {DEDUP_CONFIG})")
                
                # Get configuration
                config = DEDUP_CONFIGS.get(DEDUP_CONFIG, {})
                if DEDUP_CUSTOM_CONFIG:
                    config.update(DEDUP_CUSTOM_CONFIG)
                
                print("ðŸ“Š Deduplication configuration:")
                print(f"   - Partial name matching: {config.get('enable_partial_name_matching', True)}")
                print(f"   - Token matching: {config.get('enable_token_matching', True)}")
                print(f"   - Semantic matching: {config.get('enable_semantic_matching', True)}")
                print(f"   - Min combined score: {config.get('min_combined_score', 0.7)}")
                
                deduplicator = EnhancedEntityDeduplicator(output_dir, config)
                
                try:
                    stats = deduplicator.deduplicate_entities()
                    
                    print(f"\nâœ… Enhanced deduplication complete:")
                    print(f"   Original entities: {stats['original_entities']}")
                    print(f"   After deduplication: {stats['merged_entities']}")
                    print(f"   Entities merged: {stats['merged_count']}")
                    
                    if stats['merged_count'] > 0:
                        print(f"\nðŸ“ Deduplicated data saved to: {output_dir}/deduplicated/")
                        print(f"ðŸ“ Detailed report: {output_dir}/enhanced_deduplication_report.txt")
                        
                        # Show some examples
                        report_path = output_dir / "enhanced_deduplication_report.txt"
                        if report_path.exists():
                            with open(report_path, 'r') as f:
                                lines = f.readlines()
                                # Find and show first few merges
                                for i, line in enumerate(lines):
                                    if "â†" in line and i < len(lines) - 1:
                                        print(f"\n   Example: {line.strip()}")
                                        break
                        
                        # Ask user if they want to use deduplicated data
                        if not SKIP_CONFIRMATION:
                            use_dedup = input("\nUse deduplicated data for queries? (Y/n): ")
                            if use_dedup.lower() != 'n':
                                # Update the output directory for subsequent steps
                                output_dir = output_dir / "deduplicated"
                except Exception as e:
                    print(f"âŒ Enhanced deduplication failed: {e}")
                    if VERBOSE_MODE:
                        import traceback
                        traceback.print_exc()
            else:
                print("â­ï¸  No GraphRAG output to deduplicate")
        else:
            print("\nâ­ï¸  Skipping entity deduplication")
        
        # Step 5: Display Results Summary
        if DISPLAY_RESULTS:
            print("\nðŸ“Š Step 5: Results Summary")
            await display_results_summary(project_root)
        else:
            print("\nâ­ï¸  Skipping results display")
        
        # Step 6: Test Queries
        if TEST_QUERIES:
            print("\nðŸ” Step 6: Testing Query System")
            await test_queries(project_root)
        else:
            print("\nâ­ï¸  Skipping query testing")
        
        # Step 7: Sync to Cosmos DB
        if SYNC_TO_COSMOS:
            print("\nðŸŒ Step 7: Syncing to Cosmos DB")
            await sync_to_cosmos(project_root, skip_prompt=SKIP_CONFIRMATION)
        else:
            print("\nâ­ï¸  Skipping Cosmos DB sync")
        
        print("\nâœ… Pipeline completed successfully!")
        print("\nðŸ“š Next Steps:")
        print("   - Run queries: python scripts/microsoft_framework/test_queries.py")
        print("   - View results: Check graphrag_data/output/")
        print("   - Sync to Cosmos: Set SYNC_TO_COSMOS = True and re-run")
        
    except Exception as e:
        print(f"\nâŒ Error running pipeline: {e}")
        if VERBOSE_MODE:
            import traceback
            traceback.print_exc()

async def run_graphrag_indexing(graphrag_root: Path, verbose: bool = True):
    """Run GraphRAG indexing with optimized concurrency settings."""
    import subprocess
    
    # Set environment variables for better performance
    env = os.environ.copy()
    
    # Increase concurrency based on system capabilities
    cpu_count = os.cpu_count() or 4
    optimal_concurrency = min(cpu_count * 2, 20)  # Increased from 5
    env['GRAPHRAG_CONCURRENCY'] = str(optimal_concurrency)
    
    # Add additional performance settings
    env['GRAPHRAG_CHUNK_PARALLELISM'] = str(optimal_concurrency)
    env['GRAPHRAG_ENTITY_EXTRACTION_PARALLELISM'] = str(optimal_concurrency)
    
    print(f"ðŸš€ Running GraphRAG with concurrency level: {optimal_concurrency}")
    
    cmd = [
        PYTHON_EXE,  # Use the detected venv Python instead of sys.executable
        "-m", "graphrag", "index",
        "--root", str(graphrag_root),
        "--emit", "parquet",  # Use parquet for better performance
    ]
    
    if verbose:
        cmd.append("--verbose")
    
    # Run with optimized environment
    process = subprocess.Popen(
        cmd, 
        stdout=subprocess.PIPE, 
        stderr=subprocess.STDOUT, 
        text=True,
        env=env
    )
    
    # Stream output
    for line in iter(process.stdout.readline, ''):
        if line:
            print(f"   {line.strip()}")
    
    process.wait()
    
    if process.returncode == 0:
        print("âœ… GraphRAG indexing completed successfully")
    else:
        raise Exception(f"GraphRAG indexing failed with code {process.returncode}")

async def display_results_summary(project_root: Path):
    """Display summary of GraphRAG results."""
    print("-" * 30)
    
    from scripts.microsoft_framework import GraphRAGOutputProcessor
    
    output_dir = project_root / "graphrag_data/output"
    
    # Check if deduplicated data exists
    dedup_dir = output_dir / "deduplicated"
    if dedup_dir.exists() and list(dedup_dir.glob("*.parquet")):
        print("ðŸ“Š Using deduplicated data")
        output_dir = dedup_dir
    
    processor = GraphRAGOutputProcessor(output_dir)
    
    # Get summaries
    entity_summary = processor.get_entity_summary()
    relationship_summary = processor.get_relationship_summary()
    
    if entity_summary:
        print(f"ðŸ·ï¸ Entities extracted: {entity_summary.get('total_entities', 0)}")
        print("ðŸ“‹ Entity types:")
        for entity_type, count in entity_summary.get('entity_types', {}).items():
            print(f"   - {entity_type}: {count}")
    
    if relationship_summary:
        print(f"\nðŸ”— Relationships extracted: {relationship_summary.get('total_relationships', 0)}")
        print("ðŸ“‹ Relationship types:")
        for rel_type, count in relationship_summary.get('relationship_types', {}).items():
            print(f"   - {rel_type}: {count}")
    
    # Show file locations
    print(f"\nðŸ“ Output files location: {output_dir}")
    output_files = [
        "entities.parquet",
        "relationships.parquet", 
        "communities.parquet",
        "community_reports.parquet"
    ]
    
    for filename in output_files:
        file_path = output_dir / filename
        if file_path.exists():
            size = file_path.stat().st_size / 1024  # KB
            print(f"   âœ… {filename} ({size:.1f} KB)")
        else:
            print(f"   âŒ {filename} (not found)")

async def test_queries(project_root: Path):
    """Test the query system with example queries."""
    print("-" * 30)
    
    # Example queries for city clerk documents
    test_queries = [
        "Who is Commissioner Smith?",  # Should use Local search
        "What are the main themes in city development?",  # Should use Global search
        "How has the waterfront project evolved?",  # Should use DRIFT search
        "Tell me about ordinance 2024-01",  # Should use Local search
        "What are the overall budget trends?",  # Should use Global search
    ]
    
    query_engine = CityClerkQueryEngine(project_root / "graphrag_data")
    router = SmartQueryRouter()
    
    for query in test_queries:
        print(f"\nâ“ Query: '{query}'")
        
        # Show routing decision
        route_info = router.determine_query_method(query)
        print(f"ðŸŽ¯ Router selected: {route_info['method']} ({route_info['intent'].value})")
        
        try:
            # Execute query
            result = await query_engine.query(query)
            print(f"ðŸ“ Answer preview: {result['answer'][:200]}...")
            print(f"ðŸ”§ Parameters used: {result['parameters']}")
        except Exception as e:
            print(f"âŒ Query failed: {e}")

async def sync_to_cosmos(project_root: Path, skip_prompt: bool = False):
    """Optionally sync results to Cosmos DB."""
    print("-" * 30)
    
    if not skip_prompt:
        user_input = input("Do you want to sync GraphRAG results to Cosmos DB? (y/N): ")
        if user_input.lower() not in ['y', 'yes']:
            print("â­ï¸ Skipping Cosmos DB sync")
            return
    
    try:
        output_dir = project_root / "graphrag_data/output"
        sync = GraphRAGCosmosSync(output_dir)
        await sync.sync_to_cosmos()
        print("âœ… Successfully synced to Cosmos DB")
    except Exception as e:
        print(f"âŒ Cosmos DB sync failed: {e}")

def show_usage():
    """Show usage instructions."""
    print("""
ðŸš€ City Clerk GraphRAG Pipeline Runner

CONTROL FLAGS:
   Edit the boolean flags at the top of this file to control which modules run:
   
   RUN_INITIALIZATION - Initialize GraphRAG environment
   RUN_DOCUMENT_PREP - Convert documents to CSV format
   RUN_PROMPT_TUNING - Auto-tune prompts
   RUN_GRAPHRAG_INDEX - Run indexing process
   RUN_DEDUPLICATION - Apply enhanced entity deduplication
   DEDUP_CONFIG - Deduplication preset: 'aggressive', 'conservative', 'name_focused'
   DISPLAY_RESULTS - Show summary statistics
   TEST_QUERIES - Test example queries
   SYNC_TO_COSMOS - Sync to Cosmos DB
   
USAGE:
   python3 scripts/microsoft_framework/run_graphrag_pipeline.py [options]
   
OPTIONS:
   -h, --help     Show this help message
   --force        Force re-indexing (sets FORCE_REINDEX=True)
   --quiet        Minimal output (sets VERBOSE_MODE=False)
   --yes          Skip confirmations (sets SKIP_CONFIRMATION=True)
   --cosmos       Enable Cosmos sync (sets SYNC_TO_COSMOS=True)
   --dedup-config TYPE  Set deduplication config (aggressive/conservative/name_focused)
   --no-dedup     Disable entity deduplication

EXAMPLES:
   # Run with default settings
   python3 scripts/microsoft_framework/run_graphrag_pipeline.py
   
   # Force complete re-index
   python3 scripts/microsoft_framework/run_graphrag_pipeline.py --force --yes
   
   # Just test queries (edit flags to disable other steps)
   python3 scripts/microsoft_framework/run_graphrag_pipeline.py
    """)

if __name__ == "__main__":
    # Parse command line arguments
    if len(sys.argv) > 1:
        for arg in sys.argv[1:]:
            if arg in ['-h', '--help', 'help']:
                show_usage()
                sys.exit(0)
            elif arg == '--force':
                FORCE_REINDEX = True
            elif arg == '--quiet':
                VERBOSE_MODE = False
            elif arg == '--yes':
                SKIP_CONFIRMATION = True
            elif arg == '--cosmos':
                SYNC_TO_COSMOS = True
            elif arg.startswith('--dedup-config='):
                DEDUP_CONFIG = arg.split('=')[1]
            elif arg == '--no-dedup':
                RUN_DEDUPLICATION = False
    
    # Run the pipeline
    asyncio.run(main())


================================================================================


################################################################################
# File: scripts/graph_stages/cosmos_db_client.py
################################################################################

# File: scripts/graph_stages/cosmos_db_client.py

"""
Azure Cosmos DB Gremlin client for city clerk graph database.
Provides async operations for graph manipulation.
"""

from __future__ import annotations
import asyncio
import logging
from typing import Any, Dict, List, Optional, Union
import os
from gremlin_python.driver import client, serializer
from gremlin_python.driver.driver_remote_connection import DriverRemoteConnection
from gremlin_python.structure.graph import Graph
from dotenv import load_dotenv
import json

load_dotenv()

log = logging.getLogger('cosmos_graph_client')


class CosmosGraphClient:
    """Async client for Azure Cosmos DB Gremlin API."""
    
    def __init__(self, 
                 endpoint: Optional[str] = None,
                 key: Optional[str] = None,
                 database: Optional[str] = None,
                 container: Optional[str] = None,
                 partition_value: str = "demo"):
        """Initialize Cosmos DB client."""
        self.endpoint = endpoint or os.getenv("COSMOS_ENDPOINT")
        self.key = key or os.getenv("COSMOS_KEY")
        self.database = database or os.getenv("COSMOS_DATABASE", "cgGraph")
        self.container = container or os.getenv("COSMOS_CONTAINER", "cityClerk")
        self.partition_value = partition_value
        
        if not all([self.endpoint, self.key, self.database, self.container]):
            raise ValueError("Missing required Cosmos DB configuration")
        
        self._client = None
        self._loop = None  # Don't get the loop in __init__
    
    async def connect(self) -> None:
        """Establish connection to Cosmos DB."""
        try:
            # Get the current running loop
            self._loop = asyncio.get_running_loop()
            
            self._client = client.Client(
                f"{self.endpoint}/gremlin",
                "g",
                username=f"/dbs/{self.database}/colls/{self.container}",
                password=self.key,
                message_serializer=serializer.GraphSONSerializersV2d0()
            )
            log.info(f"âœ… Connected to Cosmos DB: {self.database}/{self.container}")
        except Exception as e:
            log.error(f"âŒ Failed to connect to Cosmos DB: {e}")
            raise
    
    async def _execute_query(self, query: str, bindings: Optional[Dict] = None) -> List[Any]:
        """Execute a Gremlin query asynchronously."""
        if not self._client:
            await self.connect()
        
        try:
            # Get the current event loop
            loop = asyncio.get_running_loop()
            
            # Run synchronous operation in thread pool
            future = loop.run_in_executor(
                None,
                lambda: self._client.submit(query, bindings or {})
            )
            callback = await future
            
            # Collect results
            results = []
            for result in callback:
                results.extend(result)
            
            return results
        except Exception as e:
            log.error(f"Query execution failed: {query[:100]}... Error: {e}")
            raise
    
    async def clear_graph(self) -> None:
        """Clear all vertices and edges from the graph."""
        log.warning("ðŸ—‘ï¸  Clearing entire graph...")
        try:
            # Drop all vertices (edges are automatically removed)
            await self._execute_query("g.V().drop()")
            log.info("âœ… Graph cleared successfully")
        except Exception as e:
            log.error(f"Failed to clear graph: {e}")
            raise
    
    async def create_vertex(self, 
                          label: str,
                          vertex_id: str,
                          properties: Dict[str, Any],
                          update_if_exists: bool = True) -> None:
        """Create a vertex with properties, optionally updating if exists."""
        
        # Check if vertex already exists
        if await self.vertex_exists(vertex_id):
            if update_if_exists:
                # Update existing vertex
                await self.update_vertex(vertex_id, properties)
                log.info(f"Updated existing vertex: {vertex_id}")
            else:
                log.info(f"Vertex already exists, skipping: {vertex_id}")
            return
        
        # Build property chain
        prop_chain = ""
        for key, value in properties.items():
            if value is not None:
                # Handle different value types
                if isinstance(value, bool):
                    prop_chain += f".property('{key}', {str(value).lower()})"
                elif isinstance(value, (int, float)):
                    prop_chain += f".property('{key}', {value})"
                elif isinstance(value, list):
                    # Convert list to JSON string
                    json_val = json.dumps(value).replace("'", "\\'")
                    prop_chain += f".property('{key}', '{json_val}')"
                else:
                    # Escape string values
                    escaped_val = str(value).replace("'", "\\'").replace('"', '\\"')
                    prop_chain += f".property('{key}', '{escaped_val}')"
        
        # Always add partition key
        prop_chain += f".property('partitionKey', '{self.partition_value}')"
        
        query = f"g.addV('{label}').property('id', '{vertex_id}'){prop_chain}"
        
        await self._execute_query(query)

    async def update_vertex(self, vertex_id: str, properties: Dict[str, Any]) -> None:
        """Update properties of an existing vertex."""
        # Build property update chain
        prop_chain = ""
        for key, value in properties.items():
            if value is not None:
                if isinstance(value, bool):
                    prop_chain += f".property('{key}', {str(value).lower()})"
                elif isinstance(value, (int, float)):
                    prop_chain += f".property('{key}', {value})"
                elif isinstance(value, list):
                    json_val = json.dumps(value).replace("'", "\\'")
                    prop_chain += f".property('{key}', '{json_val}')"
                else:
                    escaped_val = str(value).replace("'", "\\'").replace('"', '\\"')
                    prop_chain += f".property('{key}', '{escaped_val}')"
        
        query = f"g.V('{vertex_id}'){prop_chain}"
        
        try:
            await self._execute_query(query)
            log.info(f"Updated vertex {vertex_id}")
        except Exception as e:
            log.error(f"Failed to update vertex {vertex_id}: {e}")
            raise

    async def upsert_vertex(self, 
                           label: str,
                           vertex_id: str,
                           properties: Dict[str, Any]) -> bool:
        """Create or update a vertex. Returns True if created, False if updated."""
        # Check if vertex exists
        if await self.vertex_exists(vertex_id):
            await self.update_vertex(vertex_id, properties)
            return False  # Updated
        else:
            await self.create_vertex(label, vertex_id, properties)
            return True  # Created

    async def create_edge(self,
                         from_id: str,
                         to_id: str,
                         edge_type: str,
                         properties: Optional[Dict[str, Any]] = None) -> None:
        """Create an edge between two vertices."""
        # Build property chain for edge
        prop_chain = ""
        if properties:
            for key, value in properties.items():
                if value is not None:
                    if isinstance(value, bool):
                        prop_chain += f".property('{key}', {str(value).lower()})"
                    elif isinstance(value, (int, float)):
                        prop_chain += f".property('{key}', {value})"
                    else:
                        escaped_val = str(value).replace("'", "\\'")
                        prop_chain += f".property('{key}', '{escaped_val}')"
        
        query = f"g.V('{from_id}').addE('{edge_type}').to(g.V('{to_id}')){prop_chain}"
        
        try:
            await self._execute_query(query)
        except Exception as e:
            log.error(f"Failed to create edge {from_id} -> {to_id}: {e}")
            raise
    
    async def create_edge_if_not_exists(self,
                                       from_id: str,
                                       to_id: str,
                                       edge_type: str,
                                       properties: Optional[Dict[str, Any]] = None) -> bool:
        """Create an edge if it doesn't already exist. Returns True if created."""
        # Check if edge already exists
        check_query = f"g.V('{from_id}').outE('{edge_type}').where(inV().hasId('{to_id}')).count()"
        
        try:
            result = await self._execute_query(check_query)
            exists = result[0] > 0 if result else False
            
            if not exists:
                await self.create_edge(from_id, to_id, edge_type, properties)
                return True
            else:
                log.debug(f"Edge already exists: {from_id} -[{edge_type}]-> {to_id}")
                return False
        except Exception as e:
            log.error(f"Failed to check/create edge: {e}")
            raise
    
    async def vertex_exists(self, vertex_id: str) -> bool:
        """Check if a vertex exists."""
        result = await self._execute_query(f"g.V('{vertex_id}').count()")
        return result[0] > 0 if result else False
    
    async def get_vertex(self, vertex_id: str) -> Optional[Dict]:
        """Get a vertex by ID."""
        result = await self._execute_query(f"g.V('{vertex_id}').valueMap(true)")
        return result[0] if result else None
    
    async def close(self) -> None:
        """Close the connection properly."""
        if self._client:
            try:
                # Close the client synchronously since it's not async
                self._client.close()
                self._client = None
                log.info("Connection closed")
            except Exception as e:
                log.warning(f"Error during client close: {e}")
    
    async def __aenter__(self):
        await self.connect()
        return self
    
    async def __aexit__(self, *args):
        await self.close()


================================================================================


################################################################################
# File: scripts/microsoft_framework/graphrag_output_processor.py
################################################################################

# File: scripts/microsoft_framework/graphrag_output_processor.py

from pathlib import Path
import pandas as pd
import json
from typing import Dict, List, Any
import re
import ast
import logging

log = logging.getLogger(__name__)

class GraphRAGOutputProcessor:
    """Process and load GraphRAG output artifacts."""
    
    def __init__(self, output_dir: Path):
        self.output_dir = Path(output_dir)
        
    def parse_config_string(self, config_str: str) -> Dict[str, Any]:
        """Parse configuration strings from GraphRAG output."""
        try:
            # Handle strings like: "default_vector_store": { "type": "lancedb", ... }
            # First try to parse as JSON-like string
            if '"default_vector_store"' in config_str or "'default_vector_store'" in config_str:
                # Extract the dictionary part
                match = re.search(r'"default_vector_store"\s*:\s*({[^}]+})', config_str)
                if not match:
                    match = re.search(r"'default_vector_store'\s*:\s*({[^}]+})", config_str)
                
                if match:
                    dict_str = match.group(1)
                    # Replace single quotes with double quotes for JSON parsing
                    dict_str = dict_str.replace("'", '"')
                    # Handle None/null values
                    dict_str = dict_str.replace('None', 'null')
                    dict_str = dict_str.replace('True', 'true')
                    dict_str = dict_str.replace('False', 'false')
                    
                    return json.loads(dict_str)
            
            # Try to parse as Python literal
            return ast.literal_eval(config_str)
        except Exception as e:
            log.error(f"Failed to parse config string: {e}")
            return {}
    
    def extract_vector_store_config(self, artifacts: Dict[str, Any]) -> Dict[str, Any]:
        """Extract vector store configuration from artifacts."""
        vector_configs = {}
        
        # Check entities for embedded config
        if 'entities' in artifacts:
            entities_df = artifacts['entities']
            # Look for configuration in entity descriptions or metadata
            for _, entity in entities_df.iterrows():
                if 'description' in entity and 'default_vector_store' in str(entity['description']):
                    config = self.parse_config_string(str(entity['description']))
                    if config:
                        vector_configs['from_entities'] = config
        
        # Check community reports
        if 'community_reports' in artifacts:
            reports_df = artifacts['community_reports']
            for _, report in reports_df.iterrows():
                if 'summary' in report and 'default_vector_store' in str(report['summary']):
                    config = self.parse_config_string(str(report['summary']))
                    if config:
                        vector_configs['from_reports'] = config
        
        return vector_configs
    
    def load_artifacts(self) -> Dict[str, Any]:
        """Load all GraphRAG artifacts."""
        artifacts = {}
        
        # Load entities
        entities_path = self.output_dir / "entities.parquet"
        if entities_path.exists():
            try:
                import pandas as pd
                artifacts['entities'] = pd.read_parquet(entities_path)
            except Exception as e:
                log.error(f"Failed to load entities: {e}")
        
        # Load relationships
        relationships_path = self.output_dir / "relationships.parquet"
        if relationships_path.exists():
            try:
                import pandas as pd
                artifacts['relationships'] = pd.read_parquet(relationships_path)
            except Exception as e:
                log.error(f"Failed to load relationships: {e}")
        
        # Load community reports
        reports_path = self.output_dir / "community_reports.parquet"
        if reports_path.exists():
            try:
                import pandas as pd
                artifacts['community_reports'] = pd.read_parquet(reports_path)
            except Exception as e:
                log.error(f"Failed to load community reports: {e}")
        
        return artifacts
    
    def load_graphrag_artifacts(self) -> Dict[str, Any]:
        """Load all GraphRAG output artifacts."""
        artifacts = {}
        
        # Load entities
        entities_path = self.output_dir / "entities.parquet"
        if entities_path.exists():
            artifacts['entities'] = pd.read_parquet(entities_path)
        
        # Load relationships
        relationships_path = self.output_dir / "relationships.parquet"
        if relationships_path.exists():
            artifacts['relationships'] = pd.read_parquet(relationships_path)
        
        # Load communities
        communities_path = self.output_dir / "communities.parquet"
        if communities_path.exists():
            artifacts['communities'] = pd.read_parquet(communities_path)
        
        # Load community reports
        reports_path = self.output_dir / "community_reports.parquet"
        if reports_path.exists():
            artifacts['community_reports'] = pd.read_parquet(reports_path)
        
        return artifacts
    
    def get_entity_summary(self) -> Dict[str, Any]:
        """Get summary of entities."""
        entities_path = self.output_dir / "entities.parquet"
        if not entities_path.exists():
            return {}
        
        try:
            entities_df = pd.read_parquet(entities_path)
            
            summary = {
                'total_entities': len(entities_df),
                'entity_types': entities_df['type'].value_counts().to_dict() if 'type' in entities_df.columns else {}
            }
            return summary
        except Exception as e:
            log.error(f"Failed to load entities: {e}")
            return {}
    
    def get_relationship_summary(self) -> Dict[str, Any]:
        """Get summary of relationships."""
        relationships_path = self.output_dir / "relationships.parquet"
        if not relationships_path.exists():
            return {}
        
        try:
            relationships_df = pd.read_parquet(relationships_path)
            
            summary = {
                'total_relationships': len(relationships_df),
                'relationship_types': relationships_df['type'].value_counts().to_dict() if 'type' in relationships_df.columns else {}
            }
            return summary
        except Exception as e:
            log.error(f"Failed to load relationships: {e}")
            return {}
    
    def get_community_summary(self) -> Dict[str, Any]:
        """Get summary statistics of extracted communities."""
        communities_path = self.output_dir / "communities.parquet"
        if communities_path.exists():
            communities_df = pd.read_parquet(communities_path)
            
            summary = {
                'total_communities': len(communities_df),
                'community_types': communities_df['type'].value_counts().to_dict()
            }
            return summary
        else:
            return {}
    
    def parse_vector_store_config(self, text: str) -> dict:
        """Parse vector store configuration from text."""
        import re
        import json
        
        pattern = r'"default_vector_store"\s*:\s*(\{[^}]+\})'
        match = re.search(pattern, text)
        
        if match:
            try:
                config_str = match.group(1)
                config_str = config_str.replace("null", "null")
                config_str = config_str.replace("true", "true")
                config_str = config_str.replace("false", "false")
                return json.loads(config_str)
            except:
                pass
        return {}


================================================================================


################################################################################
# File: scripts/microsoft_framework/graphrag_initializer.py
################################################################################

# File: scripts/microsoft_framework/graphrag_initializer.py

import os
from pathlib import Path
import yaml
import subprocess
import sys
from typing import Dict, Any

class GraphRAGInitializer:
    """Initialize and configure Microsoft GraphRAG for city clerk documents."""
    
    def __init__(self, project_root: Path):
        self.project_root = Path(project_root)
        self.graphrag_root = self.project_root / "graphrag_data"
        
    def setup_environment(self):
        """Setup GraphRAG environment and configuration."""
        # Get the correct Python executable
        if hasattr(sys, 'real_prefix') or (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix):
            # We're in a virtualenv
            python_exe = sys.executable
        else:
            # Try to find venv Python
            venv_python = os.path.join(os.path.dirname(sys.executable), '..', 'venv', 'bin', 'python3')
            if os.path.exists(venv_python):
                python_exe = venv_python
            else:
                python_exe = sys.executable
        
        print(f"ðŸ Using Python: {python_exe}")
        
        # Create directory structure
        self.graphrag_root.mkdir(exist_ok=True)
        
        # Run GraphRAG init
        subprocess.run([
            python_exe,  # Use the correct Python
            "-m", "graphrag", "init", 
            "--root", str(self.graphrag_root),
            "--force"
        ])
        
        # Configure settings
        self._configure_settings()
        self._configure_prompts()
        
    def _configure_settings(self):
        """Configure with enhanced extraction."""
        settings = {
            "encoding_model": "cl100k_base",
            "skip_workflows": [],
            "models": {
                "default_chat_model": {
                    "api_key": "${OPENAI_API_KEY}",
                    "type": "openai_chat",
                    "model": "gpt-4.1-mini-2025-04-14",
                    "encoding_model": "cl100k_base",
                    "max_tokens": 32768,
                    "temperature": 0,
                    "api_type": "openai"
                },
                "default_embedding_model": {
                    "api_key": "${OPENAI_API_KEY}",
                    "type": "openai_embedding",
                    "model": "text-embedding-3-small",
                    "encoding_model": "cl100k_base",
                    "batch_size": 16,
                    "batch_max_tokens": 2048
                }
            },
            "input": {
                "type": "file",
                "file_type": "csv",
                "base_dir": ".",
                "source_column": "text",
                "text_column": "text",
                "title_column": "title"
            },
            "chunks": {
                "group_by_columns": [
                    "document_type",
                    "meeting_date",
                    "item_code"
                ],
                "overlap": 200,
                "size": 1200
            },
            "extract_graph": {
                "model_id": "default_chat_model",
                "prompt": "prompts/entity_extraction.txt",
                "entity_types": [
                    "agenda_item",
                    "ordinance", 
                    "resolution",
                    "document_number",
                    "cross_reference",
                    "person",
                    "organization",
                    "meeting",
                    "money",
                    "project"
                ],
                "max_gleanings": 3,
                "pattern_examples": {
                    "agenda_item": ["E-1", "F-10", "Item E-1", "(Agenda Item: E-1)"],
                    "ordinance": ["2024-01", "Ordinance 3576", "Ordinance No. 3576"],
                    "document_number": ["2024-01", "3576", "Resolution 2024-123"]
                }
            },
            "entity_extraction": {
                "entity_types": [
                    "person",
                    "organization",
                    "location",
                    "document",
                    "meeting",
                    "money",
                    "project",
                    "agenda_item",
                    "ordinance",
                    "resolution",
                    "contract"
                ],
                "max_gleanings": 2
            },
            "community_reports": {
                "max_input_length": 32768,
                "max_length": 2000
            },
            "claim_extraction": {
                "description": "Extract voting records, motions, and decisions",
                "enabled": True
            },
            "cluster_graph": {
                "max_cluster_size": 10
            },
            "storage": {
                "base_dir": "./output/artifacts",
                "type": "file"
            }
        }
        
        settings_path = self.graphrag_root / "settings.yaml"
        with open(settings_path, 'w') as f:
            yaml.dump(settings, f, sort_keys=False)
    
    def _configure_prompts(self):
        """Setup prompt configuration placeholders."""
        # This method will be called after auto-tuning
        pass


================================================================================


################################################################################
# File: settings.yaml
################################################################################

# File: settings.yaml

### GraphRAG Configuration for City Clerk Documents

### LLM settings ###
models:
  default_chat_model:
    type: openai_chat
    auth_type: api_key
    api_key: ${OPENAI_API_KEY}
    model: gpt-4.1-mini-2025-04-14
    encoding_model: cl100k_base
    max_tokens: 16384
    temperature: 0

  default_embedding_model:
    type: openai_embedding
    auth_type: api_key
    api_key: ${OPENAI_API_KEY}
    model: text-embedding-3-small
    batch_size: 16
    batch_max_tokens: 2048

### Input settings ###
input:
  type: file
  file_type: csv
  base_dir: "."
  file_pattern: "city_clerk_documents.csv"

### Chunking settings ###
chunks:
  size: 800
  overlap: 300

### Output/storage settings ###
storage:
  type: file
  base_dir: "output"

### Community detection settings ###
cluster_graph:
  max_cluster_size: 10

### Entity extraction ###
entity_extraction:
  entity_types:
    - person
    - organization 
    - location
    - document
    - meeting
    - money
    - project
    - agenda_item: "pattern: [A-Z]-?\\d+"
    - ordinance
    - resolution
    - contract
    - document_number: "pattern: \\d{4}-\\d+"
  max_gleanings: 3

claim_extraction:
  description: Extract voting records, motions, and decisions
  enabled: true
  prompt: prompts/city_clerk_claims.txt

community_reports:
  max_input_length: 16384
  max_length: 2000
  prompt: prompts/city_clerk_community_report.txt

# Legacy LLM config for backward compatibility
llm:
  api_key: ${OPENAI_API_KEY}
  api_type: openai
  max_tokens: 16384
  model: gpt-4.1-mini-2025-04-14
  temperature: 0

# Legacy embeddings config for backward compatibility  
embeddings:
  api_key: ${OPENAI_API_KEY}
  batch_max_tokens: 2048
  batch_size: 16
  model: text-embedding-3-small

query:
  drift_search:
    follow_up_depth: 5
    follow_up_expansion: 3
    include_global_context: true
    initial_community_level: 2
    max_iterations: 5
    max_tokens: 16384
    primer_queries: 3
    relevance_threshold: 0.7
    similarity_threshold: 0.8
    temperature: 0.0
    termination_strategy: convergence
  global_search:
    community_level: 2
    max_tokens: 16384
    n: 1
    rate_relevancy_model: gpt-4.1-mini-2025-04-14
    relevance_score_threshold: 0.7
    temperature: 0.0
    top_p: 1.0
    use_dynamic_community_selection: true
  local_search:
    community_prop: 0.1
    conversation_history_max_turns: 5
    max_tokens: 16384
    temperature: 0.0
    text_unit_prop: 0.5
    top_k_entities: 10
    top_k_relationships: 10

### Enhanced Entity Deduplication Settings ###
entity_deduplication:
  enabled: true
  
  # Matching strategies
  enable_partial_name_matching: true     # "Vince Lago" matches "Lago"
  enable_token_matching: true            # Token-based overlap
  enable_semantic_matching: true         # TF-IDF similarity
  enable_graph_structure_matching: true  # Neighbor overlap
  enable_abbreviation_matching: true     # "V. Lago" matches "Vince Lago"
  enable_role_based_matching: true       # "Mayor" matches "Mayor Lago"
  
  # Thresholds
  exact_match_threshold: 0.95
  high_similarity_threshold: 0.85
  partial_match_threshold: 0.7
  clustering_tolerance: 0.15
  min_combined_score: 0.7
  
  # Score weights
  weights:
    string_similarity: 0.2
    token_overlap: 0.4
    graph_structure: 0.2
    semantic_similarity: 0.2
  
  # Configuration presets
  presets:
    aggressive:
      min_combined_score: 0.65
    conservative:
      min_combined_score: 0.85
    name_focused:
      min_combined_score: 0.7


================================================================================


################################################################################
# File: scripts/microsoft_framework/graphrag_pipeline.py
################################################################################

# File: scripts/microsoft_framework/graphrag_pipeline.py

import asyncio
import subprocess
from pathlib import Path
import pandas as pd
import json
from typing import Dict, List, Any

# Import other components
from .graphrag_initializer import GraphRAGInitializer
from .document_adapter import CityClerkDocumentAdapter
from .prompt_tuner import CityClerkPromptTuner
from .graphrag_output_processor import GraphRAGOutputProcessor

class CityClerkGraphRAGPipeline:
    """Main pipeline for processing city clerk documents with GraphRAG."""
    
    def __init__(self, project_root: Path):
        self.project_root = Path(project_root)
        self.graphrag_root = self.project_root / "graphrag_data"
        self.output_dir = self.graphrag_root / "output"
        
    async def run_full_pipeline(self, force_reindex: bool = False):
        """Run the complete GraphRAG indexing pipeline."""
        
        # Step 1: Initialize GraphRAG
        print("ðŸ”§ Initializing GraphRAG...")
        initializer = GraphRAGInitializer(self.project_root)
        initializer.setup_environment()
        
        # Step 2: Prepare documents
        print("ðŸ“„ Preparing documents...")
        adapter = CityClerkDocumentAdapter(
            self.project_root / "city_clerk_documents/extracted_text"
        )
        df = adapter.prepare_documents_for_graphrag(self.graphrag_root)
        
        # Step 3: Run prompt tuning
        print("ðŸŽ¯ Tuning prompts for city clerk domain...")
        tuner = CityClerkPromptTuner(self.graphrag_root)
        tuner.run_auto_tuning()
        tuner.customize_prompts()
        
        # Step 4: Run GraphRAG indexing
        print("ðŸ—ï¸ Running GraphRAG indexing...")
        subprocess.run([
            "graphrag", "index",
            "--root", str(self.graphrag_root),
            "--verbose"
        ])
        
        # Step 5: Process outputs
        print("ðŸ“Š Processing GraphRAG outputs...")
        processor = GraphRAGOutputProcessor(self.output_dir)
        graph_data = processor.load_graphrag_artifacts()
        
        return graph_data


================================================================================


################################################################################
# File: scripts/microsoft_framework/__init__.py
################################################################################

# File: scripts/microsoft_framework/__init__.py

"""
Microsoft GraphRAG integration for City Clerk document processing.

This package provides components for integrating Microsoft GraphRAG with
the existing city clerk document processing pipeline.
"""

from .graphrag_initializer import GraphRAGInitializer
from .document_adapter import CityClerkDocumentAdapter
from .prompt_tuner import CityClerkPromptTuner
from .graphrag_pipeline import CityClerkGraphRAGPipeline
from .cosmos_synchronizer import GraphRAGCosmosSync
from .query_engine import CityClerkGraphRAGQuery, QueryType, CityClerkQueryEngine, handle_user_query
from .query_router import SmartQueryRouter, QueryIntent, QueryFocus
from .incremental_processor import IncrementalGraphRAGProcessor
from .graphrag_output_processor import GraphRAGOutputProcessor
from .entity_deduplicator import AdvancedEntityDeduplicator
from .enhanced_entity_deduplicator import EnhancedEntityDeduplicator

__all__ = [
    'GraphRAGInitializer',
    'CityClerkDocumentAdapter',
    'CityClerkPromptTuner',
    'CityClerkGraphRAGPipeline',
    'GraphRAGCosmosSync',
    'CityClerkGraphRAGQuery',
    'CityClerkQueryEngine',
    'QueryType',
    'SmartQueryRouter',
    'QueryIntent',
    'QueryFocus',
    'handle_user_query',
    'IncrementalGraphRAGProcessor',
    'GraphRAGOutputProcessor',
    'AdvancedEntityDeduplicator',
    'EnhancedEntityDeduplicator'
]


================================================================================


################################################################################
# File: scripts/graph_stages/__init__.py
################################################################################

# File: scripts/graph_stages/__init__.py

"""
Graph Pipeline Stages
=====================
Components for building city clerk document knowledge graph.
"""

from .cosmos_db_client import CosmosGraphClient
from .agenda_pdf_extractor import AgendaPDFExtractor
from .agenda_ontology_extractor import CityClerkOntologyExtractor
from .enhanced_document_linker import EnhancedDocumentLinker
from .agenda_graph_builder import AgendaGraphBuilder
from .verbatim_transcript_linker import VerbatimTranscriptLinker

__all__ = [
    'CosmosGraphClient',
    'AgendaPDFExtractor',
    'CityClerkOntologyExtractor',
    'EnhancedDocumentLinker',
    'AgendaGraphBuilder',
    'VerbatimTranscriptLinker'
]


================================================================================

