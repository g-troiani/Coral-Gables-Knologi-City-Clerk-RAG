# Concatenated Project Code - Part 3 of 3
# Generated: 2025-06-02 13:02:29
# Root Directory: /Users/gianmariatroiani/Documents/knologiÌŠ/graph_database
================================================================================

# File Index - Which Files Are in Which Parts
################################################################################

## Part 1 (10 files):
  - scripts/graph_stages/agenda_graph_builder.py
  - scripts/rag_local_web_app.py
  - scripts/graph_stages/document_linker.py
  - city_clerk_documents/graph_json/debug/entities_parsed.json
  - scripts/graph_stages/cosmos_db_client.py
  - scripts/topic_filter_and_title.py
  - scripts/stages/acceleration_utils.py
  - scripts/check_pipeline_setup.py
  - clear_database.py
  - city_clerk_documents/graph_json/debug/meeting_info_parsed.json

## Part 2 (11 files):
  - scripts/stages/embed_vectors.py
  - scripts/stages/chunk_text.py
  - scripts/pipeline_modular_optimized.py
  - scripts/stages/db_upsert.py
  - scripts/stages/llm_enrich.py
  - scripts/find_duplicates.py
  - city_clerk_documents/graph_json/debug/agenda_structure_chunk0_parsed.json
  - config.py
  - scripts/debug_pipeline_output.py
  - scripts/graph_stages/__init__.py
  - scripts/stages/__init__.py

## Part 3 (10 files):
  - scripts/graph_stages/agenda_ontology_extractor.py
  - scripts/stages/extract_clean.py
  - scripts/graph_pipeline.py
  - scripts/graph_stages/agenda_pdf_extractor.py
  - scripts/clear_database.py
  - scripts/test_vector_search.py
  - city_clerk_documents/graph_json/debug/all_agenda_items.json
  - scripts/test_graph_pipeline.py
  - city_clerk_documents/graph_json/debug/linked_documents.json
  - requirements.txt


================================================================================


################################################################################
# File: scripts/graph_stages/agenda_ontology_extractor.py
################################################################################

# File: scripts/graph_stages/agenda_ontology_extractor.py

"""
City Clerk Ontology Extractor - FIXED VERSION
Uses Groq LLM to extract structured data from city agenda documents.
"""

import logging
import json
import re
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
import os
from groq import Groq
from dotenv import load_dotenv

load_dotenv()

log = logging.getLogger('ontology_extractor')


class CityClerkOntologyExtractor:
    """Extract structured ontology from city clerk documents using LLM."""
    
    def __init__(self, 
                 groq_api_key: Optional[str] = None,
                 model: str = "llama-3.3-70b-versatile",
                 output_dir: Optional[Path] = None,
                 max_tokens: int = 32768):
        """Initialize the extractor with Groq client."""
        self.api_key = groq_api_key or os.getenv("GROQ_API_KEY")
        if not self.api_key:
            raise ValueError("GROQ_API_KEY not found in environment")
        
        self.client = Groq(api_key=self.api_key)
        self.model = model
        self.max_tokens = max_tokens
        self.output_dir = output_dir or Path("city_clerk_documents/graph_json")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Create debug directory
        self.debug_dir = self.output_dir / "debug"
        self.debug_dir.mkdir(exist_ok=True)
    
    def extract(self, pdf_path: Path) -> Dict[str, Any]:
        """Extract complete ontology from agenda PDF."""
        log.info(f"ğŸ§  Extracting ontology from {pdf_path.name}")
        
        # First, load the extracted text
        extracted_path = self.output_dir / f"{pdf_path.stem}_extracted.json"
        if extracted_path.exists():
            with open(extracted_path, 'r') as f:
                extracted_data = json.load(f)
        else:
            raise FileNotFoundError(f"No extracted data found for {pdf_path.name}. Run PDF extraction first.")
        
        # Get full text from sections
        full_text = "\n".join(section.get("text", "") for section in extracted_data.get("sections", []))
        
        # Save full text for debugging
        with open(self.debug_dir / f"{pdf_path.stem}_full_text.txt", 'w', encoding='utf-8') as f:
            f.write(full_text)
        
        # Extract meeting date from filename first (more reliable)
        meeting_date = self._extract_meeting_date_from_filename(pdf_path.stem)
        if not meeting_date:
            meeting_date = self._extract_meeting_date(pdf_path.stem, full_text[:1000])
        
        log.info(f"ğŸ“… Extracted meeting date: {meeting_date}")
        
        # Step 1: Extract meeting information
        meeting_info = self._extract_meeting_info(full_text[:4000])
        
        # Step 2: Extract complete agenda structure
        agenda_structure = self._extract_agenda_structure(full_text)
        
        # Step 3: Extract entities
        entities = self._extract_entities(full_text[:15000])
        
        # Step 4: Extract relationships between items
        relationships = self._extract_relationships(agenda_structure)
        
        # Build complete ontology
        ontology = {
            "meeting_date": meeting_date,
            "meeting_info": meeting_info,
            "agenda_structure": agenda_structure,
            "entities": entities,
            "relationships": relationships,
            "hyperlinks": extracted_data.get("hyperlinks", {}),
            "metadata": {
                "source_pdf": str(pdf_path.absolute()),
                "extraction_date": datetime.utcnow().isoformat() + "Z",
                "model": self.model
            }
        }
        
        # Save ontology
        output_path = self.output_dir / f"{pdf_path.stem}_ontology.json"
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(ontology, f, indent=2, ensure_ascii=False)
        
        log.info(f"âœ… Ontology extraction complete: {len(agenda_structure)} sections, {sum(len(s.get('items', [])) for s in agenda_structure)} items")
        return ontology
    
    def _extract_meeting_date_from_filename(self, filename: str) -> Optional[str]:
        """Extract meeting date from filename like 'Agenda 01.9.2024'."""
        # Try to extract date from filename
        date_patterns = [
            r'(\d{1,2})\.(\d{1,2})\.(\d{4})',  # 01.9.2024
            r'(\d{1,2})-(\d{1,2})-(\d{4})',    # 01-9-2024
            r'(\d{1,2})_(\d{1,2})_(\d{4})',    # 01_9_2024
        ]
        
        for pattern in date_patterns:
            match = re.search(pattern, filename)
            if match:
                month, day, year = match.groups()
                return f"{month.zfill(2)}.{day.zfill(2)}.{year}"
        
        return None
    
    def _extract_meeting_date(self, filename: str, text: str) -> str:
        """Extract meeting date in MM.DD.YYYY format."""
        # Try filename first
        date = self._extract_meeting_date_from_filename(filename)
        if date:
            return date
        
        # Try MM/DD/YYYY format in text
        date_match = re.search(r'(\d{1,2})/(\d{1,2})/(\d{4})', text)
        if date_match:
            month, day, year = date_match.groups()
            return f"{month.zfill(2)}.{day.zfill(2)}.{year}"
        
        # Default fallback
        return "01.09.2024"  # Based on the actual filename
    
    def _clean_json_response(self, json_text: str) -> str:
        """Clean and fix common JSON formatting issues from LLM responses."""
        # Remove markdown code blocks
        json_text = re.sub(r'```json\s*', '', json_text)
        json_text = re.sub(r'\s*```', '', json_text)
        
        # Remove any text before the first { or [
        json_start = json_text.find('{')
        array_start = json_text.find('[')
        
        if json_start == -1 and array_start == -1:
            return json_text
        
        if json_start == -1:
            start_pos = array_start
        elif array_start == -1:
            start_pos = json_start
        else:
            start_pos = min(json_start, array_start)
        
        json_text = json_text[start_pos:]
        
        # Fix common escape issues
        json_text = json_text.replace('\\n', ' ')
        json_text = json_text.replace('\\"', '"')
        json_text = json_text.replace('\\/', '/')
        
        # Fix truncated strings by closing them
        # Count quotes to detect unclosed strings
        in_string = False
        escape_next = False
        cleaned_chars = []
        
        for i, char in enumerate(json_text):
            if escape_next:
                escape_next = False
                cleaned_chars.append(char)
                continue
                
            if char == '\\':
                escape_next = True
                cleaned_chars.append(char)
                continue
                
            if char == '"':
                in_string = not in_string
                
            cleaned_chars.append(char)
        
        # If we end while still in a string, close it
        if in_string:
            cleaned_chars.append('"')
            # Also close any open braces/brackets
            open_braces = cleaned_chars.count('{') - cleaned_chars.count('}')
            open_brackets = cleaned_chars.count('[') - cleaned_chars.count(']')
            
            if open_braces > 0:
                cleaned_chars.append('}' * open_braces)
            if open_brackets > 0:
                cleaned_chars.append(']' * open_brackets)
        
        return ''.join(cleaned_chars)
    
    def _extract_meeting_info(self, text: str) -> Dict[str, Any]:
        """Extract meeting metadata using LLM."""
        prompt = """Analyze this city commission meeting agenda and extract meeting details.

Text:
{text}

IMPORTANT: Return ONLY the JSON object below. Do not include any other text, markdown formatting, or code blocks.

{{
    "meeting_type": "Regular Meeting or Special Meeting or Workshop",
    "meeting_time": "time if mentioned",
    "location": {{
        "name": "venue name",
        "address": "full address"
    }},
    "officials_present": {{
        "mayor": "name or null",
        "vice_mayor": "name or null",
        "commissioners": ["names"],
        "city_attorney": "name or null",
        "city_manager": "name or null",
        "city_clerk": "name or null"
    }}
}}""".format(text=text)
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a JSON extractor. Return only valid JSON, no markdown or other formatting."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_tokens=self.max_tokens  # Use the configurable value
            )
            
            # Save LLM response for debugging
            raw_response = response.choices[0].message.content.strip()
            with open(self.debug_dir / "meeting_info_llm_response.txt", 'w', encoding='utf-8') as f:
                f.write(raw_response)
            
            # Clean and parse JSON
            json_text = self._clean_json_response(raw_response)
            result = json.loads(json_text)
            
            # Save parsed result
            with open(self.debug_dir / "meeting_info_parsed.json", 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2)
            
            return result
            
        except json.JSONDecodeError as e:
            log.error(f"JSON decode error in meeting info: {e}")
            log.error(f"Raw response saved to debug/meeting_info_llm_response.txt")
            return self._default_meeting_info()
        except Exception as e:
            log.error(f"Failed to extract meeting info: {e}")
            return self._default_meeting_info()
    
    def _extract_agenda_structure(self, text: str) -> List[Dict[str, Any]]:
        """Extract complete agenda structure with all items."""
        # Split text into smaller chunks to avoid token limits
        max_chunk_size = 30000  # characters
        
        if len(text) > max_chunk_size:
            # Process in chunks
            chunks = [text[i:i+max_chunk_size] for i in range(0, len(text), max_chunk_size-1000)]
            all_sections = []
            
            for i, chunk in enumerate(chunks):
                log.info(f"Processing chunk {i+1}/{len(chunks)} for agenda structure")
                sections = self._extract_agenda_structure_chunk(chunk, i)
                all_sections.extend(sections)
            
            return all_sections
        else:
            return self._extract_agenda_structure_chunk(text, 0)
    
    def _extract_agenda_structure_chunk(self, text: str, chunk_num: int) -> List[Dict[str, Any]]:
        """Extract agenda structure from a text chunk."""
        prompt = """Extract the agenda structure from this city commission agenda.

CRITICAL: Each agenda item has an item code that appears BEFORE the document reference number.
Look for patterns like:
- "E.-9.    23-6825    A Resolution..."
- "F.-1.    23-6762    Update on..."
- "F-2     23-6779    Update regarding..."

The item code (E.-9., F.-1., etc.) is ESSENTIAL for linking to ordinance documents later.

Text:
{text}

Return ONLY a JSON array with this EXACT structure:
[
    {{
        "section_name": "RESOLUTIONS",
        "section_type": "RESOLUTION",
        "order": 1,
        "items": [
            {{
                "item_code": "E.-9.",
                "document_reference": "23-6825",
                "title": "A Resolution of the City Commission...",
                "item_type": "Resolution"
            }}
        ]
    }},
    {{
        "section_name": "CITY COMMISSION ITEMS", 
        "section_type": "COMMISSION",
        "order": 2,
        "items": [
            {{
                "item_code": "F.-1.",
                "document_reference": "23-6762",
                "title": "Update on Uber Pilot Program",
                "item_type": "Discussion"
            }}
        ]
    }}
]""".format(text=text)
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "Extract agenda structure. Return only JSON array, no formatting."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_tokens=self.max_tokens  # Use the configurable value
            )
            
            raw_response = response.choices[0].message.content.strip()
            
            # Save LLM response for debugging
            with open(self.debug_dir / f"agenda_structure_chunk{chunk_num}_llm_response.txt", 'w', encoding='utf-8') as f:
                f.write(raw_response)
            
            # Clean and parse JSON
            json_text = self._clean_json_response(raw_response)
            
            # Try to parse
            try:
                agenda_structure = json.loads(json_text)
            except json.JSONDecodeError:
                # If parsing fails, try to extract items manually from the text
                log.warning(f"Failed to parse LLM response, extracting items manually")
                agenda_structure = self._extract_items_manually(text)
            
            # Save parsed result
            with open(self.debug_dir / f"agenda_structure_chunk{chunk_num}_parsed.json", 'w', encoding='utf-8') as f:
                json.dump(agenda_structure, f, indent=2)
            
            # Add page numbers from sections if available
            for i, section in enumerate(agenda_structure):
                if "page_start" not in section:
                    section["page_start"] = i * 10 + 1  # Placeholder
                    section["page_end"] = (i + 1) * 10
            
            return agenda_structure
            
        except Exception as e:
            log.error(f"Failed to extract agenda structure chunk {chunk_num}: {e}")
            # Try manual extraction as fallback
            return self._extract_items_manually(text)
    
    def _extract_items_manually(self, text: str) -> List[Dict[str, Any]]:
        """Manually extract agenda items using regex patterns."""
        log.info("Attempting manual extraction of agenda items")
        
        sections = []
        
        # Split text into lines for easier processing
        lines = text.split('\n')
        
        current_section = {
            "section_name": "AGENDA ITEMS",
            "section_type": "GENERAL",
            "order": 1,
            "items": []
        }
        
        # Look for lines that match agenda item patterns
        for i, line in enumerate(lines):
            # Skip empty lines
            if not line.strip():
                continue
                
            # Check for section headers
            if re.match(r'^[A-Z][.\s]+(?:RESOLUTIONS?|ORDINANCES?|CITY COMMISSION ITEMS?)', line):
                # Save current section if it has items
                if current_section["items"]:
                    sections.append(current_section)
                
                # Start new section
                current_section = {
                    "section_name": line.strip(),
                    "section_type": self._determine_section_type(line),
                    "order": len(sections) + 1,
                    "items": []
                }
                continue
            
            # Pattern to match agenda items: "E.-9.    23-6825    Title..."
            # This pattern is flexible to handle variations
            item_match = re.match(
                r'^([A-Z]\.?-?\d+\.?)\s+(\d{2}-\d{4})\s+(.+)$',
                line.strip()
            )
            
            if item_match:
                item_code_raw = item_match.group(1)
                doc_ref = item_match.group(2)
                title = item_match.group(3).strip()
                
                # Normalize the item code to include dots and dashes consistently
                # E.9 -> E.-9.
                # E-9 -> E.-9.
                # E.-9 -> E.-9.
                # E.-9. -> E.-9. (no change)
                item_code = self._normalize_agenda_item_code(item_code_raw)
                
                # Determine item type
                item_type = "Item"
                if "resolution" in title.lower():
                    item_type = "Resolution"
                elif "ordinance" in title.lower():
                    item_type = "Ordinance"
                elif "update" in title.lower() or "discussion" in title.lower():
                    item_type = "Discussion"
                elif "presentation" in title.lower():
                    item_type = "Presentation"
                
                item = {
                    "item_code": item_code,
                    "document_reference": doc_ref,
                    "title": title[:300],  # Longer limit for titles
                    "item_type": item_type
                }
                
                current_section["items"].append(item)
                log.info(f"Extracted item: {item_code} - {doc_ref}")
        
        # Don't forget the last section
        if current_section["items"]:
            sections.append(current_section)
        
        total_items = sum(len(s['items']) for s in sections)
        log.info(f"Manual extraction complete: {total_items} items in {len(sections)} sections")
        
        return sections

    def _normalize_agenda_item_code(self, code: str) -> str:
        """Normalize agenda item code to consistent format for agenda display."""
        # Remove all spaces
        code = code.strip()
        
        # Ensure we have the letter part
        match = re.match(r'([A-Z])\.?-?(\d+)\.?', code)
        if match:
            letter = match.group(1)
            number = match.group(2)
            # Return in consistent format: "E.-9."
            return f"{letter}.-{number}."
        
        # If no match, return as is but ensure it ends with a dot
        if not code.endswith('.'):
            code += '.'
        return code

    def _determine_section_type(self, section_name: str) -> str:
        """Determine section type from section name."""
        section_name_upper = section_name.upper()
        if "RESOLUTION" in section_name_upper:
            return "RESOLUTION"
        elif "ORDINANCE" in section_name_upper:
            return "ORDINANCE"
        elif "COMMISSION" in section_name_upper:
            return "COMMISSION"
        elif "CONSENT" in section_name_upper:
            return "CONSENT"
        else:
            return "GENERAL"
    
    def _extract_entities(self, text: str) -> Dict[str, List[Dict[str, Any]]]:
        """Extract all entities mentioned in the document."""
        prompt = """Extract entities from this city agenda document.

Text:
{text}

IMPORTANT: Return ONLY the JSON object below. No markdown, no code blocks, no other text.

{{
    "people": [
        {{"name": "John Smith", "role": "Mayor", "context": "presiding"}}
    ],
    "organizations": [
        {{"name": "City Commission", "type": "government", "context": "governing body"}}
    ],
    "locations": [
        {{"name": "City Hall", "address": "405 Biltmore Way", "type": "government building"}}
    ],
    "monetary_amounts": [
        {{"amount": "$100,000", "purpose": "budget allocation", "context": "parks improvement"}}
    ],
    "dates": [
        {{"date": "01/23/2024", "event": "meeting date", "type": "meeting"}}
    ],
    "legal_references": [
        {{"type": "Resolution", "number": "2024-01", "title": "Budget Amendment"}}
    ]
}}""".format(text=text[:10000])  # Limit text to avoid token issues
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "Extract entities. Return only JSON, no formatting."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_tokens=self.max_tokens  # Use the configurable value
            )
            
            raw_response = response.choices[0].message.content.strip()
            
            # Save LLM response for debugging
            with open(self.debug_dir / "entities_llm_response.txt", 'w', encoding='utf-8') as f:
                f.write(raw_response)
            
            # Clean and parse JSON
            json_text = self._clean_json_response(raw_response)
            entities = json.loads(json_text)
            
            # Save parsed result
            with open(self.debug_dir / "entities_parsed.json", 'w', encoding='utf-8') as f:
                json.dump(entities, f, indent=2)
            
            return entities
            
        except json.JSONDecodeError as e:
            log.error(f"JSON decode error in entities: {e}")
            log.error(f"Raw response saved to debug/entities_llm_response.txt")
            return self._default_entities()
        except Exception as e:
            log.error(f"Failed to extract entities: {e}")
            return self._default_entities()
    
    def _extract_relationships(self, agenda_structure: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Extract relationships between agenda items."""
        relationships = []
        
        # Find related items based on common references
        all_items = []
        for section in agenda_structure:
            for item in section.get("items", []):
                item["section"] = section.get("section_name")
                all_items.append(item)
        
        # Save all items for debugging
        with open(self.debug_dir / "all_agenda_items.json", 'w', encoding='utf-8') as f:
            json.dump(all_items, f, indent=2)
        
        # Look for items that reference each other
        for i, item1 in enumerate(all_items):
            for j, item2 in enumerate(all_items[i+1:], i+1):
                # Check if items share document references
                if (item1.get("document_reference") and 
                    item1.get("document_reference") == item2.get("document_reference")):
                    relationships.append({
                        "from_code": item1.get("item_code"),
                        "to_code": item2.get("item_code"),
                        "relationship_type": "REFERENCES_SAME_DOCUMENT",
                        "description": f"Both reference document {item1.get('document_reference')}"
                    })
        
        return relationships
    
    def _default_meeting_info(self) -> Dict[str, Any]:
        """Return default meeting info structure."""
        return {
            "meeting_type": "Regular Meeting",
            "meeting_time": "9:00 a.m.",
            "location": {
                "name": "City Hall, Commission Chambers",
                "address": "405 Biltmore Way, Coral Gables, FL 33134"
            },
            "officials_present": {
                "mayor": None,
                "vice_mayor": None,
                "commissioners": [],
                "city_attorney": None,
                "city_manager": None,
                "city_clerk": None
            }
        }
    
    def _default_entities(self) -> Dict[str, List]:
        """Return default empty entities structure."""
        return {
            "people": [],
            "organizations": [],
            "locations": [],
            "monetary_amounts": [],
            "dates": [],
            "legal_references": []
        }


================================================================================


################################################################################
# File: scripts/stages/extract_clean.py
################################################################################

# File: scripts/stages/extract_clean.py

#!/usr/bin/env python3
"""
Stage 1-2 â€” *Extract PDF â†’ clean text â†’ logical sections*  
Optimized version with concurrent processing support.
Outputs `<n>.json` (+ a pretty `.txt`) in `city_clerk_documents/{json,txt}/`.
This file is **fully self-contained** â€“ no import from `stages.common`.
"""
from __future__ import annotations

import json, logging, os, pathlib, re, sys
from collections import Counter
from datetime import datetime
from textwrap import dedent
from typing import Any, Dict, List, Sequence, Optional
import asyncio
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing as mp

# â”€â”€â”€ helpers formerly in common.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_LATIN1_REPLACEMENTS = {  # windows-1252 fallback
    0x82: "â€š",
    0x83: "Æ’",
    0x84: "â€",
    0x85: "â€¦",
    0x86: "â€ ",
    0x87: "â€¡",
    0x88: "Ë†",
    0x89: "â€°",
    0x8A: "Å ",
    0x8B: "â€¹",
    0x8C: "Å’",
    0x8E: "Å½",
    0x91: "'",
    0x92: "'",
    0x93: '"',
    0x94: '"',
    0x95: "â€¢",
    0x96: "â€“",
    0x97: "â€”",
    0x98: "Ëœ",
    0x99: "â„¢",
    0x9A: "Å¡",
    0x9B: "â€º",
    0x9C: "Å“",
    0x9E: "Å¾",
    0x9F: "Å¸",
}
_TRANSLATE_LAT1 = str.maketrans(_LATIN1_REPLACEMENTS)
def latin1_scrub(txt:str)->str: return txt.translate(_TRANSLATE_LAT1)

_WS_RE = re.compile(r"[ \t]+\n")
def normalize_ws(txt:str)->str:
    return re.sub(r"[ \t]{2,}", " ", _WS_RE.sub("\n", txt)).strip()

def pct_ascii_letters(txt:str)->float:
    letters=sum(ch.isascii() and ch.isalpha() for ch in txt)
    return letters/max(1,len(txt))

def needs_ocr(txt:str)->bool:
    return (not txt.strip()) or ("\x00" in txt) or (pct_ascii_letters(txt)<0.15)

def scrub_nuls(obj:Any)->Any:
    if isinstance(obj,str):  return obj.replace("\x00","")
    if isinstance(obj,list): return [scrub_nuls(x) for x in obj]
    if isinstance(obj,dict): return {k:scrub_nuls(v) for k,v in obj.items()}
    return obj

def _make_converter()->DocumentConverter:
    opts = PdfPipelineOptions()
    opts.do_ocr = True
    return DocumentConverter({InputFormat.PDF: PdfFormatOption(pipeline_options=opts)})
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

from dotenv import load_dotenv
from openai import OpenAI
from supabase import create_client            # only needed if you later push rows
from tqdm import tqdm

# â”€â”€â”€ optional heavy deps ----------------------------------------------------
try:
    import PyPDF2                             # raw fallback
    from unstructured.partition.pdf import partition_pdf
    from docling.datamodel.base_models import InputFormat
    from docling.datamodel.pipeline_options import PdfPipelineOptions
    from docling.document_converter import DocumentConverter, PdfFormatOption
except ImportError as exc:                    # pragma: no cover
    sys.exit(f"Missing dependency â†’ {exc}.  Run `pip install -r requirements.txt`.")

# â”€â”€â”€ env / paths ------------------------------------------------------------
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

REPO_ROOT  = pathlib.Path(__file__).resolve().parents[2]
TXT_DIR    = REPO_ROOT / "city_clerk_documents" / "txt"
JSON_DIR   = REPO_ROOT / "city_clerk_documents" / "json"
TXT_DIR.mkdir(parents=True, exist_ok=True)
JSON_DIR.mkdir(parents=True, exist_ok=True)

GPT_META_MODEL = "gpt-4.1-mini-2025-04-14"

log = logging.getLogger(__name__)

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                           helper utilities                              â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_LATIN = {0x91:"'",0x92:"'",0x93:'"',0x94:'"',0x95:"â€¢",0x96:"â€“",0x97:"â€”"}
_DOI_RE = re.compile(r"\b10\.\d{4,9}/[-._;()/:A-Z0-9]+", re.I)
_JOURNAL_RE = re.compile(r"(Journal|Proceedings|Annals|Psychiatry|Psychology|Nature|Science)[^\n]{0,120}", re.I)
_ABSTRACT_RE = re.compile(r"(?<=\bAbstract\b[:\s])(.{50,2000}?)(?:\n[A-Z][^\n]{0,60}\n|\Z)", re.S)
_KEYWORDS_RE = re.compile(r"\bKey\s*words?\b[:\s]*(.+)", re.I)
_HEADING_TYPES = {"title","heading","header","subtitle","subheading"}

# â”€â”€â”€ minimal bib helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _authors(val)->List[str]:
    if val is None: return []
    if isinstance(val,list): return [str(x).strip() for x in val if x]
    return re.split(r"\s*,\s*|\s+and\s+", str(val).strip())

_DEF_META = {
    "document_type": None,
    "title": None,
    "date": None,
    "year": None,
    "month": None,
    "day": None,
    "mayor": None,
    "vice_mayor": None,
    "commissioners": [],
    "city_attorney": None,
    "city_manager": None,
    "city_clerk": None,
    "public_works_director": None,
    "agenda": None,
    "keywords": [],
}

def merge_meta(*sources:Dict[str,Any])->Dict[str,Any]:
    m=_DEF_META.copy()
    for src in sources:
        for k,v in src.items():
            if v not in (None,"",[],{}): m[k]=v
    m["commissioners"]=m["commissioners"] or []
    m["keywords"]=m["keywords"] or []
    return m

def bib_from_filename(pdf:pathlib.Path)->Dict[str,Any]:
    s=pdf.stem
    m=re.search(r"\b(19|20)\d{2}\b",s)
    yr=int(m.group(0)) if m else None
    if m:
        title=s[m.end():].strip(" -_")
    else:
        parts=s.split(" ",1); title=parts[1] if len(parts)==2 else s
    return {"year":yr,"title":title}

def bib_from_header(txt:str)->Dict[str,Any]:
    md={}
    if m:=_DOI_RE.search(txt): md["doi"]=m.group(0)
    if m:=_JOURNAL_RE.search(txt): md["journal"]=" ".join(m.group(0).split())
    if m:=_ABSTRACT_RE.search(txt): md["abstract"]=" ".join(m.group(1).split())
    if m:=_KEYWORDS_RE.search(txt):
        kws=[k.strip(" ;.,") for k in re.split(r"[;,]",m.group(1)) if k.strip()]
        md["keywords"]=kws
    return md

# â”€â”€â”€ GPT enrichment â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _first_words(txt:str,n:int=3000)->str: return " ".join(txt.split()[:n])

def gpt_metadata(text:str)->Dict[str,Any]:
    if not OPENAI_API_KEY: return {}
    cli=OpenAI(api_key=OPENAI_API_KEY)
    prompt=dedent(f"""
        Extract metadata from this city clerk document and return a JSON object with these fields:
        - document_type: one of [Resolution, Ordinance, Proclamation, Contract, Meeting Minutes, Agenda]
        - title: document title
        - date: full date string
        - year: numeric year
        - month: numeric month (1-12)
        - day: numeric day
        - mayor: name only (e.g., "John Smith")
        - vice_mayor: name only (e.g., "Jane Doe")
        - commissioners: array of commissioner names only (e.g., ["Robert Brown", "Sarah Johnson", "Michael Davis"])
        - city_attorney: name only
        - city_manager: name only
        - city_clerk: name only
        - public_works_director: name only
        - agenda: agenda items or summary if present
        - keywords: array of relevant keywords (e.g., ["budget", "zoning", "public safety"])
        
        Text:
        {text}
    """)
    rsp=cli.chat.completions.create(model=GPT_META_MODEL,temperature=0,
        messages=[{"role":"system","content":"Structured metadata extractor"},
                  {"role":"user","content":prompt}])
    txt=rsp.choices[0].message.content
    json_txt=re.search(r"{[\s\S]*}",txt)
    return json.loads(json_txt.group(0) if json_txt else "{}")        # type: ignore[arg-type]

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                         core extraction logic                            â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _docling_elements(doc)->List[Dict[str,Any]]:
    p:Dict[int,List[Dict[str,str]]]={}
    for it,_ in doc.iterate_items():
        pg=getattr(it.prov[0],"page_no",1)
        lbl=(getattr(it,"label","") or "").upper()
        typ="heading" if lbl in ("TITLE","SECTION_HEADER","HEADER") else \
            "list_item" if lbl=="LIST_ITEM" else \
            "table" if lbl=="TABLE" else "paragraph"
        p.setdefault(pg,[]).append({"type":typ,"text":str(getattr(it,"text",it)).strip()})
    out=[]
    for pn in sorted(p):
        out.append({"section":f"Page {pn}","page_number":pn,
                    "text":"\n".join(el["text"] for el in p[pn]),
                    "elements":p[pn]})
    return out

def _unstructured_elements(pdf:pathlib.Path)->List[Dict[str,Any]]:
    els=partition_pdf(str(pdf),strategy="hi_res")
    pages:Dict[int,List[Dict[str,str]]]={}
    for el in els:
        pn=getattr(el.metadata,"page_number",1)
        pages.setdefault(pn,[]).append({"type":el.category or "paragraph",
                                        "text":normalize_ws(str(el))})
    return [{"section":f"Page {pn}","page_number":pn,
             "text":"\n".join(e["text"] for e in it),"elements":it}
            for pn,it in sorted(pages.items())]

def _pypdf_elements(pdf:pathlib.Path)->List[Dict[str,Any]]:
    out=[]
    with open(pdf,"rb") as fh:
        for pn,pg in enumerate(PyPDF2.PdfReader(fh).pages,1):
            raw=normalize_ws(pg.extract_text() or "")
            paras=[p for p in re.split(r"\n{2,}",raw) if p.strip()]
            out.append({"section":f"Page {pn}","page_number":pn,
                        "text":"\n".join(paras),
                        "elements":[{"type":"paragraph","text":p} for p in paras]})
    return out

def _group_by_headings(page_secs:Sequence[Dict[str,Any]])->List[Dict[str,Any]]:
    out=[]; cur=None; last=1
    for s in page_secs:
        pn=s.get("page_number",last); last=pn
        for el in s.get("elements",[]):
            kind=(el.get("type")or"").lower(); txt=el.get("text","").strip()
            if kind in _HEADING_TYPES and txt:
                if cur: out.append(cur)
                cur={"section":txt,"page_start":pn,"page_end":pn,"elements":[el.copy()]}
            else:
                if not cur:
                    cur={"section":"(untitled)","page_start":pn,"page_end":pn,"elements":[]}
                cur["elements"].append(el.copy()); cur["page_end"]=pn
    if cur and not out: out.append(cur)
    return out

def _sections_md(sections:List[Dict[str,Any]])->str:
    md=[]
    for s in sections:
        md.append(f"# {s.get('section','(Untitled)')}")
        for el in s.get("elements",[]): md.append(el.get("text",""))
        md.append("")
    return "\n".join(md).strip()

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                         Inline extract_pdf from common                   â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def extract_pdf(
    pdf: pathlib.Path,
    txt_dir: pathlib.Path,
    json_dir: pathlib.Path,
    conv: DocumentConverter,
    *,
    overwrite: bool = False,
    ocr_lang: str = "eng",
    keep_markup: bool = True,
    docling_only: bool = False,
    stats: Counter | None = None,
    enrich_llm: bool = True,
) -> pathlib.Path:
    """Return path to JSON payload ready for downstream stages."""
    txt_path = txt_dir / f"{pdf.stem}.txt"
    json_path = json_dir / f"{pdf.stem}.json"
    if not overwrite and txt_path.exists() and json_path.exists():
        return json_path

    # â€“â€“ 1. Docling
    page_secs: List[Dict[str, Any]] = []
    bundle = None
    try:
        bundle = conv.convert(str(pdf))
        if keep_markup:
            page_secs = _docling_elements(bundle.document)
        else:
            full = bundle.document.export_to_text(page_break_marker="\f")
            page_secs = [{
                "section": "Full document",
                "page_number": 1,
                "text": full,
                "elements": [{"type": "paragraph", "text": full}],
            }]
    except Exception as exc:
        log.warning("Docling failed on %s â†’ %s", pdf.name, exc)

    # â€“â€“ 2. Unstructured fallback
    if not page_secs and not docling_only:
        try:
            page_secs = _unstructured_elements(pdf)
        except Exception as exc:
            log.warning("unstructured failed on %s â†’ %s", pdf.name, exc)

    # â€“â€“ 3. PyPDF last resort
    if not page_secs and not docling_only:
        log.info("PyPDF2 fallback on %s", pdf.name)
        page_secs = _pypdf_elements(pdf)

    if not page_secs:
        raise RuntimeError("No text extracted from PDF")

    # Latin-1 scrub + OCR repair
    for sec in page_secs:
        sec["text"] = latin1_scrub(sec.get("text", ""))
        for el in sec.get("elements", []):
            el["text"] = latin1_scrub(el.get("text", ""))
        pn = sec.get("page_number")
        need_ocr_before = bool(pn and needs_ocr(sec["text"]))
        if need_ocr_before:
            try:
                from pdfplumber import open as pdfopen
                import pytesseract

                with pdfopen(str(pdf)) as doc:
                    pil = doc.pages[pn - 1].to_image(resolution=300).original
                ocr_txt = normalize_ws(
                    pytesseract.image_to_string(pil, lang=ocr_lang)
                )
                if ocr_txt:
                    sec["text"] = ocr_txt
                    sec["elements"] = [
                        {"type": "paragraph", "text": p}
                        for p in re.split(r"\n{2,}", ocr_txt)
                        if p.strip()
                    ]
                    if stats is not None:
                        stats["ocr_pages"] += 1
            except Exception:
                pass

    # when markup is disabled we already have final sections
    logical_secs = (
        _group_by_headings(page_secs) if keep_markup else page_secs
    )

    # CRITICAL: Ensure EVERY section has a 'text' field
    # This is required by the chunking stage
    for sec in logical_secs:
        if 'elements' in sec:
            # Build text from elements if not already present
            element_texts = []
            for el in sec.get("elements", []):
                el_text = el.get("text", "")
                # Skip internal docling metadata that starts with self_ref=
                if el_text and not el_text.startswith('self_ref='):
                    element_texts.append(el_text)
            
            # Always set text field (overwrite if exists to ensure consistency)
            sec["text"] = "\n".join(element_texts)
        elif 'text' not in sec:
            # Fallback for sections without elements
            sec["text"] = ""
    
    # Also ensure page_secs have text (for debugging/consistency)
    for sec in page_secs:
        if 'text' not in sec and 'elements' in sec:
            element_texts = []
            for el in sec.get("elements", []):
                el_text = el.get("text", "")
                if el_text and not el_text.startswith('self_ref='):
                    element_texts.append(el_text)
            sec["text"] = "\n".join(element_texts)

    header_txt = " ".join(s["text"] for s in page_secs[:2])[:8000]
    heuristic_meta = merge_meta(
        bundle.document.metadata.model_dump()
        if "bundle" in locals() and hasattr(bundle.document, "metadata")
        else {},
        bib_from_filename(pdf),
        bib_from_header(header_txt),
    )

    # single GPT call for **all** metadata fields -----------------------
    llm_meta: Dict[str, Any] = {}
    if enrich_llm and OPENAI_API_KEY:
        try:
            oa = OpenAI(api_key=OPENAI_API_KEY)
            llm_meta = gpt_metadata(
                _first_words(" ".join(s["text"] for s in logical_secs))
            )
        except Exception as exc:
            log.warning("LLM metadata extraction failed on %s â†’ %s", pdf.name, exc)

    meta = merge_meta(heuristic_meta, llm_meta)

    payload = {
        **meta,
        "created_at": datetime.utcnow().isoformat() + "Z",
        "source_pdf": str(pdf.resolve()),
        "sections": logical_secs,
    }

    json_path.parent.mkdir(parents=True, exist_ok=True)
    txt_path.parent.mkdir(parents=True, exist_ok=True)
    json_path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), "utf-8")

    # --- Markdown serialisation (rich) ---
    md_text = _sections_md(logical_secs) if keep_markup else "\n".join(
        "# " + s.get("section", "(No title)") + "\n\n" + s.get("text", "") for s in logical_secs
    )
    txt_path.write_text(md_text, "utf-8")

    if stats is not None:
        stats["processed"] += 1

    return json_path

# New: Thread-safe converter pool
class ConverterPool:
    """Thread-safe pool of DocumentConverter instances."""
    def __init__(self, size: int = None):
        self.size = size or mp.cpu_count()
        self._converters = []
        self._lock = mp.Lock()
        self._initialized = False
    
    def get(self):
        """Get a converter from the pool."""
        with self._lock:
            if not self._initialized:
                self._initialize()
            return self._converters[mp.current_process()._identity[0] % self.size]
    
    def _initialize(self):
        """Lazy initialize converters."""
        for _ in range(self.size):
            self._converters.append(_make_converter())
        self._initialized = True

# Global converter pool
_converter_pool = ConverterPool()

def extract_pdf_concurrent(
    pdf: pathlib.Path,
    txt_dir: pathlib.Path,
    json_dir: pathlib.Path,
    *,
    overwrite: bool = False,
    ocr_lang: str = "eng",
    keep_markup: bool = True,
    docling_only: bool = False,
    stats: Counter | None = None,
    enrich_llm: bool = True,
) -> pathlib.Path:
    """Thread-safe version of extract_pdf that uses converter pool."""
    conv = _converter_pool.get()
    return extract_pdf(
        pdf, txt_dir, json_dir, conv,
        overwrite=overwrite,
        ocr_lang=ocr_lang,
        keep_markup=keep_markup,
        docling_only=docling_only,
        stats=stats,
        enrich_llm=enrich_llm
    )

async def extract_batch_async(
    pdfs: List[pathlib.Path],
    *,
    overwrite: bool = False,
    keep_markup: bool = True,
    ocr_lang: str = "eng",
    enrich_llm: bool = True,
    max_workers: Optional[int] = None,
) -> List[pathlib.Path]:
    """Extract multiple PDFs concurrently."""
    from .acceleration_utils import hardware
    
    stats = Counter()
    results = []
    
    # Use process pool for CPU-bound extraction
    with hardware.get_process_pool(max_workers) as executor:
        # Submit all tasks
        future_to_pdf = {
            executor.submit(
                extract_pdf_concurrent,
                pdf, TXT_DIR, JSON_DIR,
                overwrite=overwrite,
                ocr_lang=ocr_lang,
                keep_markup=keep_markup,
                enrich_llm=enrich_llm,
                stats=stats
            ): pdf
            for pdf in pdfs
        }
        
        # Process completed tasks
        from tqdm import tqdm
        for future in tqdm(as_completed(future_to_pdf), total=len(pdfs), desc="Extracting PDFs"):
            pdf = future_to_pdf[future]
            try:
                json_path = future.result()
                results.append(json_path)
            except Exception as exc:
                log.error(f"Failed to extract {pdf.name}: {exc}")
                
    log.info(f"Extraction complete: {stats['processed']} processed, {stats['ocr_pages']} OCR pages")
    return results

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                          public entry-point                             â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ---------------------------------------------------------------------------
#  Stage-1/2 wrapper (now paper-thin)
# ---------------------------------------------------------------------------
def run_one(
    pdf: pathlib.Path,
    *,
    overwrite: bool = False,
    keep_markup: bool = True,
    ocr_lang: str = "eng",
    enrich_llm: bool = True,
    conv=None,
    stats: Counter | None = None,
) -> pathlib.Path:
    conv = conv or _make_converter()
    out = extract_pdf(
        pdf,
        TXT_DIR,
        JSON_DIR,
        conv,
        overwrite=overwrite,
        ocr_lang=ocr_lang,
        keep_markup=keep_markup,
        enrich_llm=enrich_llm,
        stats=stats,
    )
    return out

def json_path_for(pdf:pathlib.Path)->pathlib.Path:
    """Helper if Stage 1 is disabled but its artefact exists."""
    return JSON_DIR / f"{pdf.stem}.json"

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format="%(message)s")
    import argparse
    p=argparse.ArgumentParser(); p.add_argument("pdf",type=pathlib.Path)
    args=p.parse_args()
    print(run_one(args.pdf))


================================================================================


################################################################################
# File: scripts/graph_pipeline.py
################################################################################

# File: scripts/graph_pipeline.py

"""
City Clerk Graph Pipeline - UPDATED FOR SUBDIRECTORY STRUCTURE
Orchestrates the complete pipeline from PDF extraction to graph building.
"""

import asyncio
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
import json
from datetime import datetime
import argparse

# Import pipeline components
from graph_stages.agenda_pdf_extractor import AgendaPDFExtractor
from graph_stages.agenda_ontology_extractor import CityClerkOntologyExtractor
from graph_stages.agenda_graph_builder import AgendaGraphBuilder
from graph_stages.cosmos_db_client import CosmosGraphClient
from graph_stages.document_linker import DocumentLinker

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
log = logging.getLogger('graph_pipeline')

# Pipeline control flags (similar to RAG pipeline)
RUN_EXTRACT_PDF = True
RUN_EXTRACT_ONTOLOGY = True
RUN_LINK_DOCUMENTS = True
RUN_BUILD_GRAPH = True
RUN_VALIDATE_LINKS = True
CLEAR_GRAPH_FIRST = False  # Warning: This will delete all existing data!


class CityClerkGraphPipeline:
    """Main pipeline orchestrator for city clerk document graph."""
    
    def __init__(self, 
                 base_dir: Path = Path("city_clerk_documents/global"),
                 output_dir: Path = Path("city_clerk_documents/graph_json")):
        self.base_dir = base_dir
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Define subdirectory structure
        self.city_dir = self.base_dir / "City Comissions 2024"
        self.agenda_dir = self.city_dir / "Agendas"
        self.ordinances_dir = self.city_dir / "Ordinances" / "2024"
        self.resolutions_dir = self.city_dir / "Resolutions"
        
        # Initialize components
        self.pdf_extractor = AgendaPDFExtractor(output_dir)
        self.ontology_extractor = CityClerkOntologyExtractor(output_dir=output_dir)
        self.document_linker = DocumentLinker()
        self.cosmos_client = None
        self.graph_builder = None
        
        # Track processing stats
        self.stats = {
            "agendas_processed": 0,
            "ordinances_linked": 0,
            "missing_links": 0,
            "errors": 0
        }
        
        # Store all missing items for final report
        self.all_missing_items = {}
    
    async def initialize(self):
        """Initialize async components."""
        # Initialize Cosmos DB client
        self.cosmos_client = CosmosGraphClient()
        await self.cosmos_client.connect()
        
        # Initialize graph builder
        self.graph_builder = AgendaGraphBuilder(self.cosmos_client)
        
        log.info("âœ… Pipeline initialized")
    
    async def process_agenda(self, agenda_path: Path) -> Dict[str, Any]:
        """Process a single agenda through all pipeline stages."""
        log.info(f"\n{'='*60}")
        log.info(f"ğŸ“‹ Processing agenda: {agenda_path.name}")
        log.info(f"{'='*60}")
        
        result = {
            "agenda": agenda_path.name,
            "status": "pending",
            "stages": {}
        }
        
        try:
            # Stage 1: Extract PDF
            if RUN_EXTRACT_PDF:
                log.info("ğŸ“„ Stage 1: Extracting PDF content...")
                extracted_data = self.pdf_extractor.extract_agenda(agenda_path)
                result["stages"]["pdf_extraction"] = {
                    "status": "success",
                    "sections": len(extracted_data.get("sections", [])),
                    "hyperlinks": len(extracted_data.get("hyperlinks", {}))
                }
            else:
                log.info("â­ï¸  Skipping PDF extraction (RUN_EXTRACT_PDF=False)")
                # Load existing extracted data
                extracted_path = self.output_dir / f"{agenda_path.stem}_extracted.json"
                if extracted_path.exists():
                    with open(extracted_path, 'r') as f:
                        extracted_data = json.load(f)
                else:
                    raise FileNotFoundError(f"No extracted data found for {agenda_path.name}")
            
            # Stage 2: Extract Ontology
            if RUN_EXTRACT_ONTOLOGY:
                log.info("ğŸ§  Stage 2: Extracting ontology with LLM...")
                ontology = self.ontology_extractor.extract(agenda_path)
                result["stages"]["ontology_extraction"] = {
                    "status": "success",
                    "meeting_date": ontology.get("meeting_date"),
                    "agenda_items": sum(len(s.get("items", [])) for s in ontology.get("agenda_structure", []))
                }
            else:
                log.info("â­ï¸  Skipping ontology extraction (RUN_EXTRACT_ONTOLOGY=False)")
                # Load existing ontology
                ontology_path = self.output_dir / f"{agenda_path.stem}_ontology.json"
                if ontology_path.exists():
                    with open(ontology_path, 'r') as f:
                        ontology = json.load(f)
                else:
                    raise FileNotFoundError(f"No ontology found for {agenda_path.name}")
            
            # Stage 3: Link Documents
            linked_docs = {}
            if RUN_LINK_DOCUMENTS:
                log.info("ğŸ”— Stage 3: Linking ordinance documents...")
                meeting_date = ontology.get("meeting_date")
                
                # Pass the ordinances directory to the document linker
                linked_docs = await self.document_linker.link_documents_for_meeting(
                    meeting_date,
                    self.ordinances_dir  # Pass the correct ordinances directory
                )
                
                total_linked = len(linked_docs.get("ordinances", [])) + len(linked_docs.get("resolutions", []))
                self.stats["ordinances_linked"] += total_linked
                
                result["stages"]["document_linking"] = {
                    "status": "success",
                    "ordinances_found": len(linked_docs.get("ordinances", [])),
                    "resolutions_found": len(linked_docs.get("resolutions", [])),
                    "total_linked": total_linked
                }
            else:
                log.info("â­ï¸  Skipping document linking (RUN_LINK_DOCUMENTS=False)")
            
            # Stage 4: Build Graph
            if RUN_BUILD_GRAPH:
                log.info("ğŸ—ï¸  Stage 4: Building graph representation...")
                graph_data = await self.graph_builder.build_graph_from_ontology(
                    ontology, 
                    agenda_path
                )
                
                # Process linked documents if available
                missing_items = []
                if linked_docs:
                    meeting_date = ontology['meeting_date']
                    meeting_date_us = self.graph_builder.ensure_us_date_format(meeting_date)
                    meeting_id = f"meeting-{meeting_date_us}"
                    
                    missing_items = await self.graph_builder.process_linked_documents(
                        linked_docs, 
                        meeting_id, 
                        meeting_date
                    )
                    
                    self.stats["missing_links"] += len(missing_items)
                    
                    # Store missing items for final report
                    if missing_items:
                        self.all_missing_items[agenda_path.name] = missing_items
                
                result["stages"]["graph_building"] = {
                    "status": "success",
                    "nodes_created": len(graph_data.get("nodes", {})),
                    "relationships_created": graph_data.get("statistics", {}).get("relationships", 0),
                    "missing_links": len(missing_items)
                }
            else:
                log.info("â­ï¸  Skipping graph building (RUN_BUILD_GRAPH=False)")
            
            # Stage 5: Validate Links
            if RUN_VALIDATE_LINKS:
                log.info("âœ… Stage 5: Validating document links...")
                validation_results = await self._validate_links(ontology, linked_docs)
                result["stages"]["link_validation"] = validation_results
            else:
                log.info("â­ï¸  Skipping link validation (RUN_VALIDATE_LINKS=False)")
            
            result["status"] = "success"
            self.stats["agendas_processed"] += 1
            
        except Exception as e:
            log.error(f"âŒ Error processing {agenda_path.name}: {e}")
            import traceback
            traceback.print_exc()
            result["status"] = "error"
            result["error"] = str(e)
            self.stats["errors"] += 1
        
        return result
    
    async def _validate_links(self, ontology: Dict, linked_docs: Dict) -> Dict[str, Any]:
        """Validate that all agenda items are properly linked."""
        validation_results = {
            "status": "success",
            "total_items": 0,
            "linked_items": 0,
            "unlinked_items": []
        }
        
        # Get all agenda items
        all_items = []
        for section in ontology.get("agenda_structure", []):
            all_items.extend(section.get("items", []))
        
        validation_results["total_items"] = len(all_items)
        
        # Get all linked document item codes
        linked_item_codes = set()
        for doc_type in ["ordinances", "resolutions"]:
            for doc in linked_docs.get(doc_type, []):
                if doc.get("item_code"):
                    normalized_code = self.graph_builder.normalize_item_code(doc["item_code"])
                    linked_item_codes.add(normalized_code)
        
        # Check each agenda item
        for item in all_items:
            normalized_code = self.graph_builder.normalize_item_code(item.get("item_code", ""))
            if normalized_code in linked_item_codes:
                validation_results["linked_items"] += 1
            else:
                validation_results["unlinked_items"].append({
                    "item_code": item.get("item_code"),
                    "title": item.get("title", "Unknown"),
                    "document_reference": item.get("document_reference")
                })
        
        return validation_results
    
    async def run(self, agenda_pattern: str = "Agenda*.pdf"):
        """Run the complete pipeline."""
        log.info("ğŸš€ Starting City Clerk Graph Pipeline")
        log.info(f"ğŸ“ Base directory: {self.base_dir}")
        log.info(f"ğŸ“ Agenda directory: {self.agenda_dir}")
        log.info(f"ğŸ“ Ordinances directory: {self.ordinances_dir}")
        
        # Check directories exist
        if not self.agenda_dir.exists():
            log.error(f"âŒ Agenda directory does not exist: {self.agenda_dir}")
            log.info(f"ğŸ’¡ Please ensure the directory structure exists")
            return
        
        if not self.ordinances_dir.exists():
            log.warning(f"âš ï¸  Ordinances directory does not exist: {self.ordinances_dir}")
            log.info(f"ğŸ’¡ Document linking may fail without ordinances")
        
        log.info(f"ğŸ“Š Pipeline stages enabled:")
        log.info(f"   - Extract PDF: {RUN_EXTRACT_PDF}")
        log.info(f"   - Extract Ontology: {RUN_EXTRACT_ONTOLOGY}")
        log.info(f"   - Link Documents: {RUN_LINK_DOCUMENTS}")
        log.info(f"   - Build Graph: {RUN_BUILD_GRAPH}")
        log.info(f"   - Validate Links: {RUN_VALIDATE_LINKS}")
        
        # Initialize components
        await self.initialize()
        
        # Clear graph if requested
        if CLEAR_GRAPH_FIRST and self.cosmos_client:
            log.warning("ğŸ—‘ï¸  Clearing existing graph data...")
            await self.cosmos_client.clear_graph()
        
        # Find agenda files
        agenda_files = sorted(self.agenda_dir.glob(agenda_pattern))
        log.info(f"ğŸ“‹ Found {len(agenda_files)} agenda files")
        
        if agenda_files:
            log.info("ğŸ“„ Agenda files found:")
            for f in agenda_files[:5]:
                log.info(f"   - {f.name}")
            if len(agenda_files) > 5:
                log.info(f"   ... and {len(agenda_files) - 5} more")
        
        # Process each agenda
        results = []
        for agenda_path in agenda_files:
            result = await self.process_agenda(agenda_path)
            results.append(result)
        
        # Generate summary report
        await self._generate_report(results)
        
        # Generate consolidated missing items report
        if self.all_missing_items:
            await self._generate_missing_items_report()
        
        # Cleanup
        if self.cosmos_client:
            await self.cosmos_client.close()
        
        log.info("âœ… Pipeline complete!")
    
    async def _generate_report(self, results: List[Dict[str, Any]]):
        """Generate pipeline execution report."""
        report = {
            "pipeline_run": datetime.utcnow().isoformat(),
            "statistics": self.stats,
            "agenda_results": results
        }
        
        # Save report
        report_path = self.output_dir / f"pipeline_report_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        # Print summary
        log.info(f"\n{'='*60}")
        log.info("ğŸ“Š Pipeline Summary:")
        log.info(f"   - Agendas processed: {self.stats['agendas_processed']}")
        log.info(f"   - Ordinances linked: {self.stats['ordinances_linked']}")
        log.info(f"   - Missing links: {self.stats['missing_links']}")
        log.info(f"   - Errors: {self.stats['errors']}")
        log.info(f"   - Report saved to: {report_path.name}")
        log.info(f"{'='*60}")
    
    async def _generate_missing_items_report(self):
        """Generate consolidated report of missing agenda items."""
        report = {
            "generated_at": datetime.utcnow().isoformat(),
            "summary": {
                "total_agendas_processed": self.stats["agendas_processed"],
                "total_missing_items": self.stats["missing_links"],
                "agendas_with_missing_items": len(self.all_missing_items)
            },
            "by_agenda": self.all_missing_items
        }
        
        report_path = self.output_dir / "consolidated_missing_items_report.json"
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        log.info(f"ğŸ“‹ Missing items report saved to: {report_path.name}")


async def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(description="City Clerk Graph Pipeline")
    parser.add_argument(
        "--base-dir", 
        type=Path,
        default=Path("city_clerk_documents/global"),
        help="Base directory containing City Comissions folder"
    )
    parser.add_argument(
        "--pattern",
        default="Agenda*.pdf",
        help="File pattern for agenda PDFs"
    )
    parser.add_argument(
        "--clear-graph",
        action="store_true",
        help="Clear existing graph before processing"
    )
    
    # Pipeline stage controls
    parser.add_argument("--skip-pdf-extract", action="store_true", help="Skip PDF extraction")
    parser.add_argument("--skip-ontology", action="store_true", help="Skip ontology extraction")
    parser.add_argument("--skip-linking", action="store_true", help="Skip document linking")
    parser.add_argument("--skip-graph", action="store_true", help="Skip graph building")
    parser.add_argument("--skip-validation", action="store_true", help="Skip link validation")
    
    args = parser.parse_args()
    
    # Override global flags based on arguments
    global CLEAR_GRAPH_FIRST, RUN_EXTRACT_PDF, RUN_EXTRACT_ONTOLOGY, RUN_LINK_DOCUMENTS, RUN_BUILD_GRAPH, RUN_VALIDATE_LINKS
    
    if args.clear_graph:
        CLEAR_GRAPH_FIRST = True
    
    if args.skip_pdf_extract:
        RUN_EXTRACT_PDF = False
    if args.skip_ontology:
        RUN_EXTRACT_ONTOLOGY = False
    if args.skip_linking:
        RUN_LINK_DOCUMENTS = False
    if args.skip_graph:
        RUN_BUILD_GRAPH = False
    if args.skip_validation:
        RUN_VALIDATE_LINKS = False
    
    # Create and run pipeline
    pipeline = CityClerkGraphPipeline(base_dir=args.base_dir)
    await pipeline.run(agenda_pattern=args.pattern)


if __name__ == "__main__":
    asyncio.run(main())


================================================================================


################################################################################
# File: scripts/graph_stages/agenda_pdf_extractor.py
################################################################################

# scripts/graph_stages/agenda_pdf_extractor.py
"""
PDF Extractor for City Clerk Agenda Documents
Extracts text, structure, and hyperlinks from agenda PDFs.
"""

import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
import json
import re
import PyPDF2
import fitz  # PyMuPDF for hyperlink extraction
from unstructured.partition.pdf import partition_pdf
from datetime import datetime

log = logging.getLogger('agenda_pdf_extractor')


class AgendaPDFExtractor:
    """Extract structured content from city agenda PDFs."""
    
    def __init__(self, output_dir: Optional[Path] = None):
        self.output_dir = output_dir or Path("city_clerk_documents/graph_json")
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def extract_agenda(self, pdf_path: Path) -> Dict[str, Any]:
        """Extract complete content from agenda PDF."""
        log.info(f"ğŸ“„ Extracting agenda from {pdf_path.name}")
        
        # Try multiple extraction methods
        sections = self._extract_with_unstructured(pdf_path)
        if not sections:
            log.warning("Unstructured extraction failed, trying PyPDF2")
            sections = self._extract_with_pypdf(pdf_path)
        
        # Extract hyperlinks
        hyperlinks = self._extract_hyperlinks(pdf_path)
        
        # Build result
        result = {
            "title": self._extract_title(sections),
            "sections": sections,
            "hyperlinks": hyperlinks,
            "metadata": {
                "source_pdf": str(pdf_path.absolute()),
                "extraction_method": "unstructured" if sections else "pypdf2",
                "extraction_date": datetime.utcnow().isoformat() + "Z",
                "total_pages": self._count_pages(pdf_path)
            }
        }
        
        # Save extracted data
        output_path = self.output_dir / f"{pdf_path.stem}_extracted.json"
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        
        log.info(f"âœ… Extraction complete: {len(sections)} sections, {len(hyperlinks)} hyperlinks")
        return result
    
    def _extract_with_unstructured(self, pdf_path: Path) -> List[Dict[str, Any]]:
        """Extract using unstructured library."""
        try:
            elements = partition_pdf(
                filename=str(pdf_path),
                strategy="hi_res",
                extract_images_in_pdf=False,
                infer_table_structure=True
            )
            
            # Group elements by page
            pages = {}
            for elem in elements:
                page_num = getattr(elem.metadata, "page_number", 1)
                if page_num not in pages:
                    pages[page_num] = []
                
                elem_dict = {
                    "type": elem.category or "paragraph",
                    "text": str(elem).strip()
                }
                pages[page_num].append(elem_dict)
            
            # Convert to sections
            sections = []
            for page_num in sorted(pages.keys()):
                section = {
                    "section": f"Page {page_num}",
                    "page_start": page_num,
                    "page_end": page_num,
                    "elements": pages[page_num],
                    "text": "\n".join(e["text"] for e in pages[page_num])
                }
                sections.append(section)
            
            return sections
            
        except Exception as e:
            log.error(f"Unstructured extraction failed: {e}")
            return []
    
    def _extract_with_pypdf(self, pdf_path: Path) -> List[Dict[str, Any]]:
        """Fallback extraction using PyPDF2."""
        sections = []
        try:
            with open(pdf_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                for page_num, page in enumerate(reader.pages, 1):
                    text = page.extract_text()
                    if text:
                        # Split into paragraphs
                        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
                        
                        section = {
                            "section": f"Page {page_num}",
                            "page_start": page_num,
                            "page_end": page_num,
                            "elements": [
                                {"type": "paragraph", "text": p} for p in paragraphs
                            ],
                            "text": text
                        }
                        sections.append(section)
        except Exception as e:
            log.error(f"PyPDF2 extraction failed: {e}")
        
        return sections
    
    def _extract_hyperlinks(self, pdf_path: Path) -> Dict[str, Dict[str, Any]]:
        """Extract hyperlinks using PyMuPDF."""
        hyperlinks = {}
        
        try:
            doc = fitz.open(pdf_path)
            
            for page_num, page in enumerate(doc, 1):
                # Get all links on the page
                links = page.get_links()
                
                for link in links:
                    if link.get("uri"):  # External link
                        # Try to find the text around the link
                        rect = fitz.Rect(link["from"])
                        text = page.get_textbox(rect).strip()
                        
                        # Look for document reference numbers (e.g., 23-6887)
                        ref_match = re.search(r'\d{2}-\d{4}', text)
                        if ref_match:
                            ref_num = ref_match.group()
                            hyperlinks[ref_num] = {
                                "url": link["uri"],
                                "page": page_num,
                                "text": text
                            }
            
            doc.close()
            
        except Exception as e:
            log.warning(f"Hyperlink extraction failed: {e}")
        
        return hyperlinks
    
    def _extract_title(self, sections: List[Dict[str, Any]]) -> str:
        """Extract document title from first page."""
        if sections and sections[0]["elements"]:
            # Look for title-like elements in first few elements
            for elem in sections[0]["elements"][:5]:
                text = elem.get("text", "")
                if "agenda" in text.lower() and len(text) < 200:
                    return text
        
        return "City Commission Agenda"
    
    def _count_pages(self, pdf_path: Path) -> int:
        """Count total pages in PDF."""
        try:
            with open(pdf_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                return len(reader.pages)
        except:
            return 0
    
    # Alias for compatibility
    def extract(self, pdf_path: Path) -> Dict[str, Any]:
        """Alias for extract_agenda."""
        return self.extract_agenda(pdf_path)


================================================================================


################################################################################
# File: scripts/clear_database.py
################################################################################

# File: scripts/clear_database.py

#!/usr/bin/env python3
"""
Database Clear Utility
======================

Safely clears Supabase database tables for the Misophonia Research system.
This script will delete all data from:
- research_documents table
- documents_chunks table

âš ï¸  WARNING: This operation is irreversible!
"""
from __future__ import annotations
import os
import sys
import logging
from typing import Optional

from dotenv import load_dotenv
from supabase import create_client

# Load environment variables
load_dotenv()

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")

if not SUPABASE_URL or not SUPABASE_KEY:
    sys.exit("âŒ Missing SUPABASE_URL or SUPABASE_SERVICE_ROLE_KEY environment variables")

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s â€” %(levelname)s â€” %(message)s"
)
log = logging.getLogger(__name__)

def init_supabase():
    """Initialize Supabase client."""
    return create_client(SUPABASE_URL, SUPABASE_KEY)

def get_table_counts(sb) -> dict:
    """Get current row counts for all tables."""
    counts = {}
    
    try:
        # Count documents
        doc_res = sb.table("research_documents").select("id", count="exact").execute()
        counts["research_documents"] = doc_res.count or 0
        
        # Count chunks
        chunk_res = sb.table("documents_chunks").select("id", count="exact").execute()
        counts["documents_chunks"] = chunk_res.count or 0
        
        # Count chunks with embeddings
        embedded_res = sb.table("documents_chunks").select("id", count="exact").not_.is_("embedding", "null").execute()
        counts["chunks_with_embeddings"] = embedded_res.count or 0
        
    except Exception as e:
        log.error(f"Error getting table counts: {e}")
        return {}
    
    return counts

def confirm_deletion() -> bool:
    """Ask user for confirmation before deletion."""
    print("\n" + "="*60)
    print("âš ï¸  DATABASE CLEAR WARNING")
    print("="*60)
    print("This will permanently delete ALL data from:")
    print("  â€¢ research_documents table")
    print("  â€¢ documents_chunks table")
    print("  â€¢ All embeddings and metadata")
    print("\nâŒ This operation CANNOT be undone!")
    print("="*60)
    
    response = input("\nType 'DELETE ALL DATA' to confirm (or anything else to cancel): ")
    return response.strip() == "DELETE ALL DATA"

def clear_table_batch(sb, table_name: str, batch_size: int = 1000) -> int:
    """Clear all rows from a specific table in batches to avoid timeouts."""
    log.info(f"Clearing table: {table_name} (batch size: {batch_size})")
    
    total_deleted = 0
    
    while True:
        try:
            # Get a batch of IDs to delete
            result = sb.table(table_name).select("id").limit(batch_size).execute()
            
            if not result.data or len(result.data) == 0:
                break
            
            ids_to_delete = [row["id"] for row in result.data]
            log.info(f"Deleting batch of {len(ids_to_delete)} rows from {table_name}")
            
            # Delete this batch
            delete_result = sb.table(table_name).delete().in_("id", ids_to_delete).execute()
            
            if hasattr(delete_result, 'error') and delete_result.error:
                log.error(f"Error deleting batch from {table_name}: {delete_result.error}")
                break
            
            batch_deleted = len(delete_result.data) if delete_result.data else 0
            total_deleted += batch_deleted
            log.info(f"âœ… Deleted {batch_deleted} rows from {table_name} (total: {total_deleted})")
            
            # If we deleted fewer than the batch size, we're done
            if batch_deleted < batch_size:
                break
                
        except Exception as e:
            log.error(f"Exception deleting batch from {table_name}: {e}")
            break
    
    log.info(f"âœ… Total deleted from {table_name}: {total_deleted}")
    return total_deleted

def clear_table(sb, table_name: str) -> int:
    """Clear all rows from a specific table."""
    return clear_table_batch(sb, table_name, batch_size=500)

def main():
    """Main function to clear the database."""
    print("ğŸ—‘ï¸  Misophonia Database Clear Utility")
    print("=" * 50)
    
    # Initialize Supabase
    sb = init_supabase()
    
    # Get current counts
    print("\nğŸ“Š Current database status:")
    counts = get_table_counts(sb)
    
    if not counts:
        print("âŒ Could not retrieve database counts. Exiting.")
        return
    
    print(f"  â€¢ Documents: {counts['research_documents']:,}")
    print(f"  â€¢ Chunks: {counts['documents_chunks']:,}")
    print(f"  â€¢ Chunks with embeddings: {counts['chunks_with_embeddings']:,}")
    
    if counts['research_documents'] == 0 and counts['documents_chunks'] == 0:
        print("\nâœ… Database is already empty!")
        return
    
    # Get confirmation
    if not confirm_deletion():
        print("\nâœ… Operation cancelled. Database unchanged.")
        return
    
    print("\nğŸ—‘ï¸  Starting database clear operation...")
    
    # Clear chunks first (has foreign key to documents)
    chunks_deleted = clear_table(sb, "documents_chunks")
    
    # Clear documents
    docs_deleted = clear_table(sb, "research_documents")
    
    # Verify deletion
    print("\nğŸ“Š Verifying deletion...")
    final_counts = get_table_counts(sb)
    
    if final_counts:
        print(f"  â€¢ Documents remaining: {final_counts['research_documents']:,}")
        print(f"  â€¢ Chunks remaining: {final_counts['documents_chunks']:,}")
        
        if final_counts['research_documents'] == 0 and final_counts['documents_chunks'] == 0:
            print("\nâœ… Database successfully cleared!")
            print(f"  â€¢ Deleted {docs_deleted:,} documents")
            print(f"  â€¢ Deleted {chunks_deleted:,} chunks")
        else:
            print("\nâš ï¸  Some data may remain in the database.")
    else:
        print("âŒ Could not verify deletion status.")

if __name__ == "__main__":
    main()


================================================================================


################################################################################
# File: scripts/test_vector_search.py
################################################################################

#!/usr/bin/env python3
################################################################################
################################################################################
"""
Vectorâ€‘search smokeâ€‘test (Supabase edition)
==========================================

â€¢ This script tests vector search functionality using Supabase's PostgREST API.

â€¢ The script calls a SQL helper function that must exist on your database:
    public.search_research_chunks(query_text TEXT,
                                  match_count INT DEFAULT 10,
                                  similarity_threshold REAL DEFAULT 0.6)
  which should:
    1. embed the incoming `query_text`
    2. invoke your `match_documents` similarity function
    3. return the topâ€‘`match_count` rows as
       (id UUID, text TEXT, metadata JSONB, similarity REAL)

  See README / earlier instructions for a readyâ€‘made implementation.

Environment variables required
------------------------------
SUPABASE_URL                 â€“ e.g. https://xxxx.supabase.co
SUPABASE_SERVICE_ROLE_KEY    â€“ or an anon key if RLS permits the RPC
OPENAI_API_KEY               â€“ only if your SQL helper embeds via an HTTP call
"""

from __future__ import annotations

import os
import sys
import time
from typing import Any, Dict, List

from dotenv import load_dotenv
from supabase import create_client
from openai import OpenAI

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ configuration & sanity checks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #

load_dotenv()

# ---------- Supabase config ----------
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")   # or anon if RLS permits
sb = create_client(SUPABASE_URL, SUPABASE_KEY)

if not SUPABASE_URL or not SUPABASE_KEY:
    sys.exit(
        "âŒ  SUPABASE_URL / SUPABASE_SERVICE_ROLE_KEY env vars are missing.\n"
        "    export them and rerun."
    )

# Sample questions to probe the index
SAMPLE_QUERIES: List[str] = [
    "What are the symptoms of misophonia?",
    "How prevalent is misophonia in university students?",
    "What is the relationship between misophonia and hyperacusis?",
    "What treatments are effective for misophonia?",
    "How does misophonia affect quality of life?",
]

# Add this after other configuration
openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

def embed(text: str) -> List[float]:
    """Return OpenAI adaâ€‘002 embedding (1536â€‘dim list of floats)."""
    resp = openai_client.embeddings.create(
        model="text-embedding-ada-002",
        input=text,
    )
    return resp.data[0].embedding

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ helper functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #


def perform_vector_search(query_vec, top_k=5, thresh=0.6):
    """
    Call the SQL RPC we just created.
    `query_vec` is a list[float] length 1536 coming from OpenAI.
    """
    try:
        resp = sb.rpc(
            "search_documents_chunks",
            {
                "query_embedding": query_vec,
                "match_count": top_k,
                "similarity_threshold": thresh,
            },
        ).execute()
        if getattr(resp, "error", None):
            raise RuntimeError(resp.error)
        return resp.data or []
    except Exception as e:
        print(f"   âš   RPC failed: {e}")
        return []


def print_results(rows: List[Dict[str, Any]]) -> None:
    """
    Nicely format the search results.
    """
    if not rows:
        print("   (no matches)\n")
        return

    for idx, row in enumerate(rows, 1):
        meta = row.get("metadata", {}) or {}
        title = meta.get("title", "Unknown title")
        year = meta.get("year", "????")
        author = meta.get("primary_author", "Unknown author")
        sim = row.get("similarity", 0.0)

        snippet = (row.get("text", "") or "").replace("\n", " ")[:280] + "â€¦"

        print(f"\nResult {idx}  â€¢  sim={sim:.3f}")
        print(f"  {title} â€” {author} ({year})")
        print(f"  {snippet}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #


def main() -> None:
    print("\nğŸ”  Supabase vector search smokeâ€‘test\n" + "â€”" * 60)
    for i, q in enumerate(SAMPLE_QUERIES, 1):
        print(f'\nQuery {i + 1}/{len(SAMPLE_QUERIES)}: "{q}"')


        # 1. get the vector
        print("Generating embedding...")
        query_vec = embed(q)  # Python list[float]

        # 2. PostgREST / Postgres expects a *string* like: [0.1,0.2,â€¦]
        vec_literal = "[" + ",".join(f"{x:.6f}" for x in query_vec) + "]"

        # 3. call the RPC
        print("Performing vector search...")
        resp = sb.rpc(
            "search_documents_chunks",
            {
                "query_embedding": vec_literal,  # <- NOT the raw text
                "match_count": 5,
                "similarity_threshold": 0.6,
            },
        ).execute()
        
        if getattr(resp, "error", None):
            print(f"   âš   RPC failed: {resp.error}\n")
            continue
            
        results = resp.data or []
        print_results(results)

        if i < len(SAMPLE_QUERIES):
            print("\nPausing 2 s before the next query â€¦")
            time.sleep(2)

    print("\nâœ”  Done")


if __name__ == "__main__":
    main()


================================================================================


################################################################################
# File: city_clerk_documents/graph_json/debug/all_agenda_items.json
################################################################################

[
  {
    "item_code": "E.-1.",
    "document_reference": "23-6723",
    "title": "An Ordinance of the City Commission amending the Cocoplum Phase 1 Security Guard District, as created by Miami-Dade County pursuant to County Ordinance 95-214, to expand the scope of services to include additional security measures including but not limited to security cameras.",
    "item_type": "Ordinance",
    "section": "AGENDA ITEMS"
  },
  {
    "item_code": "E.-3.",
    "document_reference": "23-6786",
    "title": "An Ordinance of the City Commission amending Section 1-104 \u201cJurisdiction and Applicability\u2019; Section 8-109 \u201cMoving of Existing Improvements\u201d; Section 8-106 \u201cCertificates of Appropriateness\u201d; Section 14-209.4; Section 14-215.3 \u201cNotice and Hearing Procedures\u201d; Section 15-102 \u201cNotice\u201d of the City\u2019s Zon",
    "item_type": "Ordinance",
    "section": "AGENDA ITEMS"
  },
  {
    "item_code": "E.-4.",
    "document_reference": "23-6567",
    "title": "An Ordinance of the City Commission amending St. Philip's School site plan approved under Ordinance No. 3576to replace an existing building with a new pre-K building located at 1109 Andalusia Avenue, Coral Gables, Florida; all other conditions of approval contained in Ordinance No. 3576 shall remain",
    "item_type": "Ordinance",
    "section": "AGENDA ITEMS"
  },
  {
    "item_code": "E.-5.",
    "document_reference": "23-6826",
    "title": "An Ordinance of the City Commission providing for text amendments to the City of Coral Gables Official Zoning Code (Zoning Code), amending Article 10, \u201cParking and Access,\u201d Section 10-112 \u201cMiscellaneous Parking Standards,\u201d creating provisions for considering reduction of parking requirements for aff",
    "item_type": "Ordinance",
    "section": "AGENDA ITEMS"
  },
  {
    "item_code": "E.-6.",
    "document_reference": "23-6827",
    "title": "An Ordinance of the City Commission providing for text amendments to the City of Coral Gables Official Zoning Code, amending Article 1, \u201cGeneral Provisions,\u201d Section 1-104 \u201cJurisdiction and Applicability,\u201d amending provisions for the citing of city facilities to include facilities for workforce hous",
    "item_type": "Ordinance",
    "section": "AGENDA ITEMS"
  },
  {
    "item_code": "E.-8.",
    "document_reference": "23-6881",
    "title": "An Ordinance of the City Commission providing for a text amendment to the City of Coral Gables official Zoning Code, amending Article 11. \u201cSigns,\u201d Section 11-107 \u201cReal Estate, For Sale, Lease or Rental of Property or Buildings,\u201d to apply same regulations to signs pertaining to the sale, lease, or re",
    "item_type": "Ordinance",
    "section": "AGENDA ITEMS"
  },
  {
    "item_code": "F.-10.",
    "document_reference": "23-5686",
    "title": "A Resolution of the City Commission directing the City Clerk and City Manager to schedule bimonthly presentation and protocol ceremonies. (Sponsored by Commissioner Fernandez)",
    "item_type": "Resolution",
    "section": "AGENDA ITEMS"
  },
  {
    "item_code": "F.-11.",
    "document_reference": "23-6832",
    "title": "A Resolution of the City Commission directing the City Clerk to schedule a joint meeting between the City's Historic Preservation Board and Landmarks Advisory Board to discuss the replacement of certain Florida power and Light streetlights throughout the City. (Sponsored by Commissioner Fernandez)",
    "item_type": "Resolution",
    "section": "AGENDA ITEMS"
  },
  {
    "item_code": "H.-1.",
    "document_reference": "23-6819",
    "title": "A Resolution of the City Commission accepting the recommendation of the Chief Procurement Officer to authorize access to a \u201cpiggyback\u201d contract through the Sourcewell Cooperative with Oshkosh Corporation (Pierce Manufacturing) in an amount not to exceed the available budget for firefighting equipmen",
    "item_type": "Resolution",
    "section": "AGENDA ITEMS"
  },
  {
    "item_code": "H.-2.",
    "document_reference": "23-6824",
    "title": "A Resolution of the City Commission accepting the recommendation of the Innovations and Technology Department to waive the competitive process to purchase AT&T Communication Service Maintenance and Support as a \u201cSpecial Procurement/Bid Waiver,\u201d pursuant to Section 2-691of the Procurement Code.",
    "item_type": "Resolution",
    "section": "AGENDA ITEMS"
  },
  {
    "item_code": "H.-3.",
    "document_reference": "23-6765",
    "title": "Illegal dumping mitigation technology update.",
    "item_type": "Discussion",
    "section": "AGENDA ITEMS"
  }
]


================================================================================


################################################################################
# File: scripts/test_graph_pipeline.py
################################################################################

# File: scripts/test_graph_pipeline.py

"""
Test script for City Clerk Graph Pipeline
"""
import asyncio
import os
from pathlib import Path
from dotenv import load_dotenv

load_dotenv()

async def test_pipeline():
    """Test basic pipeline functionality."""
    # Check environment variables
    required_vars = ['COSMOS_ENDPOINT', 'COSMOS_KEY', 'GROQ_API_KEY']
    missing = [var for var in required_vars if not os.getenv(var)]
    
    if missing:
        print(f"âŒ Missing environment variables: {missing}")
        print("Please set these in your .env file")
        return
    
    # Test imports
    try:
        from graph_stages.cosmos_db_client import CosmosGraphClient
        from graph_stages.agenda_pdf_extractor import AgendaPDFExtractor
        from graph_stages.agenda_ontology_extractor import CityClerkOntologyExtractor
        from graph_stages.document_linker import DocumentLinker
        from graph_stages.agenda_graph_builder import AgendaGraphBuilder
        print("âœ… All imports successful")
    except ImportError as e:
        print(f"âŒ Import error: {e}")
        return
    
    # Test Cosmos connection
    try:
        cosmos = CosmosGraphClient()
        await cosmos.connect()
        print("âœ… Cosmos DB connection successful")
        await cosmos.close()
    except Exception as e:
        print(f"âŒ Cosmos DB connection failed: {e}")
        return
    
    # Test PDF extraction
    test_dir = Path("city_clerk_documents/global")
    if test_dir.exists():
        pdfs = list(test_dir.glob("Agenda*.pdf"))
        if pdfs:
            print(f"âœ… Found {len(pdfs)} agenda PDFs")
            
            # Test extraction on first PDF
            extractor = AgendaPDFExtractor()
            result = extractor.extract(pdfs[0])
            print(f"âœ… Extracted {len(result.get('sections', []))} sections from {pdfs[0].name}")
        else:
            print("âš ï¸  No agenda PDFs found in test directory")
    else:
        print(f"âš ï¸  Test directory not found: {test_dir}")
    
    print("\nâœ… Pipeline components are functional!")

if __name__ == "__main__":
    asyncio.run(test_pipeline())


================================================================================


################################################################################
# File: city_clerk_documents/graph_json/debug/linked_documents.json
################################################################################

{
  "ordinances": [
    {
      "path": "city_clerk_documents/global/City Comissions 2024/Ordinances/2024/2024-01 - 01_09_2024.pdf",
      "filename": "2024-01 - 01_09_2024.pdf",
      "document_number": "2024-01",
      "item_code": "E-1",
      "document_type": "Ordinance",
      "title": "A\nORDINANCE NO.",
      "parsed_data": {}
    },
    {
      "path": "city_clerk_documents/global/City Comissions 2024/Ordinances/2024/2024-02 - 01_09_2024.pdf",
      "filename": "2024-02 - 01_09_2024.pdf",
      "document_number": "2024-02",
      "item_code": null,
      "document_type": "Ordinance",
      "title": "A\nORDINANCE NO.",
      "parsed_data": {}
    },
    {
      "path": "city_clerk_documents/global/City Comissions 2024/Ordinances/2024/2024-03 - 01_09_2024.pdf",
      "filename": "2024-03 - 01_09_2024.pdf",
      "document_number": "2024-03",
      "item_code": "E-3",
      "document_type": "Ordinance",
      "title": "A\nORDINANCE NO.",
      "parsed_data": {}
    }
  ],
  "resolutions": []
}


================================================================================


################################################################################
# File: requirements.txt
################################################################################

################################################################################
################################################################################
# Python dependencies for Misophonia Research RAG System

# Core dependencies
openai>=1.82.0
groq>=0.4.1
gremlinpython>=3.7.3
aiofiles>=24.1.0
python-dotenv>=1.0.0

# Graph Visualization dependencies
dash>=2.14.0
plotly>=5.17.0
networkx>=3.2.0
dash-table>=5.0.0
dash-cytoscape>=0.3.0

# Database and API dependencies
supabase>=2.0.0

# Optional for async progress bars
tqdm

# Data processing
pandas>=2.0.0
numpy

# PDF processing
PyPDF2>=3.0.0
unstructured>=0.10.0
pdfminer.six>=20221105

# PDF processing with hyperlink support
PyMuPDF>=1.23.0

# Utilities
colorama>=0.4.6

# For concurrent processing
concurrent-log-handler>=0.9.20

# Async dependencies for optimized pipeline
aiohttp>=3.8.0

# Token counting for OpenAI rate limiting
tiktoken>=0.5.0


================================================================================

