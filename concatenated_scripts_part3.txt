# Concatenated Project Code - Part 3 of 3
# Generated: 2025-06-13 17:03:10
# Root Directory: /Users/gianmariatroiani/Documents/knologi/graph_database
================================================================================

# File Index - Which Files Are in Which Parts
################################################################################

## Part 1 (12 files):
  - scripts/graph_rag_stages/phase3_querying/city_clerk_query_engine.py
  - scripts/graph_rag_stages/phase2_building/custom_graph_builder.py
  - scripts/graph_rag_stages/phase2_building/graphrag_adapter.py
  - scripts/extract_all_to_markdown.py
  - scripts/graph_rag_stages/phase3_querying/query_engine.py
  - scripts/graph_rag_stages/common/config.py
  - scripts/graph_rag_stages/phase3_querying/query_router.py
  - scripts/graph_rag_stages/phase3_querying/source_tracker.py
  - scripts/graph_rag_stages/main_pipeline.py
  - settings.yaml
  - scripts/graph_rag_stages/common/__init__.py
  - ui/__init__.py

## Part 2 (13 files):
  - ui/query_app.py
  - scripts/RAG_stages/chunk_text.py
  - scripts/graph_rag_stages/phase3_querying/structural_query_enhancer.py
  - scripts/graph_rag_stages/phase1_preprocessing/agenda_extractor.py
  - scripts/graph_rag_stages/common/cosmos_client.py
  - scripts/graph_rag_stages/phase2_building/entity_deduplicator.py
  - scripts/graph_rag_stages/phase2_building/graphrag_indexer.py
  - scripts/RAG_stages/llm_enrich.py
  - scripts/graph_rag_stages/phase2_building/__init__.py
  - scripts/graph_rag_stages/phase1_preprocessing/pdf_extractor.py
  - scripts/graph_rag_stages/phase3_querying/__init__.py
  - config.py
  - scripts/graph_rag_stages/__init__.py

## Part 3 (13 files):
  - scripts/RAG_stages/embed_vectors.py
  - scripts/RAG_stages/extract_clean.py
  - scripts/graph_rag_stages/phase3_querying/smart_query_router.py
  - scripts/graph_rag_stages/phase1_preprocessing/transcript_linker.py
  - scripts/graph_rag_stages/phase1_preprocessing/document_linker.py
  - scripts/RAG_stages/db_upsert.py
  - scripts/graph_rag_stages/common/utils.py
  - scripts/graph_rag_stages/README.md
  - scripts/RAG_stages/acceleration_utils.py
  - scripts/graph_rag_stages/phase3_querying/response_enhancer.py
  - scripts/graph_rag_stages/phase1_preprocessing/__init__.py
  - requirements.txt
  - scripts/RAG_stages/__init__.py


================================================================================


################################################################################
# File: scripts/RAG_stages/embed_vectors.py
################################################################################

# File: scripts/RAG_stages/embed_vectors.py

#!/usr/bin/env python3
"""
Stage 7 â€” Optimized embedding with rate limiting, deduplication, and conservative batching.
"""
from __future__ import annotations
import argparse, json, logging, os, sys, time
from datetime import datetime
from typing import Any, Dict, List, Optional, Set
import asyncio
import aiohttp
from dotenv import load_dotenv
from openai import OpenAI
from supabase import create_client
from tqdm import tqdm
from tqdm.asyncio import tqdm as async_tqdm
import hashlib

# Try to import tiktoken for accurate token counting
try:
    import tiktoken
    TIKTOKEN_AVAILABLE = True
except ImportError:
    TIKTOKEN_AVAILABLE = False
    logging.warning("tiktoken not available - using conservative token estimation")

load_dotenv()
SUPABASE_URL  = os.getenv("SUPABASE_URL")
SUPABASE_KEY  = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
OPENAI_API_KEY= os.getenv("OPENAI_API_KEY")
EMBEDDING_MODEL="text-embedding-ada-002"
MODEL_TOKEN_LIMIT=8192
TOKEN_GUARD=200

# ğŸ¯ DYNAMIC BATCHING - Token limits with safety margins
MAX_BATCH_TOKENS = 7692  # Conservative limit (8192 - 500 safety margin)
MAX_CHUNK_TOKENS = 6000  # Individual chunk limit
MIN_BATCH_TOKENS = 100   # Minimum viable batch size

MAX_TOTAL_TOKENS=MAX_BATCH_TOKENS  # Use dynamic batch limit instead

# Conservative defaults for rate limiting
DEFAULT_BATCH_ROWS=200  # Reduced from 5000
DEFAULT_COMMIT_ROWS=10  # Reduced from 100
DEFAULT_MAX_CONCURRENT=3  # Reduced from 50
MAX_RETRIES=5
RETRY_DELAY=2
RATE_LIMIT_DELAY=0.3  # 300ms between API calls
MAX_CALLS_PER_MINUTE=150  # Conservative limit

openai_client=OpenAI(api_key=OPENAI_API_KEY)
logging.basicConfig(level=logging.INFO,
    format="%(asctime)s â€” %(levelname)s â€” %(message)s")
log=logging.getLogger(__name__)

# Global variables for rate limiting
call_timestamps = []
total_tokens_used = 0

class AsyncEmbedder:
    """Async embedding client with strict rate limiting and connection pooling."""
    
    def __init__(self, api_key: str, max_concurrent: int = DEFAULT_MAX_CONCURRENT):
        self.api_key = api_key
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.session = None
        self.call_count = 0
        self.start_time = time.time()
        
        # Initialize tiktoken encoder if available
        if TIKTOKEN_AVAILABLE:
            self.encoder = tiktoken.encoding_for_model(EMBEDDING_MODEL)
        else:
            self.encoder = None
    
    async def __aenter__(self):
        timeout = aiohttp.ClientTimeout(total=300)
        connector = aiohttp.TCPConnector(limit=20, limit_per_host=10)  # Reduced limits
        self.session = aiohttp.ClientSession(
            headers={"Authorization": f"Bearer {self.api_key}"},
            timeout=timeout,
            connector=connector
        )
        return self
    
    async def __aexit__(self, *args):
        await self.session.close()
    
    def count_tokens(self, text: str) -> int:
        """Count tokens accurately using tiktoken if available."""
        if self.encoder:
            return len(self.encoder.encode(text))
        else:
            # Conservative fallback
            return int(len(text.split()) * 0.75) + 50
    
    async def rate_limit_check(self):
        """Enforce rate limiting."""
        current_time = time.time()
        
        # Remove timestamps older than 1 minute
        call_timestamps[:] = [ts for ts in call_timestamps if current_time - ts < 60]
        
        # Check if we're approaching the rate limit
        if len(call_timestamps) >= MAX_CALLS_PER_MINUTE - 5:
            sleep_time = 60 - (current_time - call_timestamps[0])
            if sleep_time > 0:
                log.info(f"Rate limit protection: sleeping {sleep_time:.1f}s")
                await asyncio.sleep(sleep_time)
        
        # Always enforce minimum delay between calls
        await asyncio.sleep(RATE_LIMIT_DELAY)
        
        # Record this call
        call_timestamps.append(current_time)
    
    async def embed_batch_async(self, texts: List[str]) -> List[List[float]]:
        """Embed a batch of texts asynchronously with rate limiting and token validation."""
        global total_tokens_used
        
        async with self.semaphore:
            await self.rate_limit_check()
            
            # ğŸ›¡ï¸ FINAL TOKEN VALIDATION - Last safety check before API call
            batch_tokens = sum(self.count_tokens(text) for text in texts)
            
            if batch_tokens > MAX_BATCH_TOKENS:
                error_msg = f"ğŸš¨ CRITICAL: Batch tokens {batch_tokens} exceed limit {MAX_BATCH_TOKENS}"
                log.error(error_msg)
                raise RuntimeError(error_msg)
            
            total_tokens_used += batch_tokens
            
            log.info(f"ğŸ¯ API call: {len(texts)} texts (~{batch_tokens} tokens) - WITHIN LIMITS âœ…")
            log.info(f"ğŸ“Š Total tokens used so far: ~{total_tokens_used}")
            
            for attempt in range(MAX_RETRIES):
                try:
                    async with self.session.post(
                        "https://api.openai.com/v1/embeddings",
                        json={
                            "model": EMBEDDING_MODEL,
                            "input": texts
                        }
                    ) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            self.call_count += 1
                            log.info(f"âœ… API call successful: {len(texts)} embeddings generated")
                            return [item["embedding"] for item in data["data"]]
                        elif resp.status == 400:  # Bad request - likely token limit
                            error = await resp.text()
                            log.error(f"ğŸš¨ API error 400 (likely token limit): {error}")
                            log.error(f"ğŸš¨ Batch details: {len(texts)} texts, {batch_tokens} tokens")
                            raise RuntimeError(f"Token limit API error: {error}")
                        elif resp.status == 429:  # Rate limit error
                            error = await resp.text()
                            log.warning(f"Rate limit hit: {error}")
                            # Exponential backoff for rate limits
                            sleep_time = (2 ** attempt) * 2
                            log.info(f"Exponential backoff: sleeping {sleep_time}s")
                            await asyncio.sleep(sleep_time)
                        else:
                            error = await resp.text()
                            log.warning(f"API error {resp.status}: {error}")
                            await asyncio.sleep(RETRY_DELAY)
                except Exception as e:
                    log.warning(f"Attempt {attempt + 1} failed: {e}")
                    await asyncio.sleep(RETRY_DELAY * (attempt + 1))
            
            raise RuntimeError("Failed to embed batch after retries")

def deduplicate_chunks(chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Remove duplicate text chunks based on content hash."""
    seen_hashes: Set[str] = set()
    unique_chunks = []
    duplicates_removed = 0
    
    for chunk in chunks:
        text = chunk.get("text", "").strip()
        if not text:
            continue
            
        # Create hash of the text content
        text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()
        
        if text_hash not in seen_hashes:
            seen_hashes.add(text_hash)
            unique_chunks.append(chunk)
        else:
            duplicates_removed += 1
    
    if duplicates_removed > 0:
        log.info(f"Deduplication: removed {duplicates_removed} duplicate chunks")
    
    return unique_chunks

async def process_chunks_async(
    sb,
    chunks: List[Dict[str, Any]],
    embedder: AsyncEmbedder,
    commit_size: int = DEFAULT_COMMIT_ROWS
) -> int:
    """Process chunks with async embedding and batch updates."""
    # Deduplicate chunks first
    unique_chunks = deduplicate_chunks(chunks)
    log.info(f"ğŸ“Š CHUNK PROCESSING PROGRESS:")
    log.info(f"   ğŸ“„ Original chunks: {len(chunks)}")
    log.info(f"   ğŸ“„ Unique chunks: {len(unique_chunks)}")
    if len(chunks) != len(unique_chunks):
        log.info(f"   ğŸ”„ Duplicates removed: {len(chunks) - len(unique_chunks)}")
    
    # Create token-safe slices
    slices = safe_slices(unique_chunks, commit_size)
    total_embedded = 0
    total_slices = len(slices)
    
    if total_slices == 0:
        log.warning(f"ğŸ“Š NO SLICES TO PROCESS - All chunks were filtered out")
        return 0
    
    log.info(f"ğŸ“Š EMBEDDING SLICES PROGRESS:")
    log.info(f"   ğŸ¯ Total slices to process: {total_slices}")
    log.info(f"   ğŸ“„ Total chunks to embed: {sum(len(slice_data) for slice_data in slices)}")
    
    # Process each slice
    for i, slice_data in enumerate(slices):
        slice_num = i + 1
        slice_progress = (slice_num / total_slices * 100)
        
        log.info(f"ğŸ“Š SLICE {slice_num}/{total_slices} ({slice_progress:.1f}%):")
        log.info(f"   ğŸ“„ Processing {len(slice_data)} chunks in this slice")
        
        texts = [row["text"] for row in slice_data]
        
        try:
            # Get embeddings asynchronously
            log.info(f"   ğŸ”„ Calling OpenAI API for {len(texts)} embeddings...")
            embeddings = await embedder.embed_batch_async(texts)
            log.info(f"   âœ… Received {len(embeddings)} embeddings from API")
            
            # Update database in batch
            log.info(f"   ğŸ’¾ Updating database for {len(slice_data)} chunks...")
            update_tasks = []
            for row, emb in zip(slice_data, embeddings):
                # âœ… GUARANTEED SKIP LOGIC - Layer 3: Final update verification
                def update_with_verification(r, e):
                    # Final check before updating
                    final_check = sb.table("documents_chunks").select("embedding")\
                        .eq("id", r["id"]).execute()
                    
                    if final_check.data and final_check.data[0].get("embedding"):
                        log.info(f"ğŸ›¡ï¸ Chunk {r['id']} already has embedding - SKIPPING update")
                        return {"skipped": True}
                    
                    # Safe to update
                    return sb.table("documents_chunks")\
                        .update({"embedding": e})\
                        .eq("id", r["id"])\
                        .execute()
                
                # Use thread pool for database updates
                loop = asyncio.get_event_loop()
                task = loop.run_in_executor(None, update_with_verification, row, emb)
                update_tasks.append(task)
            
            # Wait for all updates
            log.info(f"   â³ Waiting for {len(update_tasks)} database updates...")
            results = await asyncio.gather(*update_tasks, return_exceptions=True)
            successful = sum(1 for r in results if not isinstance(r, Exception) and not (isinstance(r, dict) and r.get("skipped")))
            skipped = sum(1 for r in results if isinstance(r, dict) and r.get("skipped"))
            failed = sum(1 for r in results if isinstance(r, Exception))
            total_embedded += successful
            
            # ğŸ“Š SLICE COMPLETION PROGRESS
            log.info(f"ğŸ“Š SLICE {slice_num} COMPLETE:")
            log.info(f"   âœ… Successful updates: {successful}")
            if skipped > 0:
                log.info(f"   â­ï¸  Skipped (already embedded): {skipped}")
            if failed > 0:
                log.info(f"   âŒ Failed updates: {failed}")
            log.info(f"   ğŸ“ˆ Total embedded so far: {total_embedded}")
            
        except Exception as e:
            log.error(f"âŒ SLICE {slice_num} FAILED: {e}")
            log.error(f"   ğŸ“„ Chunks in failed slice: {len(slice_data)}")
    
    # Final summary
    log.info(f"ğŸ“Š CHUNK PROCESSING COMPLETE:")
    log.info(f"   âœ… Total chunks embedded: {total_embedded}")
    log.info(f"   ğŸ“Š Slices processed: {total_slices}")
    log.info(f"   ğŸ“ˆ Success rate: {(total_embedded/len(unique_chunks)*100):.1f}%" if unique_chunks else "0%")
    
    return total_embedded

async def main_async(batch_size: int = None, commit_size: int = None, max_concurrent: int = None):
    """Async main function for embedding."""
    global EMBEDDING_MODEL, MODEL_TOKEN_LIMIT, MAX_TOTAL_TOKENS, total_tokens_used
    
    # Use provided parameters or conservative defaults
    batch_size = batch_size or DEFAULT_BATCH_ROWS
    commit_size = commit_size or DEFAULT_COMMIT_ROWS
    max_concurrent = max_concurrent or DEFAULT_MAX_CONCURRENT
    
    log.info(f"Starting with conservative settings:")
    log.info(f"  Batch size: {batch_size}")
    log.info(f"  Commit size: {commit_size}")
    log.info(f"  Max concurrent: {max_concurrent}")
    log.info(f"  Rate limit delay: {RATE_LIMIT_DELAY}s")
    
    # ğŸ¯ Dynamic batching status
    log.info(f"ğŸ¯ DYNAMIC BATCHING ENABLED:")
    log.info(f"   Max batch tokens: {MAX_BATCH_TOKENS} (with safety margin)")
    log.info(f"   Max chunk tokens: {MAX_CHUNK_TOKENS}")
    log.info(f"   Min batch tokens: {MIN_BATCH_TOKENS}")
    log.info(f"   âœ… GUARANTEED: No token limit API errors")
    
    # âœ… Check tiktoken availability for accurate token counting
    if not TIKTOKEN_AVAILABLE:
        log.warning("âš ï¸  tiktoken not available - using conservative token estimation")
        log.warning("âš ï¸  For accurate token counting, install: pip install tiktoken")
    else:
        log.info("âœ… tiktoken available - using accurate token counting")
    
    if not OPENAI_API_KEY:
        log.error("OPENAI_API_KEY missing")
        sys.exit(1)
    
    sb = init_supabase()
    existing_count = count_processed_chunks(sb)
    log.info(f"Rows already embedded: {existing_count}")
    
    # âœ… GUARANTEED SKIP LOGIC - Status check
    total_chunks_res = sb.table("documents_chunks").select("id", count="exact")\
        .eq("chunking_strategy", "token_window").execute()
    total_chunks = total_chunks_res.count or 0
    
    pending_chunks = total_chunks - existing_count
    
    log.info("ğŸ“Š Current status:")
    log.info(f"   Chunks already embedded: {existing_count}")
    log.info(f"   Chunks needing embedding: {pending_chunks}")
    log.info(f"   Total chunks: {total_chunks}")
    
    if pending_chunks == 0:
        log.info("âœ… All chunks already have embeddings - nothing to do!")
        return
    
    async with AsyncEmbedder(OPENAI_API_KEY, max_concurrent) as embedder:
        loop, total = 0, 0
        
        # ğŸ“Š PROGRESS TRACKING - Initialize counters
        total_chunks_to_process = pending_chunks
        chunks_processed = 0
        chunks_skipped = 0
        
        log.info(f"ğŸ“Š EMBEDDING PROGRESS TRACKING INITIALIZED:")
        log.info(f"   ğŸ¯ Total chunks to embed: {total_chunks_to_process}")
        log.info(f"   ğŸ¯ Starting embedding process...")
        
        while True:
            loop += 1
            rows = fetch_unprocessed_chunks(sb, limit=batch_size, offset=0)
            
            if not rows:
                log.info("âœ¨ Done â€” no more rows.")
                break
            
            # ğŸ“Š PROGRESS: Show current status before processing
            remaining_chunks = total_chunks_to_process - chunks_processed
            progress_percent = (chunks_processed / total_chunks_to_process * 100) if total_chunks_to_process > 0 else 0
            
            log.info(f"ğŸ“Š EMBEDDING PROGRESS - Loop {loop}:")
            log.info(f"   ğŸ“„ Fetched: {len(rows)} chunks")
            log.info(f"   âœ… Processed: {chunks_processed}/{total_chunks_to_process} ({progress_percent:.1f}%)")
            log.info(f"   â³ Remaining: {remaining_chunks} chunks")
            if chunks_skipped > 0:
                log.info(f"   âš ï¸  Skipped: {chunks_skipped} oversized chunks")
            
            # Process chunks asynchronously
            embedded = await process_chunks_async(sb, rows, embedder, commit_size)
            total += embedded
            chunks_processed += embedded
            
            # Update skipped count (this will be calculated in process_chunks_async)
            current_chunk_count = count_processed_chunks(sb)
            actual_embedded_this_loop = current_chunk_count - (existing_count + chunks_processed - embedded)
            
            # ğŸ“Š PROGRESS: Show results after processing
            final_progress_percent = (chunks_processed / total_chunks_to_process * 100) if total_chunks_to_process > 0 else 0
            log.info(f"ğŸ“Š LOOP {loop} COMPLETE:")
            log.info(f"   âœ… This loop: {embedded} chunks embedded")
            log.info(f"   ğŸ“ˆ Total progress: {chunks_processed}/{total_chunks_to_process} ({final_progress_percent:.1f}%)")
            log.info(f"   ğŸ”„ API calls made: {embedder.call_count}")
            log.info(f"   ğŸ“Š Total tokens used: ~{total_tokens_used}")
            
            # Check if we're making progress or stuck
            if embedded == 0 and len(rows) > 0:
                chunks_skipped += len(rows)
                log.warning(f"âš ï¸  WARNING: No chunks embedded in this loop - all {len(rows)} chunks were skipped")
                log.warning(f"âš ï¸  Total skipped so far: {chunks_skipped} chunks")
                
                # Prevent infinite loop by limiting consecutive failed attempts
                if loop > 5 and embedded == 0:
                    log.error(f"ğŸš¨ STUCK: {loop} consecutive loops with no progress - stopping to prevent infinite loop")
                    break
    
    # Save report
    ts = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    report = {
        "timestamp": ts,
        "batch_size": batch_size,
        "commit": commit_size,
        "max_concurrent": max_concurrent,
        "total_embedded": total,
        "total_with_embeddings": count_processed_chunks(sb),
        "total_tokens_used": total_tokens_used,
        "api_calls_made": embedder.call_count if 'embedder' in locals() else 0
    }
    
    # Create reports directory if it doesn't exist
    reports_dir = "reports/embedding"
    os.makedirs(reports_dir, exist_ok=True)
    
    fname = f"{reports_dir}/supabase_embedding_report_{ts}.json"
    with open(fname, "w") as fp:
        json.dump(report, fp, indent=2)
    
    log.info(f"Report saved to {fname}")

async def main_async_cli():
    """CLI version with argument parsing."""
    global EMBEDDING_MODEL, MODEL_TOKEN_LIMIT, MAX_TOTAL_TOKENS
    
    ap = argparse.ArgumentParser()
    ap.add_argument("--batch-size", type=int, default=DEFAULT_BATCH_ROWS)
    ap.add_argument("--commit", type=int, default=DEFAULT_COMMIT_ROWS)
    ap.add_argument("--skip", type=int, default=0)
    ap.add_argument("--model", default=EMBEDDING_MODEL)
    ap.add_argument("--model-limit", type=int, default=MODEL_TOKEN_LIMIT)
    ap.add_argument("--max-concurrent", type=int, default=DEFAULT_MAX_CONCURRENT,
                   help="Maximum concurrent API calls (default: 3 for rate limiting)")
    args = ap.parse_args()
    
    EMBEDDING_MODEL = args.model
    MODEL_TOKEN_LIMIT = args.model_limit
    MAX_TOTAL_TOKENS = MODEL_TOKEN_LIMIT - TOKEN_GUARD
    
    await main_async(args.batch_size, args.commit, args.max_concurrent)

# Keep original interface for compatibility
def main() -> None:
    """Original synchronous interface."""
    asyncio.run(main_async_cli())

# Keep all existing helper functions unchanged...
def init_supabase():
    if not SUPABASE_URL or not SUPABASE_KEY:
        log.error("SUPABASE creds missing"); sys.exit(1)
    return create_client(SUPABASE_URL,SUPABASE_KEY)

def count_processed_chunks(sb)->int:
    res=sb.table("documents_chunks").select("id",count="exact").not_.is_("embedding","null").execute()
    return res.count or 0

def fetch_unprocessed_chunks(sb,*,limit:int,offset:int=0)->List[Dict[str,Any]]:
    first,last=offset,offset+limit-1
    res=sb.table("documents_chunks").select("id,text,token_start,token_end")\
        .eq("chunking_strategy","token_window").is_("embedding","null")\
        .range(first,last).execute()
    
    chunks = res.data or []
    
    if chunks:
        # âœ… GUARANTEED SKIP LOGIC - Layer 2: Double-check verification
        chunk_ids = [chunk["id"] for chunk in chunks]
        verification = sb.table("documents_chunks").select("id")\
            .in_("id", chunk_ids).not_.is_("embedding", "null").execute()
        
        already_embedded_ids = {row["id"] for row in verification.data or []}
        
        if already_embedded_ids:
            log.warning(f"ğŸ›¡ï¸ Found {len(already_embedded_ids)} chunks that already have embeddings - SKIPPING them")
            chunks = [chunk for chunk in chunks if chunk["id"] not in already_embedded_ids]
        
        log.info(f"âœ… GUARANTEED: Fetched {len(chunks)} chunks WITHOUT embeddings")
        
        if not chunks:
            log.info("âœ… All chunks already have embeddings - nothing to do!")
    
    return chunks

def generate_embedding_batch(texts:List[str])->List[List[float]]:
    attempt=0
    while attempt<MAX_RETRIES:
        try:
            resp=openai_client.embeddings.create(model=EMBEDDING_MODEL,input=texts)
            return [d.embedding for d in resp.data]
        except Exception as exc:
            attempt+=1; log.warning("Embedding batch %s/%s failed: %s",attempt,MAX_RETRIES,exc)
            time.sleep(RETRY_DELAY)
    raise RuntimeError("OpenAI embedding batch failed after retries")

def chunk_tokens(row:Dict[str,Any])->int:
    try:
        t=int(row["token_end"])-int(row["token_start"])+1
        if 0<t<=16384: return t
    except: pass
    
    # Use tiktoken if available for more accurate counting
    text = row.get("text", "")
    if TIKTOKEN_AVAILABLE:
        try:
            encoder = tiktoken.encoding_for_model(EMBEDDING_MODEL)
            return len(encoder.encode(text))
        except:
            pass
    
    # Conservative fallback
    approx=int(len(text.split())*0.75)+50  # More conservative estimate
    return min(max(1,approx),MODEL_TOKEN_LIMIT)

def safe_slices(rows:List[Dict[str,Any]],max_rows:int)->List[List[Dict[str,Any]]]:
    """ğŸ¯ DYNAMIC BATCH SIZING - Guarantees no token limit errors."""
    return dynamic_batch_slices(rows)

def dynamic_batch_slices(rows: List[Dict[str, Any]]) -> List[List[Dict[str, Any]]]:
    """
    ğŸ¯ DYNAMIC BATCH SIZING - Creates variable-sized batches that GUARANTEE no API token errors.
    
    Protection Layers:
    1. Individual chunk validation (max 6,000 tokens)
    2. Dynamic batch sizing (max 7,692 tokens total)
    3. Conservative safety margins
    4. Multiple validation checks
    """
    batches = []
    current_batch = []
    current_tokens = 0
    skipped_chunks = 0
    
    # Initialize token encoder for accurate counting
    encoder = None
    if TIKTOKEN_AVAILABLE:
        try:
            encoder = tiktoken.encoding_for_model(EMBEDDING_MODEL)
        except:
            pass
    
    def count_tokens_accurate(text: str) -> int:
        """Count tokens with maximum accuracy."""
        if encoder:
            return len(encoder.encode(text))
        else:
            # Conservative fallback estimation
            return int(len(text.split()) * 0.75) + 50
    
    log.info(f"ğŸ¯ Starting dynamic batch creation with {len(rows)} chunks")
    log.info(f"ğŸ¯ Limits: {MAX_CHUNK_TOKENS} tokens/chunk, {MAX_BATCH_TOKENS} tokens/batch")
    
    for i, row in enumerate(rows):
        # Clean text and validate
        text = (row.get("text") or "").replace("\x00", "").strip()
        if not text:
            log.warning(f"ğŸ¯ Skipping empty chunk {row.get('id', 'unknown')}")
            continue
        
        # ï¸ PROTECTION LAYER 1: Individual chunk validation
        chunk_tokens = count_tokens_accurate(text)
        
        if chunk_tokens > MAX_CHUNK_TOKENS:
            log.warning(f"ğŸ¯ Skipping oversized chunk {row.get('id', 'unknown')}: {chunk_tokens} tokens (max: {MAX_CHUNK_TOKENS})")
            skipped_chunks += 1
            continue
        
        # ğŸ›¡ï¸ PROTECTION LAYER 2: Dynamic batch sizing
        would_exceed = current_tokens + chunk_tokens > MAX_BATCH_TOKENS
        
        if would_exceed and current_batch:
            # Finalize current batch
            log.info(f"ğŸ¯ Created batch with {len(current_batch)} chunks (~{current_tokens} tokens)")
            batches.append(current_batch)
            current_batch = []
            current_tokens = 0
        
        # Add chunk to current batch
        current_batch.append({"id": row["id"], "text": text})
        current_tokens += chunk_tokens
        
        # ğŸ›¡ï¸ PROTECTION LAYER 3: Safety validation
        if current_tokens > MAX_BATCH_TOKENS:
            log.error(f"ğŸš¨ CRITICAL: Batch exceeded limit! {current_tokens} > {MAX_BATCH_TOKENS}")
            # Emergency fallback - remove last chunk and finalize batch
            if len(current_batch) > 1:
                current_batch.pop()
                current_tokens -= chunk_tokens
                log.info(f"ğŸ¯ Emergency: Created batch with {len(current_batch)} chunks (~{current_tokens} tokens)")
                batches.append(current_batch)
                current_batch = [{"id": row["id"], "text": text}]
                current_tokens = chunk_tokens
            else:
                log.error(f"ğŸš¨ Single chunk too large: {chunk_tokens} tokens")
                current_batch = []
                current_tokens = 0
                skipped_chunks += 1
    
    # Add final batch if not empty
    if current_batch and current_tokens >= MIN_BATCH_TOKENS:
        log.info(f"ğŸ¯ Created final batch with {len(current_batch)} chunks (~{current_tokens} tokens)")
        batches.append(current_batch)
    elif current_batch:
        log.warning(f"ğŸ¯ Skipping tiny final batch: {current_tokens} tokens < {MIN_BATCH_TOKENS} minimum")
    
    # ğŸ›¡ï¸ PROTECTION LAYER 4: Final validation
    total_chunks = sum(len(batch) for batch in batches)
    max_batch_tokens = max((sum(count_tokens_accurate(chunk["text"]) for chunk in batch) for batch in batches), default=0)
    
    log.info(f"ğŸ¯ DYNAMIC BATCHING COMPLETE:")
    log.info(f"   ğŸ“Š {len(batches)} batches created")
    log.info(f"   ğŸ“Š {total_chunks} chunks processed")
    log.info(f"   ğŸ“Š {skipped_chunks} chunks skipped")
    log.info(f"   ğŸ“Š Largest batch: {max_batch_tokens} tokens (limit: {MAX_BATCH_TOKENS})")
    log.info(f"   âœ… GUARANTEED: No batch exceeds {MAX_BATCH_TOKENS} tokens")
    
    if max_batch_tokens > MAX_BATCH_TOKENS:
        log.error(f"ğŸš¨ CRITICAL ERROR: Batch validation failed!")
        raise RuntimeError(f"Batch token validation failed: {max_batch_tokens} > {MAX_BATCH_TOKENS}")
    
    return batches

def embed_slice(sb,slice_rows:List[Dict[str,Any]])->int:
    embeds=generate_embedding_batch([r["text"] for r in slice_rows])
    ok=0
    for row,emb in zip(slice_rows,embeds):
        attempt=0
        while attempt<MAX_RETRIES:
            res=sb.table("documents_chunks").update({"embedding":emb}).eq("id",row["id"]).execute()
            if getattr(res,"error",None):
                attempt+=1; log.warning("Update %s failed (%s/%s): %s",row["id"],attempt,MAX_RETRIES,res.error)
                time.sleep(RETRY_DELAY)
            else: ok+=1; break
    return ok

if __name__=="__main__": 
    asyncio.run(main_async_cli())


================================================================================


################################################################################
# File: scripts/RAG_stages/extract_clean.py
################################################################################

# File: scripts/RAG_stages/extract_clean.py

#!/usr/bin/env python3
"""
Stage 1-2 â€” *Extract PDF â†’ clean text â†’ logical sections*  
Optimized version with concurrent processing support.
Outputs `<n>.json` (+ a pretty `.txt`) in `city_clerk_documents/{json,txt}/`.
This file is **fully self-contained** â€“ no import from `RAG_stages.common`.
"""
from __future__ import annotations

import json, logging, os, pathlib, re, sys
from collections import Counter
from datetime import datetime
from textwrap import dedent
from typing import Any, Dict, List, Sequence, Optional
import asyncio
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing as mp

# â”€â”€â”€ helpers formerly in common.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_LATIN1_REPLACEMENTS = {  # windows-1252 fallback
    0x82: "â€š",
    0x83: "Æ’",
    0x84: "â€",
    0x85: "â€¦",
    0x86: "â€ ",
    0x87: "â€¡",
    0x88: "Ë†",
    0x89: "â€°",
    0x8A: "Å ",
    0x8B: "â€¹",
    0x8C: "Å’",
    0x8E: "Å½",
    0x91: "'",
    0x92: "'",
    0x93: '"',
    0x94: '"',
    0x95: "â€¢",
    0x96: "â€“",
    0x97: "â€”",
    0x98: "Ëœ",
    0x99: "â„¢",
    0x9A: "Å¡",
    0x9B: "â€º",
    0x9C: "Å“",
    0x9E: "Å¾",
    0x9F: "Å¸",
}
_TRANSLATE_LAT1 = str.maketrans(_LATIN1_REPLACEMENTS)
def latin1_scrub(txt:str)->str: return txt.translate(_TRANSLATE_LAT1)

_WS_RE = re.compile(r"[ \t]+\n")
def normalize_ws(txt:str)->str:
    return re.sub(r"[ \t]{2,}", " ", _WS_RE.sub("\n", txt)).strip()

def pct_ascii_letters(txt:str)->float:
    letters=sum(ch.isascii() and ch.isalpha() for ch in txt)
    return letters/max(1,len(txt))

def needs_ocr(txt:str)->bool:
    return (not txt.strip()) or ("\x00" in txt) or (pct_ascii_letters(txt)<0.15)

def scrub_nuls(obj:Any)->Any:
    if isinstance(obj,str):  return obj.replace("\x00","")
    if isinstance(obj,list): return [scrub_nuls(x) for x in obj]
    if isinstance(obj,dict): return {k:scrub_nuls(v) for k,v in obj.items()}
    return obj

def _make_converter()->DocumentConverter:
    opts = PdfPipelineOptions()
    opts.do_ocr = True
    return DocumentConverter({InputFormat.PDF: PdfFormatOption(pipeline_options=opts)})
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

from dotenv import load_dotenv
from groq import Groq
from supabase import create_client            # only needed if you later push rows
from tqdm import tqdm

# â”€â”€â”€ optional heavy deps ----------------------------------------------------
try:
    import PyPDF2                             # raw fallback
    from unstructured.partition.pdf import partition_pdf
    from docling.datamodel.base_models import InputFormat
    from docling.datamodel.pipeline_options import PdfPipelineOptions
    from docling.document_converter import DocumentConverter, PdfFormatOption
except ImportError as exc:                    # pragma: no cover
    sys.exit(f"Missing dependency â†’ {exc}.  Run `pip install -r requirements.txt`.")

# â”€â”€â”€ env / paths ------------------------------------------------------------
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

REPO_ROOT  = pathlib.Path(__file__).resolve().parents[2]
TXT_DIR    = REPO_ROOT / "city_clerk_documents" / "txt"
JSON_DIR   = REPO_ROOT / "city_clerk_documents" / "json"
TXT_DIR.mkdir(parents=True, exist_ok=True)
JSON_DIR.mkdir(parents=True, exist_ok=True)

GPT_META_MODEL = "gpt-4.1-mini-2025-04-14"

log = logging.getLogger(__name__)

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                           helper utilities                              â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_LATIN = {0x91:"'",0x92:"'",0x93:'"',0x94:'"',0x95:"â€¢",0x96:"â€“",0x97:"â€”"}
_DOI_RE = re.compile(r"\b10\.\d{4,9}/[-._;()/:A-Z0-9]+", re.I)
_JOURNAL_RE = re.compile(r"(Journal|Proceedings|Annals|Psychiatry|Psychology|Nature|Science)[^\n]{0,120}", re.I)
_ABSTRACT_RE = re.compile(r"(?<=\bAbstract\b[:\s])(.{50,2000}?)(?:\n[A-Z][^\n]{0,60}\n|\Z)", re.S)
_KEYWORDS_RE = re.compile(r"\bKey\s*words?\b[:\s]*(.+)", re.I)
_HEADING_TYPES = {"title","heading","header","subtitle","subheading"}

# â”€â”€â”€ minimal bib helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _authors(val)->List[str]:
    if val is None: return []
    if isinstance(val,list): return [str(x).strip() for x in val if x]
    return re.split(r"\s*,\s*|\s+and\s+", str(val).strip())

_DEF_META = {
    "document_type": None,
    "title": None,
    "date": None,
    "year": None,
    "month": None,
    "day": None,
    "mayor": None,
    "vice_mayor": None,
    "commissioners": [],
    "city_attorney": None,
    "city_manager": None,
    "city_clerk": None,
    "public_works_director": None,
    "agenda": None,
    "keywords": [],
}

def merge_meta(*sources:Dict[str,Any])->Dict[str,Any]:
    m=_DEF_META.copy()
    for src in sources:
        for k,v in src.items():
            if v not in (None,"",[],{}): m[k]=v
    m["commissioners"]=m["commissioners"] or []
    m["keywords"]=m["keywords"] or []
    return m

def bib_from_filename(pdf:pathlib.Path)->Dict[str,Any]:
    s=pdf.stem
    m=re.search(r"\b(19|20)\d{2}\b",s)
    yr=int(m.group(0)) if m else None
    if m:
        title=s[m.end():].strip(" -_")
    else:
        parts=s.split(" ",1); title=parts[1] if len(parts)==2 else s
    return {"year":yr,"title":title}

def bib_from_header(txt:str)->Dict[str,Any]:
    md={}
    if m:=_DOI_RE.search(txt): md["doi"]=m.group(0)
    if m:=_JOURNAL_RE.search(txt): md["journal"]=" ".join(m.group(0).split())
    if m:=_ABSTRACT_RE.search(txt): md["abstract"]=" ".join(m.group(1).split())
    if m:=_KEYWORDS_RE.search(txt):
        kws=[k.strip(" ;.,") for k in re.split(r"[;,]",m.group(1)) if k.strip()]
        md["keywords"]=kws
    return md

# â”€â”€â”€ GPT enrichment â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _first_words(txt:str,n:int=3000)->str: return " ".join(txt.split()[:n])

def gpt_metadata(text:str)->Dict[str,Any]:
    if not OPENAI_API_KEY: return {}
    cli=Groq()
    prompt=dedent(f"""
        Extract metadata from this city clerk document and return a JSON object with these fields:
        - document_type: one of [Resolution, Ordinance, Proclamation, Contract, Meeting Minutes, Agenda]
        - title: document title
        - date: full date string
        - year: numeric year
        - month: numeric month (1-12)
        - day: numeric day
        - mayor: name only (e.g., "John Smith")
        - vice_mayor: name only (e.g., "Jane Doe")
        - commissioners: array of commissioner names only (e.g., ["Robert Brown", "Sarah Johnson", "Michael Davis"])
        - city_attorney: name only
        - city_manager: name only
        - city_clerk: name only
        - public_works_director: name only
        - agenda: agenda items or summary if present
        - keywords: array of relevant keywords (e.g., ["budget", "zoning", "public safety"])
        
        Text:
        {text}
    """)
    rsp=cli.chat.completions.create(
        model="meta-llama/llama-4-maverick-17b-128e-instruct",
        temperature=0,
        max_completion_tokens=8192,
        top_p=1,
        stream=False,
        stop=None,
        messages=[{"role":"system","content":"Structured metadata extractor"},
                  {"role":"user","content":prompt}])
    txt=rsp.choices[0].message.content
    json_txt=re.search(r"{[\s\S]*}",txt)
    return json.loads(json_txt.group(0) if json_txt else "{}")        # type: ignore[arg-type]

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                         core extraction logic                            â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _docling_elements(doc)->List[Dict[str,Any]]:
    p:Dict[int,List[Dict[str,str]]]={}
    for it,_ in doc.iterate_items():
        pg=getattr(it.prov[0],"page_no",1)
        lbl=(getattr(it,"label","") or "").upper()
        typ="heading" if lbl in ("TITLE","SECTION_HEADER","HEADER") else \
            "list_item" if lbl=="LIST_ITEM" else \
            "table" if lbl=="TABLE" else "paragraph"
        p.setdefault(pg,[]).append({"type":typ,"text":str(getattr(it,"text",it)).strip()})
    out=[]
    for pn in sorted(p):
        out.append({"section":f"Page {pn}","page_number":pn,
                    "text":"\n".join(el["text"] for el in p[pn]),
                    "elements":p[pn]})
    return out

def _unstructured_elements(pdf:pathlib.Path)->List[Dict[str,Any]]:
    els=partition_pdf(str(pdf),strategy="hi_res")
    pages:Dict[int,List[Dict[str,str]]]={}
    for el in els:
        pn=getattr(el.metadata,"page_number",1)
        pages.setdefault(pn,[]).append({"type":el.category or "paragraph",
                                        "text":normalize_ws(str(el))})
    return [{"section":f"Page {pn}","page_number":pn,
             "text":"\n".join(e["text"] for e in it),"elements":it}
            for pn,it in sorted(pages.items())]

def _pypdf_elements(pdf:pathlib.Path)->List[Dict[str,Any]]:
    out=[]
    with open(pdf,"rb") as fh:
        for pn,pg in enumerate(PyPDF2.PdfReader(fh).pages,1):
            raw=normalize_ws(pg.extract_text() or "")
            paras=[p for p in re.split(r"\n{2,}",raw) if p.strip()]
            out.append({"section":f"Page {pn}","page_number":pn,
                        "text":"\n".join(paras),
                        "elements":[{"type":"paragraph","text":p} for p in paras]})
    return out

def _group_by_headings(page_secs:Sequence[Dict[str,Any]])->List[Dict[str,Any]]:
    out=[]; cur=None; last=1
    for s in page_secs:
        pn=s.get("page_number",last); last=pn
        for el in s.get("elements",[]):
            kind=(el.get("type")or"").lower(); txt=el.get("text","").strip()
            if kind in _HEADING_TYPES and txt:
                if cur: out.append(cur)
                cur={"section":txt,"page_start":pn,"page_end":pn,"elements":[el.copy()]}
            else:
                if not cur:
                    cur={"section":"(untitled)","page_start":pn,"page_end":pn,"elements":[]}
                cur["elements"].append(el.copy()); cur["page_end"]=pn
    if cur and not out: out.append(cur)
    return out

def _sections_md(sections:List[Dict[str,Any]])->str:
    md=[]
    for s in sections:
        md.append(f"# {s.get('section','(Untitled)')}")
        for el in s.get("elements",[]): md.append(el.get("text",""))
        md.append("")
    return "\n".join(md).strip()

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                         Inline extract_pdf from common                   â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def extract_pdf(
    pdf: pathlib.Path,
    txt_dir: pathlib.Path,
    json_dir: pathlib.Path,
    conv: DocumentConverter,
    *,
    overwrite: bool = False,
    ocr_lang: str = "eng",
    keep_markup: bool = True,
    docling_only: bool = False,
    stats: Counter | None = None,
    enrich_llm: bool = True,
) -> pathlib.Path:
    """Return path to JSON payload ready for downstream RAG_stages."""
    txt_path = txt_dir / f"{pdf.stem}.txt"
    json_path = json_dir / f"{pdf.stem}.json"
    if not overwrite and txt_path.exists() and json_path.exists():
        return json_path

    # â€“â€“ 1. Docling
    page_secs: List[Dict[str, Any]] = []
    bundle = None
    try:
        bundle = conv.convert(str(pdf))
        if keep_markup:
            page_secs = _docling_elements(bundle.document)
        else:
            full = bundle.document.export_to_text(page_break_marker="\f")
            page_secs = [{
                "section": "Full document",
                "page_number": 1,
                "text": full,
                "elements": [{"type": "paragraph", "text": full}],
            }]
    except Exception as exc:
        log.warning("Docling failed on %s â†’ %s", pdf.name, exc)

    # â€“â€“ 2. Unstructured fallback
    if not page_secs and not docling_only:
        try:
            page_secs = _unstructured_elements(pdf)
        except Exception as exc:
            log.warning("unstructured failed on %s â†’ %s", pdf.name, exc)

    # â€“â€“ 3. PyPDF last resort
    if not page_secs and not docling_only:
        log.info("PyPDF2 fallback on %s", pdf.name)
        page_secs = _pypdf_elements(pdf)

    if not page_secs:
        raise RuntimeError("No text extracted from PDF")

    # Latin-1 scrub + OCR repair
    for sec in page_secs:
        sec["text"] = latin1_scrub(sec.get("text", ""))
        for el in sec.get("elements", []):
            el["text"] = latin1_scrub(el.get("text", ""))
        pn = sec.get("page_number")
        need_ocr_before = bool(pn and needs_ocr(sec["text"]))
        if need_ocr_before:
            try:
                from pdfplumber import open as pdfopen
                import pytesseract

                with pdfopen(str(pdf)) as doc:
                    pil = doc.pages[pn - 1].to_image(resolution=300).original
                ocr_txt = normalize_ws(
                    pytesseract.image_to_string(pil, lang=ocr_lang)
                )
                if ocr_txt:
                    sec["text"] = ocr_txt
                    sec["elements"] = [
                        {"type": "paragraph", "text": p}
                        for p in re.split(r"\n{2,}", ocr_txt)
                        if p.strip()
                    ]
                    if stats is not None:
                        stats["ocr_pages"] += 1
            except Exception:
                pass

    # when markup is disabled we already have final sections
    logical_secs = (
        _group_by_headings(page_secs) if keep_markup else page_secs
    )

    # CRITICAL: Ensure EVERY section has a 'text' field
    # This is required by the chunking stage
    for sec in logical_secs:
        if 'elements' in sec:
            # Build text from elements if not already present
            element_texts = []
            for el in sec.get("elements", []):
                el_text = el.get("text", "")
                # Skip internal docling metadata that starts with self_ref=
                if el_text and not el_text.startswith('self_ref='):
                    element_texts.append(el_text)
            
            # Always set text field (overwrite if exists to ensure consistency)
            sec["text"] = "\n".join(element_texts)
        elif 'text' not in sec:
            # Fallback for sections without elements
            sec["text"] = ""
    
    # Also ensure page_secs have text (for debugging/consistency)
    for sec in page_secs:
        if 'text' not in sec and 'elements' in sec:
            element_texts = []
            for el in sec.get("elements", []):
                el_text = el.get("text", "")
                if el_text and not el_text.startswith('self_ref='):
                    element_texts.append(el_text)
            sec["text"] = "\n".join(element_texts)

    header_txt = " ".join(s["text"] for s in page_secs[:2])[:8000]
    heuristic_meta = merge_meta(
        bundle.document.metadata.model_dump()
        if "bundle" in locals() and hasattr(bundle.document, "metadata")
        else {},
        bib_from_filename(pdf),
        bib_from_header(header_txt),
    )

    # single GPT call for **all** metadata fields -----------------------
    llm_meta: Dict[str, Any] = {}
    if enrich_llm and OPENAI_API_KEY:
        try:
            oa = Groq()
            llm_meta = gpt_metadata(
                _first_words(" ".join(s["text"] for s in logical_secs))
            )
        except Exception as exc:
            log.warning("LLM metadata extraction failed on %s â†’ %s", pdf.name, exc)

    meta = merge_meta(heuristic_meta, llm_meta)

    payload = {
        **meta,
        "created_at": datetime.utcnow().isoformat() + "Z",
        "source_pdf": str(pdf.resolve()),
        "sections": logical_secs,
    }

    json_path.parent.mkdir(parents=True, exist_ok=True)
    txt_path.parent.mkdir(parents=True, exist_ok=True)
    json_path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), "utf-8")

    # --- Markdown serialisation (rich) ---
    md_text = _sections_md(logical_secs) if keep_markup else "\n".join(
        "# " + s.get("section", "(No title)") + "\n\n" + s.get("text", "") for s in logical_secs
    )
    txt_path.write_text(md_text, "utf-8")

    if stats is not None:
        stats["processed"] += 1

    return json_path

# New: Thread-safe converter pool
class ConverterPool:
    """Thread-safe pool of DocumentConverter instances."""
    def __init__(self, size: int = None):
        self.size = size or mp.cpu_count()
        self._converters = []
        self._lock = mp.Lock()
        self._initialized = False
    
    def get(self):
        """Get a converter from the pool."""
        with self._lock:
            if not self._initialized:
                self._initialize()
            return self._converters[mp.current_process()._identity[0] % self.size]
    
    def _initialize(self):
        """Lazy initialize converters."""
        for _ in range(self.size):
            self._converters.append(_make_converter())
        self._initialized = True

# Global converter pool
_converter_pool = ConverterPool()

def extract_pdf_concurrent(
    pdf: pathlib.Path,
    txt_dir: pathlib.Path,
    json_dir: pathlib.Path,
    *,
    overwrite: bool = False,
    ocr_lang: str = "eng",
    keep_markup: bool = True,
    docling_only: bool = False,
    stats: Counter | None = None,
    enrich_llm: bool = True,
) -> pathlib.Path:
    """Thread-safe version of extract_pdf that uses converter pool."""
    conv = _converter_pool.get()
    return extract_pdf(
        pdf, txt_dir, json_dir, conv,
        overwrite=overwrite,
        ocr_lang=ocr_lang,
        keep_markup=keep_markup,
        docling_only=docling_only,
        stats=stats,
        enrich_llm=enrich_llm
    )

async def extract_batch_async(
    pdfs: List[pathlib.Path],
    *,
    overwrite: bool = False,
    keep_markup: bool = True,
    ocr_lang: str = "eng",
    enrich_llm: bool = True,
    max_workers: Optional[int] = None,
) -> List[pathlib.Path]:
    """Extract multiple PDFs concurrently."""
    from .acceleration_utils import hardware
    
    stats = Counter()
    results = []
    
    # Use process pool for CPU-bound extraction
    with hardware.get_process_pool(max_workers) as executor:
        # Submit all tasks
        future_to_pdf = {
            executor.submit(
                extract_pdf_concurrent,
                pdf, TXT_DIR, JSON_DIR,
                overwrite=overwrite,
                ocr_lang=ocr_lang,
                keep_markup=keep_markup,
                enrich_llm=enrich_llm,
                stats=stats
            ): pdf
            for pdf in pdfs
        }
        
        # Process completed tasks
        from tqdm import tqdm
        for future in tqdm(as_completed(future_to_pdf), total=len(pdfs), desc="Extracting PDFs"):
            pdf = future_to_pdf[future]
            try:
                json_path = future.result()
                results.append(json_path)
            except Exception as exc:
                log.error(f"Failed to extract {pdf.name}: {exc}")
                
    log.info(f"Extraction complete: {stats['processed']} processed, {stats['ocr_pages']} OCR pages")
    return results

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                          public entry-point                             â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ---------------------------------------------------------------------------
#  Stage-1/2 wrapper (now paper-thin)
# ---------------------------------------------------------------------------
def run_one(
    pdf: pathlib.Path,
    *,
    overwrite: bool = False,
    keep_markup: bool = True,
    ocr_lang: str = "eng",
    enrich_llm: bool = True,
    conv=None,
    stats: Counter | None = None,
) -> pathlib.Path:
    conv = conv or _make_converter()
    out = extract_pdf(
        pdf,
        TXT_DIR,
        JSON_DIR,
        conv,
        overwrite=overwrite,
        ocr_lang=ocr_lang,
        keep_markup=keep_markup,
        enrich_llm=enrich_llm,
        stats=stats,
    )
    return out

def json_path_for(pdf:pathlib.Path)->pathlib.Path:
    """Helper if Stage 1 is disabled but its artefact exists."""
    return JSON_DIR / f"{pdf.stem}.json"

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format="%(message)s")
    import argparse
    p=argparse.ArgumentParser(); p.add_argument("pdf",type=pathlib.Path)
    args=p.parse_args()
    print(run_one(args.pdf))


================================================================================


################################################################################
# File: scripts/graph_rag_stages/phase3_querying/smart_query_router.py
################################################################################

# File: scripts/graph_rag_stages/phase3_querying/smart_query_router.py

from enum import Enum
from typing import Dict, Any, Optional, List, Tuple
import re
import logging

logger = logging.getLogger(__name__)

class QueryIntent(Enum):
    ENTITY_SPECIFIC = "entity_specific"  # Use Local
    HOLISTIC = "holistic"               # Use Global  
    EXPLORATORY = "exploratory"         # Use DRIFT
    TEMPORAL = "temporal"               # Use DRIFT

class QueryFocus(Enum):
    SPECIFIC_ENTITY = "specific_entity"          # User wants info about ONE specific item
    MULTIPLE_SPECIFIC = "multiple_specific"      # User wants info about MULTIPLE specific items
    COMPARISON = "comparison"                    # User wants to compare entities
    CONTEXTUAL = "contextual"                    # User wants relationships/context
    GENERAL = "general"                          # No specific entity mentioned

class SmartQueryRouter:
    """Automatically route queries to the optimal search method with intelligent intent detection."""
    
    def __init__(self):
        # Entity extraction patterns
        self.entity_patterns = {
            'agenda_item': [
                r'(?:agenda\s+)?(?:item|items)\s+([A-Z]-?\d+)',
                r'(?:item|items)\s+([A-Z]-?\d+)',
                r'([A-Z]-\d+)(?:\s+agenda)?',
                r'\b([A-Z]-\d+)\b'  # Just the code itself
            ],
            'ordinance': [
                r'ordinance(?:\s+(?:number|no\.?|#))?\s*(\d{4}-\d+|\d+)',
                r'(?:city\s+)?ordinance\s+(\d{4}-\d+|\d+)',
                r'\b(\d{4}-\d+)\b(?=.*ordinance)',
                r'ordinance\s+(\w+)'
            ],
            'resolution': [
                r'resolution(?:\s+(?:number|no\.?|#))?\s*(\d{4}-\d+|\d+)',
                r'(?:city\s+)?resolution\s+(\d{4}-\d+|\d+)',
                r'\b(\d{4}-\d+)\b(?=.*resolution)',
                r'resolution\s+(\w+)'
            ]
        }
        
        # Intent indicators
        self.specific_indicators = {
            'singular_determiners': ['the', 'this', 'that', 'a', 'an'],
            'identity_verbs': ['is', 'are', 'was', 'were', 'means', 'mean', 'refers to', 'refer to', 'concerns', 'concern', 'about'],
            'detail_nouns': ['details', 'information', 'content', 'text', 'provision', 'provisions', 'summary', 'summaries', 'description', 'descriptions'],
            'specific_question_words': ['what', 'which', 'show', 'tell', 'explain', 'describe', 'list'],
            'limiting_adverbs': ['only', 'just', 'specifically', 'exactly', 'precisely', 'individually', 'separately']
        }
        
        self.comparison_indicators = {
            'comparison_verbs': ['compare', 'contrast', 'differ', 'differentiate', 'distinguish'],
            'comparison_words': ['versus', 'vs', 'against', 'compared to', 'difference', 'differences', 'similarity', 'similarities'],
            'comparison_phrases': ['how do', 'what is the difference', 'what are the differences']
        }
        
        self.contextual_indicators = {
            'plural_forms': ['items', 'ordinances', 'resolutions', 'documents'],
            'relationship_words': ['related', 'connected', 'associated', 'linked', 'relationship', 
                                  'connections', 'references', 'mentions', 'together', 'context',
                                  'affects', 'impacts', 'influences', 'between', 'among'],
            'exploration_verbs': ['explore', 'analyze', 'understand', 'investigate'],
            'scope_expanders': ['all', 'other', 'various', 'multiple', 'several', 'any']
        }
        
        # Holistic patterns for global search
        self.holistic_patterns = [
            r"what are the (?:main|top|key) (themes|topics|issues)",
            r"summarize (?:the|all) (.*)",
            r"overall (.*)",
            r"trends in (.*)",
            r"patterns across (.*)"
        ]
        
        # Temporal patterns for drift search
        self.temporal_patterns = [
            r"how has (.*) (?:changed|evolved)",
            r"timeline of (.*)",
            r"history of (.*)",
            r"development of (.*) over time",
            r"evolution of (.*)",
            r"changes in (.*)"
        ]
    
    def determine_query_method(self, query: str) -> Dict[str, Any]:
        """Determine query method with source tracking enabled."""
        query_lower = query.lower()
        
        # First check for holistic queries (global search)
        for pattern in self.holistic_patterns:
            if re.search(pattern, query_lower):
                result = {
                    "method": "global",
                    "intent": QueryIntent.HOLISTIC,
                    "params": {
                        "community_level": self._determine_community_level(query),
                        "response_type": "multiple paragraphs"
                    }
                }
                # Add source tracking to params
                result['params']['track_sources'] = True
                result['params']['include_source_metadata'] = True
                result['params']['citation_style'] = 'inline'
                return result
        
        # Check for temporal/exploratory queries (drift search)
        for pattern in self.temporal_patterns:
            if re.search(pattern, query_lower):
                result = {
                    "method": "drift",
                    "intent": QueryIntent.TEMPORAL,
                    "params": {
                        "initial_community_level": 2,
                        "max_follow_ups": 5
                    }
                }
                # Add source tracking to params
                result['params']['track_sources'] = True
                result['params']['include_source_metadata'] = True
                result['params']['citation_style'] = 'inline'
                return result
        
        # Extract ALL entity references
        all_entities = self._extract_all_entities(query)
        
        if not all_entities:
            # No specific entity found - use local search as default
            result = {
                "method": "local",
                "intent": QueryIntent.EXPLORATORY,
                "params": {
                    "top_k_entities": 10,
                    "include_community_context": True
                }
            }
            # Add source tracking to params
            result['params']['track_sources'] = True
            result['params']['include_source_metadata'] = True
            result['params']['citation_style'] = 'inline'
            return result
        
        # Handle single entity
        if len(all_entities) == 1:
            entity_info = all_entities[0]
            query_focus = self._determine_single_entity_focus(query_lower, entity_info)
            
            if query_focus == QueryFocus.SPECIFIC_ENTITY:
                result = {
                    "method": "local",
                    "intent": QueryIntent.ENTITY_SPECIFIC,
                    "params": {
                        "entity_filter": {
                            "type": entity_info['type'].upper(),
                            "value": entity_info['value']
                        },
                        "top_k_entities": 1,
                        "include_community_context": False,
                        "strict_entity_focus": True,
                        "disable_community": True
                    }
                }
            else:  # CONTEXTUAL
                result = {
                    "method": "local",
                    "intent": QueryIntent.ENTITY_SPECIFIC,
                    "params": {
                        "entity_filter": {
                            "type": entity_info['type'].upper(),
                            "value": entity_info['value']
                        },
                        "top_k_entities": 10,
                        "include_community_context": True,
                        "strict_entity_focus": False,
                        "disable_community": False
                    }
                }
            
            # Add source tracking to params
            result['params']['track_sources'] = True
            result['params']['include_source_metadata'] = True
            result['params']['citation_style'] = 'inline'
            return result
        
        # Handle multiple entities
        else:
            query_focus = self._determine_multi_entity_focus(query_lower, all_entities)
            
            if query_focus == QueryFocus.MULTIPLE_SPECIFIC:
                # User wants specific info about each entity separately
                result = {
                    "method": "local",
                    "intent": QueryIntent.ENTITY_SPECIFIC,
                    "params": {
                        "multiple_entities": all_entities,
                        "top_k_entities": 1,
                        "include_community_context": False,
                        "strict_entity_focus": True,
                        "disable_community": True,
                        "aggregate_results": True  # Combine results for each entity
                    }
                }
            elif query_focus == QueryFocus.COMPARISON:
                # User wants to compare entities
                result = {
                    "method": "local",
                    "intent": QueryIntent.ENTITY_SPECIFIC,
                    "params": {
                        "multiple_entities": all_entities,
                        "top_k_entities": 5,
                        "include_community_context": True,  # Need context for comparison
                        "strict_entity_focus": False,
                        "disable_community": False,
                        "comparison_mode": True
                    }
                }
            else:  # CONTEXTUAL
                # User wants relationships between entities
                result = {
                    "method": "local",
                    "intent": QueryIntent.ENTITY_SPECIFIC,
                    "params": {
                        "multiple_entities": all_entities,
                        "top_k_entities": 10,
                        "include_community_context": True,
                        "strict_entity_focus": False,
                        "disable_community": False,
                        "focus_on_relationships": True
                    }
                }
            
            # Add source tracking to params
            result['params']['track_sources'] = True
            result['params']['include_source_metadata'] = True
            result['params']['citation_style'] = 'inline'
            return result
    
    def _extract_all_entities(self, query: str) -> List[Dict[str, str]]:
        """Extract ALL entity references from query."""
        query_lower = query.lower()
        entities = []
        found_positions = {}  # Track positions to avoid duplicates
        
        for entity_type, patterns in self.entity_patterns.items():
            for pattern in patterns:
                for match in re.finditer(pattern, query_lower, re.IGNORECASE):
                    value = match.group(1)
                    position = match.start()
                    
                    # Normalize value
                    if entity_type == 'agenda_item':
                        value = value.upper()
                        if not '-' in value and len(value) > 1:
                            value = f"{value[0]}-{value[1:]}"
                    
                    # Check if we already found an entity at this position
                    if position not in found_positions:
                        found_positions[position] = True
                        entities.append({
                            'type': entity_type,
                            'value': value,
                            'position': position
                        })
        
        # Sort by position and remove position info
        entities = sorted(entities, key=lambda x: x['position'])
        for entity in entities:
            del entity['position']
        
        # Remove duplicates while preserving order
        seen = set()
        unique_entities = []
        for entity in entities:
            key = f"{entity['type']}:{entity['value']}"
            if key not in seen:
                seen.add(key)
                unique_entities.append(entity)
        
        return unique_entities
    
    def _determine_single_entity_focus(self, query_lower: str, entity_info: Dict) -> QueryFocus:
        """Determine focus for single entity queries."""
        specific_score = 0
        contextual_score = 0
        
        tokens = query_lower.split()
        
        # Check for limiting words
        for word in self.specific_indicators['limiting_adverbs']:
            if word in tokens:
                specific_score += 3
        
        # Check for relationship words
        for word in self.contextual_indicators['relationship_words']:
            if word in tokens:
                contextual_score += 3
        
        # Simple "what is X" patterns
        if re.match(r'^(what|whats|what\'s)\s+(is|are)\s+', query_lower):
            specific_score += 2
        
        # Very short queries tend to be specific
        if len(tokens) <= 3:
            specific_score += 2
        
        # Check for detail-seeking patterns
        for noun in self.specific_indicators['detail_nouns']:
            if noun in query_lower:
                specific_score += 1
        
        logger.info(f"Single entity focus scores - Specific: {specific_score}, Contextual: {contextual_score}")
        
        return QueryFocus.SPECIFIC_ENTITY if specific_score > contextual_score else QueryFocus.CONTEXTUAL
    
    def _determine_multi_entity_focus(self, query_lower: str, entities: List[Dict]) -> QueryFocus:
        """Determine focus for multi-entity queries."""
        tokens = query_lower.split()
        
        # Check for comparison indicators
        comparison_score = 0
        for verb in self.comparison_indicators['comparison_verbs']:
            if verb in tokens:
                comparison_score += 3
        
        for word in self.comparison_indicators['comparison_words']:
            if word in query_lower:
                comparison_score += 2
        
        for phrase in self.comparison_indicators['comparison_phrases']:
            if phrase in query_lower:
                comparison_score += 2
        
        # Check for specific information indicators
        specific_score = 0
        
        # "What are E-1 and E-2?" suggests wanting specific info
        if re.match(r'^(what|whats|what\'s)\s+(are|is)\s+', query_lower):
            specific_score += 2
        
        # Check for "separately" or "individually"
        if any(word in tokens for word in ['separately', 'individually', 'each']):
            specific_score += 3
        
        # Check for detail nouns with plural entities
        for noun in self.specific_indicators['detail_nouns']:
            if noun in query_lower:
                specific_score += 1
        
        # Check for contextual/relationship indicators
        contextual_score = 0
        for word in self.contextual_indicators['relationship_words']:
            if word in tokens:
                contextual_score += 2
        
        # Check for "and" patterns that suggest relationships
        # e.g., "relationship between E-1 and E-2"
        if re.search(r'between.*and', query_lower):
            contextual_score += 3
        
        logger.info(f"Multi-entity focus scores - Comparison: {comparison_score}, Specific: {specific_score}, Contextual: {contextual_score}")
        
        # Determine focus based on highest score
        if comparison_score >= specific_score and comparison_score >= contextual_score:
            return QueryFocus.COMPARISON
        elif specific_score > contextual_score:
            return QueryFocus.MULTIPLE_SPECIFIC
        else:
            return QueryFocus.CONTEXTUAL
    
    def _determine_community_level(self, query: str) -> int:
        """Determine optimal community level based on query scope."""
        if any(word in query.lower() for word in ["entire", "all", "overall", "whole"]):
            return 0  # Highest level
        elif any(word in query.lower() for word in ["department", "district", "area"]):
            return 1  # Mid level
        else:
            return 2  # Lower level for more specific summaries


================================================================================


################################################################################
# File: scripts/graph_rag_stages/phase1_preprocessing/transcript_linker.py
################################################################################

# File: scripts/graph_rag_stages/phase1_preprocessing/transcript_linker.py

"""
Processes verbatim transcript documents and links them to agenda items.
"""

import logging
from pathlib import Path
from typing import Dict, List, Optional
import re
from datetime import datetime

from .pdf_extractor import PDFExtractor
from ..common.utils import sanitize_filename

log = logging.getLogger(__name__)


class TranscriptLinker:
    """Processes verbatim transcript documents and links them to agenda items."""
    
    def __init__(self, output_dir: Path):
        self.output_dir = output_dir
        self.pdf_extractor = PDFExtractor()
        
        # Pattern to extract date and item info from filename
        self.filename_pattern = re.compile(
            r'(\d{2})_(\d{2})_(\d{4})\s*-\s*Verbatim Transcripts\s*-\s*(.+)\.pdf',
            re.IGNORECASE
        )

    async def extract_and_save_transcript(self, pdf_path: Path) -> None:
        """Extract and save a verbatim transcript with agenda item linking."""
        log.info(f"ğŸ¤ Processing transcript: {pdf_path.name}")
        
        # Parse filename to extract meeting info
        match = self.filename_pattern.match(pdf_path.name)
        if not match:
            log.warning(f"Could not parse transcript filename: {pdf_path.name}")
            # Fall back to generic processing
            await self._process_generic_transcript(pdf_path)
            return
        
        month, day, year = match.groups()[:3]
        item_info = match.group(4).strip()
        meeting_date = f"{month}.{day}.{year}"
        
        # Extract text using PDF extractor
        full_text, pages = self.pdf_extractor.extract_text_from_pdf(pdf_path)
        if not full_text:
            log.warning(f"No text extracted from {pdf_path.name}, skipping.")
            return

        # Parse item codes from filename
        parsed_items = self._parse_item_codes(item_info)
        
        # Determine transcript type
        transcript_type = self._determine_transcript_type(item_info, parsed_items)
        
        # Create transcript data structure
        transcript_data = {
            'source_file': pdf_path.name,
            'doc_id': self._generate_doc_id(pdf_path),
            'full_text': full_text,
            'meeting_date': meeting_date,
            'item_codes': parsed_items['item_codes'],
            'section_codes': parsed_items['section_codes'],
            'transcript_type': transcript_type,
            'item_info_raw': item_info,
            'metadata': {
                'extraction_method': 'docling',
                'num_pages': len(pages),
                'total_chars': len(full_text),
                'extraction_timestamp': datetime.now().isoformat()
            }
        }

        # Save as enriched markdown
        self._save_as_markdown(pdf_path, transcript_data)

    async def _process_generic_transcript(self, pdf_path: Path) -> None:
        """Process transcript with generic filename pattern."""
        # Extract text
        full_text, pages = self.pdf_extractor.extract_text_from_pdf(pdf_path)
        if not full_text:
            log.warning(f"No text extracted from {pdf_path.name}, skipping.")
            return

        # Try to extract date from filename
        date_match = re.search(r'(\d{2})[._](\d{2})[._](\d{4})', pdf_path.name)
        meeting_date = 'unknown'
        if date_match:
            month, day, year = date_match.groups()
            meeting_date = f"{month}.{day}.{year}"

        # Create basic transcript data
        transcript_data = {
            'source_file': pdf_path.name,
            'doc_id': self._generate_doc_id(pdf_path),
            'full_text': full_text,
            'meeting_date': meeting_date,
            'item_codes': [],
            'section_codes': [],
            'transcript_type': 'unknown',
            'item_info_raw': 'parsed from filename',
            'metadata': {
                'extraction_method': 'docling',
                'num_pages': len(pages),
                'total_chars': len(full_text),
                'extraction_timestamp': datetime.now().isoformat()
            }
        }

        # Save as enriched markdown
        self._save_as_markdown(pdf_path, transcript_data)

    def _parse_item_codes(self, item_info: str) -> Dict[str, List[str]]:
        """Parse item codes from the filename item info section."""
        result = {
            'item_codes': [],
            'section_codes': []
        }
        
        # Check for public comment first
        if re.search(r'public\s+comment', item_info, re.IGNORECASE):
            result['section_codes'].append('PUBLIC_COMMENT')
            return result
        
        # Special cases
        if re.search(r'meeting\s+minutes', item_info, re.IGNORECASE):
            result['item_codes'].append('MEETING_MINUTES')
            return result
        
        if re.search(r'public|full\s+meeting', item_info, re.IGNORECASE) and not re.search(r'comment', item_info, re.IGNORECASE):
            result['item_codes'].append('FULL_MEETING')
            return result
        
        # Handle multiple items with "and"
        if ' and ' in item_info.lower():
            parts = re.split(r'\s+and\s+', item_info, flags=re.IGNORECASE)
            for part in parts:
                codes = self._extract_single_item_codes(part.strip())
                result['item_codes'].extend(codes)
        
        # Handle space-separated items (e.g., "E-5 E-6 E-7")
        elif re.match(r'^([A-Z]-?\d+\s*)+$', item_info):
            items = item_info.split()
            for item in items:
                if re.match(r'^[A-Z]-?\d+$', item):
                    normalized = self._normalize_item_code(item)
                    if normalized:
                        result['item_codes'].append(normalized)
        
        # Handle comma-separated items
        elif ',' in item_info:
            parts = item_info.split(',')
            for part in parts:
                codes = self._extract_single_item_codes(part.strip())
                result['item_codes'].extend(codes)
        
        # Single item or other format
        else:
            codes = self._extract_single_item_codes(item_info)
            result['item_codes'].extend(codes)
        
        # Remove duplicates while preserving order
        result['item_codes'] = list(dict.fromkeys(result['item_codes']))
        result['section_codes'] = list(dict.fromkeys(result['section_codes']))
        
        return result

    def _extract_single_item_codes(self, text: str) -> List[str]:
        """Extract item codes from a single text segment."""
        codes = []
        
        # Patterns for item codes
        patterns = [
            r'([A-Z])\.?\-?(\d+)\.?',  # Letter-based items (E-1, E1, E.-1.)
            r'(\d+)\-(\d+)'             # Number-only items (2-1)
        ]
        
        for pattern in patterns:
            for match in re.finditer(pattern, text):
                if pattern.startswith('(\\d'):  # Number-only pattern
                    codes.append(f"{match.group(1)}-{match.group(2)}")
                else:
                    # Letter-number format
                    letter = match.group(1)
                    number = match.group(2)
                    codes.append(f"{letter}-{number}")
        
        return codes

    def _normalize_item_code(self, code: str) -> str:
        """Normalize item code to consistent format (e.g., E-1)."""
        code = code.strip('. ')
        
        # Letter-number pattern
        letter_match = re.match(r'^([A-Z])\.?\-?(\d+)\.?$', code)
        if letter_match:
            letter = letter_match.group(1)
            number = letter_match.group(2)
            return f"{letter}-{number}"
        
        # Number-number pattern
        number_match = re.match(r'^(\d+)\-(\d+)$', code)
        if number_match:
            return code  # Already in correct format
        
        return code

    def _determine_transcript_type(self, item_info: str, parsed_items: Dict) -> str:
        """Determine the type of transcript based on parsed information."""
        if 'PUBLIC_COMMENT' in parsed_items['section_codes']:
            return 'public_comment'
        elif parsed_items['section_codes']:
            return 'section'
        elif len(parsed_items['item_codes']) > 3:
            return 'multi_item'
        elif len(parsed_items['item_codes']) == 1:
            return 'single_item'
        else:
            return 'item_group'

    def _save_as_markdown(self, pdf_path: Path, transcript_data: Dict) -> None:
        """Save transcript as enriched markdown for GraphRAG."""
        # Build comprehensive header
        header = self._build_transcript_header(transcript_data)
        
        # Add questions section
        questions_section = self._build_item_questions(transcript_data['item_codes'])
        
        # Combine with full text
        full_content = header + questions_section + "\n\n# VERBATIM TRANSCRIPT CONTENT\n\n" + transcript_data.get('full_text', '')
        
        # Generate filename
        meeting_date = transcript_data['meeting_date'].replace('.', '_') if transcript_data['meeting_date'] != 'unknown' else 'unknown'
        item_info_clean = re.sub(r'[^a-zA-Z0-9-]', '_', transcript_data['item_info_raw'])
        
        filename = sanitize_filename(f"verbatim_{meeting_date}_{item_info_clean}.md")
        md_path = self.output_dir / filename
        
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(full_content)
        
        log.info(f"ğŸ“ Saved transcript markdown to: {md_path}")

    def _build_transcript_header(self, transcript_data: Dict) -> str:
        """Build comprehensive transcript header."""
        items_str = ', '.join(transcript_data['item_codes']) if transcript_data['item_codes'] else 'N/A'
        
        header = f"""---
DOCUMENT METADATA AND CONTEXT
=============================

**DOCUMENT IDENTIFICATION:**
- Document Type: VERBATIM_TRANSCRIPT
- Source File: {transcript_data.get('source_file', 'N/A')}
- Meeting Date: {transcript_data.get('meeting_date', 'N/A')}

**TRANSCRIPT DETAILS:**
- Agenda Items Discussed: {items_str}
- Transcript Type: {transcript_data.get('transcript_type', 'N/A')}
- Page Count: {transcript_data['metadata'].get('num_pages', 'N/A')}

**SEARCHABLE IDENTIFIERS:**
- DOCUMENT_TYPE: VERBATIM_TRANSCRIPT
- MEETING_DATE: {transcript_data.get('meeting_date', 'N/A')}
{self._format_item_identifiers(transcript_data['item_codes'])}

**NATURAL LANGUAGE DESCRIPTION:**
This is the verbatim transcript from the {transcript_data.get('meeting_date', 'unknown')} City Commission meeting covering the discussion of {self._describe_items(transcript_data)}.

**QUERY HELPERS:**
{self._build_transcript_query_helpers(transcript_data)}

---

"""
        return header

    def _format_item_identifiers(self, item_codes: List[str]) -> str:
        """Format agenda items as searchable identifiers."""
        lines = []
        for item in item_codes:
            lines.append(f"- AGENDA_ITEM: {item}")
        return '\n'.join(lines)

    def _describe_items(self, transcript_data: Dict) -> str:
        """Create natural language description of items."""
        if transcript_data['item_codes']:
            if len(transcript_data['item_codes']) == 1:
                return f"agenda item {transcript_data['item_codes'][0]}"
            else:
                return f"agenda items {', '.join(transcript_data['item_codes'])}"
        elif 'PUBLIC_COMMENT' in transcript_data.get('section_codes', []):
            return "public comments section"
        else:
            return "the meeting proceedings"

    def _build_transcript_query_helpers(self, transcript_data: Dict) -> str:
        """Build query helpers for transcripts."""
        helpers = []
        for item in transcript_data.get('item_codes', []):
            helpers.append(f"- To find discussion about {item}, search for 'Item {item}' or '{item} discussion'")
        helpers.append(f"- To find all discussions from this meeting, search for '{transcript_data.get('meeting_date', 'unknown')}'")
        helpers.append("- This transcript contains the exact words spoken during the meeting")
        return '\n'.join(helpers)

    def _build_item_questions(self, item_codes: List[str]) -> str:
        """Build Q&A style entries for items."""
        questions = ["## AGENDA ITEMS COVERED\n"]
        for item in item_codes:
            questions.append(f"### What was discussed about Item {item}?")
            questions.append(f"The discussion of Item {item} is transcribed in this document.\n")
        return '\n'.join(questions)

    def _generate_doc_id(self, pdf_path: Path) -> str:
        """Generate canonical document ID."""
        import hashlib
        return f"DOC_{hashlib.sha1(str(pdf_path.absolute()).encode()).hexdigest()[:12]}"


================================================================================


################################################################################
# File: scripts/graph_rag_stages/phase1_preprocessing/document_linker.py
################################################################################

# File: scripts/graph_rag_stages/phase1_preprocessing/document_linker.py

"""
Generic document processor that extracts text and links documents to agenda items.
"""

import logging
from pathlib import Path
from typing import Dict, List, Optional
import re
from datetime import datetime

from .pdf_extractor import PDFExtractor
from ..common.utils import get_llm_client, call_llm_with_retry, sanitize_filename

log = logging.getLogger(__name__)


class DocumentLinker:
    """Links ordinance and resolution documents to agenda items."""
    
    def __init__(self, output_dir: Path):
        self.output_dir = output_dir
        self.pdf_extractor = PDFExtractor()
        self.llm_client = get_llm_client()
        self.model = "gpt-4"

    async def extract_and_save_document(self, pdf_path: Path) -> None:
        """Extract and save a generic document with agenda item linking."""
        log.info(f"ğŸ“„ Processing document: {pdf_path.name}")
        
        # Extract text using base PDF extractor
        full_text, pages = self.pdf_extractor.extract_text_from_pdf(pdf_path)
        if not full_text:
            log.warning(f"No text extracted from {pdf_path.name}, skipping.")
            return

        # Try to extract document metadata
        doc_metadata = await self._extract_document_metadata(full_text, pdf_path)
        
        # Try to find linked agenda item
        agenda_item_code = await self._extract_agenda_item_code(full_text, pdf_path.stem)
        
        # Create document data structure
        document_data = {
            'source_file': pdf_path.name,
            'doc_id': self._generate_doc_id(pdf_path),
            'full_text': full_text,
            'document_type': doc_metadata.get('document_type', 'document'),
            'title': doc_metadata.get('title', pdf_path.stem),
            'agenda_item_code': agenda_item_code,
            'metadata': {
                'extraction_method': 'docling',
                'num_pages': len(pages),
                'total_chars': len(full_text),
                'extraction_timestamp': datetime.now().isoformat(),
                **doc_metadata
            }
        }

        # Save as enriched markdown
        self._save_as_markdown(pdf_path, document_data)

    async def _extract_document_metadata(self, text: str, pdf_path: Path) -> Dict:
        """Extract document metadata using pattern matching and LLM assistance."""
        metadata = {}
        
        # Determine document type from filename and content
        filename_lower = pdf_path.name.lower()
        text_sample = text[:2000].lower()
        
        if 'ordinance' in filename_lower or 'ordinance' in text_sample:
            metadata['document_type'] = 'ordinance'
        elif 'resolution' in filename_lower or 'resolution' in text_sample:
            metadata['document_type'] = 'resolution'
        elif 'minutes' in filename_lower or 'minutes' in text_sample:
            metadata['document_type'] = 'minutes'
        elif 'transcript' in filename_lower or 'transcript' in text_sample:
            metadata['document_type'] = 'transcript'
        else:
            metadata['document_type'] = 'document'
        
        # Extract title
        metadata['title'] = self._extract_title(text)
        
        # Extract date information
        date_info = self._extract_date_info(text, pdf_path)
        metadata.update(date_info)
        
        # Extract document number from filename if present
        doc_number_match = re.search(r'(\d{4}-\d{2,})', pdf_path.name)
        if doc_number_match:
            metadata['document_number'] = doc_number_match.group(1)
        
        return metadata

    async def _extract_agenda_item_code(self, text: str, document_id: str) -> Optional[str]:
        """Extract agenda item code from document text using LLM."""
        log.info(f"ğŸ§  Extracting agenda item code for {document_id}")
        
        # Prepare focused prompt for agenda item extraction
        messages = [
            {
                "role": "system",
                "content": """You are an expert at finding agenda item references in city council documents.
Look for agenda item codes that typically appear in formats like:
- (Agenda Item: E-1)
- Agenda Item E-3
- Item H-3
- H.-3. or E.-2.
- E-2 (simple format)

Search the ENTIRE document carefully. The agenda item can appear anywhere.
Respond with just the code (e.g., "E-2") or "NOT_FOUND" if none exists."""
            },
            {
                "role": "user",
                "content": f"Find the agenda item code in this document:\n\n{text[:8000]}"  # Limit length
            }
        ]
        
        try:
            response = await call_llm_with_retry(
                self.llm_client,
                messages,
                model=self.model,
                temperature=0.1
            )
            
            # Clean and normalize the response
            code = response.strip()
            if code and code != "NOT_FOUND":
                normalized_code = self._normalize_item_code(code)
                log.info(f"âœ… Found agenda item code: {normalized_code}")
                return normalized_code
            else:
                log.info(f"âŒ No agenda item code found")
                return None
                
        except Exception as e:
            log.error(f"Failed to extract agenda item code: {e}")
            return None

    def _normalize_item_code(self, code: str) -> str:
        """Normalize item code to consistent format (e.g., E-1)."""
        if not code:
            return code
        
        # Remove trailing dots and spaces
        code = code.rstrip('. ')
        
        # Remove dots between letter and dash: "E.-1" -> "E-1"
        code = re.sub(r'([A-Z])\.(-)', r'\1\2', code)
        
        # Handle cases without dash: "E.1" -> "E-1"
        code = re.sub(r'([A-Z])\.(\d)', r'\1-\2', code)
        
        # Remove any remaining dots
        code = code.replace('.', '')
        
        return code

    def _extract_title(self, text: str) -> str:
        """Extract document title from text."""
        # Look for common title patterns
        title_patterns = [
            r'(AN?\s+(ORDINANCE|RESOLUTION)[^.]+\.)',
            r'(ORDINANCE\s+NO\.\s*\d+[^.]+\.)',
            r'(RESOLUTION\s+NO\.\s*\d+[^.]+\.)'
        ]
        
        for pattern in title_patterns:
            title_match = re.search(pattern, text[:2000], re.IGNORECASE)
            if title_match:
                return title_match.group(1).strip()
        
        # Fallback to first substantive line
        lines = text.split('\n')
        for line in lines[:20]:
            line = line.strip()
            if len(line) > 20 and not line.isdigit() and not line.isupper():
                return line[:200]  # Limit length
        
        return "Untitled Document"

    def _extract_date_info(self, text: str, pdf_path: Path) -> Dict:
        """Extract date information from text and filename."""
        date_info = {}
        
        # Try to extract from filename first
        date_match = re.search(r'(\d{2})\.(\d{2})\.(\d{4})', pdf_path.name)
        if date_match:
            month, day, year = date_match.groups()
            date_info['meeting_date'] = f"{month}.{day}.{year}"
        
        # Look for "day of Month, Year" pattern in text
        date_pattern = r'day\s+of\s+(\w+),?\s+(\d{4})'
        date_match = re.search(date_pattern, text[:2000])
        if date_match:
            date_info['adoption_date'] = date_match.group(0)
        
        return date_info

    def _save_as_markdown(self, pdf_path: Path, document_data: Dict) -> None:
        """Save document as enriched markdown for GraphRAG."""
        # Build header
        header = self._build_document_header(document_data)
        
        # Combine with full text
        full_content = header + "\n\n# DOCUMENT CONTENT\n\n" + document_data.get('full_text', '')
        
        # Generate filename
        doc_type = document_data['metadata'].get('document_type', 'document')
        doc_number = document_data['metadata'].get('document_number', pdf_path.stem)
        
        filename = sanitize_filename(f"{doc_type}_{doc_number}.md")
        md_path = self.output_dir / filename
        
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(full_content)
        
        log.info(f"ğŸ“ Saved document markdown to: {md_path}")

    def _build_document_header(self, document_data: Dict) -> str:
        """Build document header with metadata."""
        metadata = document_data['metadata']
        
        header = f"""---
DOCUMENT METADATA AND CONTEXT
=============================

**DOCUMENT IDENTIFICATION:**
- Document Type: {metadata.get('document_type', 'DOCUMENT').upper()}
- Title: {document_data.get('title', 'N/A')}
- Source File: {document_data.get('source_file', 'N/A')}

**AGENDA LINKAGE:**
- Linked Agenda Item: {document_data.get('agenda_item_code', 'N/A')}

**DOCUMENT DETAILS:**
- Document Number: {metadata.get('document_number', 'N/A')}
- Meeting Date: {metadata.get('meeting_date', 'N/A')}
- Adoption Date: {metadata.get('adoption_date', 'N/A')}

**SEARCHABLE IDENTIFIERS:**
- DOCUMENT_TYPE: {metadata.get('document_type', 'DOCUMENT').upper()}
- AGENDA_ITEM: {document_data.get('agenda_item_code', 'N/A')}

---

"""
        return header

    def _generate_doc_id(self, pdf_path: Path) -> str:
        """Generate canonical document ID."""
        import hashlib
        return f"DOC_{hashlib.sha1(str(pdf_path.absolute()).encode()).hexdigest()[:12]}"


================================================================================


################################################################################
# File: scripts/RAG_stages/db_upsert.py
################################################################################

# File: scripts/RAG_stages/db_upsert.py

#!/usr/bin/env python3
"""
Stage 6 â€” Optimized database operations with batching and connection pooling.
"""
from __future__ import annotations
import json, logging, os, pathlib, sys
from datetime import datetime
from typing import Any, Dict, List, Sequence, Optional
import asyncio
from concurrent.futures import ThreadPoolExecutor
import threading

from dotenv import load_dotenv
from supabase import create_client
from tqdm import tqdm

load_dotenv()
SUPABASE_URL  = os.getenv("SUPABASE_URL")
SUPABASE_KEY  = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
OPENAI_API_KEY= os.getenv("OPENAI_API_KEY")

META_FIELDS = [
    "document_type", "title", "date", "year", "month", "day",
    "mayor", "vice_mayor", "commissioners",
    "city_attorney", "city_manager", "city_clerk", "public_works_director",
    "agenda", "keywords"
]

log = logging.getLogger(__name__)

# Thread-safe connection pool
class SupabasePool:
    """Thread-safe Supabase client pool."""
    def __init__(self, size: int = 10):
        self.size = size
        self._clients = []
        self._lock = threading.Lock()
        self._initialized = False
    
    def get(self):
        """Get a client from the pool."""
        with self._lock:
            if not self._initialized:
                self._initialize()
            # Round-robin selection
            import random
            return self._clients[random.randint(0, self.size - 1)]
    
    def _initialize(self):
        """Initialize client pool."""
        for _ in range(self.size):
            self._clients.append(init_supabase())
        self._initialized = True

# Global pool
_sb_pool = SupabasePool()

# â”€â”€â”€ Supabase & sanitiser helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def scrub_nuls(obj:Any)->Any:
    if isinstance(obj,str):  return obj.replace("\x00","")
    if isinstance(obj,list): return [scrub_nuls(x) for x in obj]
    if isinstance(obj,dict): return {k:scrub_nuls(v) for k,v in obj.items()}
    return obj

def init_supabase():
    if not (SUPABASE_URL and SUPABASE_KEY):
        sys.exit("â›”  SUPABASE env vars missing")
    return create_client(SUPABASE_URL,SUPABASE_KEY)

def upsert_document(sb,meta:Dict[str,Any])->str:
    meta = scrub_nuls(meta)
    doc_type = meta.get("document_type")
    date = meta.get("date")
    title = meta.get("title")
    if doc_type and date and title:
        existing = (
            sb.table("city_clerk_documents")
            .select("id")
            .eq("document_type", doc_type)
            .eq("date", date)
            .eq("title", title)
            .limit(1)
            .execute()
            .data
        )
        if existing:
            doc_id = existing[0]["id"]
            sb.table("city_clerk_documents").update(meta).eq("id", doc_id).execute()
            return doc_id
    res = sb.table("city_clerk_documents").insert(meta).execute()
    if hasattr(res, "error") and res.error:
        raise RuntimeError(f"Document insert failed: {res.error}")
    return res.data[0]["id"]

# Optimized batch operations
def insert_chunks_optimized(sb, doc_id: str, chunks: Sequence[Dict[str, Any]], 
                          src_json: pathlib.Path, batch_size: int = 1000):
    """Insert chunks with larger batches for better performance."""
    ts = datetime.utcnow().isoformat()
    inserted_ids = []
    
    # Prepare all rows
    rows = [
        {
            "document_id": doc_id,
            "chunk_index": ch["chunk_index"],
            "token_start": ch["token_start"],
            "token_end": ch["token_end"],
            "page_start": ch["page_start"],
            "page_end": ch["page_end"],
            "text": scrub_nuls(ch["text"]),
            "metadata": scrub_nuls(ch.get("metadata", {})),
            "chunking_strategy": "token_window",
            "source_file": str(src_json),
            "created_at": ts,
        }
        for ch in chunks
    ]
    
    # Insert in larger batches
    for i in range(0, len(rows), batch_size):
        batch = rows[i:i + batch_size]
        try:
            res = sb.table("documents_chunks").insert(batch).execute()
            if hasattr(res, "error") and res.error:
                log.error("Batch insert failed: %s", res.error)
                # Fall back to smaller batches
                for j in range(0, len(batch), 100):
                    mini_batch = batch[j:j + 100]
                    mini_res = sb.table("documents_chunks").insert(mini_batch).execute()
                    if hasattr(mini_res, "data"):
                        inserted_ids.extend([r["id"] for r in mini_res.data])
            else:
                inserted_ids.extend([r["id"] for r in res.data])
        except Exception as e:
            log.error(f"Failed to insert batch {i//batch_size + 1}: {e}")
            # Try individual inserts as last resort
            for row in batch:
                try:
                    single_res = sb.table("documents_chunks").insert(row).execute()
                    if hasattr(single_res, "data") and single_res.data:
                        inserted_ids.append(single_res.data[0]["id"])
                except:
                    pass
    
    return inserted_ids

def insert_chunks(sb,doc_id:str,chunks:Sequence[Dict[str,Any]],src_json:pathlib.Path):
    """Original interface using optimized implementation."""
    return insert_chunks_optimized(sb, doc_id, chunks, src_json, batch_size=500)

async def upsert_batch_async(
    documents: List[Dict[str, Any]],
    max_concurrent: int = 5
) -> None:
    """Upsert multiple documents concurrently."""
    semaphore = asyncio.Semaphore(max_concurrent)
    
    async def upsert_one(doc_data):
        async with semaphore:
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(
                None,
                lambda: upsert(
                    doc_data["json_path"],
                    doc_data["chunks"],
                    do_embed=doc_data.get("do_embed", False)
                )
            )
    
    tasks = [upsert_one(doc) for doc in documents]
    
    from tqdm.asyncio import tqdm_asyncio
    await tqdm_asyncio.gather(*tasks, desc="Upserting to database")

# Keep original interface but use optimized implementation
def upsert(json_doc: pathlib.Path, chunks: List[Dict[str, Any]] | None,
           *, do_embed: bool = False) -> None:
    """Original interface with optimized implementation."""
    sb = _sb_pool.get()  # Use connection from pool
    
    data = json.loads(json_doc.read_text())
    row = {k: data.get(k) for k in META_FIELDS} | {
        "source_pdf": data.get("source_pdf", str(json_doc))
    }
    
    # Ensure commissioners is a list
    if "commissioners" in row:
        if isinstance(row["commissioners"], str):
            row["commissioners"] = [row["commissioners"]]
        elif not isinstance(row["commissioners"], list):
            row["commissioners"] = []
    else:
        row["commissioners"] = []
    
    # Ensure keywords is a list
    if "keywords" in row:
        if not isinstance(row["keywords"], list):
            row["keywords"] = []
    else:
        row["keywords"] = []
    
    # Convert agenda from array to text if needed
    if "agenda" in row:
        if isinstance(row["agenda"], list):
            row["agenda"] = "; ".join(str(item) for item in row["agenda"] if item)
        elif row["agenda"] is None:
            row["agenda"] = None
        else:
            row["agenda"] = str(row["agenda"])
    
    # Ensure all text fields are strings or None
    text_fields = ["document_type", "title", "date", "mayor", "vice_mayor", 
                   "city_attorney", "city_manager", "city_clerk", "public_works_director"]
    for field in text_fields:
        if field in row and row[field] is not None:
            row[field] = str(row[field])
    
    # Ensure numeric fields are integers or None
    numeric_fields = ["year", "month", "day"]
    for field in numeric_fields:
        if field in row and row[field] is not None:
            try:
                row[field] = int(row[field])
            except (ValueError, TypeError):
                row[field] = None
    
    doc_id = upsert_document(sb, row)
    
    if not chunks:
        log.info("No chunks to insert for %s", json_doc.stem)
        return
    
    # Use optimized batch insert
    inserted_ids = insert_chunks_optimized(sb, doc_id, chunks, json_doc)
    log.info("â†‘ %s chunks inserted for %s", len(chunks), json_doc.stem)
    
    if do_embed and inserted_ids:
        from RAG_stages import embed_vectors
        embed_vectors.main()

if __name__ == "__main__":
    import argparse
    logging.basicConfig(level=logging.INFO, format="%(message)s")
    p=argparse.ArgumentParser()
    p.add_argument("json",type=pathlib.Path)
    p.add_argument("--chunks",type=pathlib.Path)
    p.add_argument("--embed",action="store_true")
    args=p.parse_args()
    
    chunks = None
    if args.chunks and args.chunks.exists():
        chunks = json.loads(args.chunks.read_text())
    
    upsert(args.json, chunks, do_embed=args.embed)


================================================================================


################################################################################
# File: scripts/graph_rag_stages/common/utils.py
################################################################################

# File: scripts/graph_rag_stages/common/utils.py

"""
General utilities for the unified GraphRAG pipeline.
"""

import logging
import json
import re
from pathlib import Path
from typing import Dict, Any, Optional, List
import openai
from openai import AsyncOpenAI
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

def setup_logging(level: str = "INFO", log_file: Optional[Path] = None) -> None:
    """
    Setup logging configuration for the pipeline.
    
    Args:
        level: Logging level (DEBUG, INFO, WARNING, ERROR)
        log_file: Optional file to write logs to
    """
    log_level = getattr(logging, level.upper(), logging.INFO)
    
    handlers = [logging.StreamHandler()]
    if log_file:
        handlers.append(logging.FileHandler(log_file))
    
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=handlers
    )


def ensure_directory_exists(directory: Path) -> None:
    """
    Ensure that a directory exists, creating it if necessary.
    
    Args:
        directory: Path to the directory
    """
    directory.mkdir(parents=True, exist_ok=True)


def get_llm_client(api_key: Optional[str] = None) -> AsyncOpenAI:
    """
    Get configured OpenAI client for LLM operations.
    
    Args:
        api_key: Optional API key, will use environment variable if not provided
        
    Returns:
        Configured AsyncOpenAI client
    """
    api_key = api_key or os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("OpenAI API key is required")
    
    return AsyncOpenAI(api_key=api_key)


def clean_json_response(response_text: str) -> Dict[str, Any]:
    """
    Clean and parse JSON response from LLM, handling common formatting issues.
    
    Args:
        response_text: Raw response text from LLM
        
    Returns:
        Parsed JSON dictionary
    """
    # Remove markdown code blocks if present
    if "```json" in response_text:
        response_text = response_text.split("```json")[1].split("```")[0]
    elif "```" in response_text:
        response_text = response_text.split("```")[1].split("```")[0]
    
    # Remove any leading/trailing whitespace
    response_text = response_text.strip()
    
    try:
        return json.loads(response_text)
    except json.JSONDecodeError as e:
        # Try to fix common JSON issues
        fixed_text = response_text.replace("'", '"')  # Single to double quotes
        fixed_text = re.sub(r',\s*}', '}', fixed_text)  # Remove trailing commas
        fixed_text = re.sub(r',\s*]', ']', fixed_text)  # Remove trailing commas in arrays
        
        try:
            return json.loads(fixed_text)
        except json.JSONDecodeError:
            logging.error(f"Failed to parse JSON response: {response_text[:200]}...")
            raise e


def extract_metadata_from_header(content: str) -> Dict[str, Any]:
    """
    Extract metadata from markdown header section.
    
    Args:
        content: Markdown content with metadata header
        
    Returns:
        Dictionary of extracted metadata
    """
    metadata = {}
    
    # Look for metadata section between --- markers
    if content.startswith("---"):
        try:
            _, header_section, _ = content.split("---", 2)
            for line in header_section.strip().split("\n"):
                if ":" in line:
                    key, value = line.split(":", 1)
                    metadata[key.strip()] = value.strip()
        except ValueError:
            pass  # No proper YAML header found
    
    # Also look for simple key-value pairs at the beginning
    lines = content.split("\n")
    for line in lines[:20]:  # Check first 20 lines
        if line.startswith("**") and ":**" in line:
            # Extract from bold formatting like **Title:** Something
            key_match = re.search(r'\*\*(.*?)\*\*:\s*(.*)', line)
            if key_match:
                key = key_match.group(1).lower().replace(" ", "_")
                value = key_match.group(2)
                metadata[key] = value
    
    return metadata


def sanitize_filename(filename: str) -> str:
    """
    Sanitize filename by removing or replacing invalid characters.
    
    Args:
        filename: Original filename
        
    Returns:
        Sanitized filename safe for filesystem
    """
    # Replace problematic characters
    filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
    # Remove multiple consecutive underscores
    filename = re.sub(r'_{2,}', '_', filename)
    # Remove leading/trailing underscores and dots
    filename = filename.strip("_.")
    # Limit length
    if len(filename) > 200:
        filename = filename[:200]
    
    return filename


def chunk_text(text: str, chunk_size: int = 4000, overlap: int = 200) -> List[str]:
    """
    Split text into overlapping chunks for processing.
    
    Args:
        text: Text to chunk
        chunk_size: Maximum size of each chunk
        overlap: Number of characters to overlap between chunks
        
    Returns:
        List of text chunks
    """
    if len(text) <= chunk_size:
        return [text]
    
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        
        # Try to break at a sentence or paragraph boundary
        if end < len(text):
            # Look for sentence endings within the last 200 characters
            search_start = max(end - 200, start)
            sentence_end = text.rfind(".", search_start, end)
            if sentence_end > start:
                end = sentence_end + 1
            else:
                # Look for paragraph breaks
                para_end = text.rfind("\n\n", search_start, end)
                if para_end > start:
                    end = para_end + 2
        
        chunks.append(text[start:end])
        
        if end >= len(text):
            break
            
        start = end - overlap
    
    return chunks


def format_file_size(size_bytes: int) -> str:
    """
    Format file size in human-readable format.
    
    Args:
        size_bytes: Size in bytes
        
    Returns:
        Formatted size string
    """
    for unit in ['B', 'KB', 'MB', 'GB']:
        if size_bytes < 1024:
            return f"{size_bytes:.1f} {unit}"
        size_bytes /= 1024
    return f"{size_bytes:.1f} TB"


async def call_llm_with_retry(
    client: AsyncOpenAI,
    messages: List[Dict[str, str]],
    model: str = "gpt-4",
    max_retries: int = 3,
    **kwargs
) -> str:
    """
    Call LLM with retry logic for handling rate limits and transient errors.
    
    Args:
        client: OpenAI client
        messages: List of message dictionaries
        model: Model to use
        max_retries: Maximum number of retries
        **kwargs: Additional arguments for the API call
        
    Returns:
        Response text from the LLM
    """
    import asyncio
    
    for attempt in range(max_retries + 1):
        try:
            response = await client.chat.completions.create(
                model=model,
                messages=messages,
                **kwargs
            )
            return response.choices[0].message.content
        except Exception as e:
            if attempt == max_retries:
                raise e
            
            # Wait before retrying (exponential backoff)
            wait_time = 2 ** attempt
            logging.warning(f"LLM call failed (attempt {attempt + 1}), retrying in {wait_time}s: {e}")
            await asyncio.sleep(wait_time)


================================================================================


################################################################################
# File: scripts/graph_rag_stages/README.md
################################################################################

<!-- 
File: scripts/graph_rag_stages/README.md
 -->

# Unified GraphRAG Pipeline

This is the unified, modular GraphRAG pipeline that combines and replaces the previous `graph_stages` and `microsoft_framework` directories. The pipeline is designed to be testable, maintainable, and easily configurable.

## Structure

```
graph_rag_stages/
â”œâ”€â”€ main_pipeline.py              # Main orchestrator with boolean flags
â”œâ”€â”€ phase1_preprocessing/          # Data extraction and processing
â”‚   â”œâ”€â”€ pdf_extractor.py          # PDF text extraction with Docling
â”‚   â”œâ”€â”€ agenda_extractor.py       # Agenda-specific extraction with LLM
â”‚   â”œâ”€â”€ document_linker.py        # Generic document processing
â”‚   â””â”€â”€ transcript_linker.py      # Verbatim transcript processing
â”œâ”€â”€ phase2_building/               # Graph construction (dual approach)
â”‚   â”œâ”€â”€ custom_graph_builder.py   # Custom graph for Cosmos DB
â”‚   â”œâ”€â”€ graphrag_adapter.py       # Data preparation for GraphRAG
â”‚   â”œâ”€â”€ graphrag_indexer.py       # Microsoft GraphRAG indexing
â”‚   â””â”€â”€ entity_deduplicator.py    # Post-processing optimization
â”œâ”€â”€ phase3_querying/               # Query processing and response
â”‚   â”œâ”€â”€ query_engine.py           # Main query interface
â”‚   â”œâ”€â”€ query_router.py           # Intelligent query routing
â”‚   â”œâ”€â”€ response_enhancer.py      # Response post-processing
â”‚   â””â”€â”€ source_tracker.py         # Provenance tracking
â”œâ”€â”€ common/                        # Shared utilities
â”‚   â”œâ”€â”€ config.py                 # Configuration management
â”‚   â”œâ”€â”€ cosmos_client.py          # Cosmos DB client
â”‚   â””â”€â”€ utils.py                  # Common utility functions
â””â”€â”€ ui/                           # User interfaces
    â””â”€â”€ query_app.py              # Query application UI
```

## Usage

### Main Pipeline Execution

The pipeline is controlled via boolean flags in `main_pipeline.py`:

```python
# Configure what runs
RUN_DATA_PREPROCESSING = True        # Extract and process PDFs
RUN_CUSTOM_GRAPH_PIPELINE = False   # Build custom graph (Cosmos DB)
RUN_GRAPHRAG_INDEXING_PIPELINE = True # Build GraphRAG index
RUN_QUERY_ENGINE = True             # Setup query capabilities

# Run the pipeline with arguments
python -m graph_rag_stages.main_pipeline --source-dir "path/to/pdfs"
```

### Individual Components

You can also use individual components:

```python
# Data preprocessing only
from graph_rag_stages.phase1_preprocessing import run_extraction_pipeline
await run_extraction_pipeline(source_dir, output_dir)

# GraphRAG indexing only
from graph_rag_stages.phase2_building import run_graphrag_indexing_pipeline
await run_graphrag_indexing_pipeline(markdown_dir, graphrag_dir)

# Query engine
from graph_rag_stages.phase3_querying import QueryEngine
engine = QueryEngine(graphrag_root)
response = await engine.answer_query("What agenda items were discussed?")
```

### Configuration

Configure the pipeline via:
1. Environment variables (`.env` file)
2. `settings.yaml` file in project root  
3. Direct configuration in code
4. Command line arguments

Required environment variables:
```bash
OPENAI_API_KEY=your_openai_key
COSMOS_ENDPOINT=your_cosmos_endpoint    # Optional, for custom graph
COSMOS_KEY=your_cosmos_key              # Optional, for custom graph
```

Command line usage:
```bash
# Use default source directory
python -m graph_rag_stages.main_pipeline

# Specify custom source directory
python -m graph_rag_stages.main_pipeline --source-dir "path/to/your/pdfs"
```

## Pipeline Stages

### 1. Data Preprocessing (`phase1_preprocessing/`)

Converts source PDFs into enriched markdown files:
- **PDF Extraction**: Uses Docling for OCR and structure preservation
- **Agenda Processing**: LLM-enhanced extraction of agenda items and metadata  
- **Document Linking**: Connects ordinances/resolutions to agenda items
- **Transcript Processing**: Handles verbatim transcripts with item linking

**Output**: Enriched markdown files with comprehensive metadata headers

### 2. Graph Building (`phase2_building/`)

Two parallel approaches for graph construction:

#### Custom Graph Pipeline (Optional)
- Builds knowledge graph in Azure Cosmos DB
- Creates entities for documents, meetings, agenda items
- Establishes relationships between entities
- Enables graph-based queries via Gremlin

#### GraphRAG Pipeline  
- Adapts markdown to GraphRAG CSV format
- Runs Microsoft GraphRAG indexing
- Optional entity deduplication for quality improvement
- Creates entity/relationship extraction and community detection

**Output**: Either Cosmos DB graph or GraphRAG parquet files

### 3. Query and Response (`phase3_querying/`)

Intelligent query processing system:
- **Query Routing**: Determines optimal query method (global vs local)
- **Query Execution**: Interfaces with GraphRAG via subprocess
- **Response Enhancement**: Cleans and enriches responses
- **Source Tracking**: Provides provenance and citations

**Output**: Enhanced responses with source attribution

## Key Features

### Boolean Control System
Easy on/off switching for pipeline components:
```python
RUN_DATA_PREPROCESSING = True      # Process source documents
RUN_CUSTOM_GRAPH_PIPELINE = False # Skip Cosmos DB graph  
RUN_GRAPHRAG_INDEXING_PIPELINE = True # Build GraphRAG index
```

### Modular Architecture
Each stage is independently testable and can be run separately.

### Dual Graph Approach
- **Custom Graph**: Traditional knowledge graph in Cosmos DB
- **GraphRAG**: Microsoft's approach with LLM-enhanced indexing

### Rich Metadata Headers
Generated markdown includes comprehensive headers for improved GraphRAG performance:
```markdown
---
DOCUMENT METADATA AND CONTEXT
=============================

**DOCUMENT IDENTIFICATION:**
- Document Type: AGENDA
- Meeting Date: 01.09.2024

**SEARCHABLE IDENTIFIERS:**
- AGENDA_ITEM: E-1
- AGENDA_ITEM: E-2
---
```

### Intelligent Query Routing
Automatically determines the best query method based on query characteristics.

### Configurable Source Directory
The pipeline now accepts command-line arguments for flexible source directory specification.

## Migration from Old Structure

The unified pipeline replaces:
- `scripts/graph_stages/` â†’ `graph_rag_stages/phase1_preprocessing/` + `graph_rag_stages/phase2_building/custom_graph_builder.py`
- `scripts/microsoft_framework/` â†’ `graph_rag_stages/phase2_building/` (GraphRAG components) + `graph_rag_stages/phase3_querying/`

All existing functionality has been preserved and enhanced in the new structure.

## Dependencies

Core dependencies:
- `docling` - PDF processing
- `openai` - LLM operations  
- `graphrag` - Microsoft GraphRAG
- `pandas` - Data manipulation
- `azure-cosmos` - Cosmos DB (if using custom graph)
- `fitz` (PyMuPDF) - PDF hyperlink extraction

Install with:
```bash
pip install -r requirements.txt
```

## Corrections Applied

This version addresses the following corrections:
1. **Valid Python Identifiers**: Renamed directories from `1_data_preprocessing` to `phase1_preprocessing`, etc.
2. **Proper Import Structure**: Updated all imports to use valid package names
3. **Command Line Arguments**: Added argument parsing for configurable source directory
4. **Complete Implementation**: Filled in placeholder logic throughout the pipeline
5. **Robust Error Handling**: Enhanced error checking and validation
6. **Comprehensive Documentation**: Updated documentation to reflect all changes


================================================================================


################################################################################
# File: scripts/RAG_stages/acceleration_utils.py
################################################################################

# File: scripts/RAG_stages/acceleration_utils.py

"""
Hardware acceleration utilities for the pipeline.
Provides Apple Silicon optimization when available, with CPU fallback.
"""
import os
import platform
import multiprocessing as mp
from typing import Optional, Callable, Any, List
import logging
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import asyncio
from functools import partial

log = logging.getLogger(__name__)

class HardwareAccelerator:
    """Detects and manages hardware acceleration capabilities."""
    
    def __init__(self):
        self.is_apple_silicon = self._detect_apple_silicon()
        self.cpu_count = mp.cpu_count()
        self.optimal_workers = self._calculate_optimal_workers()
        
        # Set environment for better performance on macOS
        if self.is_apple_silicon:
            os.environ['OPENBLAS_NUM_THREADS'] = '1'
            os.environ['MKL_NUM_THREADS'] = '1'
            os.environ['OMP_NUM_THREADS'] = '1'
        
        log.info(f"Hardware: {'Apple Silicon' if self.is_apple_silicon else platform.processor()}")
        log.info(f"CPU cores: {self.cpu_count}, Optimal workers: {self.optimal_workers}")
    
    def _detect_apple_silicon(self) -> bool:
        """Detect if running on Apple Silicon."""
        if platform.system() != 'Darwin':
            return False
        try:
            import subprocess
            result = subprocess.run(['sysctl', '-n', 'hw.optional.arm64'], 
                                  capture_output=True, text=True)
            return result.stdout.strip() == '1'
        except:
            return False
    
    def _calculate_optimal_workers(self) -> int:
        """Calculate optimal number of workers based on hardware."""
        if self.is_apple_silicon:
            # Apple Silicon has efficiency and performance cores
            # Use 75% of cores to leave room for system
            return max(1, int(self.cpu_count * 0.75))
        else:
            # Traditional CPU - use all but one core
            return max(1, self.cpu_count - 1)
    
    def get_process_pool(self, max_workers: Optional[int] = None) -> ProcessPoolExecutor:
        """Get optimized process pool executor."""
        workers = max_workers or self.optimal_workers
        return ProcessPoolExecutor(
            max_workers=workers,
            mp_context=mp.get_context('spawn')  # Required for macOS
        )
    
    def get_thread_pool(self, max_workers: Optional[int] = None) -> ThreadPoolExecutor:
        """Get optimized thread pool executor for I/O operations."""
        workers = max_workers or min(32, self.cpu_count * 4)
        return ThreadPoolExecutor(max_workers=workers)

# Global instance
hardware = HardwareAccelerator()

async def run_cpu_bound_concurrent(func: Callable, items: List[Any], 
                                 max_workers: Optional[int] = None,
                                 desc: str = "Processing") -> List[Any]:
    """Run CPU-bound function concurrently on multiple items."""
    loop = asyncio.get_event_loop()
    with hardware.get_process_pool(max_workers) as executor:
        futures = [loop.run_in_executor(executor, func, item) for item in items]
        
        from tqdm.asyncio import tqdm_asyncio
        results = await tqdm_asyncio.gather(*futures, desc=desc)
        return results

async def run_io_bound_concurrent(func: Callable, items: List[Any],
                                max_workers: Optional[int] = None,
                                desc: str = "Processing") -> List[Any]:
    """Run I/O-bound function concurrently on multiple items."""
    loop = asyncio.get_event_loop()
    with hardware.get_thread_pool(max_workers) as executor:
        futures = [loop.run_in_executor(executor, func, item) for item in items]
        
        from tqdm.asyncio import tqdm_asyncio
        results = await tqdm_asyncio.gather(*futures, desc=desc)
        return results


================================================================================


################################################################################
# File: scripts/graph_rag_stages/phase3_querying/response_enhancer.py
################################################################################

# File: scripts/graph_rag_stages/phase3_querying/response_enhancer.py

"""
Response enhancer that improves GraphRAG responses with additional context and formatting.
"""

import logging
from typing import Dict, Any
import re

log = logging.getLogger(__name__)


class ResponseEnhancer:
    """Enhances GraphRAG responses with additional context and formatting."""
    
    def __init__(self):
        pass

    async def enhance_response(self, query: str, raw_response: Dict[str, Any]) -> Dict[str, Any]:
        """
        Enhance the raw GraphRAG response.
        
        Args:
            query: The original user query
            raw_response: Raw response from GraphRAG
            
        Returns:
            Enhanced response dictionary
        """
        log.debug("ğŸ”§ Enhancing GraphRAG response")
        
        # Start with the raw response
        enhanced = raw_response.copy()
        
        # Clean and format the answer
        if 'answer' in enhanced:
            enhanced['answer'] = self._clean_answer_text(enhanced['answer'])
        
        # Add query context
        enhanced['query'] = query
        enhanced['enhanced'] = True
        
        # Add helpful context
        enhanced['context'] = self._generate_context_hints(query)
        
        return enhanced

    def _clean_answer_text(self, answer: str) -> str:
        """Clean and format the answer text."""
        if not answer:
            return "I couldn't find a specific answer to your question."
        
        # Remove common artifacts
        cleaned = answer.strip()
        
        # Remove timestamp artifacts if present
        cleaned = re.sub(r'\[\d{4}-\d{2}-\d{2}.*?\]', '', cleaned)
        
        # Clean up extra whitespace
        cleaned = re.sub(r'\s+', ' ', cleaned)
        
        # Ensure proper sentence ending
        if cleaned and not cleaned.endswith(('.', '!', '?')):
            cleaned += '.'
        
        return cleaned

    def _generate_context_hints(self, query: str) -> Dict[str, Any]:
        """Generate helpful context hints based on the query."""
        hints = {
            'query_type': self._classify_query_type(query),
            'suggestions': []
        }
        
        # Add suggestions based on query type
        query_lower = query.lower()
        
        if 'agenda' in query_lower or 'item' in query_lower:
            hints['suggestions'].append("For specific agenda items, try including the item code (e.g., 'E-1', 'H-2')")
        
        if 'meeting' in query_lower:
            hints['suggestions'].append("For meeting-specific information, try including the date (MM.DD.YYYY)")
        
        if 'ordinance' in query_lower or 'resolution' in query_lower:
            hints['suggestions'].append("For legislation, try searching by document number or title")
        
        return hints

    def _classify_query_type(self, query: str) -> str:
        """Classify the type of query for context."""
        query_lower = query.lower()
        
        if any(word in query_lower for word in ['agenda', 'item']):
            return 'agenda_inquiry'
        elif any(word in query_lower for word in ['ordinance', 'resolution']):
            return 'legislation_inquiry'
        elif any(word in query_lower for word in ['meeting', 'discussion']):
            return 'meeting_inquiry'
        elif any(word in query_lower for word in ['overview', 'summary', 'trend']):
            return 'analysis_inquiry'
        else:
            return 'general_inquiry'


================================================================================


################################################################################
# File: scripts/graph_rag_stages/phase1_preprocessing/__init__.py
################################################################################

# File: scripts/graph_rag_stages/phase1_preprocessing/__init__.py

"""
Initializes the preprocessing module and provides the main orchestration
function to run the entire data extraction and enrichment process.
"""
import asyncio
import logging
from pathlib import Path
from typing import List, Coroutine

from .agenda_extractor import AgendaExtractor
from .document_linker import DocumentLinker
from .transcript_linker import TranscriptLinker

log = logging.getLogger(__name__)

async def run_extraction_pipeline(base_dir: Path, markdown_output_dir: Path):
    """
    High-level function to run the entire data extraction process.
    """
    log.info(f"Starting extraction pipeline for source directory: {base_dir}")
    markdown_output_dir.mkdir(parents=True, exist_ok=True)

    agenda_extractor = AgendaExtractor(markdown_output_dir)
    doc_linker = DocumentLinker(markdown_output_dir)
    transcript_linker = TranscriptLinker(markdown_output_dir)

    agenda_pdfs = list((base_dir / "Agendas").glob("*.pdf"))
    ordinance_pdfs = list((base_dir / "Ordinances").rglob("*.pdf"))
    resolution_pdfs = list((base_dir / "Resolutions").rglob("*.pdf"))
    
    verbatim_dir_path = base_dir / "Verbatim Items"
    if not verbatim_dir_path.exists():
        verbatim_dir_path = base_dir / "Verbating Items"
    transcript_pdfs = list(verbatim_dir_path.rglob("*.pdf")) if verbatim_dir_path.exists() else []

    log.info(f"Discovered {len(agenda_pdfs)} agendas, {len(ordinance_pdfs)} ordinances, "
             f"{len(resolution_pdfs)} resolutions, and {len(transcript_pdfs)} transcripts.")

    tasks: List[Coroutine] = []
    for pdf in agenda_pdfs: tasks.append(agenda_extractor.extract_and_save_agenda(pdf))
    for pdf in ordinance_pdfs: tasks.append(doc_linker.link_and_save_document(pdf, "ordinance"))
    for pdf in resolution_pdfs: tasks.append(doc_linker.link_and_save_document(pdf, "resolution"))
    for pdf in transcript_pdfs: tasks.append(transcript_linker.link_and_save_transcript(pdf))

    log.info(f"Executing {len(tasks)} extraction tasks in parallel...")
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    success_count = sum(1 for r in results if r is not None and not isinstance(r, Exception))
    error_count = len(results) - success_count
    
    log.info(f"Extraction pipeline finished. Successful: {success_count}, Failed: {error_count}.")
    if error_count > 0:
        log.warning("Some files failed to process. Check logs for details.")

__all__ = ["run_extraction_pipeline"]


================================================================================


################################################################################
# File: requirements.txt
################################################################################

################################################################################
################################################################################
# Python dependencies for Misophonia Research RAG System

# Core dependencies
openai>=1.82.0
groq>=0.4.1
gremlinpython>=3.7.3
aiofiles>=24.1.0
python-dotenv>=1.0.0

# Graph Visualization dependencies
dash>=2.14.0
plotly>=5.17.0
networkx>=3.2.0
dash-table>=5.0.0
dash-cytoscape>=0.3.0
dash-bootstrap-components>=1.0.0

# Database and API dependencies
supabase>=2.0.0

# GraphRAG dependencies
graphrag==2.3.0
pyyaml>=6.0.0
pyarrow>=14.0.0
scipy>=1.11.0

# Optional for async progress bars
tqdm

# Data processing
pandas>=2.0.0
numpy

# PDF processing
PyPDF2>=3.0.0
unstructured[pdf]==0.10.30
pytesseract>=0.3.10
pdf2image>=1.16.3
docling
pdfminer.six>=20221105

# PDF processing with hyperlink support
PyMuPDF>=1.23.0

# Utilities
colorama>=0.4.6

# For concurrent processing
concurrent-log-handler>=0.9.20

# Async dependencies for optimized pipeline
aiohttp>=3.8.0

# Token counting for OpenAI rate limiting
tiktoken>=0.5.0

# Enhanced deduplication dependencies
python-Levenshtein>=0.20.0
scikit-learn>=1.3.0


================================================================================


################################################################################
# File: scripts/RAG_stages/__init__.py
################################################################################

# File: scripts/RAG_stages/__init__.py

"""Stage helpers live here so `pipeline_integrated` can still be the single-file
reference implementation while every stage can be invoked on its own."""

"""
Namespace package so the stage modules can be imported with
    from RAG_stages import <module>
"""
__all__ = [
    "common",
    "extract_clean",
    "llm_enrich",
    "chunk_text",
    "db_upsert",
    "embed_vectors",
]


================================================================================

