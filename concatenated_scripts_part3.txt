# Concatenated Project Code - Part 3 of 3
# Generated: 2025-06-12 09:30:51
# Root Directory: /Users/gianmariatroiani/Documents/knologi/graph_database
================================================================================

# File Index - Which Files Are in Which Parts
################################################################################

## Part 1 (15 files):
  - scripts/microsoft_framework/query_engine.py
  - scripts/graph_stages/agenda_pdf_extractor.py
  - scripts/graph_stages/agenda_ontology_extractor.py
  - scripts/microsoft_framework/document_adapter.py
  - scripts/graph_stages/document_linker.py
  - scripts/microsoft_framework/prompt_tuner.py
  - scripts/graph_stages/pdf_extractor.py
  - scripts/microsoft_framework/create_custom_prompts.py
  - check_status.py
  - investigate_graph.py
  - scripts/microsoft_framework/city_clerk_settings_template.yaml
  - scripts/microsoft_framework/incremental_processor.py
  - verify_deduplication.py
  - scripts/microsoft_framework/__init__.py
  - run_enhanced_dedup.py

## Part 2 (15 files):
  - scripts/graph_stages/agenda_graph_builder.py
  - concatenate_scripts_broken.py
  - scripts/graph_stages/enhanced_document_linker.py
  - scripts/graph_stages/verbatim_transcript_linker.py
  - scripts/microsoft_framework/query_router.py
  - scripts/extract_all_to_markdown.py
  - scripts/microsoft_framework/graphrag_output_processor.py
  - scripts/microsoft_framework/graphrag_initializer.py
  - scripts/microsoft_framework/cosmos_synchronizer.py
  - settings.yaml
  - scripts/microsoft_framework/graphrag_pipeline.py
  - check_enhanced_results.py
  - check_ordinances.py
  - requirements.txt
  - scripts/microsoft_framework/query_graphrag.py

## Part 3 (15 files):
  - scripts/microsoft_framework/enhanced_entity_deduplicator.py
  - scripts/graph_stages/ontology_extractor.py
  - graphrag_query_ui.py
  - scripts/microsoft_framework/run_graphrag_pipeline.py
  - scripts/microsoft_framework/entity_deduplicator.py
  - scripts/graph_stages/cosmos_db_client.py
  - scripts/microsoft_framework/README.md
  - scripts/extract_all_pdfs_direct.py
  - extract_documents_for_graphrag.py
  - explore_graphrag_sources.py
  - scripts/microsoft_framework/source_tracker.py
  - config.py
  - scripts/json_to_markdown_converter.py
  - scripts/microsoft_framework/run_graphrag_direct.py
  - scripts/graph_stages/__init__.py


================================================================================


################################################################################
# File: scripts/microsoft_framework/enhanced_entity_deduplicator.py
################################################################################

# File: scripts/microsoft_framework/enhanced_entity_deduplicator.py

#!/usr/bin/env python3
"""
Enhanced Entity Deduplication for GraphRAG output.

This module provides sophisticated entity deduplication capabilities including:
- Partial name matching ("Vince Lago" matches "Lago")
- Token-based overlap
- Semantic similarity using TF-IDF
- Graph structure analysis
- Abbreviation matching ("V. Lago" matches "Vince Lago")
- Role-based matching ("Mayor" matches "Mayor Lago")
- Multiple scoring strategies with configurable weights
"""

import pandas as pd
import networkx as nx
from pathlib import Path
import difflib
from typing import Dict, List, Tuple, Set, Any, Optional, Union
import logging
import json
import re
from datetime import datetime
from collections import defaultdict
import math
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
import multiprocessing as mp

# Optional dependencies with fallbacks
try:
    import Levenshtein
    HAS_LEVENSHTEIN = True
except ImportError:
    HAS_LEVENSHTEIN = False
    
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    import numpy as np
    HAS_SKLEARN = True
except ImportError:
    HAS_SKLEARN = False

logger = logging.getLogger(__name__)

def _compare_entity_pairs_worker(args):
    """Worker function for parallel entity pair comparison."""
    entity_pairs, config, weights = args
    
    candidates = []
    
    for entity1, entity2 in entity_pairs:
        # Calculate similarity scores
        scores = _calculate_similarity_scores_standalone(entity1, entity2, config)
        
        # Calculate combined score
        combined_score = _calculate_combined_score_standalone(scores, weights)
        
        # Check threshold
        if combined_score >= config.get('min_combined_score', 0.7):
            # Validate candidate
            scores['combined_score'] = combined_score
            if _validate_merge_candidate_standalone(entity1, entity2, scores):
                # Determine merge reason
                merge_reason = _determine_merge_reason_standalone(scores)
                
                candidates.append({
                    'entity1_title': entity1['title'],
                    'entity2_title': entity2['title'],
                    'entity1_id': entity1.get('id', entity1.get('human_readable_id', '')),
                    'entity2_id': entity2.get('id', entity2.get('human_readable_id', '')),
                    'combined_score': combined_score,
                    'individual_scores': scores,
                    'merge_reason': merge_reason,
                    'primary_entity': _determine_primary_entity_standalone(entity1, entity2)
                })
    
    return candidates

def _calculate_similarity_scores_standalone(entity1: Dict, entity2: Dict, config: Dict) -> Dict[str, float]:
    """Standalone version of similarity calculation for parallel processing."""
    scores = {}
    
    title1 = entity1['title'].lower().strip()
    title2 = entity2['title'].lower().strip()
    
    # String similarity
    scores['string_similarity'] = _string_similarity_standalone(title1, title2)
    
    # Token overlap
    if config.get('enable_token_matching', True):
        scores['token_overlap'] = _token_overlap_similarity_standalone(title1, title2)
    else:
        scores['token_overlap'] = 0.0
    
    # Partial name matching
    if config.get('enable_partial_name_matching', True):
        scores['partial_name_match'] = _partial_name_similarity_standalone(title1, title2)
    else:
        scores['partial_name_match'] = 0.0
    
    # Abbreviation matching
    if config.get('enable_abbreviation_matching', True):
        scores['abbreviation_match'] = _abbreviation_similarity_standalone(title1, title2)
    else:
        scores['abbreviation_match'] = 0.0
    
    # Role-based matching
    if config.get('enable_role_based_matching', True):
        scores['role_match'] = _role_based_similarity_standalone(title1, title2)
    else:
        scores['role_match'] = 0.0
    
    # Graph structure similarity
    if config.get('enable_graph_structure_matching', True):
        scores['graph_structure'] = _graph_structure_similarity_standalone(entity1, entity2, config)
    else:
        scores['graph_structure'] = 0.0
    
    # Semantic similarity (simplified for parallel processing)
    scores['semantic_similarity'] = 0.0  # Skip for parallel version to avoid pickling issues
    
    return scores

def _string_similarity_standalone(str1: str, str2: str) -> float:
    """Standalone string similarity calculation."""
    if HAS_LEVENSHTEIN:
        lev_sim = 1 - (Levenshtein.distance(str1, str2) / max(len(str1), len(str2), 1))
    else:
        lev_sim = 0.0
    seq_sim = difflib.SequenceMatcher(None, str1, str2).ratio()
    return max(lev_sim, seq_sim)

def _token_overlap_similarity_standalone(str1: str, str2: str) -> float:
    """Standalone token overlap similarity calculation."""
    tokens1 = set(re.findall(r'\b\w+\b', str1.lower()))
    tokens2 = set(re.findall(r'\b\w+\b', str2.lower()))
    
    # Remove stop words
    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
    tokens1 = tokens1 - stop_words
    tokens2 = tokens2 - stop_words
    
    if not tokens1 or not tokens2:
        return 0.0
    
    intersection = tokens1.intersection(tokens2)
    union = tokens1.union(tokens2)
    
    if not union:
        return 0.0
    
    jaccard = len(intersection) / len(union)
    
    # Boost for subset matches
    if tokens1.issubset(tokens2) or tokens2.issubset(tokens1):
        min_tokens = min(len(tokens1), len(tokens2))
        if min_tokens >= 2 or (min_tokens == 1 and jaccard >= 0.5):
            return min(1.0, jaccard + 0.2)
    
    return jaccard

def _partial_name_similarity_standalone(str1: str, str2: str) -> float:
    """Standalone partial name similarity calculation."""
    tokens1 = re.findall(r'\b\w+\b', str1.lower())
    tokens2 = re.findall(r'\b\w+\b', str2.lower())
    
    if len(tokens1) <= len(tokens2):
        shorter, longer = tokens1, tokens2
    else:
        shorter, longer = tokens2, tokens1
    
    if not shorter:
        return 0.0
    
    matches = sum(1 for token in shorter if token in longer)
    return matches / len(shorter)

def _abbreviation_similarity_standalone(str1: str, str2: str) -> float:
    """Standalone abbreviation similarity calculation."""
    def get_initials(text):
        words = re.findall(r'\b\w+\b', text)
        return ''.join(word[0].lower() for word in words if word)
    
    def has_abbreviation_pattern(short, long):
        short_clean = re.sub(r'[^\w\s]', '', short).strip()
        long_clean = re.sub(r'[^\w\s]', '', long).strip()
        
        initials = get_initials(long_clean)
        if short_clean.replace('.', '').replace(' ', '').lower() == initials:
            return 1.0
        
        short_parts = short_clean.split()
        long_parts = long_clean.split()
        
        if len(short_parts) == len(long_parts):
            matches = 0
            for s_part, l_part in zip(short_parts, long_parts):
                s_clean = s_part.replace('.', '').lower()
                l_clean = l_part.lower()
                
                if s_clean == l_clean or (len(s_clean) == 1 and l_clean.startswith(s_clean)):
                    matches += 1
            
            return matches / len(short_parts)
        
        return 0.0
    
    score1 = has_abbreviation_pattern(str1, str2)
    score2 = has_abbreviation_pattern(str2, str1)
    return max(score1, score2)

def _role_based_similarity_standalone(str1: str, str2: str) -> float:
    """Standalone role-based similarity calculation."""
    roles = ['mayor', 'commissioner', 'councilman', 'councilwoman', 'director', 'manager', 'chief']
    
    def extract_role_and_name(text):
        text_lower = text.lower()
        for role in roles:
            if role in text_lower:
                name_part = text_lower.replace(role, '').strip()
                return role, name_part
        return None, text_lower
    
    role1, name1 = extract_role_and_name(str1)
    role2, name2 = extract_role_and_name(str2)
    
    if role1 and not role2:
        return _string_similarity_standalone(name1, str2)
    elif role2 and not role1:
        return _string_similarity_standalone(name2, str1)
    elif role1 and role2:
        if role1 == role2:
            return _string_similarity_standalone(name1, name2)
        else:
            return 0.0
    
    return 0.0

def _graph_structure_similarity_standalone(entity1: Dict, entity2: Dict, config: Dict) -> float:
    """Standalone graph structure similarity calculation."""
    neighbors1 = entity1.get('neighbors', set())
    neighbors2 = entity2.get('neighbors', set())
    
    if not neighbors1 and not neighbors2:
        return 1.0
    elif not neighbors1 or not neighbors2:
        return 0.0
    
    intersection = len(neighbors1.intersection(neighbors2))
    union = len(neighbors1.union(neighbors2))
    
    jaccard_sim = intersection / union if union > 0 else 0.0
    
    coeff1 = entity1.get('clustering_coeff', 0.0)
    coeff2 = entity2.get('clustering_coeff', 0.0)
    coeff_diff = abs(coeff1 - coeff2)
    coeff_sim = 1.0 - min(coeff_diff / config.get('clustering_tolerance', 0.15), 1.0)
    
    return (jaccard_sim + coeff_sim) / 2.0

def _calculate_combined_score_standalone(scores: Dict[str, float], weights: Dict[str, float]) -> float:
    """Standalone combined score calculation."""
    if scores.get('token_overlap', 0) > 0.7 and scores.get('string_similarity', 0) < 0.8:
        if scores.get('graph_structure', 0) < 0.6:
            return 0.0
    
    combined = 0.0
    total_weight = 0.0
    
    for score_type, weight in weights.items():
        if score_type in scores:
            combined += scores[score_type] * weight
            total_weight += weight
    
    bonus_scores = ['partial_name_match', 'abbreviation_match', 'role_match']
    max_bonus = 0.0
    for bonus_type in bonus_scores:
        if bonus_type in scores:
            max_bonus = max(max_bonus, scores[bonus_type])
    
    bonus_factor = min(max_bonus * 0.2, 0.2)
    
    if total_weight > 0:
        final_score = (combined / total_weight) + bonus_factor
        return min(final_score, 1.0)
    
    return 0.0

def _validate_merge_candidate_standalone(entity1: Dict, entity2: Dict, scores: Dict[str, float]) -> bool:
    """Standalone validation for merge candidates."""
    if scores.get('token_overlap', 0) > scores.get('string_similarity', 0):
        has_strong_evidence = (
            scores.get('graph_structure', 0) > 0.7 or
            scores.get('semantic_similarity', 0) > 0.8 or
            scores.get('abbreviation_match', 0) > 0.8 or
            scores.get('role_match', 0) > 0.7
        )
        
        if not has_strong_evidence:
            return False
    
    if entity1.get('description') and entity2.get('description'):
        desc1_len = len(str(entity1['description']))
        desc2_len = len(str(entity2['description']))
        
        if max(desc1_len, desc2_len) > 3 * min(desc1_len, desc2_len):
            return scores.get('combined_score', 0) > 0.9
    
    return True

def _determine_merge_reason_standalone(scores: Dict[str, float]) -> str:
    """Standalone merge reason determination."""
    main_scores = {
        'string_similarity': 'High string similarity',
        'token_overlap': 'Token overlap',
        'graph_structure': 'Similar graph structure',
        'semantic_similarity': 'Semantic similarity'
    }
    
    special_scores = {
        'partial_name_match': 'Partial name match',
        'abbreviation_match': 'Abbreviation pattern',
        'role_match': 'Role-based match'
    }
    
    for score_type, reason in special_scores.items():
        if scores.get(score_type, 0.0) > 0.7:
            return reason
    
    max_score = 0.0
    max_reason = "Combined similarity"
    
    for score_type, reason in main_scores.items():
        if scores.get(score_type, 0.0) > max_score:
            max_score = scores[score_type]
            max_reason = reason
    
    return max_reason

def _determine_primary_entity_standalone(entity1: Dict, entity2: Dict) -> str:
    """Standalone primary entity determination."""
    degree1 = entity1.get('degree_centrality', 0)
    degree2 = entity2.get('degree_centrality', 0)
    
    if degree1 > degree2:
        return entity1['title']
    elif degree2 > degree1:
        return entity2['title']
    
    if len(entity1['title']) > len(entity2['title']):
        return entity1['title']
    else:
        return entity2['title']

# Configuration presets
DEDUP_CONFIGS = {
    'aggressive': {
        'min_combined_score': 0.75,  # INCREASED from 0.65
        'enable_partial_name_matching': True,
        'enable_token_matching': True,
        'enable_semantic_matching': True,
        'enable_graph_structure_matching': True,
        'enable_abbreviation_matching': True,
        'enable_role_based_matching': True,
        'exact_match_threshold': 0.85,
        'high_similarity_threshold': 0.8,
        'partial_match_threshold': 0.6,
        'clustering_tolerance': 0.2,
        'weights': {
            'string_similarity': 0.2,
            'token_overlap': 0.2,
            'graph_structure': 0.4,  # Emphasize network evidence
            'semantic_similarity': 0.2
        }
    },
    'conservative': {
        'min_combined_score': 0.85,  # Already strict
        'enable_partial_name_matching': True,
        'enable_token_matching': True,
        'enable_semantic_matching': True,
        'enable_graph_structure_matching': True,
        'enable_abbreviation_matching': True,
        'enable_role_based_matching': False,
        'exact_match_threshold': 0.95,
        'high_similarity_threshold': 0.9,
        'partial_match_threshold': 0.8,
        'clustering_tolerance': 0.1,
        'weights': {
            'string_similarity': 0.4,
            'token_overlap': 0.1,    # Reduced to avoid false positives
            'graph_structure': 0.4,
            'semantic_similarity': 0.1
        }
    },
    'name_focused': {
        'min_combined_score': 0.8,   # INCREASED from 0.7
        'enable_partial_name_matching': True,
        'enable_token_matching': True,
        'enable_semantic_matching': True,
        'enable_graph_structure_matching': True,
        'enable_abbreviation_matching': True,
        'enable_role_based_matching': True,
        'exact_match_threshold': 0.95,
        'high_similarity_threshold': 0.85,
        'partial_match_threshold': 0.7,
        'clustering_tolerance': 0.15,
        'weights': {
            'string_similarity': 0.2,
            'token_overlap': 0.3,     # REDUCED from 0.4
            'graph_structure': 0.3,   # INCREASED from 0.2
            'semantic_similarity': 0.2
        }
    }
}


class EnhancedEntityDeduplicator:
    """
    Enhanced entity deduplication using multiple similarity metrics and advanced matching strategies.
    """
    
    def __init__(self, output_dir: Path, config: Dict[str, Any] = None):
        """
        Initialize the enhanced entity deduplicator.
        
        Args:
            output_dir: Path to GraphRAG output directory
            config: Configuration dictionary with matching strategies and thresholds
        """
        self.output_dir = Path(output_dir)
        self.config = config or DEDUP_CONFIGS['name_focused']
        
        # Initialize data containers
        self.entities_df = None
        self.relationships_df = None
        self.graph = None
        self.tfidf_vectorizer = None
        self.tfidf_matrix = None
        
        # Merge tracking
        self.merge_report = {
            'timestamp': datetime.now().isoformat(),
            'config': self.config.copy(),
            'merges': [],
            'statistics': {}
        }
        
        logger.info(f"Initialized enhanced deduplicator for {output_dir}")
        logger.info(f"Configuration: {self.config.get('min_combined_score', 0.7)} min score")
    
    def deduplicate_entities(self) -> Dict[str, Any]:
        """
        Main enhanced deduplication process.
        
        Returns:
            Dictionary with deduplication statistics
        """
        print("🚀 Starting Enhanced Entity Deduplication Process")
        print("=" * 60)
        logger.info("Starting enhanced entity deduplication process")
        
        # Load data
        print("\n📂 Step 1/6: Loading data...")
        self._load_data()
        original_entity_count = len(self.entities_df)
        print(f"   ✓ Loaded {original_entity_count:,} entities and {len(self.relationships_df):,} relationships")
        
        # Build graph and calculate features
        print("\n🏗️ Step 2/6: Building graph structure...")
        self.graph = self._build_graph(self.entities_df, self.relationships_df)
        
        print("\n📊 Step 3/6: Analyzing graph features...")
        self.entities_df = self._calculate_graph_features(self.entities_df)
        
        # Initialize semantic similarity if enabled
        if self.config.get('enable_semantic_matching', True) and HAS_SKLEARN:
            print("\n🧠 Step 4/6: Setting up semantic analysis...")
            self._initialize_semantic_similarity()
        else:
            print("\n⏭️ Step 4/6: Skipping semantic analysis (disabled or scikit-learn unavailable)")
        
        # Find merge candidates using enhanced strategies
        print("\n🔍 Step 5/6: Finding merge candidates...")
        merge_candidates = self._find_merge_candidates(self.entities_df, self.relationships_df)
        
        if merge_candidates:
            # Execute merges
            print("\n🔧 Step 6/6: Executing merges...")
            merged_entities_df = self._execute_merges(self.entities_df, merge_candidates)
        else:
            print("\n✨ Step 6/6: No merges needed - all entities are sufficiently distinct")
            merged_entities_df = self.entities_df
        
        # Save results
        print("\n💾 Saving results...")
        self._save_deduplicated_data(merged_entities_df)
        self._save_enhanced_report(merge_candidates)
        
        # Calculate statistics
        final_entity_count = len(merged_entities_df)
        merged_count = original_entity_count - final_entity_count
        
        stats = {
            'original_entities': original_entity_count,
            'merged_entities': final_entity_count,
            'merged_count': merged_count,
            'merge_candidates': len(merge_candidates),
            'output_dir': str(self.output_dir / "deduplicated"),
            'config_used': self.config
        }
        
        print("\n✅ Enhanced Deduplication Complete!")
        print("=" * 60)
        print(f"📊 Final Results:")
        print(f"   • Original entities: {original_entity_count:,}")
        print(f"   • Final entities: {final_entity_count:,}")
        print(f"   • Entities merged: {merged_count:,}")
        print(f"   • Reduction: {(merged_count/original_entity_count)*100:.1f}%")
        print(f"   • Output saved to: output/deduplicated/")
        
        logger.info(f"Enhanced deduplication complete: {original_entity_count} -> {final_entity_count} entities")
        return stats
    
    def _load_data(self):
        """Load GraphRAG entities and relationships data."""
        entities_path = self.output_dir / "entities.parquet"
        relationships_path = self.output_dir / "relationships.parquet"
        
        if not entities_path.exists():
            raise FileNotFoundError(f"Entities file not found: {entities_path}")
        if not relationships_path.exists():
            raise FileNotFoundError(f"Relationships file not found: {relationships_path}")
        
        self.entities_df = pd.read_parquet(entities_path)
        self.relationships_df = pd.read_parquet(relationships_path)
        
        logger.info(f"Loaded {len(self.entities_df)} entities and {len(self.relationships_df)} relationships")
    
    def _build_graph(self, entities_df: pd.DataFrame, relationships_df: pd.DataFrame) -> nx.Graph:
        """Build NetworkX graph from relationships for analysis."""
        graph = nx.Graph()
        
        # Add entities as nodes
        for _, entity in entities_df.iterrows():
            graph.add_node(entity['title'], **entity.to_dict())
        
        # Add relationships as edges
        for _, rel in relationships_df.iterrows():
            if 'source' in rel and 'target' in rel:
                graph.add_edge(rel['source'], rel['target'], **rel.to_dict())
        
        logger.info(f"Built graph with {graph.number_of_nodes()} nodes and {graph.number_of_edges()} edges")
        return graph
    
    def _calculate_graph_features(self, entities_df: pd.DataFrame) -> pd.DataFrame:
        """Calculate graph-based features for entities."""
        df = entities_df.copy()
        
        print("🔍 Calculating graph features...")
        
        # Calculate clustering coefficients and degree centrality
        clustering_coeffs = {}
        degree_centrality = {}
        
        entity_names = df['title'].tolist()
        for entity_name in tqdm(entity_names, desc="Computing clustering & centrality", unit="entities"):
            if entity_name in self.graph:
                clustering_coeffs[entity_name] = nx.clustering(self.graph, entity_name)
                degree_centrality[entity_name] = self.graph.degree(entity_name)
            else:
                clustering_coeffs[entity_name] = 0.0
                degree_centrality[entity_name] = 0
        
        df['clustering_coeff'] = df['title'].map(clustering_coeffs)
        df['degree_centrality'] = df['title'].map(degree_centrality)
        
        # Calculate neighbor sets for structure comparison
        neighbor_sets = {}
        for entity_name in tqdm(entity_names, desc="Building neighbor sets", unit="entities"):
            if entity_name in self.graph:
                neighbor_sets[entity_name] = set(self.graph.neighbors(entity_name))
            else:
                neighbor_sets[entity_name] = set()
        
        df['neighbors'] = df['title'].map(neighbor_sets)
        
        logger.info("Calculated graph features for all entities")
        return df
    
    def _initialize_semantic_similarity(self):
        """Initialize TF-IDF vectorizer for semantic similarity."""
        if not HAS_SKLEARN:
            logger.warning("scikit-learn not available, skipping semantic similarity")
            return
        
        print("🧠 Initializing semantic similarity analysis...")
        
        # Prepare text corpus from entity descriptions
        descriptions = []
        for _, entity in tqdm(self.entities_df.iterrows(), desc="Preparing text data", 
                             total=len(self.entities_df), unit="entities"):
            desc = entity.get('description', '') or ''
            title = entity.get('title', '') or ''
            combined_text = f"{title} {desc}".strip()
            descriptions.append(combined_text if combined_text else title)
        
        # Initialize TF-IDF vectorizer
        print("📊 Fitting TF-IDF vectorizer...")
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=1000,
            stop_words='english',
            lowercase=True,
            ngram_range=(1, 2)
        )
        
        try:
            self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(descriptions)
            logger.info(f"Initialized semantic similarity with {self.tfidf_matrix.shape[1]} features")
        except Exception as e:
            logger.warning(f"Failed to initialize semantic similarity: {e}")
            self.tfidf_matrix = None
    
    def _find_merge_candidates(self, entities_df: pd.DataFrame, relationships_df: pd.DataFrame) -> List[Dict]:
        """Find entity merge candidates using enhanced matching strategies with optimized processing."""
        entity_list = entities_df.to_dict('records')
        total_comparisons = len(entity_list) * (len(entity_list) - 1) // 2
        
        print(f"\n🔍 Analyzing {total_comparisons:,} entity pairs for potential merges...")
        
        # Use optimized sequential processing with better chunking
        return self._find_merge_candidates_optimized(entity_list, total_comparisons)
    
    def _process_entity_chunk(self, entity_pairs: List[Tuple[Dict, Dict]]) -> List[Dict]:
        """Process a chunk of entity pairs using instance methods."""
        candidates = []
        
        for entity1, entity2 in entity_pairs:
            # Calculate similarity scores
            scores = self._calculate_similarity_scores(entity1, entity2)
            
            # Calculate combined score
            combined_score = self._calculate_combined_score(scores)
            
            # Check threshold
            if combined_score >= self.config.get('min_combined_score', 0.7):
                # Validate candidate
                scores['combined_score'] = combined_score
                if self._validate_merge_candidate(entity1, entity2, scores):
                    # Determine merge reason
                    merge_reason = self._determine_merge_reason(scores)
                    
                    candidates.append({
                        'entity1_title': entity1['title'],
                        'entity2_title': entity2['title'],
                        'entity1_id': entity1.get('id', entity1.get('human_readable_id', '')),
                        'entity2_id': entity2.get('id', entity2.get('human_readable_id', '')),
                        'combined_score': combined_score,
                        'individual_scores': scores,
                        'merge_reason': merge_reason,
                        'primary_entity': self._determine_primary_entity(entity1, entity2)
                    })
        
        return candidates
    
    def _find_merge_candidates_optimized(self, entity_list: List[Dict], total_comparisons: int) -> List[Dict]:
        """Optimized version with smart filtering and progress tracking."""
        candidates = []
        
        print("   Using optimized processing with smart filtering")
        
        # First pass: Quick filtering based on simple criteria
        print("   📋 Pre-filtering entities for faster processing...")
        
        # Group entities by first letter and length for quick filtering
        entity_groups = {}
        for i, entity in enumerate(entity_list):
            title = entity['title'].lower().strip()
            # Group by first letter and rough length category
            key = (title[0] if title else '', len(title) // 5)  # Length buckets of 5
            if key not in entity_groups:
                entity_groups[key] = []
            entity_groups[key].append((i, entity))
        
        # Only compare entities within similar groups or with high connectivity
        comparison_pairs = []
        for group_entities in entity_groups.values():
            # Within-group comparisons (more likely to match)
            for i, (idx1, entity1) in enumerate(group_entities):
                for j, (idx2, entity2) in enumerate(group_entities[i+1:], i+1):
                    comparison_pairs.append((entity1, entity2))
        
        # Add cross-group comparisons for highly connected entities
        high_connectivity_threshold = 5  # entities with 5+ connections
        high_connectivity_entities = [
            entity for entity in entity_list 
            if entity.get('degree_centrality', 0) >= high_connectivity_threshold
        ]
        
        # Cross-group comparisons for high-connectivity entities
        for i, entity1 in enumerate(high_connectivity_entities):
            for entity2 in high_connectivity_entities[i+1:]:
                # Avoid duplicates
                pair_exists = any(
                    (p[0]['title'] == entity1['title'] and p[1]['title'] == entity2['title']) or
                    (p[0]['title'] == entity2['title'] and p[1]['title'] == entity1['title'])
                    for p in comparison_pairs
                )
                if not pair_exists:
                    comparison_pairs.append((entity1, entity2))
        
        actual_comparisons = len(comparison_pairs)
        reduction_percent = (1 - actual_comparisons/total_comparisons) * 100
        
        print(f"   ✂️ Reduced from {total_comparisons:,} to {actual_comparisons:,} comparisons ({reduction_percent:.1f}% reduction)")
        
        # Process the filtered comparisons with progress
        processed = 0
        update_frequency = max(1, actual_comparisons // 100)  # Update every 1%
        
        with tqdm(total=actual_comparisons, desc="Comparing entity pairs", unit="pairs") as pbar:
            for entity1, entity2 in comparison_pairs:
                # Calculate similarity scores
                scores = self._calculate_similarity_scores(entity1, entity2)
                
                # Calculate combined score
                combined_score = self._calculate_combined_score(scores)
                
                # Check if it meets the threshold
                if combined_score >= self.config.get('min_combined_score', 0.7):
                    # Additional validation
                    scores['combined_score'] = combined_score
                    if self._validate_merge_candidate(entity1, entity2, scores):
                        # Determine merge reason
                        merge_reason = self._determine_merge_reason(scores)
                        
                        candidates.append({
                            'entity1_title': entity1['title'],
                            'entity2_title': entity2['title'],
                            'entity1_id': entity1.get('id', entity1.get('human_readable_id', '')),
                            'entity2_id': entity2.get('id', entity2.get('human_readable_id', '')),
                            'combined_score': combined_score,
                            'individual_scores': scores,
                            'merge_reason': merge_reason,
                            'primary_entity': self._determine_primary_entity(entity1, entity2)
                        })
                
                processed += 1
                if processed % update_frequency == 0:
                    pbar.update(update_frequency)
                    pbar.set_description(f"Comparing pairs (found {len(candidates)} candidates)")
            
            # Update any remaining
            pbar.update(actual_comparisons - pbar.n)
        
        # Sort by combined score (highest first)
        candidates.sort(key=lambda x: x['combined_score'], reverse=True)
        
        logger.info(f"Found {len(candidates)} merge candidates using optimized processing")
        return candidates
    
    def _find_merge_candidates_sequential(self, entity_list: List[Dict]) -> List[Dict]:
        """Sequential version for smaller datasets or fallback."""
        candidates = []
        processed_pairs = set()
        total_comparisons = len(entity_list) * (len(entity_list) - 1) // 2
        
        comparisons_made = 0
        with tqdm(total=total_comparisons, desc="Comparing entity pairs", unit="pairs") as pbar:
            for i, entity1 in enumerate(entity_list):
                for j, entity2 in enumerate(entity_list[i+1:], i+1):
                    pair_key = tuple(sorted([entity1['title'], entity2['title']]))
                    if pair_key in processed_pairs:
                        continue
                    processed_pairs.add(pair_key)
                    
                    # Calculate multiple similarity scores
                    scores = self._calculate_similarity_scores(entity1, entity2)
                    
                    # Combine scores using weighted average
                    combined_score = self._calculate_combined_score(scores)
                    
                    # Check if it meets the threshold
                    if combined_score >= self.config.get('min_combined_score', 0.7):
                        # Additional validation
                        scores['combined_score'] = combined_score
                        if self._validate_merge_candidate(entity1, entity2, scores):
                            # Determine merge reason
                            merge_reason = self._determine_merge_reason(scores)
                            
                            candidates.append({
                                'entity1_title': entity1['title'],
                                'entity2_title': entity2['title'],
                                'entity1_id': entity1.get('id', i),
                                'entity2_id': entity2.get('id', j),
                                'combined_score': combined_score,
                                'individual_scores': scores,
                                'merge_reason': merge_reason,
                                'primary_entity': self._determine_primary_entity(entity1, entity2)
                            })
                    
                    comparisons_made += 1
                    pbar.update(1)
                    
                    # Update description with current progress stats
                    if comparisons_made % 10000 == 0:
                        pbar.set_description(f"Comparing pairs (found {len(candidates)} candidates)")
        
        # Sort by combined score (highest first)
        candidates.sort(key=lambda x: x['combined_score'], reverse=True)
        
        logger.info(f"Found {len(candidates)} merge candidates using sequential processing")
        return candidates
    
    def _calculate_similarity_scores(self, entity1: Dict, entity2: Dict) -> Dict[str, float]:
        """Calculate multiple similarity scores between two entities."""
        scores = {}
        
        title1 = entity1['title'].lower().strip()
        title2 = entity2['title'].lower().strip()
        
        # 1. String similarity
        scores['string_similarity'] = self._string_similarity(title1, title2)
        
        # 2. Token overlap
        if self.config.get('enable_token_matching', True):
            scores['token_overlap'] = self._token_overlap_similarity(title1, title2)
        else:
            scores['token_overlap'] = 0.0
        
        # 3. Partial name matching
        if self.config.get('enable_partial_name_matching', True):
            scores['partial_name_match'] = self._partial_name_similarity(title1, title2)
        else:
            scores['partial_name_match'] = 0.0
        
        # 4. Abbreviation matching
        if self.config.get('enable_abbreviation_matching', True):
            scores['abbreviation_match'] = self._abbreviation_similarity(title1, title2)
        else:
            scores['abbreviation_match'] = 0.0
        
        # 5. Role-based matching
        if self.config.get('enable_role_based_matching', True):
            scores['role_match'] = self._role_based_similarity(title1, title2)
        else:
            scores['role_match'] = 0.0
        
        # 6. Graph structure similarity
        if self.config.get('enable_graph_structure_matching', True):
            scores['graph_structure'] = self._graph_structure_similarity(entity1, entity2)
        else:
            scores['graph_structure'] = 0.0
        
        # 7. Semantic similarity
        if self.config.get('enable_semantic_matching', True) and self.tfidf_matrix is not None:
            scores['semantic_similarity'] = self._semantic_similarity(entity1, entity2)
        else:
            scores['semantic_similarity'] = 0.0
        
        return scores
    
    def _string_similarity(self, str1: str, str2: str) -> float:
        """Calculate string similarity using multiple methods."""
        if HAS_LEVENSHTEIN:
            # Use Levenshtein distance if available
            lev_sim = 1 - (Levenshtein.distance(str1, str2) / max(len(str1), len(str2), 1))
        else:
            lev_sim = 0.0
        
        # SequenceMatcher similarity
        seq_sim = difflib.SequenceMatcher(None, str1, str2).ratio()
        
        # Return the maximum of available similarities
        return max(lev_sim, seq_sim)
    
    def _token_overlap_similarity(self, str1: str, str2: str) -> float:
        """Calculate token overlap score with stricter criteria."""
        tokens1 = set(self._tokenize_and_clean(str1))
        tokens2 = set(self._tokenize_and_clean(str2))
        
        if not tokens1 or not tokens2:
            return 0.0
        
        intersection = tokens1.intersection(tokens2)
        union = tokens1.union(tokens2)
        
        if not union:
            return 0.0
        
        jaccard = len(intersection) / len(union)
        
        # STRICTER: Only boost if one is complete subset AND it's a meaningful match
        if tokens1.issubset(tokens2) or tokens2.issubset(tokens1):
            # Require at least 2 tokens in the subset for meaningful match
            min_tokens = min(len(tokens1), len(tokens2))
            if min_tokens >= 2 or (min_tokens == 1 and jaccard >= 0.5):
                return min(1.0, jaccard + 0.2)  # Reduced boost from 0.3
        
        return jaccard
    
    def _tokenize_and_clean(self, text: str) -> set:
        """Helper method to tokenize and clean text."""
        # Tokenize and clean
        tokens = set(re.findall(r'\b\w+\b', text.lower()))
        
        # Remove common stop words
        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
        return tokens - stop_words
    
    def _partial_name_similarity(self, str1: str, str2: str) -> float:
        """Check if one name is a partial match of another."""
        tokens1 = re.findall(r'\b\w+\b', str1.lower())
        tokens2 = re.findall(r'\b\w+\b', str2.lower())
        
        # Check if all tokens of shorter name are in longer name
        if len(tokens1) <= len(tokens2):
            shorter, longer = tokens1, tokens2
        else:
            shorter, longer = tokens2, tokens1
        
        if not shorter:
            return 0.0
        
        matches = sum(1 for token in shorter if token in longer)
        return matches / len(shorter)
    
    def _abbreviation_similarity(self, str1: str, str2: str) -> float:
        """Check for abbreviation patterns."""
        # Extract initials and check against full names
        def get_initials(text):
            words = re.findall(r'\b\w+\b', text)
            return ''.join(word[0].lower() for word in words if word)
        
        def has_abbreviation_pattern(short, long):
            # Check if short form could be abbreviation of long form
            short_clean = re.sub(r'[^\w\s]', '', short).strip()
            long_clean = re.sub(r'[^\w\s]', '', long).strip()
            
            # Check initials
            initials = get_initials(long_clean)
            if short_clean.replace('.', '').replace(' ', '').lower() == initials:
                return 1.0
            
            # Check partial abbreviations (e.g., "V. Lago" vs "Vince Lago")
            short_parts = short_clean.split()
            long_parts = long_clean.split()
            
            if len(short_parts) == len(long_parts):
                matches = 0
                for s_part, l_part in zip(short_parts, long_parts):
                    s_clean = s_part.replace('.', '').lower()
                    l_clean = l_part.lower()
                    
                    if s_clean == l_clean or (len(s_clean) == 1 and l_clean.startswith(s_clean)):
                        matches += 1
                
                return matches / len(short_parts)
            
            return 0.0
        
        # Check both directions
        score1 = has_abbreviation_pattern(str1, str2)
        score2 = has_abbreviation_pattern(str2, str1)
        
        return max(score1, score2)
    
    def _role_based_similarity(self, str1: str, str2: str) -> float:
        """Check for role-based matches."""
        roles = ['mayor', 'commissioner', 'councilman', 'councilwoman', 'director', 'manager', 'chief']
        
        def extract_role_and_name(text):
            text_lower = text.lower()
            for role in roles:
                if role in text_lower:
                    # Extract the name part
                    name_part = text_lower.replace(role, '').strip()
                    return role, name_part
            return None, text_lower
        
        role1, name1 = extract_role_and_name(str1)
        role2, name2 = extract_role_and_name(str2)
        
        # If one has a role and the other doesn't, check if names match
        if role1 and not role2:
            return self._string_similarity(name1, str2)
        elif role2 and not role1:
            return self._string_similarity(name2, str1)
        elif role1 and role2:
            # Both have roles, check if they match and names are similar
            if role1 == role2:
                return self._string_similarity(name1, name2)
            else:
                return 0.0
        
        return 0.0
    
    def _graph_structure_similarity(self, entity1: Dict, entity2: Dict) -> float:
        """Calculate graph structure similarity."""
        # Get neighbors from pre-calculated neighbor sets
        neighbors1 = entity1.get('neighbors', set())
        neighbors2 = entity2.get('neighbors', set())
        
        if not neighbors1 and not neighbors2:
            return 1.0  # Both isolated
        elif not neighbors1 or not neighbors2:
            return 0.0  # One isolated, one connected
        
        # Jaccard similarity of neighbor sets
        intersection = len(neighbors1.intersection(neighbors2))
        union = len(neighbors1.union(neighbors2))
        
        jaccard_sim = intersection / union if union > 0 else 0.0
        
        # Also compare clustering coefficients
        coeff1 = entity1.get('clustering_coeff', 0.0)
        coeff2 = entity2.get('clustering_coeff', 0.0)
        coeff_diff = abs(coeff1 - coeff2)
        coeff_sim = 1.0 - min(coeff_diff / self.config.get('clustering_tolerance', 0.15), 1.0)
        
        return (jaccard_sim + coeff_sim) / 2.0
    
    def _semantic_similarity(self, entity1: Dict, entity2: Dict) -> float:
        """Calculate semantic similarity using TF-IDF."""
        if self.tfidf_matrix is None:
            return 0.0
        
        try:
            # Get entity indices (this is a simplified approach)
            entity1_idx = None
            entity2_idx = None
            
            for idx, row in self.entities_df.iterrows():
                if row['title'] == entity1['title']:
                    entity1_idx = idx
                elif row['title'] == entity2['title']:
                    entity2_idx = idx
                
                if entity1_idx is not None and entity2_idx is not None:
                    break
            
            if entity1_idx is None or entity2_idx is None:
                return 0.0
            
            # Calculate cosine similarity
            vec1 = self.tfidf_matrix[entity1_idx:entity1_idx+1]
            vec2 = self.tfidf_matrix[entity2_idx:entity2_idx+1]
            
            similarity = cosine_similarity(vec1, vec2)[0, 0]
            return similarity
        except Exception as e:
            logger.debug(f"Error calculating semantic similarity: {e}")
            return 0.0
    
    def _calculate_combined_score(self, scores: Dict[str, float]) -> float:
        """Combine scores with strict requirements for partial matches."""
        # For partial name matches, require strong corroborating evidence
        if scores.get('token_overlap', 0) > 0.7 and scores.get('string_similarity', 0) < 0.8:
            # This is likely a partial name match (e.g., "Lago" vs "Vince Lago")
            # Require high neighbor overlap as confirmation
            if scores.get('graph_structure', 0) < 0.6:
                return 0.0  # Reject without strong network evidence
        
        # Original weighted combination
        weights = self.config.get('weights', {
            'string_similarity': 0.2,
            'token_overlap': 0.4,
            'graph_structure': 0.2,
            'semantic_similarity': 0.2
        })
        
        # Combine main scores
        combined = 0.0
        total_weight = 0.0
        
        for score_type, weight in weights.items():
            if score_type in scores:
                combined += scores[score_type] * weight
                total_weight += weight
        
        # Add bonus scores for special matches
        bonus_scores = ['partial_name_match', 'abbreviation_match', 'role_match']
        max_bonus = 0.0
        for bonus_type in bonus_scores:
            if bonus_type in scores:
                max_bonus = max(max_bonus, scores[bonus_type])
        
        # Apply bonus (up to 20% boost)
        bonus_factor = min(max_bonus * 0.2, 0.2)
        
        if total_weight > 0:
            final_score = (combined / total_weight) + bonus_factor
            return min(final_score, 1.0)
        
        return 0.0
    
    def _determine_merge_reason(self, scores: Dict[str, float]) -> str:
        """Determine the primary reason for merging."""
        # Find the highest contributing factor
        main_scores = {
            'string_similarity': 'High string similarity',
            'token_overlap': 'Token overlap',
            'graph_structure': 'Similar graph structure',
            'semantic_similarity': 'Semantic similarity'
        }
        
        special_scores = {
            'partial_name_match': 'Partial name match',
            'abbreviation_match': 'Abbreviation pattern',
            'role_match': 'Role-based match'
        }
        
        # Check special patterns first
        for score_type, reason in special_scores.items():
            if scores.get(score_type, 0.0) > 0.7:
                return reason
        
        # Find highest main score
        max_score = 0.0
        max_reason = "Combined similarity"
        
        for score_type, reason in main_scores.items():
            if scores.get(score_type, 0.0) > max_score:
                max_score = scores[score_type]
                max_reason = reason
        
        return max_reason
    
    def _determine_primary_entity(self, entity1: Dict, entity2: Dict) -> str:
        """Determine which entity should be kept as primary."""
        # Prefer entity with higher degree centrality
        degree1 = entity1.get('degree_centrality', 0)
        degree2 = entity2.get('degree_centrality', 0)
        
        if degree1 > degree2:
            return entity1['title']
        elif degree2 > degree1:
            return entity2['title']
        
        # If same degree, prefer longer/more descriptive name
        if len(entity1['title']) > len(entity2['title']):
            return entity1['title']
        else:
            return entity2['title']
    
    def _validate_merge_candidate(self, entity1: Dict, entity2: Dict, 
                                scores: Dict[str, float]) -> bool:
        """Additional validation to prevent false positives."""
        # Special validation for partial name matches
        if scores.get('token_overlap', 0) > scores.get('string_similarity', 0):
            # This suggests a partial match like "Lago" vs "Vince Lago"
            
            # Require either:
            # 1. Very high neighbor overlap (>70%)
            # 2. High semantic similarity (>80%) 
            # 3. Abbreviation or role match
            
            has_strong_evidence = (
                scores.get('graph_structure', 0) > 0.7 or
                scores.get('semantic_similarity', 0) > 0.8 or
                scores.get('abbreviation_match', 0) > 0.8 or
                scores.get('role_match', 0) > 0.7
            )
            
            if not has_strong_evidence:
                return False
        
        # Prevent merging entities with very different descriptions
        if entity1.get('description') and entity2.get('description'):
            desc1_len = len(str(entity1['description']))
            desc2_len = len(str(entity2['description']))
            
            # If descriptions differ significantly in length, be more cautious
            if max(desc1_len, desc2_len) > 3 * min(desc1_len, desc2_len):
                # Require higher combined score for very different descriptions
                return scores.get('combined_score', 0) > 0.9
        
        return True
    
    def _execute_merges(self, entities_df: pd.DataFrame, merge_candidates: List[Dict]) -> pd.DataFrame:
        """Execute the entity merges."""
        merged_df = entities_df.copy()
        entities_to_remove = set()
        merge_map = {}  # Maps old entity -> new entity
        
        # Add aliases column if it doesn't exist
        if 'aliases' not in merged_df.columns:
            merged_df['aliases'] = ''
        
        print(f"\n🔧 Executing {len(merge_candidates)} entity merges...")
        
        for candidate in tqdm(merge_candidates, desc="Executing merges", unit="merges"):
            entity1_title = candidate['entity1_title']
            entity2_title = candidate['entity2_title']
            primary_entity = candidate['primary_entity']
            
            # Skip if either entity was already merged
            if entity1_title in entities_to_remove or entity2_title in entities_to_remove:
                continue
            
            # Determine which entity to remove
            if primary_entity == entity1_title:
                keep_entity = entity1_title
                remove_entity = entity2_title
            else:
                keep_entity = entity2_title
                remove_entity = entity1_title
            
            # Update the primary entity
            primary_mask = merged_df['title'] == keep_entity
            remove_mask = merged_df['title'] == remove_entity
            
            if primary_mask.any() and remove_mask.any():
                primary_row = merged_df[primary_mask].iloc[0]
                remove_row = merged_df[remove_mask].iloc[0]
                
                # Merge descriptions
                primary_desc = primary_row.get('description', '') or ''
                remove_desc = remove_row.get('description', '') or ''
                
                if remove_desc and remove_desc not in primary_desc:
                    merged_desc = f"{primary_desc}\n[Also known as: {remove_entity}] {remove_desc}".strip()
                else:
                    merged_desc = f"{primary_desc}\n[Also known as: {remove_entity}]".strip()
                
                # Update aliases
                existing_aliases = primary_row.get('aliases', '') or ''
                if existing_aliases:
                    new_aliases = f"{existing_aliases}|{remove_entity}"
                else:
                    new_aliases = remove_entity
                
                # Apply updates
                merged_df.loc[primary_mask, 'description'] = merged_desc
                merged_df.loc[primary_mask, 'aliases'] = new_aliases
                
                # Mark for removal
                entities_to_remove.add(remove_entity)
                merge_map[remove_entity] = keep_entity
                
                # Record merge
                self.merge_report['merges'].append({
                    'primary_entity': keep_entity,
                    'merged_entity': remove_entity,
                    'combined_score': candidate['combined_score'],
                    'merge_reason': candidate['merge_reason'],
                    'individual_scores': candidate['individual_scores']
                })
        
        # Remove merged entities
        merged_df = merged_df[~merged_df['title'].isin(entities_to_remove)]
        
        logger.info(f"Executed {len(entities_to_remove)} merges")
        return merged_df
    
    def _save_deduplicated_data(self, merged_entities_df: pd.DataFrame):
        """Save deduplicated data to output directory."""
        output_subdir = self.output_dir / "deduplicated"
        output_subdir.mkdir(exist_ok=True)
        
        # Save merged entities
        entities_output = output_subdir / "entities.parquet"
        merged_entities_df.to_parquet(entities_output, index=False)
        
        # Copy other files unchanged
        other_files = [
            "relationships.parquet",
            "communities.parquet", 
            "community_reports.parquet"
        ]
        
        for filename in other_files:
            source_path = self.output_dir / filename
            target_path = output_subdir / filename
            
            if source_path.exists():
                import shutil
                shutil.copy2(source_path, target_path)
        
        logger.info(f"Saved deduplicated data to {output_subdir}")
    
    def _save_enhanced_report(self, merge_candidates: List[Dict]):
        """Save detailed enhanced deduplication report."""
        # Save JSON report
        report_path = self.output_dir / "enhanced_deduplication_report.json"
        self.merge_report['statistics'] = {
            'total_candidates': len(merge_candidates),
            'executed_merges': len(self.merge_report['merges']),
            'config_used': self.config
        }
        
        with open(report_path, 'w') as f:
            json.dump(self.merge_report, f, indent=2)
        
        # Save human-readable text report
        text_report_path = self.output_dir / "enhanced_deduplication_report.txt"
        with open(text_report_path, 'w') as f:
            f.write("Enhanced Entity Deduplication Report\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"Timestamp: {self.merge_report['timestamp']}\n")
            f.write(f"Configuration: {self.config.get('min_combined_score', 0.7)} min score\n")
            f.write(f"Total candidates found: {len(merge_candidates)}\n")
            f.write(f"Merges executed: {len(self.merge_report['merges'])}\n\n")
            
            f.write("Executed Merges:\n")
            f.write("-" * 20 + "\n")
            
            for merge in self.merge_report['merges']:
                f.write(f"\n'{merge['primary_entity']}' ← '{merge['merged_entity']}'\n")
                f.write(f"  Score: {merge['combined_score']:.3f}\n")
                f.write(f"  Reason: {merge['merge_reason']}\n")
                f.write(f"  Details: {merge['individual_scores']}\n")
            
            if merge_candidates:
                f.write(f"\n\nTop candidates (not merged):\n")
                f.write("-" * 30 + "\n")
                
                # Show top 10 candidates that weren't merged
                unmerged = [c for c in merge_candidates 
                           if not any(m['merged_entity'] in [c['entity1_title'], c['entity2_title']] 
                                     for m in self.merge_report['merges'])]
                
                for candidate in unmerged[:10]:
                    f.write(f"\n'{candidate['entity1_title']}' ↔ '{candidate['entity2_title']}'\n")
                    f.write(f"  Score: {candidate['combined_score']:.3f}\n")
                    f.write(f"  Reason: {candidate['merge_reason']}\n")
        
        logger.info(f"Saved enhanced deduplication report to {text_report_path}")


# Main execution for standalone testing
if __name__ == "__main__":
    import argparse
    import sys
    from pathlib import Path
    
    # Add project root to path
    project_root = Path(__file__).parent.parent.parent
    sys.path.append(str(project_root))
    
    # Setup logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    parser = argparse.ArgumentParser(description='Run enhanced entity deduplication')
    parser.add_argument('--output-dir', '-o', type=str, default='graphrag_data/output',
                       help='Path to GraphRAG output directory')
    parser.add_argument('--config', '-c', choices=['aggressive', 'conservative', 'name_focused'],
                       default='name_focused', help='Deduplication configuration preset')
    parser.add_argument('--min-score', type=float, help='Override minimum combined score')
    
    args = parser.parse_args()
    
    output_dir = project_root / args.output_dir
    
    if not output_dir.exists():
        print(f"❌ Output directory not found: {output_dir}")
        sys.exit(1)
    
    # Get configuration
    config = DEDUP_CONFIGS.get(args.config, DEDUP_CONFIGS['name_focused']).copy()
    if args.min_score:
        config['min_combined_score'] = args.min_score
    
    print(f"🔍 Running enhanced entity deduplication on {output_dir}")
    print(f"   Configuration: {args.config}")
    print(f"   Min combined score: {config['min_combined_score']}")
    
    try:
        deduplicator = EnhancedEntityDeduplicator(output_dir, config)
        stats = deduplicator.deduplicate_entities()
        
        print(f"\n✅ Enhanced deduplication complete!")
        print(f"   Original entities: {stats['original_entities']}")
        print(f"   After deduplication: {stats['merged_entities']}")
        print(f"   Entities merged: {stats['merged_count']}")
        print(f"   Merge candidates: {stats['merge_candidates']}")
        print(f"   Output saved to: {stats['output_dir']}")
        
    except Exception as e:
        print(f"❌ Enhanced deduplication failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


================================================================================


################################################################################
# File: scripts/graph_stages/ontology_extractor.py
################################################################################

# File: scripts/graph_stages/ontology_extractor.py

from pathlib import Path
from typing import Dict, List, Optional, Tuple
import json
import logging
import re
from datetime import datetime
from groq import Groq
import os

log = logging.getLogger(__name__)

class OntologyExtractor:
    """Extract rich ontology from agenda data using LLM."""
    
    def __init__(self, output_dir: Optional[Path] = None):
        """Initialize the ontology extractor."""
        self.output_dir = output_dir or Path("city_clerk_documents/graph_json")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Debug directory for LLM responses
        self.debug_dir = Path("debug")
        self.debug_dir.mkdir(exist_ok=True)
        
        # Initialize OpenAI client
        self.client = Groq()
        # Use gpt-4.1-mini-2025-04-14
        self.model = "gpt-4.1-mini-2025-04-14"
    
    def extract_ontology(self, agenda_file: Path) -> Dict[str, any]:
        """Extract rich ontology from agenda data."""
        log.info(f"🧠 Extracting ontology from {agenda_file.name}")
        
        # Load extracted data (from docling)
        extracted_file = self.output_dir / f"{agenda_file.stem}_extracted.json"
        if not extracted_file.exists():
            log.error(f"❌ Extracted file not found: {extracted_file}")
            return self._create_empty_ontology(agenda_file.name)
        
        with open(extracted_file, 'r', encoding='utf-8') as f:
            agenda_data = json.load(f)
        
        full_text = agenda_data.get('full_text', '')
        
        # Get the pre-extracted agenda items
        extracted_items = agenda_data.get('agenda_items', [])
        log.info(f"📊 Found {len(extracted_items)} pre-extracted agenda items")
        
        # Save debug info
        with open(self.debug_dir / "extracted_items.json", 'w') as f:
            json.dump(extracted_items, f, indent=2)
        
        # Extract meeting date
        meeting_date = self._extract_meeting_date(agenda_file.name)
        log.info(f"📅 Extracted meeting date: {meeting_date}")
        
        # Extract meeting info
        meeting_info = self._extract_meeting_info(full_text, meeting_date)
        
        # Extract sections and their items
        sections = self._extract_sections_with_items(full_text, extracted_items)
        
        # Extract entities from the entire document
        entities = self._extract_entities(full_text)
        
        # Extract relationships between entities
        relationships = self._extract_relationships(entities, sections)
        
        # Extract URLs using regex
        urls = self._extract_urls_regex(full_text)
        
        # Enhance agenda items with URLs (only if they don't already have URLs from PyMuPDF)
        for section in sections:
            for item in section.get('items', []):
                # Only extract URLs if the item doesn't already have them from PyMuPDF
                if not item.get('urls'):
                    item['urls'] = self._find_urls_for_item(item, urls, full_text)
                # If item already has URLs from PyMuPDF, keep them as they are more accurate
        
        # Build ontology
        ontology = {
            'source_file': agenda_file.name,
            'meeting_date': meeting_date,
            'meeting_info': meeting_info,
            'sections': sections,
            'entities': entities,
            'relationships': relationships,
            'metadata': {
                'extraction_method': 'llm+regex',
                'num_sections': len(sections),
                'num_items': sum(len(s.get('items', [])) for s in sections),
                'num_entities': len(entities),
                'num_relationships': len(relationships),
                'num_urls': len(urls)
            }
        }
        
        # Save ontology
        output_file = self.output_dir / f"{agenda_file.stem}_ontology.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(ontology, f, indent=2, ensure_ascii=False)
        
        log.info(f"✅ Ontology extraction complete: {len(sections)} sections, {sum(len(s.get('items', [])) for s in sections)} items")
        
        return ontology
    
    def _extract_meeting_date(self, filename: str) -> str:
        """Extract meeting date from filename."""
        # Pattern: Agenda MM.DD.YYYY.pdf or Agenda MM.D.YYYY.pdf
        date_pattern = r'Agenda\s+(\d{1,2})\.(\d{1,2})\.(\d{4})'
        match = re.search(date_pattern, filename)
        if match:
            month = match.group(1).zfill(2)
            day = match.group(2).zfill(2)
            year = match.group(3)
            return f"{month}.{day}.{year}"
        return "01.01.2024"  # Default fallback
    
    def _extract_meeting_info(self, text: str, meeting_date: str) -> Dict[str, any]:
        """Extract detailed meeting information using LLM."""
        prompt = f"""Extract meeting information from this city commission agenda. Find:

1. Meeting type (Regular, Special, Workshop)
2. Meeting time
3. Meeting location/venue
4. Commission members present (if listed)
5. City officials (Mayor, City Manager, City Attorney, City Clerk)

Return ONLY the JSON object below, no other text:
{{
  "type": "Regular Meeting",
  "time": "5:30 PM",
  "location": "City Commission Chambers",
  "commissioners": ["Name1", "Name2"],
  "officials": {{
    "mayor": "Mayor Name",
    "city_manager": "Manager Name",
    "city_attorney": "Attorney Name",
    "city_clerk": "Clerk Name"
  }}
}}

Text (first 3000 chars):
{text[:3000]}"""

        try:
            response = self.client.chat.completions.create(
                model="meta-llama/llama-4-maverick-17b-128e-instruct",
                messages=[
                    {"role": "system", "content": "You are a JSON extraction assistant. Return ONLY valid JSON, no markdown formatting or code blocks."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_completion_tokens=8192,
                top_p=1,
                stream=False,
                stop=None
            )
            
            response_text = response.choices[0].message.content.strip()
            
            # Debug: save raw response
            debug_file = self.debug_dir / "meeting_info_response.txt"
            with open(debug_file, 'w') as f:
                f.write(response_text)
            
            # Clean the response
            response_text = self._clean_json_response(response_text)
            
            # Parse JSON
            result = json.loads(response_text)
            
            # Validate it's a dict
            if not isinstance(result, dict):
                log.error(f"Meeting info is not a dict: {type(result)}")
                return self._default_meeting_info()
                
            return result
            
        except Exception as e:
            log.error(f"Failed to extract meeting info: {e}")
            return self._default_meeting_info()

    def _default_meeting_info(self) -> Dict[str, any]:
        """Return default meeting info structure."""
        return {
            "type": "Regular Meeting",
            "time": "5:30 PM",
            "location": "City Commission Chambers",
            "commissioners": [],
            "officials": {}
        }
    
    def _extract_section_headers_from_text(self, text: str) -> List[Dict[str, any]]:
        """Extract section headers from the agenda text."""
        sections = []
        
        # Define all possible section headers
        section_headers = [
            'PRESENTATIONS AND PROTOCOL DOCUMENTS',
            'APPROVAL OF MINUTES',
            'PUBLIC COMMENTS',
            'CONSENT AGENDA',
            'ORDINANCES ON SECOND READING',
            'ORDINANCES ON FIRST READING',
            'PUBLIC HEARINGS',  # Sometimes used instead of ORDINANCES
            'RESOLUTIONS',
            'CITY COMMISSION ITEMS',
            'BOARDS/COMMITTEES ITEMS',
            'BOARDS AND COMMITTEES ITEMS',  # Alternative spelling
            'CITY MANAGER ITEMS',
            'CITY ATTORNEY ITEMS',
            'CITY CLERK ITEMS',
            'DISCUSSION ITEMS',
            'ADJOURNMENT'
        ]
        
        lines = text.split('\n')
        found_sections = []
        
        for i, line in enumerate(lines[:500]):  # Check first 500 lines for headers
            line_stripped = line.strip()
            if not line_stripped:
                continue
            
            # Check for letter-prefixed section (e.g., "A. PRESENTATIONS AND PROTOCOL DOCUMENTS")
            letter_match = re.match(r'^([A-Z])\.\s+(.+)$', line_stripped)
            if letter_match:
                letter = letter_match.group(1)
                section_name = letter_match.group(2).strip()
                
                # Check if this matches one of our known headers
                section_type = None
                for header in section_headers:
                    if header in section_name.upper():
                        section_type = header.replace(' ', '_').replace('/', '_')
                        break
                
                if not section_type:
                    section_type = f'SECTION_{letter}'
                
                found_sections.append({
                    'section_letter': letter,
                    'section_name': section_name,
                    'section_type': section_type,
                    'line_number': i,
                    'items': []
                })
                log.info(f"Found section: {letter}. {section_name}")
                continue
            
            # Check for non-letter section headers
            line_upper = line_stripped.upper()
            for header in section_headers:
                # Check for exact match or if the header is contained in the line
                if header == line_upper or header in line_upper:
                    # Try to find if there's a letter before this section
                    section_letter = None
                    if i > 0:
                        # Check previous lines for a letter
                        for j in range(max(0, i-3), i):
                            prev_line = lines[j].strip()
                            single_letter = re.match(r'^([A-Z])\.?$', prev_line)
                            if single_letter:
                                section_letter = single_letter.group(1)
                                break
                    
                    # If no letter found, assign based on order
                    if not section_letter and found_sections:
                        last_letter = found_sections[-1].get('section_letter', '@')
                        section_letter = chr(ord(last_letter) + 1)
                    elif not section_letter:
                        section_letter = 'A'
                    
                    found_sections.append({
                        'section_letter': section_letter,
                        'section_name': line_stripped,
                        'section_type': header.replace(' ', '_').replace('/', '_'),
                        'line_number': i,
                        'items': []
                    })
                    log.info(f"Found section: {section_letter}. {line_stripped}")
                    break
        
        # Sort sections by line number to maintain order
        found_sections.sort(key=lambda x: x['line_number'])
        
        # Add order field
        for i, section in enumerate(found_sections):
            section['order'] = i + 1
            del section['line_number']  # Remove line number as it's not needed anymore
        
        return found_sections
    
    def _assign_items_to_sections(self, sections: List[Dict], items: List[Dict]) -> List[Dict]:
        """Assign items to their appropriate sections based on item codes."""
        # Create a map of letter to section
        letter_to_section = {}
        for section in sections:
            if 'section_letter' in section:
                letter_to_section[section['section_letter']] = section
        
        # Assign items to sections based on their letter prefix
        for item in items:
            item_code = item.get('item_code', '')
            if not item_code:
                continue
            
            # Extract letter prefix (A.-1. -> A, D.-2. -> D, 1.-1. -> 1)
            letter_match = re.match(r'^([A-Z0-9])', item_code)
            if letter_match:
                letter = letter_match.group(1)
                
                if letter in letter_to_section:
                    section = letter_to_section[letter]
                    
                    # Add enhanced fields to item
                    enhanced_item = {
                        **item,
                        'section': section['section_name'],
                        'description': item.get('title', '')[:200],
                        'sponsors': [],
                        'departments': [],
                        'actions': [],
                        'stakeholders': [],
                        'urls': item.get('urls', [])  # Preserve existing URLs instead of overwriting
                    }
                    
                    section['items'].append(enhanced_item)
                    log.debug(f"Assigned item {item_code} to section {section['section_name']}")
        
        # Remove empty sections and reorder
        non_empty_sections = []
        for i, section in enumerate(sections):
            if section.get('items'):
                section['order'] = i + 1
                non_empty_sections.append(section)
                log.info(f"Section {section['section_name']} has {len(section['items'])} items")
        
        return non_empty_sections

    def _group_items_by_prefix(self, items: List[Dict]) -> List[Dict[str, any]]:
        """Group items by their letter prefix without assuming section types."""
        sections_map = {}
        
        for item in items:
            item_code = item.get('item_code', '')
            if not item_code:
                continue
            
            # Extract letter/number prefix (A.-1. -> A, D.-2. -> D, 1.-1. -> 1)
            prefix_match = re.match(r'^([A-Z0-9])', item_code)
            if prefix_match:
                prefix = prefix_match.group(1)
                
                if prefix not in sections_map:
                    # Create generic section name
                    if prefix.isalpha():
                        section_name = f'Section {prefix}'
                    else:
                        section_name = f'Numbered Items - Section {prefix}'
                    
                    sections_map[prefix] = {
                        'section_letter': prefix,
                        'section_name': section_name,
                        'section_type': f'SECTION_{prefix}',
                        'items': []
                    }
                
                # Add enhanced fields to item
                enhanced_item = {
                    **item,
                    'section': sections_map[prefix]['section_name'],
                    'description': item.get('title', '')[:200],
                    'sponsors': [],
                    'departments': [],
                    'actions': [],
                    'stakeholders': [],
                    'urls': item.get('urls', [])  # Preserve existing URLs instead of overwriting
                }
                
                sections_map[prefix]['items'].append(enhanced_item)
        
        # Convert to list and sort
        sections = []
        
        # Sort alphabetically first (A, B, C...), then numerically (1, 2, 3...)
        sorted_keys = sorted(sections_map.keys(), key=lambda x: (x.isdigit(), x))
        
        for i, key in enumerate(sorted_keys):
            section = sections_map[key]
            section['order'] = i + 1
            sections.append(section)
            log.info(f"Created section '{section['section_name']}' with {len(section['items'])} items")
        
        return sections

    def _determine_item_type(self, title: str, section_type: str) -> str:
        """Determine item type from title and section."""
        title_lower = title.lower()
        
        # Check title first for explicit type indicators
        if 'an ordinance' in title_lower:
            return 'Ordinance'
        elif 'a resolution' in title_lower:
            return 'Resolution'
        elif 'proclamation' in title_lower:
            return 'Proclamation'
        elif 'recognition' in title_lower:
            return 'Recognition'
        elif 'congratulations' in title_lower:
            return 'Recognition'
        elif 'presentation' in title_lower:
            return 'Presentation'
        elif 'appointment' in title_lower or 'appointing' in title_lower:
            return 'Appointment'
        elif 'minutes' in title_lower and 'approval' in title_lower:
            return 'Minutes Approval'
        
        # Use section type as hint if no explicit type in title
        if section_type:
            if 'ORDINANCE' in section_type:
                return 'Ordinance'
            elif 'RESOLUTION' in section_type:
                return 'Resolution'
            elif 'PRESENTATION' in section_type:
                return 'Presentation'
            elif 'MINUTES' in section_type:
                return 'Minutes Approval'
            elif 'CONSENT' in section_type:
                # Consent items could be various types
                return 'Consent Item'
        
        # Generic fallback
        return 'Agenda Item'
    
    def _extract_sections_with_items(self, text: str, extracted_items: List[Dict]) -> List[Dict[str, any]]:
        """Extract sections and organize items within them using LLM."""
        
        # If we have extracted items from the PDF extractor, use them
        if extracted_items:
            log.info(f"Using {len(extracted_items)} pre-extracted agenda items")
            
            # First, try to extract section headers from the text
            sections = self._extract_section_headers_from_text(text)
            
            if sections:
                log.info(f"Found {len(sections)} sections in agenda text")
                # Assign items to the extracted sections
                sections = self._assign_items_to_sections(sections, extracted_items)
            else:
                log.warning("No sections found in text, grouping items by prefix")
                # If no sections found, group items by their letter prefix
                sections = self._group_items_by_prefix(extracted_items)
            
            # Add any items that weren't assigned to a section
            unassigned_items = []
            assigned_codes = set()
            
            for section in sections:
                for item in section.get('items', []):
                    assigned_codes.add(item.get('item_code'))
            
            for item in extracted_items:
                if item.get('item_code') not in assigned_codes:
                    unassigned_items.append(item)
            
            if unassigned_items:
                log.warning(f"Found {len(unassigned_items)} unassigned items")
                # Create a miscellaneous section for unassigned items
                misc_section = {
                    'section_letter': 'MISC',
                    'section_name': 'Other Items',
                    'section_type': 'OTHER',
                    'order': len(sections) + 1,
                    'items': []
                }
                
                for item in unassigned_items:
                    enhanced_item = {
                        **item,
                        'section': 'Other Items',
                        'description': item.get('title', '')[:200],
                        'sponsors': [],
                        'departments': [],
                        'actions': [],
                        'stakeholders': [],
                        'urls': item.get('urls', [])  # Preserve existing URLs instead of overwriting
                    }
                    misc_section['items'].append(enhanced_item)
                
                sections.append(misc_section)
            
            log.info(f"Created {len(sections)} sections with {sum(len(s.get('items', [])) for s in sections)} total items")
            return sections
        
        # If no extracted items, fall back to LLM extraction
        log.warning("No pre-extracted items found, using LLM extraction")
        # First, get section structure from LLM
        prompt = f"""Identify all major sections in this city commission agenda. Common sections include:
- PRESENTATIONS AND PROCLAMATIONS
- CONSENT AGENDA  
- ORDINANCES ON FIRST READING
- ORDINANCES ON SECOND READING
- RESOLUTIONS
- CITY MANAGER REPORTS
- CITY ATTORNEY REPORTS
- GENERAL DISCUSSION

For each section found, provide:
1. section_name: The exact name as it appears
2. section_type: One of [presentations, consent, ordinances_first, ordinances_second, resolutions, reports, discussion, other]
3. description: Brief description of what this section contains

Return as JSON array:
[
  {{
    "section_name": "CONSENT AGENDA",
    "section_type": "consent",
    "description": "Items for routine approval"
  }}
]

Text (first 5000 chars):
{text[:5000]}"""

        sections = []
        
        try:
            response = self.client.chat.completions.create(
                model="meta-llama/llama-4-maverick-17b-128e-instruct",
                messages=[
                    {"role": "system", "content": "Extract agenda sections. Return only valid JSON array."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_completion_tokens=8192,
                top_p=1,
                stream=False,
                stop=None
            )
            
            response_text = response.choices[0].message.content.strip()
            response_text = self._clean_json_response(response_text)
            
            section_list = json.loads(response_text)
            
            # Now assign items to sections based on their location in text
            for section in section_list:
                section['items'] = []
                
                # Find items that belong to this section
                section_name = section['section_name']
                for item in extracted_items:
                    # Enhanced item with more details
                    enhanced_item = {
                        **item,
                        'section': section_name,
                        'description': '',
                        'sponsors': [],
                        'departments': [],
                        'actions': [],
                        'urls': item.get('urls', [])  # Preserve existing URLs
                    }
                    
                    # Try to find item in text and extract context
                    item_code = item.get('item_code', '')
                    if item_code:
                        # Find the item in the text and extract surrounding context
                        pattern = rf'{re.escape(item_code)}.*?(?=(?:[A-Z]\.-\d+\.|$))'
                        match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
                        if match:
                            context = match.group(0)[:1000]  # Get context around item
                            
                            # Extract additional details using LLM
                            details = self._extract_item_details(item_code, context)
                            enhanced_item.update(details)
                    
                    # Assign to appropriate section based on item type or position
                    if item.get('item_type') == 'Ordinance':
                        if section['section_type'] in ['ordinances_first', 'ordinances_second']:
                            section['items'].append(enhanced_item)
                    elif item.get('item_type') == 'Resolution' and section['section_type'] == 'resolutions':
                        section['items'].append(enhanced_item)
                    elif section['section_type'] == 'consent' and item_code.startswith('D'):
                        section['items'].append(enhanced_item)
            
            sections = section_list
            
        except Exception as e:
            log.error(f"Failed to extract sections: {e}")
            # Fallback: create basic sections
            sections = self._create_basic_sections(extracted_items)
        
        return sections
    
    def _extract_item_details(self, item_code: str, context: str) -> Dict[str, any]:
        """Extract detailed information about a specific agenda item."""
        prompt = f"""Extract details for agenda item {item_code} from this context:

Find:
1. Full description/summary
2. Sponsoring commissioners or departments
3. Departments involved
4. Recommended actions (approve, deny, discuss, defer, etc.)
5. Key stakeholders mentioned

Return as JSON:
{{
  "description": "Brief description",
  "sponsors": ["Commissioner Name"],
  "departments": ["Planning", "Finance"],
  "actions": ["approve", "authorize"],
  "stakeholders": ["Organization name"]
}}

Context:
{context}"""

        try:
            response = self.client.chat.completions.create(
                model="meta-llama/llama-4-maverick-17b-128e-instruct",
                messages=[
                    {"role": "system", "content": "You are a JSON extraction assistant. Return ONLY valid JSON with no additional text."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_completion_tokens=8192,
                top_p=1,
                stream=False,
                stop=None
            )
            
            response_text = response.choices[0].message.content.strip()
            response_text = self._clean_json_response(response_text)
            
            return json.loads(response_text)
            
        except Exception as e:
            log.error(f"Failed to extract item details for {item_code}: {e}")
            return {
                "description": "",
                "sponsors": [],
                "departments": [],
                "actions": [],
                "stakeholders": []
            }
    
    def _extract_entities(self, text: str) -> List[Dict[str, any]]:
        """Extract all entities (people, organizations, departments) from the document."""
        # Process in chunks
        max_chars = 10000
        chunks = [text[i:i+max_chars] for i in range(0, len(text), max_chars)]
        
        all_entities = []
        seen_entities = set()
        
        for i, chunk in enumerate(chunks[:5]):  # Process first 5 chunks
            prompt = f"""Extract all named entities from this government document:

Find:
1. People (commissioners, officials, citizens)
2. Organizations (companies, non-profits, agencies)
3. Departments (city departments, divisions)
4. Locations (addresses, buildings, areas)

For each entity, determine:
- name: Full name
- type: person, organization, department, location
- role: Their role if mentioned (e.g., "Commissioner", "Director", "Applicant")
- context: Brief context where they appear

Return as JSON array:
[
  {{
    "name": "John Smith",
    "type": "person",
    "role": "Commissioner",
    "context": "Sponsoring ordinance E-1"
  }}
]

Text chunk {i+1}:
{chunk}"""

            try:
                response = self.client.chat.completions.create(
                    model="meta-llama/llama-4-maverick-17b-128e-instruct",
                    messages=[
                        {"role": "system", "content": "You are a JSON extraction assistant. You must return ONLY a valid JSON array with no additional text, explanations, or markdown formatting."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0,
                    max_completion_tokens=8192,
                    top_p=1,
                    stream=False,
                    stop=None
                )
                
                response_text = response.choices[0].message.content.strip()
                
                # Debug: save raw response
                debug_file = self.debug_dir / f"entities_response_chunk_{i}.txt"
                with open(debug_file, 'w') as f:
                    f.write(response_text)
                
                response_text = self._clean_json_response(response_text)
                
                entities = json.loads(response_text)
                if not isinstance(entities, list):
                    log.error(f"Expected list but got {type(entities)} for chunk {i+1}")
                    entities = []
                
                # Deduplicate
                for entity in entities:
                    entity_key = f"{entity.get('type', '')}:{entity.get('name', '').lower()}"
                    if entity_key not in seen_entities:
                        seen_entities.add(entity_key)
                        all_entities.append(entity)
                
            except Exception as e:
                log.error(f"Failed to extract entities from chunk {i+1}: {e}")
                # Try basic regex extraction as fallback
                chunk_entities = self._basic_entity_extraction(chunk)
                all_entities.extend(chunk_entities)
        
        return all_entities
    
    def _basic_entity_extraction(self, text: str) -> List[Dict[str, any]]:
        """Basic entity extraction using patterns as fallback."""
        entities = []
        
        # Extract commissioners/council members
        commissioner_pattern = r'Commissioner\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)'
        for match in re.finditer(commissioner_pattern, text):
            entities.append({
                "name": match.group(1),
                "type": "person",
                "role": "Commissioner",
                "context": "City Commissioner"
            })
        
        # Extract Mayor
        mayor_pattern = r'Mayor\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)'
        for match in re.finditer(mayor_pattern, text):
            entities.append({
                "name": match.group(1),
                "type": "person",
                "role": "Mayor",
                "context": "City Mayor"
            })
        
        # Extract departments
        dept_pattern = r'(?:Department of|Dept\. of)\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)'
        for match in re.finditer(dept_pattern, text):
            entities.append({
                "name": f"Department of {match.group(1)}",
                "type": "department",
                "role": "",
                "context": "City Department"
            })
        
        # Extract City Manager, City Attorney, City Clerk
        for role in ["City Manager", "City Attorney", "City Clerk"]:
            pattern = rf'{role}\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)'
            for match in re.finditer(pattern, text):
                entities.append({
                    "name": match.group(1),
                    "type": "person",
                    "role": role,
                    "context": f"{role} of the City"
                })
        
        return entities
    
    def _extract_relationships(self, entities: List[Dict], sections: List[Dict]) -> List[Dict[str, any]]:
        """Extract relationships between entities and agenda items."""
        relationships = []
        
        # Extract relationships from agenda items
        for section in sections:
            for item in section.get('items', []):
                item_code = item.get('item_code', '')
                
                # Sponsors relationship
                for sponsor in item.get('sponsors', []):
                    relationships.append({
                        'source': sponsor,
                        'source_type': 'person',
                        'relationship': 'sponsors',
                        'target': item_code,
                        'target_type': 'agenda_item'
                    })
                
                # Department relationships
                for dept in item.get('departments', []):
                    relationships.append({
                        'source': dept,
                        'source_type': 'department',
                        'relationship': 'responsible_for',
                        'target': item_code,
                        'target_type': 'agenda_item'
                    })
                
                # Stakeholder relationships
                for stakeholder in item.get('stakeholders', []):
                    relationships.append({
                        'source': stakeholder,
                        'source_type': 'organization',
                        'relationship': 'involved_in',
                        'target': item_code,
                        'target_type': 'agenda_item'
                    })
        
        return relationships
    
    def _extract_urls_regex(self, text: str) -> List[Dict[str, str]]:
        """Extract all URLs from the text using regex."""
        urls = []
        
        # Comprehensive URL pattern
        url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+(?:[/?#][^\s<>"{}|\\^`\[\]]*)?'
        
        for match in re.finditer(url_pattern, text):
            url = match.group(0).rstrip('.,;:)')  # Clean trailing punctuation
            
            # Find surrounding context
            start = max(0, match.start() - 100)
            end = min(len(text), match.end() + 100)
            context = text[start:end]
            
            # Try to find associated agenda item
            item_pattern = r'([A-Z])[.-](\d+)'
            item_matches = list(re.finditer(item_pattern, context))
            
            associated_item = None
            if item_matches:
                # Find closest item reference
                closest_match = min(item_matches, 
                                  key=lambda m: abs(m.start() - (match.start() - start)))
                associated_item = f"{closest_match.group(1)}-{closest_match.group(2)}"
            
            urls.append({
                'url': url,
                'context': context.replace('\n', ' ').strip(),
                'associated_item': associated_item
            })
        
        return urls
    
    def _find_urls_for_item(self, item: Dict, all_urls: List[Dict], full_text: str) -> List[str]:
        """Find URLs associated with a specific agenda item."""
        item_code = item.get('item_code', '')
        doc_ref = item.get('document_reference', '')
        
        if not item_code:
            return []
        
        # Find the item's position in text
        item_pattern = rf'{re.escape(item_code)}.*?{re.escape(doc_ref)}' if doc_ref else rf'{re.escape(item_code)}'
        item_match = re.search(item_pattern, full_text, re.DOTALL)
        
        if item_match:
            item_start = item_match.start()
            # Look for next item to bound our search
            next_item_pattern = r'[A-Z]\.-\d+\.?\s+\d{2}-\d{4,5}'
            next_match = re.search(next_item_pattern, full_text[item_start + len(item_match.group(0)):])
            item_end = item_start + len(item_match.group(0)) + (next_match.start() if next_match else 1000)
            
            # Find URLs in this item's text range
            item_urls = []
            for url_info in all_urls:
                url_pos = full_text.find(url_info['url'])
                if item_start <= url_pos <= item_end:
                    item_urls.append(url_info['url'])
            
            return item_urls
        
        # Fallback to original method if no match found
        item_urls = []
        
        # Find URLs that mention this item code
        for url_info in all_urls:
            if url_info.get('associated_item') == item_code:
                item_urls.append(url_info['url'])
            elif item_code in url_info.get('context', ''):
                item_urls.append(url_info['url'])
        
        # Also search near the item in the text
        item_pattern = rf'{re.escape(item_code)}[^A-Z]*'
        match = re.search(item_pattern, full_text, re.IGNORECASE)
        if match:
            # Look for URLs within 500 chars of the item
            start = match.start()
            end = min(len(full_text), start + 1000)
            item_context = full_text[start:end]
            
            url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+(?:[/?#][^\s<>"{}|\\^`\[\]]*)?'
            for url_match in re.finditer(url_pattern, item_context):
                url = url_match.group(0).rstrip('.,;:)')
                if url not in item_urls:
                    item_urls.append(url)
        
        return item_urls
    
    def _create_basic_sections(self, items: List[Dict]) -> List[Dict[str, any]]:
        """Create basic sections as fallback."""
        sections = []
        
        # Group by item type
        ordinances = [item for item in items if item.get('item_type') == 'Ordinance']
        resolutions = [item for item in items if item.get('item_type') == 'Resolution']
        others = [item for item in items if item.get('item_type') not in ['Ordinance', 'Resolution']]
        
        if ordinances:
            sections.append({
                'section_name': 'ORDINANCES',
                'section_type': 'ordinances_first',
                'description': 'Ordinances for consideration',
                'items': ordinances
            })
        
        if resolutions:
            sections.append({
                'section_name': 'RESOLUTIONS',
                'section_type': 'resolutions',
                'description': 'Resolutions for consideration',
                'items': resolutions
            })
        
        if others:
            sections.append({
                'section_name': 'OTHER ITEMS',
                'section_type': 'other',
                'description': 'Other agenda items',
                'items': others
            })
        
        return sections
    
    def _clean_json_response(self, response: str) -> str:
        """Clean LLM response to extract valid JSON."""
        # Remove thinking tags if present
        if '<think>' in response:
            # Extract content after </think>
            parts = response.split('</think>')
            if len(parts) > 1:
                response = parts[1].strip()
        
        # Remove markdown code blocks
        if '```json' in response:
            response = response.split('```json')[1].split('```')[0]
        elif '```' in response:
            response = response.split('```')[1].split('```')[0]
        
        # Remove any non-JSON content before/after
        response = response.strip()
        
        # Find JSON array or object
        if '[' in response:
            # Find the first [ and matching ]
            start_idx = response.find('[')
            if start_idx != -1:
                bracket_count = 0
                for i in range(start_idx, len(response)):
                    if response[i] == '[':
                        bracket_count += 1
                    elif response[i] == ']':
                        bracket_count -= 1
                        if bracket_count == 0:
                            return response[start_idx:i+1]
        
        if '{' in response:
            # Find the first { and matching }
            start_idx = response.find('{')
            if start_idx != -1:
                brace_count = 0
                in_string = False
                escape_next = False
                
                for i in range(start_idx, len(response)):
                    char = response[i]
                    
                    if escape_next:
                        escape_next = False
                        continue
                        
                    if char == '\\':
                        escape_next = True
                        continue
                        
                    if char == '"' and not escape_next:
                        in_string = not in_string
                        
                    if not in_string:
                        if char == '{':
                            brace_count += 1
                        elif char == '}':
                            brace_count -= 1
                            if brace_count == 0:
                                return response[start_idx:i+1]
        
        return response
    
    def _create_empty_ontology(self, filename: str) -> Dict[str, any]:
        """Create empty ontology structure."""
        return {
            'source_file': filename,
            'meeting_date': self._extract_meeting_date(filename),
            'meeting_info': {
                'type': 'Regular Meeting',
                'time': '5:30 PM',
                'location': 'City Commission Chambers',
                'commissioners': [],
                'officials': {}
            },
            'sections': [],
            'entities': [],
            'relationships': [],
            'metadata': {
                'extraction_method': 'empty',
                'num_sections': 0,
                'num_items': 0,
                'num_entities': 0,
                'num_relationships': 0
            }
        }

    def _extract_and_associate_urls(self, text: str, agenda_items: List[Dict]) -> Dict[str, List[str]]:
        """Extract URLs and associate them with nearby agenda items."""
        url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+(?:[/?#][^\s<>"{}|\\^`\[\]]*)?'
        
        # Find all URLs with their positions
        url_matches = []
        for match in re.finditer(url_pattern, text):
            url = match.group(0).rstrip('.,;:)')
            position = match.start()
            
            # Find the nearest agenda item reference before this URL
            nearest_item = self._find_nearest_agenda_item(text, position, agenda_items)
            
            url_matches.append({
                'url': url,
                'position': position,
                'associated_item': nearest_item
            })
        
        return url_matches

    def _find_nearest_agenda_item(self, text: str, url_position: int, agenda_items: List[Dict]) -> Optional[str]:
        """Find the nearest agenda item reference before a URL position."""
        best_item = None
        best_distance = float('inf')
        
        for item in agenda_items:
            item_code = item.get('item_code', '')
            if not item_code:
                continue
                
            # Find all occurrences of this item code before the URL
            item_pattern = rf'{re.escape(item_code)}'
            for match in re.finditer(item_pattern, text[:url_position]):
                distance = url_position - match.end()
                if distance < best_distance:
                    best_distance = distance
                    best_item = item_code
        
        return best_item


================================================================================


################################################################################
# File: graphrag_query_ui.py
################################################################################

# File: graphrag_query_ui.py

#!/usr/bin/env python3
"""
GraphRAG Query UI
A web interface for querying the City Clerk GraphRAG knowledge base.
"""

import dash
from dash import dcc, html, Input, Output, State, ctx
import dash_bootstrap_components as dbc
from dash.exceptions import PreventUpdate
import asyncio
from pathlib import Path
import sys
import json
from datetime import datetime
import logging
from typing import Dict, Any


# Add project root to path
# Handle both cases: script in root or in a subdirectory
current_file = Path(__file__).resolve()
if current_file.parent.name == "scripts" or current_file.parent.name == "ui":
    project_root = current_file.parent.parent
else:
    project_root = current_file.parent
sys.path.append(str(project_root))

from scripts.microsoft_framework import (
    CityClerkQueryEngine,
    SmartQueryRouter,
    QueryIntent
)

# Set up logging
logging.basicConfig(level=logging.INFO)
log = logging.getLogger(__name__)

# Initialize the app with a nice theme
app = dash.Dash(
    __name__, 
    external_stylesheets=[dbc.themes.BOOTSTRAP],
    title="GraphRAG City Clerk Query System"
)

# Initialize query engine and router
GRAPHRAG_ROOT = project_root / "graphrag_data"
query_engine = None
query_router = SmartQueryRouter()

# Store query history
query_history = []

# Define the layout
app.layout = dbc.Container([
    dbc.Row([
        dbc.Col([
            html.H1("🏛️ City Clerk GraphRAG Query System", className="text-center mb-4"),
            html.Hr(),
        ])
    ]),
    
    # Query Input Section
    dbc.Row([
        dbc.Col([
            dbc.Card([
                dbc.CardHeader(html.H4("🔍 Enter Your Query")),
                dbc.CardBody([
                    dbc.Textarea(
                        id="query-input",
                        placeholder="Ask about agenda items, ordinances, resolutions, or city proceedings...\n\nExamples:\n- What is agenda item E-1?\n- Tell me about ordinance 2024-01\n- What are the main development themes?\n- How has zoning policy evolved?",
                        style={"height": "150px"},
                        className="mb-3"
                    ),
                    
                    dbc.Row([
                        dbc.Col([
                            html.Label("Query Method:", className="fw-bold"),
                            dbc.RadioItems(
                                id="query-method",
                                options=[
                                    {"label": "🤖 Auto-Select (Recommended)", "value": "auto"},
                                    {"label": "🎯 Local Search", "value": "local"},
                                    {"label": "🌐 Global Search", "value": "global"},
                                    {"label": "🔄 DRIFT Search", "value": "drift"}
                                ],
                                value="auto",
                                inline=False
                            ),
                        ], md=6),
                        
                        dbc.Col([
                            html.Label("Query Options:", className="fw-bold"),
                            dbc.Checklist(
                                id="query-options",
                                options=[
                                    {"label": "Include community context", "value": "community"},
                                    {"label": "Show routing details", "value": "routing"},
                                    {"label": "Show data sources", "value": "sources"},
                                    {"label": "Verbose results", "value": "verbose"}
                                ],
                                value=["community", "routing", "sources"],
                                inline=False
                            ),
                        ], md=6),
                    ]),
                    
                    dbc.Row([
                        dbc.Col([
                            dbc.Button(
                                "🚀 Submit Query",
                                id="submit-query",
                                color="primary",
                                size="lg",
                                className="w-100 mt-3",
                                n_clicks=0
                            ),
                        ], md=6),
                        dbc.Col([
                            dbc.Button(
                                "🧹 Clear",
                                id="clear-all",
                                color="secondary",
                                size="lg",
                                className="w-100 mt-3",
                                n_clicks=0
                            ),
                        ], md=6),
                    ]),
                ])
            ], className="mb-4"),
        ])
    ]),
    
    # Loading indicator
    dcc.Loading(
        id="loading",
        type="default",
        children=[
            html.Div(id="loading-output")
        ]
    ),
    
    # Routing Information
    dbc.Row([
        dbc.Col([
            dbc.Collapse(
                dbc.Card([
                    dbc.CardHeader(html.H5("🎯 Query Routing Analysis")),
                    dbc.CardBody(id="routing-info")
                ], className="mb-4"),
                id="routing-collapse",
                is_open=False
            ),
        ])
    ]),
    
    # Results Section
    dbc.Row([
        dbc.Col([
            dbc.Card([
                dbc.CardHeader(html.H4("📊 Query Results")),
                dbc.CardBody(id="query-results", style={"min-height": "300px"})
            ], className="mb-4"),
        ])
    ]),
    
    # Query History
    dbc.Row([
        dbc.Col([
            dbc.Card([
                dbc.CardHeader([
                    html.H5("📜 Query History", className="d-inline"),
                    dbc.Button(
                        "Clear History",
                        id="clear-history",
                        color="danger",
                        size="sm",
                        className="float-end",
                        n_clicks=0
                    )
                ]),
                dbc.CardBody(id="query-history")
            ]),
        ])
    ]),
    
    # Hidden div for storing state
    html.Div(id="query-state", style={"display": "none"})
    
], fluid=True, className="p-4")

def create_data_sources_display(data_sources):
    """Create a formatted display of data sources used in the query."""
    
    # Handle empty or None data_sources
    if not data_sources:
        return html.Div([
            html.Hr(style={'margin': '20px 0', 'border-top': '1px solid #e0e0e0'}),
            html.Div([
                html.H4("📊 Data Sources", style={'margin-bottom': '10px', 'color': '#333'}),
                html.P("No data sources tracked for this query.", style={
                    'background-color': '#f5f5f5',
                    'padding': '10px',
                    'border-radius': '5px',
                    'color': '#666'
                })
            ])
        ])
    
    # Get lists of items
    entities = data_sources.get('entities', [])
    relationships = data_sources.get('relationships', [])
    sources = data_sources.get('sources', [])
    text_units = data_sources.get('text_units', [])
    
    # Create summary
    summary_parts = []
    
    if entities:
        entity_ids = [str(e.get('id', 'Unknown')) for e in entities[:10]]
        ids_str = ', '.join(entity_ids)
        if len(entities) > 10:
            ids_str += f", ... +{len(entities) - 10} more"
        summary_parts.append(f"Entities ({ids_str})")
    
    if relationships:
        rel_ids = [str(r.get('id', 'Unknown')) for r in relationships[:10]]
        ids_str = ', '.join(rel_ids)
        if len(relationships) > 10:
            ids_str += f", ... +{len(relationships) - 10} more"
        summary_parts.append(f"Relationships ({ids_str})")
    
    if sources:
        source_ids = [str(s.get('id', 'Unknown')) for s in sources[:10]]
        ids_str = ', '.join(source_ids)
        if len(sources) > 10:
            ids_str += f", ... +{len(sources) - 10} more"
        summary_parts.append(f"Sources ({ids_str})")
    
    summary_text = "Data: " + "; ".join(summary_parts) + "." if summary_parts else "Data: No sources tracked."
    
    # Create expandable sections
    details_sections = []
    
    # Entities section with better formatting
    if entities:
        entity_items = []
        for entity in entities[:20]:
            entity_items.append(
                html.Li([
                    html.Div([
                        html.Strong(f"[{entity.get('id', 'Unknown')}] {entity.get('title', 'Unknown')}"),
                        html.Span(f" ({entity.get('type', 'Unknown')})", 
                                style={'color': '#666', 'font-size': '0.9em'})
                    ]),
                    html.P(entity.get('description', 'No description available')[:200] + '...' 
                          if len(entity.get('description', '')) > 200 else entity.get('description', ''),
                          style={'margin': '5px 0 0 20px', 'color': '#555', 'font-size': '0.9em'})
                ], style={'margin-bottom': '10px', 'list-style': 'none'})
            )
        
        if len(entities) > 20:
            entity_items.append(
                html.Li(f"... and {len(entities) - 20} more entities", 
                       style={'font-style': 'italic', 'color': '#666'})
            )
        
        details_sections.append(
            html.Details([
                html.Summary([
                    html.Span("📊 ", style={'font-size': '1.2em'}),
                    f"Entities Used ({len(entities)})"
                ], style={'cursor': 'pointer', 'font-weight': 'bold', 'padding': '10px 0'}),
                html.Ul(entity_items, style={'padding-left': '20px', 'margin-top': '10px'})
            ], style={'margin': '15px 0', 'border-left': '3px solid #4a90e2', 'padding-left': '10px'})
        )
    
    # Relationships section
    if relationships:
        rel_items = []
        for rel in relationships[:15]:
            rel_items.append(
                html.Li([
                    html.Div([
                        html.Strong(f"[{rel.get('id', 'Unknown')}] "),
                        html.Span(f"{rel.get('source', 'Unknown')} → {rel.get('target', 'Unknown')}", 
                                style={'color': '#2c5aa0'})
                    ]),
                    html.P([
                        html.Em(rel.get('description', 'No description')[:150] + '...' 
                               if len(rel.get('description', '')) > 150 else rel.get('description', '')),
                        html.Span(f" (weight: {rel.get('weight', 0):.2f})", 
                                style={'color': '#666', 'font-size': '0.9em'})
                    ], style={'margin': '5px 0 0 20px', 'color': '#555', 'font-size': '0.9em'})
                ], style={'margin-bottom': '10px', 'list-style': 'none'})
            )
        
        if len(relationships) > 15:
            rel_items.append(
                html.Li(f"... and {len(relationships) - 15} more relationships", 
                       style={'font-style': 'italic', 'color': '#666'})
            )
        
        details_sections.append(
            html.Details([
                html.Summary([
                    html.Span("🔗 ", style={'font-size': '1.2em'}),
                    f"Relationships Used ({len(relationships)})"
                ], style={'cursor': 'pointer', 'font-weight': 'bold', 'padding': '10px 0'}),
                html.Ul(rel_items, style={'padding-left': '20px', 'margin-top': '10px'})
            ], style={'margin': '15px 0', 'border-left': '3px solid #e24a4a', 'padding-left': '10px'})
        )
    
    # Sources section
    if sources:
        source_items = []
        for source in sources[:10]:
            source_items.append(
                html.Li([
                    html.Div([
                        html.Strong(f"[{source.get('id', 'Unknown')}] {source.get('title', 'Unknown')}"),
                        html.Span(f" ({source.get('type', 'document')})", 
                                style={'color': '#666', 'font-size': '0.9em'})
                    ]),
                    html.P(source.get('text_preview', '')[:150] + '...' 
                          if len(source.get('text_preview', '')) > 150 else source.get('text_preview', ''),
                          style={'margin': '5px 0 0 20px', 'color': '#555', 'font-size': '0.9em', 'font-style': 'italic'})
                ], style={'margin-bottom': '10px', 'list-style': 'none'})
            )
        
        if len(sources) > 10:
            source_items.append(
                html.Li(f"... and {len(sources) - 10} more sources", 
                       style={'font-style': 'italic', 'color': '#666'})
            )
        
        details_sections.append(
            html.Details([
                html.Summary([
                    html.Span("📄 ", style={'font-size': '1.2em'}),
                    f"Source Documents ({len(sources)})"
                ], style={'cursor': 'pointer', 'font-weight': 'bold', 'padding': '10px 0'}),
                html.Ul(source_items, style={'padding-left': '20px', 'margin-top': '10px'})
            ], style={'margin': '15px 0', 'border-left': '3px solid #4ae255', 'padding-left': '10px'})
        )
    
    # Combine everything
    return html.Div([
        html.Hr(style={'margin': '20px 0', 'border-top': '1px solid #e0e0e0'}),
        html.Div([
            html.H4("📊 Data Sources", style={'margin-bottom': '15px', 'color': '#333'}),
            html.Div(summary_text, style={
                'background-color': '#f5f5f5',
                'padding': '12px',
                'border-radius': '5px',
                'font-family': 'monospace',
                'font-size': '14px',
                'color': '#333',
                'border': '1px solid #ddd'
            }),
            html.Div(details_sections, style={'margin-top': '20px'})
        ], style={
            'background-color': '#fafafa',
            'padding': '20px',
            'border-radius': '8px',
            'border': '1px solid #e0e0e0'
        })
    ])

# Callback for handling queries
@app.callback(
    [Output("query-results", "children"),
     Output("routing-info", "children"),
     Output("routing-collapse", "is_open"),
     Output("query-history", "children"),
     Output("loading-output", "children")],
    [Input("submit-query", "n_clicks"),
     Input("clear-all", "n_clicks"),
     Input("clear-history", "n_clicks")],
    [State("query-input", "value"),
     State("query-method", "value"),
     State("query-options", "value")]
)
def handle_query(submit_clicks, clear_clicks, clear_history_clicks, query_text, method, options):
    global query_history
    
    # Determine which button was clicked
    triggered = ctx.triggered_id
    
    if triggered == "clear-all":
        return "", "", False, render_query_history(), ""
    
    if triggered == "clear-history":
        query_history = []
        return dash.no_update, dash.no_update, dash.no_update, render_query_history(), ""
    
    if triggered != "submit-query" or not query_text:
        raise PreventUpdate
    
    # Initialize query engine if needed
    global query_engine
    if query_engine is None:
        try:
            query_engine = CityClerkQueryEngine(GRAPHRAG_ROOT)
        except Exception as e:
            return render_error(f"Failed to initialize query engine: {e}"), "", False, dash.no_update, ""
    
    # Show loading message
    loading_msg = html.Div([
        html.H5("🔄 Processing your query..."),
        html.P(f"Query: {query_text[:100]}..."),
        html.P(f"Method: {method}")
    ])
    
    try:
        # Determine method
        if method == "auto":
            # Use router to determine method
            route_info = query_router.determine_query_method(query_text)
            actual_method = route_info['method']
            routing_details = route_info
        else:
            actual_method = method
            routing_details = {"method": method, "params": {}}
        
        # Run query asynchronously
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        # Add options to params
        params = routing_details.get('params', {})
        if "community" not in options:
            params['include_community_context'] = False
        
        # Run the query
        result = loop.run_until_complete(
            query_engine.query(
                query=query_text,
                method=actual_method if method != "auto" else None,
                **params
            )
        )
        
        # Extract data sources
        data_sources = result.get('data_sources', result.get('context_data', {}))
        
        # Format the main answer with proper markdown
        answer_content = dcc.Markdown(
            result.get('answer', 'No response generated.'),
            style={
                'padding': '20px',
                'backgroundColor': '#f8f9fa',
                'borderRadius': '8px',
                'lineHeight': '1.6',
                'whiteSpace': 'pre-wrap'  # Preserve formatting
            }
        )
        
        # Create data sources display if requested
        sources_display = html.Div()
        if "sources" in options:
            sources_display = create_data_sources_display(data_sources)
        
        # Combine results
        results_content = html.Div([
            html.H3("Answer:", style={'marginBottom': '15px'}),
            answer_content,
            sources_display
        ])
        
        # Add to history
        query_history.insert(0, {
            "timestamp": datetime.now(),
            "query": query_text,
            "method": actual_method,
            "auto_routed": method == "auto"
        })
        
        # Limit history to 10 items
        query_history = query_history[:10]
        
        routing_content = render_routing_info(routing_details, actual_method) if "routing" in options else ""
        show_routing = "routing" in options
        
        return results_content, routing_content, show_routing, render_query_history(), ""
        
    except Exception as e:
        log.error(f"Query failed: {e}")
        return render_error(f"Query failed: {str(e)}"), "", False, dash.no_update, ""

def render_results(result, options):
    """Render query results with all source information."""
    
    answer = result.get('answer', 'No answer available')
    sources_info = result.get('sources_info', {})
    entity_chunks = result.get('entity_chunks', [])
    metadata = result.get('routing_metadata', {})
    
    # Clean up the answer (remove metadata lines)
    if isinstance(answer, str):
        lines = answer.split('\n')
        cleaned_lines = [line for line in lines if not line.startswith(('INFO:', 'WARNING:', 'DEBUG:', 'SUCCESS:'))]
        answer = '\n'.join(cleaned_lines).strip()
    
    content = [
        html.H5("📝 Answer:", className="mb-3"),
        dcc.Markdown(answer, className="p-3 bg-light rounded"),
    ]
    
    # Show data sources if requested
    if "sources" in options and sources_info:
        content.extend([
            html.Hr(),
            html.H5("📊 Data Sources:", className="mb-3"),
            render_all_sources(sources_info, entity_chunks)
        ])
    
    # Show verbose metadata if requested
    if "verbose" in options and metadata:
        content.extend([
            html.Hr(),
            html.H6("🔍 Query Metadata:"),
            html.Pre(json.dumps(metadata, indent=2), className="bg-dark text-light p-3 rounded")
        ])
    
    return html.Div(content)

def render_all_sources(sources_info, entity_chunks):
    """Render all source information comprehensively."""
    content = []
    
    # Show raw references first
    raw_refs = sources_info.get('raw_references', {})
    if any(raw_refs.values()):
        ref_text = []
        if raw_refs.get('entities'):
            ref_text.append(f"Entities: {', '.join(raw_refs['entities'])}")
        if raw_refs.get('reports'):
            ref_text.append(f"Reports: {', '.join(raw_refs['reports'])}")
        if raw_refs.get('sources'):
            ref_text.append(f"Sources: {', '.join(raw_refs['sources'])}")
        
        content.append(
            dbc.Alert([
                html.Strong("📋 References in Answer: "),
                html.Br(),
                html.Code(' | '.join(ref_text))
            ], color="info", className="mb-3")
        )
    
    # Show resolved reports (for GLOBAL search)
    reports = sources_info.get('reports', [])
    if reports:
        content.append(html.H6("📑 Community Reports Used:", className="mb-2"))
        for report in reports[:10]:  # Limit to first 10
            content.append(
                dbc.Card([
                    dbc.CardBody([
                        html.Strong(f"Report #{report['id']}"),
                        html.Span(f" (Level {report.get('level', '?')})", className="text-muted"),
                        html.P(report.get('summary', 'No summary available'), 
                               className="small text-muted mt-1 mb-0")
                    ])
                ], className="mb-2", color="light", outline=True)
            )
        if len(reports) > 10:
            content.append(html.P(f"... and {len(reports) - 10} more reports", className="text-muted"))
    
    # Show resolved entities (for LOCAL search)
    entities = sources_info.get('entities', [])
    if entities:
        content.append(html.H6("🎯 Entities Referenced:", className="mb-2 mt-3"))
        for entity in entities[:10]:  # Limit to first 10
            content.append(
                dbc.Card([
                    dbc.CardBody([
                        html.Div([
                            html.Span(entity['type'].upper(), 
                                     className=f"badge bg-{get_entity_color(entity['type'])} me-2"),
                            html.Strong(entity['title']),
                            html.Span(f" (#{entity['id']})", className="text-muted small")
                        ]),
                        html.P(entity.get('description', ''), 
                               className="small text-muted mt-1 mb-0")
                    ])
                ], className="mb-2", color="light", outline=True)
            )
        if len(entities) > 10:
            content.append(html.P(f"... and {len(entities) - 10} more entities", className="text-muted"))
    
    # Show entity chunks (the actual retrieved content)
    if entity_chunks:
        content.append(html.H6("📄 Retrieved Content Chunks:", className="mb-2 mt-3"))
        for chunk in entity_chunks[:5]:  # Show first 5 chunks
            source_info = chunk.get('source', {})
            content.append(
                dbc.Card([
                    dbc.CardBody([
                        html.Div([
                            html.Span(chunk['type'].upper(), 
                                     className=f"badge bg-{get_entity_color(chunk['type'])} me-2"),
                            html.Strong(chunk['title'])
                        ]),
                        html.P(chunk.get('description', '')[:200] + "..." 
                               if len(chunk.get('description', '')) > 200 
                               else chunk.get('description', ''), 
                               className="small mt-2"),
                        html.Hr(className="my-2"),
                        html.Small([
                            html.Strong("Source: "),
                            f"{source_info.get('type', 'Unknown')} - {source_info.get('meeting_date', 'N/A')}",
                            html.Br(),
                            html.Strong("File: "),
                            html.Code(source_info.get('source_file', 'Unknown'), className="small")
                        ], className="text-muted")
                    ])
                ], className="mb-2")
            )
        if len(entity_chunks) > 5:
            content.append(html.P(f"... and {len(entity_chunks) - 5} more chunks", className="text-muted"))
    
    return html.Div(content)

def get_entity_color(entity_type):
    """Get color for entity type badge."""
    color_map = {
        'AGENDA_ITEM': 'primary',
        'ORDINANCE': 'success', 
        'RESOLUTION': 'warning',
        'PERSON': 'info',
        'ORGANIZATION': 'secondary',
        'MEETING': 'danger',
        'DOCUMENT': 'dark'
    }
    return color_map.get(entity_type.upper(), 'light')

def render_entity_card(entity, highlight=False, is_related=False):
    """Render a single entity card with proper formatting."""
    
    # Determine card color based on entity type
    color_map = {
        'AGENDA_ITEM': 'primary',
        'ORDINANCE': 'success',
        'RESOLUTION': 'warning',
        'PERSON': 'info',
        'ORGANIZATION': 'secondary',
        'referenced_entity': 'danger'
    }
    
    border_color = color_map.get(entity.get('entity_type', entity.get('type', '')), 'light')
    
    card_content = [
        html.H6([
            html.Span(
                entity.get('entity_type', entity.get('type', '')).upper(), 
                className=f"badge bg-{border_color} me-2"
            ),
            entity['title'],
            html.Span(
                f" (Entity #{entity.get('id', entity.get('entity_id', ''))})",
                className="text-muted small"
            ) if entity.get('id') or entity.get('entity_id') else ""
        ]),
        html.P(
            entity.get('description', ''), 
            className="text-muted small mb-2",
            style={"maxHeight": "100px", "overflow": "auto"}
        ),
    ]
    
    # Add relationship info if this is a related entity
    if is_related and entity.get('relationship'):
        card_content.insert(1, html.P([
            html.Strong("Relationship: "),
            html.Em(entity['relationship'][:100] + "..." if len(entity['relationship']) > 100 else entity['relationship'])
        ], className="small"))
    
    # Add source document info if available
    source_doc = entity.get('source_document', {})
    if source_doc:
        card_content.append(
            html.Div([
                html.Hr(className="my-2"),
                html.Small([
                    html.Strong("Source: "),
                    f"{source_doc.get('type', 'Document')} - {source_doc.get('meeting_date', 'N/A')}",
                    html.Br(),
                    html.Strong("File: "),
                    html.Code(source_doc.get('source_file', 'Unknown'), className="small")
                ], className="text-muted")
            ])
        )
    
    return dbc.Card(
        dbc.CardBody(card_content),
        className="mb-2",
        color=border_color if highlight else None,
        outline=True,
        style={"border-width": "2px"} if highlight else {}
    )

def render_routing_info(routing_details, actual_method):
    """Render routing analysis information."""
    
    intent = routing_details.get('intent')
    params = routing_details.get('params', {})
    
    content = [
        html.P([
            html.Strong("Selected Method: "),
            html.Span(actual_method.upper(), className="badge bg-primary")
        ]),
    ]
    
    if intent:
        content.append(html.P([
            html.Strong("Detected Intent: "),
            html.Span(intent.value if hasattr(intent, 'value') else str(intent))
        ]))
    
    # Show detected entities
    if 'entity_filter' in params:
        entity = params['entity_filter']
        content.append(html.P([
            html.Strong("Primary Entity: "),
            html.Code(f"{entity['type']}: {entity['value']}")
        ]))
    
    if 'multiple_entities' in params:
        entities = params['multiple_entities']
        content.append(html.P([
            html.Strong("Detected Entities: "),
            html.Ul([
                html.Li(html.Code(f"{e['type']}: {e['value']}"))
                for e in entities
            ])
        ]))
    
    # Show key parameters
    key_params = ['top_k_entities', 'community_level', 'comparison_mode', 'strict_entity_focus']
    param_list = []
    for param in key_params:
        if param in params:
            param_list.append(html.Li(f"{param}: {params[param]}"))
    
    if param_list:
        content.append(html.Div([
            html.Strong("Parameters:"),
            html.Ul(param_list)
        ]))
    
    return html.Div(content)

def render_query_history():
    """Render the query history."""
    if not query_history:
        return html.P("No queries yet", className="text-muted")
    
    history_items = []
    for item in query_history:
        badge_color = "success" if item['auto_routed'] else "info"
        history_items.append(
            html.Li([
                html.Small(item['timestamp'].strftime("%H:%M:%S"), className="text-muted me-2"),
                html.Span(item['method'].upper(), className=f"badge bg-{badge_color} me-2"),
                html.Span(item['query'][:100] + "..." if len(item['query']) > 100 else item['query'])
            ], className="mb-2")
        )
    
    return html.Ul(history_items, className="list-unstyled")

def render_error(error_msg):
    """Render an error message."""
    return dbc.Alert([
        html.H5("❌ Error", className="alert-heading"),
        html.P(error_msg)
    ], color="danger")

# Add some custom CSS
app.index_string = '''
<!DOCTYPE html>
<html>
    <head>
        {%metas%}
        <title>{%title%}</title>
        {%favicon%}
        {%css%}
        <style>
            body {
                background-color: #f8f9fa;
            }
            .card {
                box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            }
            .card-header {
                background-color: #e9ecef;
            }
            pre {
                white-space: pre-wrap;
                word-wrap: break-word;
            }
        </style>
    </head>
    <body>
        {%app_entry%}
        <footer>
            {%config%}
            {%scripts%}
            {%renderer%}
        </footer>
    </body>
</html>
'''

if __name__ == "__main__":
    print("🚀 Starting GraphRAG Query UI...")
    print(f"📁 GraphRAG Root: {GRAPHRAG_ROOT}")
    print("🌐 Open http://localhost:8050 in your browser")
    print("Press Ctrl+C to stop")
    
    app.run(debug=True, host='0.0.0.0', port=8050)


================================================================================


################################################################################
# File: scripts/microsoft_framework/run_graphrag_pipeline.py
################################################################################

# File: scripts/microsoft_framework/run_graphrag_pipeline.py

#!/usr/bin/env python3
"""
Runner script for the City Clerk GraphRAG Pipeline.

This script demonstrates how to run the complete GraphRAG pipeline and view results.
"""

import asyncio
import os
import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

# Detect virtual environment
def get_venv_python():
    """Get the correct Python executable from virtual environment."""
    # Check if we're in a venv
    if sys.prefix != sys.base_prefix:
        return sys.executable
    
    # Try common venv locations
    venv_paths = [
        'venv/bin/python3',
        'venv/bin/python',
        '.venv/bin/python3',
        '.venv/bin/python',
        'city_clerk_rag/bin/python3',
        'city_clerk_rag/bin/python'
    ]
    
    for venv_path in venv_paths:
        full_path = os.path.join(os.getcwd(), venv_path)
        if os.path.exists(full_path):
            return full_path
    
    # Fallback
    return sys.executable

# Use this in all subprocess calls
PYTHON_EXE = get_venv_python()
print(f"🐍 Using Python: {PYTHON_EXE}")

from scripts.microsoft_framework import (
    CityClerkGraphRAGPipeline,
    CityClerkQueryEngine,
    SmartQueryRouter,
    GraphRAGCosmosSync,
    handle_user_query,
    GraphRAGInitializer,
    CityClerkDocumentAdapter,
    CityClerkPromptTuner,
    GraphRAGOutputProcessor
)
from scripts.microsoft_framework.enhanced_entity_deduplicator import EnhancedEntityDeduplicator, DEDUP_CONFIGS

# ============================================================================
# PIPELINE CONTROL FLAGS - Set these to control which modules run
# ============================================================================

# Core Pipeline Steps
RUN_INITIALIZATION = True      # Initialize GraphRAG environment and settings
RUN_DOCUMENT_PREP = True       # Convert extracted JSONs to GraphRAG CSV format
RUN_PROMPT_TUNING = True       # Auto-tune prompts for city clerk domain
RUN_GRAPHRAG_INDEX = True      # Run the actual GraphRAG indexing process

# Post-Processing Steps  
DISPLAY_RESULTS = True         # Show summary of extracted entities/relationships
TEST_QUERIES = True            # Run example queries to test the system
SYNC_TO_COSMOS = False         # Sync GraphRAG results to Cosmos DB

# Advanced Options
FORCE_REINDEX = False          # Force re-indexing even if output exists
VERBOSE_MODE = True            # Show detailed progress information
SKIP_CONFIRMATION = False      # Skip confirmation prompts

# Enhanced Deduplication Control
RUN_DEDUPLICATION = True       
DEDUP_CONFIG = 'conservative'   # CHANGED from 'name_focused' to 'conservative'
DEDUP_CUSTOM_CONFIG = {
    'min_combined_score': 0.8,  # INCREASED from 0.7
    'enable_partial_name_matching': True,
    'enable_abbreviation_matching': True,
    'weights': {
        'string_similarity': 0.3,
        'token_overlap': 0.2,
        'graph_structure': 0.4,  # INCREASED weight on graph structure
        'semantic_similarity': 0.1
    }
}

# ============================================================================

async def main():
    """Main pipeline execution with modular control."""
    
    # Check environment variables
    if not os.getenv("OPENAI_API_KEY"):
        print("❌ Error: OPENAI_API_KEY environment variable not set")
        print("Please set it with: export OPENAI_API_KEY='your-api-key'")
        return
    
    print("🚀 City Clerk GraphRAG Pipeline")
    print("=" * 50)
    print("📋 Module Configuration:")
    print(f"   Initialize Environment: {'✅' if RUN_INITIALIZATION else '⏭️'}")
    print(f"   Prepare Documents:      {'✅' if RUN_DOCUMENT_PREP else '⏭️'}")
    print(f"   Tune Prompts:          {'✅' if RUN_PROMPT_TUNING else '⏭️'}")
    print(f"   Run GraphRAG Index:    {'✅' if RUN_GRAPHRAG_INDEX else '⏭️'}")
    print(f"   Display Results:       {'✅' if DISPLAY_RESULTS else '⏭️'}")
    print(f"   Test Queries:          {'✅' if TEST_QUERIES else '⏭️'}")
    print(f"   Sync to Cosmos:        {'✅' if SYNC_TO_COSMOS else '⏭️'}")
    print("=" * 50)
    
    if not SKIP_CONFIRMATION:
        confirm = input("\nProceed with this configuration? (y/N): ")
        if confirm.lower() not in ['y', 'yes']:
            print("❌ Pipeline cancelled")
            return
    
    try:
        graphrag_root = project_root / "graphrag_data"
        
        # Step 1: Initialize GraphRAG Environment
        if RUN_INITIALIZATION:
            print("\n📋 Step 1: Initializing GraphRAG Environment")
            print("-" * 30)
            
            initializer = GraphRAGInitializer(project_root)
            initializer.setup_environment()
            print("✅ GraphRAG environment initialized")
        else:
            print("\n⏭️  Skipping GraphRAG initialization")
            if not graphrag_root.exists():
                print("❌ GraphRAG root doesn't exist! Enable RUN_INITIALIZATION")
                return
        
        # Step 2: Prepare Documents
        if RUN_DOCUMENT_PREP:
            print("\n📋 Step 2: Preparing Documents for GraphRAG")
            print("-" * 30)
            
            adapter = CityClerkDocumentAdapter(
                project_root / "city_clerk_documents/extracted_text"
            )
            
            # Use JSON files directly for better structure preservation
            df = adapter.prepare_documents_for_graphrag(graphrag_root)
            print(f"✅ Prepared {len(df)} isolated documents for GraphRAG")
            print("   Each agenda item is now a completely separate entity")
        else:
            print("\n⏭️  Skipping document preparation")
            csv_path = graphrag_root / "city_clerk_documents.csv"
            if not csv_path.exists():
                print("❌ No prepared documents found! Enable RUN_DOCUMENT_PREP")
                return
        
        # Step 3: Prompt Tuning
        if RUN_PROMPT_TUNING:
            print("\n📋 Step 3: Tuning Prompts for City Clerk Domain")
            print("-" * 30)
            
            tuner = CityClerkPromptTuner(graphrag_root)
            prompts_dir = graphrag_root / "prompts"

            # If we are forcing a re-index or skipping confirmation, we should always regenerate prompts
            # to ensure the latest versions from the scripts are used.
            if FORCE_REINDEX or SKIP_CONFIRMATION:
                print("📝 Forcing prompt regeneration to apply new rules...")
                if prompts_dir.exists():
                    import shutil
                    shutil.rmtree(prompts_dir)
                tuner.create_manual_prompts()
                print("✅ Prompts regenerated successfully.")
            
            # Original interactive logic for manual runs
            else:
                if prompts_dir.exists() and list(prompts_dir.glob("*.txt")):
                    print("📁 Existing prompts found")
                    reuse = input("Use existing prompts? (Y/n): ")
                    if reuse.lower() != 'n':
                        print("🔄 Re-creating manual prompts...")
                        tuner.create_manual_prompts()
                        print("✅ Prompts created manually")
                    else:
                        print("✅ Using existing prompts")
                else:
                    print("📝 Creating prompts manually...")
                    tuner.create_manual_prompts()
                    print("✅ Prompts created")
        else:
            print("\n⏭️  Skipping prompt tuning")
        
        # Step 4: Run GraphRAG Indexing
        if RUN_GRAPHRAG_INDEX:
            print("\n📋 Step 4: Running GraphRAG Indexing")
            print("-" * 30)
            
            # Check if output already exists
            output_dir = graphrag_root / "output"
            if output_dir.exists() and list(output_dir.glob("*.parquet")):
                print("📁 Existing GraphRAG output found")
                if not FORCE_REINDEX:
                    reindex = input("Re-run indexing? This may take time (y/N): ")
                    if reindex.lower() != 'y':
                        print("✅ Using existing index")
                    else:
                        print("🏗️ Re-indexing documents...")
                        await run_graphrag_indexing(graphrag_root, VERBOSE_MODE)
                else:
                    print("🏗️ Force re-indexing documents...")
                    await run_graphrag_indexing(graphrag_root, VERBOSE_MODE)
            else:
                print("🏗️ Running GraphRAG indexing (this may take several minutes)...")
                await run_graphrag_indexing(graphrag_root, VERBOSE_MODE)
        else:
            print("\n⏭️  Skipping GraphRAG indexing")
        
        # Step 4.5: Enhanced Entity Deduplication
        if RUN_DEDUPLICATION and RUN_GRAPHRAG_INDEX:
            print("\n📋 Step 4.5: Enhanced Entity Deduplication")
            print("-" * 30)
            
            output_dir = graphrag_root / "output"
            if output_dir.exists() and list(output_dir.glob("*.parquet")):
                print(f"🔍 Running enhanced deduplication (config: {DEDUP_CONFIG})")
                
                # Get configuration
                config = DEDUP_CONFIGS.get(DEDUP_CONFIG, {})
                if DEDUP_CUSTOM_CONFIG:
                    config.update(DEDUP_CUSTOM_CONFIG)
                
                print("📊 Deduplication configuration:")
                print(f"   - Partial name matching: {config.get('enable_partial_name_matching', True)}")
                print(f"   - Token matching: {config.get('enable_token_matching', True)}")
                print(f"   - Semantic matching: {config.get('enable_semantic_matching', True)}")
                print(f"   - Min combined score: {config.get('min_combined_score', 0.7)}")
                
                deduplicator = EnhancedEntityDeduplicator(output_dir, config)
                
                try:
                    stats = deduplicator.deduplicate_entities()
                    
                    print(f"\n✅ Enhanced deduplication complete:")
                    print(f"   Original entities: {stats['original_entities']}")
                    print(f"   After deduplication: {stats['merged_entities']}")
                    print(f"   Entities merged: {stats['merged_count']}")
                    
                    if stats['merged_count'] > 0:
                        print(f"\n📁 Deduplicated data saved to: {output_dir}/deduplicated/")
                        print(f"📝 Detailed report: {output_dir}/enhanced_deduplication_report.txt")
                        
                        # Show some examples
                        report_path = output_dir / "enhanced_deduplication_report.txt"
                        if report_path.exists():
                            with open(report_path, 'r') as f:
                                lines = f.readlines()
                                # Find and show first few merges
                                for i, line in enumerate(lines):
                                    if "←" in line and i < len(lines) - 1:
                                        print(f"\n   Example: {line.strip()}")
                                        break
                        
                        # Ask user if they want to use deduplicated data
                        if not SKIP_CONFIRMATION:
                            use_dedup = input("\nUse deduplicated data for queries? (Y/n): ")
                            if use_dedup.lower() != 'n':
                                # Update the output directory for subsequent steps
                                output_dir = output_dir / "deduplicated"
                except Exception as e:
                    print(f"❌ Enhanced deduplication failed: {e}")
                    if VERBOSE_MODE:
                        import traceback
                        traceback.print_exc()
            else:
                print("⏭️  No GraphRAG output to deduplicate")
        else:
            print("\n⏭️  Skipping entity deduplication")
        
        # Step 5: Display Results Summary
        if DISPLAY_RESULTS:
            print("\n📊 Step 5: Results Summary")
            await display_results_summary(project_root)
        else:
            print("\n⏭️  Skipping results display")
        
        # Step 6: Test Queries
        if TEST_QUERIES:
            print("\n🔍 Step 6: Testing Query System")
            await test_queries(project_root)
        else:
            print("\n⏭️  Skipping query testing")
        
        # Step 7: Sync to Cosmos DB
        if SYNC_TO_COSMOS:
            print("\n🌐 Step 7: Syncing to Cosmos DB")
            await sync_to_cosmos(project_root, skip_prompt=SKIP_CONFIRMATION)
        else:
            print("\n⏭️  Skipping Cosmos DB sync")
        
        print("\n✅ Pipeline completed successfully!")
        print("\n📚 Next Steps:")
        print("   - Run queries: python scripts/microsoft_framework/test_queries.py")
        print("   - View results: Check graphrag_data/output/")
        print("   - Sync to Cosmos: Set SYNC_TO_COSMOS = True and re-run")
        
    except Exception as e:
        print(f"\n❌ Error running pipeline: {e}")
        if VERBOSE_MODE:
            import traceback
            traceback.print_exc()

async def run_graphrag_indexing(graphrag_root: Path, verbose: bool = True):
    """Run GraphRAG indexing with optimized concurrency settings."""
    import subprocess
    
    # Set environment variables for better performance
    env = os.environ.copy()
    
    # Increase concurrency based on system capabilities
    cpu_count = os.cpu_count() or 4
    optimal_concurrency = min(cpu_count * 2, 20)  # Increased from 5
    env['GRAPHRAG_CONCURRENCY'] = str(optimal_concurrency)
    
    # Add additional performance settings
    env['GRAPHRAG_CHUNK_PARALLELISM'] = str(optimal_concurrency)
    env['GRAPHRAG_ENTITY_EXTRACTION_PARALLELISM'] = str(optimal_concurrency)
    
    print(f"🚀 Running GraphRAG with concurrency level: {optimal_concurrency}")
    
    cmd = [
        PYTHON_EXE,  # Use the detected venv Python instead of sys.executable
        "-m", "graphrag", "index",
        "--root", str(graphrag_root),
        "--emit", "parquet",  # Use parquet for better performance
    ]
    
    if verbose:
        cmd.append("--verbose")
    
    # Run with optimized environment
    process = subprocess.Popen(
        cmd, 
        stdout=subprocess.PIPE, 
        stderr=subprocess.STDOUT, 
        text=True,
        env=env
    )
    
    # Stream output
    for line in iter(process.stdout.readline, ''):
        if line:
            print(f"   {line.strip()}")
    
    process.wait()
    
    if process.returncode == 0:
        print("✅ GraphRAG indexing completed successfully")
    else:
        raise Exception(f"GraphRAG indexing failed with code {process.returncode}")

async def display_results_summary(project_root: Path):
    """Display summary of GraphRAG results."""
    print("-" * 30)
    
    from scripts.microsoft_framework import GraphRAGOutputProcessor
    
    output_dir = project_root / "graphrag_data/output"
    
    # Check if deduplicated data exists
    dedup_dir = output_dir / "deduplicated"
    if dedup_dir.exists() and list(dedup_dir.glob("*.parquet")):
        print("📊 Using deduplicated data")
        output_dir = dedup_dir
    
    processor = GraphRAGOutputProcessor(output_dir)
    
    # Get summaries
    entity_summary = processor.get_entity_summary()
    relationship_summary = processor.get_relationship_summary()
    
    if entity_summary:
        print(f"🏷️ Entities extracted: {entity_summary.get('total_entities', 0)}")
        print("📋 Entity types:")
        for entity_type, count in entity_summary.get('entity_types', {}).items():
            print(f"   - {entity_type}: {count}")
    
    if relationship_summary:
        print(f"\n🔗 Relationships extracted: {relationship_summary.get('total_relationships', 0)}")
        print("📋 Relationship types:")
        for rel_type, count in relationship_summary.get('relationship_types', {}).items():
            print(f"   - {rel_type}: {count}")
    
    # Show file locations
    print(f"\n📁 Output files location: {output_dir}")
    output_files = [
        "entities.parquet",
        "relationships.parquet", 
        "communities.parquet",
        "community_reports.parquet"
    ]
    
    for filename in output_files:
        file_path = output_dir / filename
        if file_path.exists():
            size = file_path.stat().st_size / 1024  # KB
            print(f"   ✅ {filename} ({size:.1f} KB)")
        else:
            print(f"   ❌ {filename} (not found)")

async def test_queries(project_root: Path):
    """Test the query system with example queries."""
    print("-" * 30)
    
    # Example queries for city clerk documents
    test_queries = [
        "Who is Commissioner Smith?",  # Should use Local search
        "What are the main themes in city development?",  # Should use Global search
        "How has the waterfront project evolved?",  # Should use DRIFT search
        "Tell me about ordinance 2024-01",  # Should use Local search
        "What are the overall budget trends?",  # Should use Global search
    ]
    
    query_engine = CityClerkQueryEngine(project_root / "graphrag_data")
    router = SmartQueryRouter()
    
    for query in test_queries:
        print(f"\n❓ Query: '{query}'")
        
        # Show routing decision
        route_info = router.determine_query_method(query)
        print(f"🎯 Router selected: {route_info['method']} ({route_info['intent'].value})")
        
        try:
            # Execute query
            result = await query_engine.query(query)
            print(f"📝 Answer preview: {result['answer'][:200]}...")
            print(f"🔧 Parameters used: {result['parameters']}")
        except Exception as e:
            print(f"❌ Query failed: {e}")

async def sync_to_cosmos(project_root: Path, skip_prompt: bool = False):
    """Optionally sync results to Cosmos DB."""
    print("-" * 30)
    
    if not skip_prompt:
        user_input = input("Do you want to sync GraphRAG results to Cosmos DB? (y/N): ")
        if user_input.lower() not in ['y', 'yes']:
            print("⏭️ Skipping Cosmos DB sync")
            return
    
    try:
        output_dir = project_root / "graphrag_data/output"
        sync = GraphRAGCosmosSync(output_dir)
        await sync.sync_to_cosmos()
        print("✅ Successfully synced to Cosmos DB")
    except Exception as e:
        print(f"❌ Cosmos DB sync failed: {e}")

def show_usage():
    """Show usage instructions."""
    print("""
🚀 City Clerk GraphRAG Pipeline Runner

CONTROL FLAGS:
   Edit the boolean flags at the top of this file to control which modules run:
   
   RUN_INITIALIZATION - Initialize GraphRAG environment
   RUN_DOCUMENT_PREP - Convert documents to CSV format
   RUN_PROMPT_TUNING - Auto-tune prompts
   RUN_GRAPHRAG_INDEX - Run indexing process
   RUN_DEDUPLICATION - Apply enhanced entity deduplication
   DEDUP_CONFIG - Deduplication preset: 'aggressive', 'conservative', 'name_focused'
   DISPLAY_RESULTS - Show summary statistics
   TEST_QUERIES - Test example queries
   SYNC_TO_COSMOS - Sync to Cosmos DB
   
USAGE:
   python3 scripts/microsoft_framework/run_graphrag_pipeline.py [options]
   
OPTIONS:
   -h, --help     Show this help message
   --force        Force re-indexing (sets FORCE_REINDEX=True)
   --quiet        Minimal output (sets VERBOSE_MODE=False)
   --yes          Skip confirmations (sets SKIP_CONFIRMATION=True)
   --cosmos       Enable Cosmos sync (sets SYNC_TO_COSMOS=True)
   --dedup-config TYPE  Set deduplication config (aggressive/conservative/name_focused)
   --no-dedup     Disable entity deduplication

EXAMPLES:
   # Run with default settings
   python3 scripts/microsoft_framework/run_graphrag_pipeline.py
   
   # Force complete re-index
   python3 scripts/microsoft_framework/run_graphrag_pipeline.py --force --yes
   
   # Just test queries (edit flags to disable other steps)
   python3 scripts/microsoft_framework/run_graphrag_pipeline.py
    """)

if __name__ == "__main__":
    # Parse command line arguments
    if len(sys.argv) > 1:
        for arg in sys.argv[1:]:
            if arg in ['-h', '--help', 'help']:
                show_usage()
                sys.exit(0)
            elif arg == '--force':
                FORCE_REINDEX = True
            elif arg == '--quiet':
                VERBOSE_MODE = False
            elif arg == '--yes':
                SKIP_CONFIRMATION = True
            elif arg == '--cosmos':
                SYNC_TO_COSMOS = True
            elif arg.startswith('--dedup-config='):
                DEDUP_CONFIG = arg.split('=')[1]
            elif arg == '--no-dedup':
                RUN_DEDUPLICATION = False
    
    # Run the pipeline
    asyncio.run(main())


================================================================================


################################################################################
# File: scripts/microsoft_framework/entity_deduplicator.py
################################################################################

# File: scripts/microsoft_framework/entity_deduplicator.py

#!/usr/bin/env python3
"""
Advanced Entity Deduplication for GraphRAG output.

This module provides sophisticated entity deduplication capabilities that go beyond
simple string matching by considering:
- String similarity using multiple algorithms
- Graph-based clustering coefficients
- Entity relationship patterns
- Configurable merging strategies
"""

import pandas as pd
import networkx as nx
from pathlib import Path
import difflib
from typing import Dict, List, Tuple, Set, Any, Optional
import logging
import json
from datetime import datetime

logger = logging.getLogger(__name__)


class AdvancedEntityDeduplicator:
    """
    Advanced entity deduplication using multiple similarity metrics and graph analysis.
    """
    
    def __init__(
        self,
        output_dir: Path,
        similarity_threshold: float = 0.95,
        clustering_tolerance: float = 0.1,
        merge_strategy: str = "keep_most_connected"
    ):
        """
        Initialize the entity deduplicator.
        
        Args:
            output_dir: Path to GraphRAG output directory
            similarity_threshold: Minimum string similarity for merging (0-1)
            clustering_tolerance: Maximum difference in clustering coefficients
            merge_strategy: Strategy for which entity to keep ("keep_most_connected" or "keep_first")
        """
        self.output_dir = Path(output_dir)
        self.similarity_threshold = similarity_threshold
        self.clustering_tolerance = clustering_tolerance
        self.merge_strategy = merge_strategy
        
        # Initialize data containers
        self.entities_df = None
        self.relationships_df = None
        self.graph = None
        self.merge_groups = []
        self.merge_report = {
            'timestamp': datetime.now().isoformat(),
            'parameters': {
                'similarity_threshold': similarity_threshold,
                'clustering_tolerance': clustering_tolerance,
                'merge_strategy': merge_strategy
            },
            'merges': []
        }
        
        logger.info(f"Initialized deduplicator for {output_dir}")
        logger.info(f"Settings: threshold={similarity_threshold}, tolerance={clustering_tolerance}")
    
    def deduplicate_entities(self) -> Dict[str, Any]:
        """
        Main deduplication process.
        
        Returns:
            Dictionary with deduplication statistics
        """
        logger.info("Starting entity deduplication process")
        
        # Load data
        self._load_data()
        original_entity_count = len(self.entities_df)
        
        # Build graph for analysis
        self._build_graph()
        
        # Calculate clustering coefficients
        clustering_coeffs = self._calculate_clustering_coefficients()
        
        # Find similar entities
        similar_groups = self._find_similar_entities(clustering_coeffs)
        
        # Merge entities
        merged_entities_df = self._merge_entities(similar_groups)
        
        # Save results
        self._save_deduplicated_data(merged_entities_df)
        self._save_merge_report()
        
        # Calculate statistics
        final_entity_count = len(merged_entities_df)
        merged_count = original_entity_count - final_entity_count
        
        stats = {
            'original_entities': original_entity_count,
            'merged_entities': final_entity_count,
            'merged_count': merged_count,
            'merge_groups': len(similar_groups),
            'output_dir': str(self.output_dir / "deduplicated")
        }
        
        logger.info(f"Deduplication complete: {original_entity_count} -> {final_entity_count} entities")
        return stats
    
    def _load_data(self):
        """Load GraphRAG entities and relationships data."""
        entities_path = self.output_dir / "entities.parquet"
        relationships_path = self.output_dir / "relationships.parquet"
        
        if not entities_path.exists():
            raise FileNotFoundError(f"Entities file not found: {entities_path}")
        if not relationships_path.exists():
            raise FileNotFoundError(f"Relationships file not found: {relationships_path}")
        
        self.entities_df = pd.read_parquet(entities_path)
        self.relationships_df = pd.read_parquet(relationships_path)
        
        logger.info(f"Loaded {len(self.entities_df)} entities and {len(self.relationships_df)} relationships")
    
    def _build_graph(self):
        """Build NetworkX graph from relationships for analysis."""
        self.graph = nx.Graph()
        
        # Add entities as nodes
        for _, entity in self.entities_df.iterrows():
            self.graph.add_node(entity['title'], **entity.to_dict())
        
        # Add relationships as edges
        for _, rel in self.relationships_df.iterrows():
            if 'source' in rel and 'target' in rel:
                self.graph.add_edge(rel['source'], rel['target'], **rel.to_dict())
        
        logger.info(f"Built graph with {self.graph.number_of_nodes()} nodes and {self.graph.number_of_edges()} edges")
    
    def _calculate_clustering_coefficients(self) -> Dict[str, float]:
        """
        Calculate clustering coefficients for all entities.
        
        Returns:
            Dictionary mapping entity names to clustering coefficients
        """
        clustering_coeffs = {}
        
        for entity_name in self.entities_df['title']:
            if entity_name in self.graph:
                clustering_coeffs[entity_name] = nx.clustering(self.graph, entity_name)
            else:
                clustering_coeffs[entity_name] = 0.0
        
        logger.info(f"Calculated clustering coefficients for {len(clustering_coeffs)} entities")
        return clustering_coeffs
    
    def _find_similar_entities(self, clustering_coeffs: Dict[str, float]) -> List[List[str]]:
        """
        Find groups of similar entities based on string similarity and clustering coefficients.
        
        Args:
            clustering_coeffs: Dictionary of clustering coefficients
            
        Returns:
            List of groups, where each group is a list of similar entity names
        """
        similar_groups = []
        processed_entities = set()
        
        entity_names = self.entities_df['title'].tolist()
        
        for i, entity1 in enumerate(entity_names):
            if entity1 in processed_entities:
                continue
            
            current_group = [entity1]
            
            for j, entity2 in enumerate(entity_names[i+1:], i+1):
                if entity2 in processed_entities:
                    continue
                
                # Check string similarity
                similarity = self._string_similarity(entity1, entity2)
                
                if similarity >= self.similarity_threshold:
                    # Check clustering coefficient similarity
                    coeff1 = clustering_coeffs.get(entity1, 0.0)
                    coeff2 = clustering_coeffs.get(entity2, 0.0)
                    coeff_diff = abs(coeff1 - coeff2)
                    
                    if coeff_diff <= self.clustering_tolerance:
                        current_group.append(entity2)
                        processed_entities.add(entity2)
            
            if len(current_group) > 1:
                similar_groups.append(current_group)
                for entity in current_group:
                    processed_entities.add(entity)
            else:
                processed_entities.add(entity1)
        
        logger.info(f"Found {len(similar_groups)} groups of similar entities")
        for i, group in enumerate(similar_groups):
            logger.debug(f"Group {i+1}: {group}")
        
        return similar_groups
    
    def _string_similarity(self, str1: str, str2: str) -> float:
        """
        Calculate string similarity using SequenceMatcher.
        
        Args:
            str1: First string
            str2: Second string
            
        Returns:
            Similarity score between 0 and 1
        """
        # Normalize strings for comparison
        str1_norm = str1.lower().strip()
        str2_norm = str2.lower().strip()
        
        # Use SequenceMatcher for similarity
        similarity = difflib.SequenceMatcher(None, str1_norm, str2_norm).ratio()
        
        return similarity
    
    def _merge_entities(self, similar_groups: List[List[str]]) -> pd.DataFrame:
        """
        Merge similar entities based on the configured strategy.
        
        Args:
            similar_groups: List of groups of similar entity names
            
        Returns:
            DataFrame with merged entities
        """
        merged_df = self.entities_df.copy()
        entities_to_remove = set()
        
        for group in similar_groups:
            if len(group) <= 1:
                continue
            
            # Determine which entity to keep
            if self.merge_strategy == "keep_most_connected":
                # Keep entity with highest degree centrality
                degrees = {entity: self.graph.degree(entity) if entity in self.graph else 0 
                          for entity in group}
                primary_entity = max(degrees, key=degrees.get)
            else:  # keep_first
                primary_entity = group[0]
            
            # Get entities to merge
            entities_to_merge = [e for e in group if e != primary_entity]
            
            # Update primary entity description to include merged information
            primary_row = merged_df[merged_df['title'] == primary_entity].iloc[0]
            merged_descriptions = [primary_row.get('description', '')]
            
            for entity in entities_to_merge:
                entity_row = merged_df[merged_df['title'] == entity].iloc[0]
                entity_desc = entity_row.get('description', '')
                if entity_desc and entity_desc not in merged_descriptions:
                    merged_descriptions.append(entity_desc)
                
                # Mark for removal
                entities_to_remove.add(entity)
            
            # Update primary entity description
            if len(merged_descriptions) > 1:
                merged_desc = f"{merged_descriptions[0]} [MERGED: Contains information from {', '.join(entities_to_merge)}]"
                merged_df.loc[merged_df['title'] == primary_entity, 'description'] = merged_desc
            
            # Record merge for report
            self.merge_report['merges'].append({
                'primary_entity': primary_entity,
                'merged_entities': entities_to_merge,
                'merge_reason': f"String similarity >= {self.similarity_threshold}",
                'strategy': self.merge_strategy
            })
        
        # Remove merged entities
        merged_df = merged_df[~merged_df['title'].isin(entities_to_remove)]
        
        logger.info(f"Merged {len(entities_to_remove)} entities into {len(similar_groups)} primary entities")
        return merged_df
    
    def _save_deduplicated_data(self, merged_entities_df: pd.DataFrame):
        """
        Save deduplicated data to output directory.
        
        Args:
            merged_entities_df: DataFrame with merged entities
        """
        output_subdir = self.output_dir / "deduplicated"
        output_subdir.mkdir(exist_ok=True)
        
        # Save merged entities
        entities_output = output_subdir / "entities.parquet"
        merged_entities_df.to_parquet(entities_output, index=False)
        
        # Copy other files unchanged
        other_files = [
            "relationships.parquet",
            "communities.parquet", 
            "community_reports.parquet"
        ]
        
        for filename in other_files:
            source_path = self.output_dir / filename
            target_path = output_subdir / filename
            
            if source_path.exists():
                import shutil
                shutil.copy2(source_path, target_path)
        
        logger.info(f"Saved deduplicated data to {output_subdir}")
    
    def _save_merge_report(self):
        """Save detailed merge report."""
        report_path = self.output_dir / "deduplicated" / "merge_report.json"
        
        with open(report_path, 'w') as f:
            json.dump(self.merge_report, f, indent=2)
        
        logger.info(f"Saved merge report to {report_path}")


# Main execution for standalone testing
if __name__ == "__main__":
    import argparse
    import sys
    from pathlib import Path
    
    # Add project root to path
    project_root = Path(__file__).parent.parent.parent
    sys.path.append(str(project_root))
    
    # Setup logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    parser = argparse.ArgumentParser(description='Run advanced entity deduplication')
    parser.add_argument('--output-dir', '-o', type=str, default='graphrag_data/output',
                       help='Path to GraphRAG output directory')
    parser.add_argument('--threshold', '-t', type=float, default=0.95,
                       help='String similarity threshold (0-1)')
    parser.add_argument('--tolerance', type=float, default=0.1,
                       help='Clustering coefficient tolerance')
    parser.add_argument('--strategy', choices=['keep_most_connected', 'keep_first'],
                       default='keep_most_connected', help='Merge strategy')
    
    args = parser.parse_args()
    
    output_dir = project_root / args.output_dir
    
    if not output_dir.exists():
        print(f"❌ Output directory not found: {output_dir}")
        sys.exit(1)
    
    print(f"🔍 Running entity deduplication on {output_dir}")
    print(f"   Similarity threshold: {args.threshold}")
    print(f"   Clustering tolerance: {args.tolerance}")
    print(f"   Merge strategy: {args.strategy}")
    
    try:
        deduplicator = AdvancedEntityDeduplicator(
            output_dir,
            similarity_threshold=args.threshold,
            clustering_tolerance=args.tolerance,
            merge_strategy=args.strategy
        )
        
        stats = deduplicator.deduplicate_entities()
        
        print(f"\n✅ Deduplication complete!")
        print(f"   Original entities: {stats['original_entities']}")
        print(f"   After deduplication: {stats['merged_entities']}")
        print(f"   Entities merged: {stats['merged_count']}")
        print(f"   Merge groups: {stats['merge_groups']}")
        print(f"   Output saved to: {stats['output_dir']}")
        
    except Exception as e:
        print(f"❌ Deduplication failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


================================================================================


################################################################################
# File: scripts/graph_stages/cosmos_db_client.py
################################################################################

# File: scripts/graph_stages/cosmos_db_client.py

"""
Azure Cosmos DB Gremlin client for city clerk graph database.
Provides async operations for graph manipulation.
"""

from __future__ import annotations
import asyncio
import logging
from typing import Any, Dict, List, Optional, Union
import os
from gremlin_python.driver import client, serializer
from gremlin_python.driver.driver_remote_connection import DriverRemoteConnection
from gremlin_python.structure.graph import Graph
from dotenv import load_dotenv
import json

load_dotenv()

log = logging.getLogger('cosmos_graph_client')


class CosmosGraphClient:
    """Async client for Azure Cosmos DB Gremlin API."""
    
    def __init__(self, 
                 endpoint: Optional[str] = None,
                 key: Optional[str] = None,
                 database: Optional[str] = None,
                 container: Optional[str] = None,
                 partition_value: str = "demo"):
        """Initialize Cosmos DB client."""
        self.endpoint = endpoint or os.getenv("COSMOS_ENDPOINT")
        self.key = key or os.getenv("COSMOS_KEY")
        self.database = database or os.getenv("COSMOS_DATABASE", "cgGraph")
        self.container = container or os.getenv("COSMOS_CONTAINER", "cityClerk")
        self.partition_value = partition_value
        
        if not all([self.endpoint, self.key, self.database, self.container]):
            raise ValueError("Missing required Cosmos DB configuration")
        
        self._client = None
        self._loop = None  # Don't get the loop in __init__
    
    async def connect(self) -> None:
        """Establish connection to Cosmos DB."""
        try:
            # Get the current running loop
            self._loop = asyncio.get_running_loop()
            
            self._client = client.Client(
                f"{self.endpoint}/gremlin",
                "g",
                username=f"/dbs/{self.database}/colls/{self.container}",
                password=self.key,
                message_serializer=serializer.GraphSONSerializersV2d0()
            )
            log.info(f"✅ Connected to Cosmos DB: {self.database}/{self.container}")
        except Exception as e:
            log.error(f"❌ Failed to connect to Cosmos DB: {e}")
            raise
    
    async def _execute_query(self, query: str, bindings: Optional[Dict] = None) -> List[Any]:
        """Execute a Gremlin query asynchronously."""
        if not self._client:
            await self.connect()
        
        try:
            # Get the current event loop
            loop = asyncio.get_running_loop()
            
            # Run synchronous operation in thread pool
            future = loop.run_in_executor(
                None,
                lambda: self._client.submit(query, bindings or {})
            )
            callback = await future
            
            # Collect results
            results = []
            for result in callback:
                results.extend(result)
            
            return results
        except Exception as e:
            log.error(f"Query execution failed: {query[:100]}... Error: {e}")
            raise
    
    async def clear_graph(self) -> None:
        """Clear all vertices and edges from the graph."""
        log.warning("🗑️  Clearing entire graph...")
        try:
            # Drop all vertices (edges are automatically removed)
            await self._execute_query("g.V().drop()")
            log.info("✅ Graph cleared successfully")
        except Exception as e:
            log.error(f"Failed to clear graph: {e}")
            raise
    
    async def create_vertex(self, 
                          label: str,
                          vertex_id: str,
                          properties: Dict[str, Any],
                          update_if_exists: bool = True) -> None:
        """Create a vertex with properties, optionally updating if exists."""
        
        # Check if vertex already exists
        if await self.vertex_exists(vertex_id):
            if update_if_exists:
                # Update existing vertex
                await self.update_vertex(vertex_id, properties)
                log.info(f"Updated existing vertex: {vertex_id}")
            else:
                log.info(f"Vertex already exists, skipping: {vertex_id}")
            return
        
        # Build property chain
        prop_chain = ""
        for key, value in properties.items():
            if value is not None:
                # Handle different value types
                if isinstance(value, bool):
                    prop_chain += f".property('{key}', {str(value).lower()})"
                elif isinstance(value, (int, float)):
                    prop_chain += f".property('{key}', {value})"
                elif isinstance(value, list):
                    # Convert list to JSON string
                    json_val = json.dumps(value).replace("'", "\\'")
                    prop_chain += f".property('{key}', '{json_val}')"
                else:
                    # Escape string values
                    escaped_val = str(value).replace("'", "\\'").replace('"', '\\"')
                    prop_chain += f".property('{key}', '{escaped_val}')"
        
        # Always add partition key
        prop_chain += f".property('partitionKey', '{self.partition_value}')"
        
        query = f"g.addV('{label}').property('id', '{vertex_id}'){prop_chain}"
        
        await self._execute_query(query)

    async def update_vertex(self, vertex_id: str, properties: Dict[str, Any]) -> None:
        """Update properties of an existing vertex."""
        # Build property update chain
        prop_chain = ""
        for key, value in properties.items():
            if value is not None:
                if isinstance(value, bool):
                    prop_chain += f".property('{key}', {str(value).lower()})"
                elif isinstance(value, (int, float)):
                    prop_chain += f".property('{key}', {value})"
                elif isinstance(value, list):
                    json_val = json.dumps(value).replace("'", "\\'")
                    prop_chain += f".property('{key}', '{json_val}')"
                else:
                    escaped_val = str(value).replace("'", "\\'").replace('"', '\\"')
                    prop_chain += f".property('{key}', '{escaped_val}')"
        
        query = f"g.V('{vertex_id}'){prop_chain}"
        
        try:
            await self._execute_query(query)
            log.info(f"Updated vertex {vertex_id}")
        except Exception as e:
            log.error(f"Failed to update vertex {vertex_id}: {e}")
            raise

    async def upsert_vertex(self, 
                           label: str,
                           vertex_id: str,
                           properties: Dict[str, Any]) -> bool:
        """Create or update a vertex. Returns True if created, False if updated."""
        # Check if vertex exists
        if await self.vertex_exists(vertex_id):
            await self.update_vertex(vertex_id, properties)
            return False  # Updated
        else:
            await self.create_vertex(label, vertex_id, properties)
            return True  # Created

    async def create_edge(self,
                         from_id: str,
                         to_id: str,
                         edge_type: str,
                         properties: Optional[Dict[str, Any]] = None) -> None:
        """Create an edge between two vertices."""
        # Build property chain for edge
        prop_chain = ""
        if properties:
            for key, value in properties.items():
                if value is not None:
                    if isinstance(value, bool):
                        prop_chain += f".property('{key}', {str(value).lower()})"
                    elif isinstance(value, (int, float)):
                        prop_chain += f".property('{key}', {value})"
                    else:
                        escaped_val = str(value).replace("'", "\\'")
                        prop_chain += f".property('{key}', '{escaped_val}')"
        
        query = f"g.V('{from_id}').addE('{edge_type}').to(g.V('{to_id}')){prop_chain}"
        
        try:
            await self._execute_query(query)
        except Exception as e:
            log.error(f"Failed to create edge {from_id} -> {to_id}: {e}")
            raise
    
    async def create_edge_if_not_exists(self,
                                       from_id: str,
                                       to_id: str,
                                       edge_type: str,
                                       properties: Optional[Dict[str, Any]] = None) -> bool:
        """Create an edge if it doesn't already exist. Returns True if created."""
        # Check if edge already exists
        check_query = f"g.V('{from_id}').outE('{edge_type}').where(inV().hasId('{to_id}')).count()"
        
        try:
            result = await self._execute_query(check_query)
            exists = result[0] > 0 if result else False
            
            if not exists:
                await self.create_edge(from_id, to_id, edge_type, properties)
                return True
            else:
                log.debug(f"Edge already exists: {from_id} -[{edge_type}]-> {to_id}")
                return False
        except Exception as e:
            log.error(f"Failed to check/create edge: {e}")
            raise
    
    async def vertex_exists(self, vertex_id: str) -> bool:
        """Check if a vertex exists."""
        result = await self._execute_query(f"g.V('{vertex_id}').count()")
        return result[0] > 0 if result else False
    
    async def get_vertex(self, vertex_id: str) -> Optional[Dict]:
        """Get a vertex by ID."""
        result = await self._execute_query(f"g.V('{vertex_id}').valueMap(true)")
        return result[0] if result else None
    
    async def close(self) -> None:
        """Close the connection properly."""
        if self._client:
            try:
                # Close the client synchronously since it's not async
                self._client.close()
                self._client = None
                log.info("Connection closed")
            except Exception as e:
                log.warning(f"Error during client close: {e}")
    
    async def __aenter__(self):
        await self.connect()
        return self
    
    async def __aexit__(self, *args):
        await self.close()


================================================================================


################################################################################
# File: scripts/microsoft_framework/README.md
################################################################################

<!-- 
File: scripts/microsoft_framework/README.md
 -->

# City Clerk GraphRAG System

Microsoft GraphRAG integration for city clerk document processing with advanced entity extraction, community detection, and intelligent query routing.

## 🚀 Quick Start

### 1. Set up your environment:
```bash
export OPENAI_API_KEY='your-api-key-here'
```

### 2. Run the complete pipeline:
```bash
./run_graphrag.sh run
```

### 3. Test queries interactively:
```bash
./run_graphrag.sh query
```

## 📋 Prerequisites

1. **Environment Variables**:
   ```bash
   export OPENAI_API_KEY='your-openai-api-key'
   ```

2. **Extracted Documents**: 
   - City clerk documents should be extracted as JSON files in `city_clerk_documents/extracted_text/`
   - Run your existing document extraction pipeline first

3. **Dependencies**:
   - Python 3.8+
   - GraphRAG library
   - All dependencies in `requirements.txt`

## 🛠️ Installation & Setup

### Option 1: Using the Shell Script (Recommended)
```bash
# Setup environment and dependencies
./run_graphrag.sh setup

# Run the complete pipeline
./run_graphrag.sh run
```

### Option 2: Manual Setup
```bash
# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Run pipeline
python3 scripts/microsoft_framework/run_graphrag_pipeline.py
```

## 🔍 Query System

The system supports three types of queries with automatic routing:

### 1. 🎯 Local Search (Entity-Specific)
Best for specific entities and their immediate relationships:
```
"Who is Commissioner Smith?"
"What is ordinance 2024-01?"
"Tell me about agenda item E-1"
```

### 2. 🌐 Global Search (Holistic)
Best for broad themes and dataset-wide analysis:
```
"What are the main themes in city development?"
"Summarize all budget discussions"
"Overall trends in housing policy"
```

### 3. 🔄 DRIFT Search (Temporal/Complex)
Best for temporal changes and complex exploratory queries:
```
"How has the waterfront project evolved?"
"Timeline of budget decisions"
"Development of housing policy over time"
```

## 📊 Pipeline Steps

The GraphRAG pipeline includes:

1. **🔧 Environment Setup** - Initialize GraphRAG with city clerk configuration
2. **📄 Document Adaptation** - Convert extracted JSON documents to GraphRAG format
3. **🎯 Prompt Tuning** - Auto-tune prompts for city government domain
4. **🏗️ GraphRAG Indexing** - Extract entities, relationships, and communities
5. **📊 Results Processing** - Load and summarize GraphRAG outputs
6. **🔍 Query Testing** - Test with example queries
7. **🌐 Cosmos DB Sync** - Optionally sync to existing Cosmos DB

## 📁 Output Structure

After running the pipeline, you'll find:

```
graphrag_data/
├── settings.yaml           # GraphRAG configuration
├── city_clerk_documents.csv # Input documents in GraphRAG format
├── prompts/                # Auto-tuned prompts
│   ├── entity_extraction.txt
│   └── community_report.txt
└── output/                 # GraphRAG results
    ├── entities.parquet    # Extracted entities
    ├── relationships.parquet # Entity relationships
    ├── communities.parquet # Community clusters
    └── community_reports.parquet # Community summaries
```

## 🎮 Usage Examples

### Run Complete Pipeline
```bash
./run_graphrag.sh run
```

### Interactive Query Session
```bash
./run_graphrag.sh query
```

### View Results Summary
```bash
./run_graphrag.sh results
```

### Clean Up Data
```bash
./run_graphrag.sh clean
```

### Example Queries to Try

**Entity-specific (Local Search):**
- "Who is Mayor Johnson?"
- "What is resolution 2024-15?"
- "Tell me about the parks department"

**Holistic (Global Search):**
- "What are the main development themes?"
- "Summarize all transportation discussions"
- "Overall budget allocation patterns"

**Temporal (DRIFT Search):**
- "How has zoning policy evolved?"
- "Timeline of infrastructure projects"
- "Development of affordable housing initiatives"

## ⚙️ Configuration

### Model Settings
The system is configured to use:
- **Model**: `gpt-4.1-mini-2025-04-14`
- **Max Tokens**: `32768`
- **Temperature**: `0` (deterministic)

### Entity Types
Configured for city government entities:
- `person`, `organization`, `location`
- `document`, `meeting`, `agenda_item`
- `money`, `project`, `ordinance`, `resolution`, `contract`

### Query Configuration
- **Global Search**: Community-level analysis with dynamic selection
- **Local Search**: Top-K entity retrieval with community context
- **DRIFT Search**: Iterative exploration with follow-up expansion

## 🔧 Advanced Usage

### Python API
```python
from scripts.microsoft_framework import CityClerkQueryEngine

# Initialize query engine
engine = CityClerkQueryEngine("./graphrag_data")

# Auto-routed query
result = await engine.query("What are the main budget themes?")

# Specific method
result = await engine.query(
    "Who is Commissioner Smith?", 
    method="local",
    top_k_entities=15
)
```

### Custom Query Routing
```python
from scripts.microsoft_framework import SmartQueryRouter

router = SmartQueryRouter()
route_info = router.determine_query_method("Your query here")
print(f"Recommended method: {route_info['method']}")
```

### Cosmos DB Integration
```python
from scripts.microsoft_framework import GraphRAGCosmosSync

sync = GraphRAGCosmosSync("./graphrag_data/output")
await sync.sync_to_cosmos()
```

## 📈 Performance Tips

1. **Incremental Processing**: Use `IncrementalGraphRAGProcessor` for new documents
2. **Community Levels**: Adjust community levels for different query scopes
3. **Query Optimization**: Use specific entity names and agenda codes when known
4. **Batch Processing**: Process documents in batches for large datasets

## 🐛 Troubleshooting

### Common Issues

**GraphRAG not found:**
```bash
pip install graphrag
```

**No documents found:**
- Ensure documents are in `city_clerk_documents/extracted_text/`
- Run document extraction pipeline first

**API Key issues:**
```bash
export OPENAI_API_KEY='your-key-here'
```

**Memory issues:**
- Reduce `max_tokens` in settings
- Process documents in smaller batches

### Debug Mode
```bash
# Run with verbose output
python3 scripts/microsoft_framework/run_graphrag_pipeline.py --verbose

# Check GraphRAG logs
tail -f graphrag_data/logs/*.log
```

## 🤝 Integration with Existing System

This GraphRAG system integrates seamlessly with your existing infrastructure:

- **Reuses**: Docling PDF extraction, URL preservation, Cosmos DB client
- **Extends**: Adds advanced entity extraction and community detection
- **Maintains**: Existing graph schema and document processing pipeline
- **Enhances**: Query capabilities with intelligent routing

## 📚 More Information

- [Microsoft GraphRAG Documentation](https://microsoft.github.io/graphrag/)
- [Query Configuration Guide](./city_clerk_settings_template.yaml)
- [Entity Types and Prompts](./prompt_tuner.py)

---

For issues or questions, check the troubleshooting section or review the pipeline logs in `graphrag_data/logs/`.


================================================================================


################################################################################
# File: scripts/extract_all_pdfs_direct.py
################################################################################

# File: scripts/extract_all_pdfs_direct.py

#!/usr/bin/env python3
"""
Direct extraction of all PDFs without relying on specific directory structure.
"""

import asyncio
from pathlib import Path
import logging
import re

from graph_stages.pdf_extractor import PDFExtractor
from graph_stages.agenda_pdf_extractor import AgendaPDFExtractor
from graph_stages.enhanced_document_linker import EnhancedDocumentLinker
from graph_stages.verbatim_transcript_linker import VerbatimTranscriptLinker

logging.basicConfig(level=logging.INFO)
log = logging.getLogger(__name__)

async def extract_all_pdfs():
    """Extract all PDFs found in city clerk documents."""
    
    base_dir = Path("city_clerk_documents/global")
    
    # Find ALL PDFs recursively
    all_pdfs = list(base_dir.rglob("*.pdf"))
    log.info(f"Found {len(all_pdfs)} total PDFs")
    
    # Categorize PDFs by type
    agendas = []
    ordinances = []
    resolutions = []
    verbatims = []
    unknown = []
    
    for pdf in all_pdfs:
        path_str = str(pdf).lower()
        filename = pdf.name.lower()
        
        if 'agenda' in filename and filename.startswith('agenda'):
            agendas.append(pdf)
        elif 'ordinance' in path_str:
            ordinances.append(pdf)
        elif 'resolution' in path_str:
            resolutions.append(pdf)
        elif 'verbat' in path_str or 'transcript' in filename:
            verbatims.append(pdf)
        else:
            unknown.append(pdf)
    
    log.info(f"\nCategorized PDFs:")
    log.info(f"  📋 Agendas: {len(agendas)}")
    log.info(f"  📜 Ordinances: {len(ordinances)}")
    log.info(f"  📜 Resolutions: {len(resolutions)}")
    log.info(f"  🎤 Verbatims: {len(verbatims)}")
    log.info(f"  ❓ Unknown: {len(unknown)}")
    
    # Process each type
    stats = {'success': 0, 'errors': 0}
    
    # 1. Process Agendas
    if agendas:
        log.info("\n📋 Processing Agendas...")
        extractor = AgendaPDFExtractor()
        for pdf in agendas:
            try:
                log.info(f"  Processing: {pdf.name}")
                agenda_data = extractor.extract_agenda(pdf)
                output_path = Path("city_clerk_documents/extracted_text") / f"{pdf.stem}_extracted.json"
                output_path.parent.mkdir(exist_ok=True)
                extractor.save_extracted_agenda(agenda_data, output_path)
                stats['success'] += 1
            except Exception as e:
                log.error(f"  Failed: {e}")
                stats['errors'] += 1
    
    # 2. Process Ordinances
    if ordinances:
        log.info("\n📜 Processing Ordinances...")
        linker = EnhancedDocumentLinker()
        for pdf in ordinances:
            try:
                log.info(f"  Processing: {pdf.name}")
                meeting_date = extract_date_from_path(pdf)
                if meeting_date:
                    doc_info = await linker._process_document(pdf, meeting_date, "ordinance")
                    if doc_info:
                        linker._save_extracted_text(pdf, doc_info, "ordinance")
                        stats['success'] += 1
                else:
                    log.warning(f"  No date found for: {pdf.name}")
            except Exception as e:
                log.error(f"  Failed: {e}")
                stats['errors'] += 1
    
    # 3. Process Resolutions
    if resolutions:
        log.info("\n📜 Processing Resolutions...")
        linker = EnhancedDocumentLinker()
        for pdf in resolutions:
            try:
                log.info(f"  Processing: {pdf.name}")
                meeting_date = extract_date_from_path(pdf)
                if meeting_date:
                    doc_info = await linker._process_document(pdf, meeting_date, "resolution")
                    if doc_info:
                        linker._save_extracted_text(pdf, doc_info, "resolution")
                        stats['success'] += 1
                else:
                    log.warning(f"  No date found for: {pdf.name}")
            except Exception as e:
                log.error(f"  Failed: {e}")
                stats['errors'] += 1
    
    # 4. Process Verbatims
    if verbatims:
        log.info("\n🎤 Processing Verbatim Transcripts...")
        transcript_linker = VerbatimTranscriptLinker()
        for pdf in verbatims:
            try:
                log.info(f"  Processing: {pdf.name}")
                meeting_date = extract_date_from_path(pdf)
                if meeting_date:
                    transcript_info = await transcript_linker._process_transcript(pdf, meeting_date)
                    if transcript_info:
                        transcript_linker._save_extracted_text(pdf, transcript_info)
                        stats['success'] += 1
                else:
                    log.warning(f"  No date found for: {pdf.name}")
            except Exception as e:
                log.error(f"  Failed: {e}")
                stats['errors'] += 1
    
    log.info(f"\n✅ Extraction complete:")
    log.info(f"   Success: {stats['success']}")
    log.info(f"   Errors: {stats['errors']}")

def extract_date_from_path(pdf_path: Path) -> str:
    """Extract date from filename or path."""
    # Try different date patterns
    patterns = [
        r'(\d{2})[._](\d{2})[._](\d{4})',  # MM_DD_YYYY or MM.DD.YYYY
        r'(\d{4})-\d+\s*-\s*(\d{2})_(\d{2})_(\d{4})',  # Ordinance pattern
    ]
    
    for pattern in patterns:
        match = re.search(pattern, pdf_path.name)
        if match:
            groups = match.groups()
            if len(groups) == 3:
                month, day, year = groups
            else:  # ordinance pattern
                year, month, day, year2 = groups
            return f"{month}.{day}.{year}"
    
    # Try parent directory for year
    if '2024' in str(pdf_path):
        # Default to a date if we know it's 2024
        return "01.01.2024"
    
    return None

if __name__ == "__main__":
    asyncio.run(extract_all_pdfs())


================================================================================


################################################################################
# File: extract_documents_for_graphrag.py
################################################################################

# File: extract_documents_for_graphrag.py

#!/usr/bin/env python3
"""
Extract documents using enhanced PDF extractor for GraphRAG processing.
This script uses the intelligent metadata header functionality.
"""

import sys
from pathlib import Path
import asyncio

# Add current directory to path
sys.path.append('.')

from scripts.graph_stages.pdf_extractor import PDFExtractor

def extract_documents_for_graphrag():
    """Extract a few sample documents for GraphRAG testing."""
    
    print("🚀 Starting document extraction for GraphRAG testing")
    print("="*60)
    
    # Define source and output directories
    pdf_dir = Path("city_clerk_documents/global copy/City Comissions 2024/Agendas")
    output_dir = Path("city_clerk_documents/extracted_text")
    markdown_dir = Path("city_clerk_documents/extracted_markdown")
    
    # Create output directories
    output_dir.mkdir(parents=True, exist_ok=True)
    markdown_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"📁 Source directory: {pdf_dir}")
    print(f"📁 JSON output directory: {output_dir}")
    print(f"📁 Markdown output directory: {markdown_dir}")
    
    # Find agenda PDFs (limit to a few for testing)
    agenda_files = sorted(pdf_dir.glob("Agenda*.pdf"))[:3]  # Just first 3 for testing
    
    if not agenda_files:
        print("❌ No agenda PDFs found!")
        return False
    
    print(f"📄 Found {len(agenda_files)} agenda files to process:")
    for pdf in agenda_files:
        print(f"   - {pdf.name}")
    
    # Initialize extractor
    extractor = PDFExtractor(pdf_dir, output_dir)
    
    # Process each PDF
    for pdf_path in agenda_files:
        try:
            print(f"\n📄 Processing: {pdf_path.name}")
            
            # Extract with intelligent metadata headers
            markdown_path, enhanced_content = extractor.extract_and_save_with_metadata(
                pdf_path, markdown_dir
            )
            print(f"✅ Enhanced markdown saved to: {markdown_path}")
            
            # Also extract regular JSON for compatibility
            full_text, pages = extractor.extract_text_from_pdf(pdf_path)
            extracted_data = {
                'full_text': full_text,
                'pages': pages,
                'document_type': extractor._determine_doc_type(pdf_path.name),
                'metadata': {
                    'filename': pdf_path.name,
                    'num_pages': len(pages),
                    'total_chars': len(full_text),
                    'extraction_method': 'docling_enhanced'
                }
            }
            
            # Save JSON
            import json
            json_path = output_dir / f"{pdf_path.stem}_extracted.json"
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(extracted_data, f, indent=2, ensure_ascii=False)
            print(f"✅ JSON saved to: {json_path}")
            
        except Exception as e:
            print(f"❌ Error processing {pdf_path.name}: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    print(f"\n✅ Document extraction complete!")
    print(f"📊 Extracted {len(agenda_files)} documents")
    print(f"📁 Files available in:")
    print(f"   - JSON: {output_dir}")
    print(f"   - Enhanced Markdown: {markdown_dir}")
    
    return True

if __name__ == "__main__":
    success = extract_documents_for_graphrag()
    if success:
        print("\n🎯 Ready for GraphRAG pipeline!")
    else:
        print("\n❌ Extraction failed")
        sys.exit(1)


================================================================================


################################################################################
# File: explore_graphrag_sources.py
################################################################################

# File: explore_graphrag_sources.py

#!/usr/bin/env python3
"""
Explore GraphRAG data sources and trace entity lineage.
"""

import pandas as pd
from pathlib import Path
import sys

def explore_sources():
    """Explore the GraphRAG knowledge base to understand data sources."""
    
    graphrag_root = Path("graphrag_data")
    
    print("🔍 GraphRAG Data Source Explorer")
    print("=" * 50)
    
    # Load all the parquet files
    print("\n📊 Loading GraphRAG outputs...")
    
    entities_df = pd.read_parquet(graphrag_root / "output/entities.parquet")
    relationships_df = pd.read_parquet(graphrag_root / "output/relationships.parquet")
    docs_df = pd.read_csv(graphrag_root / "city_clerk_documents.csv")
    
    print(f"✅ Loaded {len(entities_df)} entities")
    print(f"✅ Loaded {len(relationships_df)} relationships")
    print(f"✅ Loaded {len(docs_df)} source documents")
    
    # Show entity type breakdown
    print("\n📈 Entity Types:")
    print(entities_df['type'].value_counts())
    
    # Interactive exploration
    while True:
        print("\n" + "="*50)
        print("Enter an entity to explore (e.g., E-1, 2024-01) or 'quit':")
        query = input("> ").strip()
        
        if query.lower() == 'quit':
            break
        
        # Find matching entities
        matches = entities_df[
            entities_df['title'].str.contains(query, case=False, na=False) |
            entities_df['description'].str.contains(query, case=False, na=False)
        ]
        
        if matches.empty:
            print(f"❌ No entities found matching '{query}'")
            continue
        
        print(f"\n📍 Found {len(matches)} matching entities:")
        
        for idx, entity in matches.iterrows():
            print(f"\n🏷️ Entity ID: {idx}")
            print(f"   Title: {entity['title']}")
            print(f"   Type: {entity['type']}")
            print(f"   Description: {entity['description'][:200]}...")
            
            # Find relationships
            related = relationships_df[
                (relationships_df['source'] == idx) | 
                (relationships_df['target'] == idx)
            ]
            
            if not related.empty:
                print(f"   🔗 Relationships: {len(related)}")
                for _, rel in related.head(3).iterrows():
                    print(f"      - {rel['description'][:100]}...")
            
            # Try to trace back to source document
            print("\n   📄 Possible Source Documents:")
            for _, doc in docs_df.iterrows():
                if query.upper() in str(doc['item_code']).upper() or \
                   query in str(doc['document_number']) or \
                   query.lower() in str(doc['title']).lower():
                    print(f"      - {doc['source_file']}")
                    print(f"        Type: {doc['document_type']}")
                    print(f"        Meeting: {doc['meeting_date']}")
                    break

if __name__ == "__main__":
    explore_sources()


================================================================================


################################################################################
# File: scripts/microsoft_framework/source_tracker.py
################################################################################

# File: scripts/microsoft_framework/source_tracker.py

"""Source tracking component for GraphRAG queries."""

from typing import Dict, List, Any, Set, Tuple
import logging

logger = logging.getLogger(__name__)

class SourceTracker:
    """Track sources used during GraphRAG queries."""
    
    def __init__(self):
        self.reset()
    
    def reset(self):
        """Reset tracking state."""
        self.entities_used: Dict[int, Dict[str, Any]] = {}
        self.relationships_used: Dict[int, Dict[str, Any]] = {}
        self.sources_used: Dict[int, Dict[str, Any]] = {}
        self.text_units_used: Dict[int, Dict[str, Any]] = {}
        self.communities_used: Dict[int, Dict[str, Any]] = {}
    
    def track_entity(self, entity_id: int, entity_data: Dict[str, Any]):
        """Track an entity being used."""
        self.entities_used[entity_id] = {
            'id': entity_id,
            'title': entity_data.get('title', 'Unknown'),
            'type': entity_data.get('type', 'Unknown'),
            'description': entity_data.get('description', '')[:200],
            'source_id': entity_data.get('source_id', '')
        }
    
    def track_relationship(self, rel_id: int, rel_data: Dict[str, Any]):
        """Track a relationship being used."""
        self.relationships_used[rel_id] = {
            'id': rel_id,
            'source': rel_data.get('source', ''),
            'target': rel_data.get('target', ''),
            'description': rel_data.get('description', '')[:200],
            'weight': rel_data.get('weight', 0)
        }
    
    def track_source(self, source_id: int, source_data: Dict[str, Any]):
        """Track a source document being used."""
        self.sources_used[source_id] = {
            'id': source_id,
            'title': source_data.get('title', 'Unknown'),
            'type': source_data.get('document_type', 'document'),
            'file': source_data.get('source_file', '')
        }
    
    def get_summary(self) -> Dict[str, Any]:
        """Get summary of all tracked sources."""
        return {
            'entities': list(self.entities_used.values()),
            'relationships': list(self.relationships_used.values()),
            'sources': list(self.sources_used.values()),
            'text_units': list(self.text_units_used.values()),
            'communities': list(self.communities_used.values())
        }
    
    def get_citation_map(self) -> Dict[str, List[int]]:
        """Get a map of content to source IDs for citations."""
        return {
            'entity_ids': list(self.entities_used.keys()),
            'relationship_ids': list(self.relationships_used.keys()),
            'source_ids': list(self.sources_used.keys())
        }


================================================================================


################################################################################
# File: config.py
################################################################################

# File: config.py

#!/usr/bin/env python3
"""
Configuration file for graph database visualization
Manages database credentials and connection settings
"""

import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Azure Cosmos DB Gremlin Configuration
COSMOS_ENDPOINT = os.getenv("COSMOS_ENDPOINT", "wss://aida-graph-db.gremlin.cosmos.azure.com:443")
COSMOS_KEY = os.getenv("COSMOS_KEY", "")  # This will be set from .env file
DATABASE = os.getenv("COSMOS_DATABASE", "cgGraph")
CONTAINER = os.getenv("COSMOS_CONTAINER", "cityClerk")
PARTITION_KEY = os.getenv("COSMOS_PARTITION_KEY", "partitionKey")
PARTITION_VALUE = os.getenv("COSMOS_PARTITION_VALUE", "demo")

def validate_config():
    """Validate that all required configuration is available"""
    required_vars = {
        "COSMOS_KEY": COSMOS_KEY,
        "COSMOS_ENDPOINT": COSMOS_ENDPOINT,
        "DATABASE": DATABASE,
        "CONTAINER": CONTAINER
    }
    
    missing_vars = [var for var, value in required_vars.items() if not value]
    
    if missing_vars:
        print("❌ Missing required configuration:")
        for var in missing_vars:
            print(f"   - {var}")
        print("\n🔧 Please create a .env file with your credentials:")
        print("   COSMOS_KEY=your_actual_cosmos_key_here")
        print("   COSMOS_ENDPOINT=wss://aida-graph-db.gremlin.cosmos.azure.com:443")
        print("   COSMOS_DATABASE=cgGraph")
        print("   COSMOS_CONTAINER=cityClerk")
        return False
    
    return True

if __name__ == "__main__":
    print("🔧 Configuration Check:")
    if validate_config():
        print("✅ All configuration variables are set!")
    else:
        print("❌ Configuration incomplete!")


================================================================================


################################################################################
# File: scripts/json_to_markdown_converter.py
################################################################################

# File: scripts/json_to_markdown_converter.py

#!/usr/bin/env python3
"""Convert existing JSON extractions to markdown."""

import json
from pathlib import Path
import sys
sys.path.append('.')

from scripts.graph_stages.agenda_pdf_extractor import AgendaPDFExtractor

def convert_jsons_to_markdown():
    """Convert all existing JSON files to markdown."""
    
    json_dir = Path("city_clerk_documents/extracted_text")
    
    # Process agenda JSONs
    print("Converting agenda JSONs to markdown...")
    extractor = AgendaPDFExtractor()
    
    for json_file in json_dir.glob("Agenda*_extracted.json"):
        print(f"Processing: {json_file.name}")
        
        with open(json_file, 'r') as f:
            agenda_data = json.load(f)
        
        # Call the markdown save method
        extractor._save_agenda_as_markdown(agenda_data, json_file)
    
    print("✅ Conversion complete!")
    
    # Check results
    markdown_dir = Path("city_clerk_documents/extracted_markdown")
    md_files = list(markdown_dir.glob("*.md"))
    print(f"\nMarkdown files: {len(md_files)}")
    print("- Agendas:", len([f for f in md_files if f.name.startswith('agenda_')]))
    print("- Ordinances:", len([f for f in md_files if f.name.startswith('ordinance_')]))
    print("- Resolutions:", len([f for f in md_files if f.name.startswith('resolution_')]))
    print("- Verbatims:", len([f for f in md_files if f.name.startswith('verbatim_')]))

if __name__ == "__main__":
    convert_jsons_to_markdown()


================================================================================


################################################################################
# File: scripts/microsoft_framework/run_graphrag_direct.py
################################################################################

# File: scripts/microsoft_framework/run_graphrag_direct.py

#!/usr/bin/env python3
"""Run GraphRAG commands directly without subprocess."""

import sys
import os

def run_graphrag_index(root_dir, verbose=True):
    """Run GraphRAG indexing directly."""
    # Set up arguments
    sys.argv = [
        'graphrag', 'index',
        '--root', str(root_dir)
    ]
    if verbose:
        sys.argv.append('--verbose')
    
    # Import and run GraphRAG
    try:
        from graphrag.cli import app
        app()
    except ImportError:
        print("❌ GraphRAG not found in current environment")
        print(f"Python: {sys.executable}")
        print(f"Path: {sys.path}")
        raise

if __name__ == "__main__":
    if len(sys.argv) > 1:
        root = sys.argv[1]
    else:
        root = "graphrag_data"
    
    run_graphrag_index(root)


================================================================================


################################################################################
# File: scripts/graph_stages/__init__.py
################################################################################

# File: scripts/graph_stages/__init__.py

"""
Graph Pipeline Stages
=====================
Components for building city clerk document knowledge graph.
"""

from .cosmos_db_client import CosmosGraphClient
from .agenda_pdf_extractor import AgendaPDFExtractor
from .agenda_ontology_extractor import CityClerkOntologyExtractor
from .enhanced_document_linker import EnhancedDocumentLinker
from .agenda_graph_builder import AgendaGraphBuilder
from .verbatim_transcript_linker import VerbatimTranscriptLinker

__all__ = [
    'CosmosGraphClient',
    'AgendaPDFExtractor',
    'CityClerkOntologyExtractor',
    'EnhancedDocumentLinker',
    'AgendaGraphBuilder',
    'VerbatimTranscriptLinker'
]


================================================================================

