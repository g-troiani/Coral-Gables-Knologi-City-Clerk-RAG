# Concatenated Project Code - Part 2 of 3
# Generated: 2025-06-05 10:54:45
# Root Directory: /Users/gianmariatroiani/Documents/knologiÃä/graph_database
================================================================================

# File Index - Which Files Are in Which Parts
################################################################################

## Part 1 (6 files):
  - scripts/graph_stages/enhanced_document_linker.py
  - scripts/graph_stages/pdf_extractor.py
  - scripts/test_ontology_urls.py
  - scripts/minimal_url_test.py
  - requirements.txt
  - scripts/test_e1_urls.py

## Part 2 (6 files):
  - scripts/graph_stages/verbatim_transcript_linker.py
  - test_query_system.py
  - scripts/test_url_extraction.py
  - scripts/test_graph_urls.py
  - graph_clear_database.py
  - scripts/check_extracted_urls.py

## Part 3 (6 files):
  - scripts/graph_stages/document_linker.py
  - scripts/graph_stages/cosmos_db_client.py
  - scripts/debug_visualizer.py
  - scripts/check_graph_empty.py
  - config.py
  - scripts/graph_stages/__init__.py


================================================================================


################################################################################
# File: scripts/graph_stages/verbatim_transcript_linker.py
################################################################################

# File: scripts/graph_stages/verbatim_transcript_linker.py

"""
Verbatim Transcript Linker
Links verbatim transcript documents to their corresponding agenda items.
Handles various transcript types including individual items, item ranges, and public comments.
"""

import logging
import re
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Set
import json
from datetime import datetime
import PyPDF2
import os

log = logging.getLogger('verbatim_transcript_linker')


class VerbatimTranscriptLinker:
    """Links verbatim transcript documents to agenda items in the graph."""
    
    def __init__(self):
        """Initialize the verbatim transcript linker."""
        # Pattern to extract date and item info from filename
        self.filename_pattern = re.compile(
            r'(\d{2})_(\d{2})_(\d{4})\s*-\s*Verbatim Transcripts\s*-\s*(.+)\.pdf',
            re.IGNORECASE
        )
        
        # Debug directory for logging - ensure parent exists
        self.debug_dir = Path("city_clerk_documents/graph_json/debug/verbatim")
        self.debug_dir.mkdir(parents=True, exist_ok=True)
    
    async def link_transcripts_for_meeting(self, 
                                         meeting_date: str,
                                         verbatim_dir: Path) -> Dict[str, List[Dict]]:
        """Find and link all verbatim transcripts for a specific meeting date."""
        log.info(f"üé§ Linking verbatim transcripts for meeting date: {meeting_date}")
        log.info(f"üìÅ Verbatim directory: {verbatim_dir}")
        
        # Debug logging for troubleshooting
        log.info(f"üîç Looking for verbatim transcripts in: {verbatim_dir}")
        log.info(f"üîç Directory exists: {verbatim_dir.exists()}")
        if verbatim_dir.exists():
            all_files = list(verbatim_dir.glob("*.pdf"))
            log.info(f"üîç Total PDF files in directory: {len(all_files)}")
            if all_files:
                log.info(f"üîç Sample files: {[f.name for f in all_files[:3]]}")
        
        # Convert meeting date format: "01.09.2024" -> "01_09_2024"
        date_underscore = meeting_date.replace(".", "_")
        
        # Initialize results
        linked_transcripts = {
            "item_transcripts": [],      # Transcripts for specific agenda items
            "public_comments": [],       # Public comment transcripts
            "section_transcripts": []    # Transcripts for entire sections
        }
        
        if not verbatim_dir.exists():
            log.warning(f"‚ö†Ô∏è  Verbatim directory not found: {verbatim_dir}")
            return linked_transcripts
        
        # Find all transcript files for this date
        # Try multiple patterns to ensure we catch all files
        patterns = [
            f"{date_underscore}*Verbatim*.pdf",
            f"{date_underscore} - Verbatim*.pdf",
            f"*{date_underscore}*Verbatim*.pdf"
        ]

        transcript_files = []
        for pattern in patterns:
            files = list(verbatim_dir.glob(pattern))
            log.info(f"üîç Pattern '{pattern}' found {len(files)} files")
            transcript_files.extend(files)

        # Remove duplicates
        transcript_files = list(set(transcript_files))
        
        log.info(f"üìÑ Found {len(transcript_files)} transcript files")
        
        # Process each transcript file
        for transcript_path in transcript_files:
            try:
                transcript_info = await self._process_transcript(transcript_path, meeting_date)
                if transcript_info:
                    # Categorize based on transcript type
                    if transcript_info['transcript_type'] == 'public_comment':
                        linked_transcripts['public_comments'].append(transcript_info)
                    elif transcript_info['transcript_type'] == 'section':
                        linked_transcripts['section_transcripts'].append(transcript_info)
                    else:
                        linked_transcripts['item_transcripts'].append(transcript_info)
                        
            except Exception as e:
                log.error(f"Error processing transcript {transcript_path.name}: {e}")
        
        # Save linked transcripts info for debugging
        self._save_linking_report(meeting_date, linked_transcripts)
        
        # Log summary
        total_linked = (len(linked_transcripts['item_transcripts']) + 
                       len(linked_transcripts['public_comments']) + 
                       len(linked_transcripts['section_transcripts']))
        
        log.info(f"‚úÖ Verbatim transcript linking complete:")
        log.info(f"   üé§ Item transcripts: {len(linked_transcripts['item_transcripts'])}")
        log.info(f"   üé§ Public comments: {len(linked_transcripts['public_comments'])}")
        log.info(f"   üé§ Section transcripts: {len(linked_transcripts['section_transcripts'])}")
        log.info(f"   üìÑ Total linked: {total_linked}")
        
        return linked_transcripts
    
    async def _process_transcript(self, transcript_path: Path, meeting_date: str) -> Optional[Dict[str, Any]]:
        """Process a single transcript file and extract item references."""
        try:
            # Parse filename
            match = self.filename_pattern.match(transcript_path.name)
            if not match:
                log.warning(f"Could not parse transcript filename: {transcript_path.name}")
                return None
            
            month, day, year = match.groups()[:3]
            item_info = match.group(4).strip()
            
            # Parse item codes from the item info
            parsed_items = self._parse_item_codes(item_info)
            
            # Extract text from PDF (first few pages for context)
            text_excerpt = self._extract_pdf_text(transcript_path, max_pages=3)
            
            # Determine transcript type and normalize item codes
            transcript_type = self._determine_transcript_type(item_info, parsed_items)
            
            transcript_info = {
                "path": str(transcript_path),
                "filename": transcript_path.name,
                "meeting_date": meeting_date,
                "item_info_raw": item_info,
                "item_codes": parsed_items['item_codes'],
                "section_codes": parsed_items['section_codes'],
                "transcript_type": transcript_type,
                "page_count": self._get_pdf_page_count(transcript_path),
                "text_excerpt": text_excerpt[:500] if text_excerpt else ""
            }
            
            log.info(f"üìÑ Processed transcript: {transcript_path.name}")
            log.info(f"   Items: {parsed_items['item_codes']}")
            log.info(f"   Type: {transcript_type}")
            
            return transcript_info
            
        except Exception as e:
            log.error(f"Error processing transcript {transcript_path.name}: {e}")
            return None
    
    def _parse_item_codes(self, item_info: str) -> Dict[str, List[str]]:
        """Parse item codes from the filename item info section."""
        result = {
            'item_codes': [],
            'section_codes': []
        }
        
        # Check for public comment first
        if re.search(r'public\s+comment', item_info, re.IGNORECASE):
            result['section_codes'].append('PUBLIC_COMMENT')
            return result
        
        # Special case: Meeting Minutes or other general labels
        if re.search(r'meeting\s+minutes', item_info, re.IGNORECASE):
            result['item_codes'].append('MEETING_MINUTES')
            return result
        
        # Special case: Full meeting transcript
        if re.search(r'public|full\s+meeting', item_info, re.IGNORECASE) and not re.search(r'comment', item_info, re.IGNORECASE):
            result['item_codes'].append('FULL_MEETING')
            return result
        
        # Special case: Discussion Items (K section)
        if re.match(r'^K\s*$', item_info.strip()):
            result['section_codes'].append('K')
            return result
        
        # Clean the item info
        item_info = item_info.strip()
        
        # Handle multiple items with "and" or "AND"
        # Examples: "F-7 and F-10", "2-1 AND 2-2"
        if ' and ' in item_info.lower():
            parts = re.split(r'\s+and\s+', item_info, flags=re.IGNORECASE)
            for part in parts:
                codes = self._extract_single_item_codes(part.strip())
                result['item_codes'].extend(codes)
        
        # Handle space-separated items
        # Examples: "E-5 E-6 E-7 E-8 E-9 E-10"
        elif re.match(r'^([A-Z]-?\d+\s*)+$', item_info):
            # Split by spaces and extract each item
            items = item_info.split()
            for item in items:
                if re.match(r'^[A-Z]-?\d+$', item):
                    normalized = self._normalize_item_code(item)
                    if normalized:
                        result['item_codes'].append(normalized)
        
        # Handle comma-separated items
        elif ',' in item_info:
            parts = item_info.split(',')
            for part in parts:
                codes = self._extract_single_item_codes(part.strip())
                result['item_codes'].extend(codes)
        
        # Single item or other format
        else:
            codes = self._extract_single_item_codes(item_info)
            result['item_codes'].extend(codes)
        
        # Remove duplicates while preserving order
        result['item_codes'] = list(dict.fromkeys(result['item_codes']))
        result['section_codes'] = list(dict.fromkeys(result['section_codes']))
        
        return result
    
    def _extract_single_item_codes(self, text: str) -> List[str]:
        """Extract item codes from a single text segment."""
        codes = []
        
        # Pattern for item codes: letter-number, letter.number, or just number-number
        # Handles: E-1, E1, E.-1., E.1, 2-1, etc.
        patterns = [
            r'([A-Z])\.?\-?(\d+)\.?',  # Letter-based items
            r'(\d+)\-(\d+)'             # Number-only items like 2-1
        ]
        
        for pattern in patterns:
            for match in re.finditer(pattern, text):
                if pattern.startswith('(\\d'):  # Number-only pattern
                    # For number-only, just use as is
                    codes.append(f"{match.group(1)}-{match.group(2)}")
                else:
                    # For letter-number format
                    letter = match.group(1)
                    number = match.group(2)
                    codes.append(f"{letter}-{number}")
        
        return codes
    
    def _normalize_item_code(self, code: str) -> str:
        """Normalize item code to consistent format (e.g., E-1, 2-1)."""
        # Remove dots and ensure dash format
        code = code.strip('. ')
        
        # Pattern: letter followed by optional punctuation and number
        letter_match = re.match(r'^([A-Z])\.?\-?(\d+)\.?$', code)
        if letter_match:
            letter = letter_match.group(1)
            number = letter_match.group(2)
            return f"{letter}-{number}"
        
        # Pattern: number-number format
        number_match = re.match(r'^(\d+)\-(\d+)$', code)
        if number_match:
            return code  # Already in correct format
        
        return code
    
    def _determine_transcript_type(self, item_info: str, parsed_items: Dict) -> str:
        """Determine the type of transcript based on parsed information."""
        if 'PUBLIC_COMMENT' in parsed_items['item_codes']:
            return 'public_comment'
        elif parsed_items['section_codes']:
            return 'section'
        elif len(parsed_items['item_codes']) > 3:
            return 'multi_item'
        elif len(parsed_items['item_codes']) == 1:
            return 'single_item'
        else:
            return 'item_group'
    
    def _extract_pdf_text(self, pdf_path: Path, max_pages: int = 3) -> str:
        """Extract text from first few pages of PDF."""
        try:
            with open(pdf_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                text_parts = []
                
                # Extract text from first few pages
                pages_to_read = min(len(reader.pages), max_pages)
                for i in range(pages_to_read):
                    text = reader.pages[i].extract_text()
                    if text:
                        text_parts.append(text)
                
                return "\n".join(text_parts)
        except Exception as e:
            log.error(f"Failed to extract text from {pdf_path.name}: {e}")
            return ""
    
    def _get_pdf_page_count(self, pdf_path: Path) -> int:
        """Get total page count of PDF."""
        try:
            with open(pdf_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                return len(reader.pages)
        except:
            return 0
    
    def _save_linking_report(self, meeting_date: str, linked_transcripts: Dict):
        """Save detailed report of transcript linking."""
        report = {
            "meeting_date": meeting_date,
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total_transcripts": sum(len(v) for v in linked_transcripts.values()),
                "item_transcripts": len(linked_transcripts["item_transcripts"]),
                "public_comments": len(linked_transcripts["public_comments"]),
                "section_transcripts": len(linked_transcripts["section_transcripts"])
            },
            "transcripts": linked_transcripts
        }
        
        report_filename = f"verbatim_linking_report_{meeting_date.replace('.', '_')}.json"
        report_path = self.debug_dir / report_filename
        
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        log.info(f"üìä Verbatim linking report saved to: {report_path}")
    
    def _validate_meeting_date(self, meeting_date: str) -> bool:
        """Validate meeting date format MM.DD.YYYY"""
        return bool(re.match(r'^\d{2}\.\d{2}\.\d{4}$', meeting_date))


================================================================================


################################################################################
# File: test_query_system.py
################################################################################

# File: test_query_system.py

#!/usr/bin/env python3
"""
Test script for querying the city clerk system
Demonstrates both graph database and RAG query capabilities
"""

import os
import sys
import asyncio
from dotenv import load_dotenv

# Add the current directory to Python path
sys.path.append('.')
sys.path.append('scripts')

load_dotenv()

class QueryTester:
    """Test both graph and RAG query systems"""
    
    def __init__(self):
        self.graph_client = None
        self.rag_available = False
        
    async def setup_graph_client(self):
        """Setup graph database client"""
        try:
            from scripts.graph_stages.cosmos_db_client import CosmosGraphClient
            self.graph_client = CosmosGraphClient()
            print("‚úÖ Graph database client connected")
        except Exception as e:
            print(f"‚ùå Failed to connect to graph database: {e}")
            
    def setup_rag_client(self):
        """Check RAG system availability"""
        try:
            # Check if required environment variables are set
            required_vars = ['OPENAI_API_KEY', 'SUPABASE_URL', 'SUPABASE_SERVICE_ROLE_KEY']
            missing = [var for var in required_vars if not os.getenv(var)]
            
            if missing:
                print(f"‚ùå RAG system missing: {', '.join(missing)}")
                return False
                
            print("‚úÖ RAG system environment variables found")
            self.rag_available = True
            return True
        except Exception as e:
            print(f"‚ùå RAG system check failed: {e}")
            return False
    
    async def test_graph_queries(self):
        """Test various graph database queries"""
        if not self.graph_client:
            print("‚ö†Ô∏è  Graph client not available - skipping graph tests")
            return
            
        print("\nüîç Testing Graph Database Queries")
        print("=" * 50)
        
        # Test queries
        test_queries = [
            ("Count all vertices", "g.V().count()"),
            ("Count all edges", "g.E().count()"),
            ("List vertex types", "g.V().label().dedup()"),
            ("List edge types", "g.E().label().dedup()"),
            ("Sample meetings", "g.V().hasLabel('Meeting').limit(3).valueMap()"),
            ("Sample agenda items", "g.V().hasLabel('AgendaItem').limit(3).valueMap()"),
            ("Find people", "g.V().hasLabel('Person').limit(5).values('name')"),
            ("Meeting relationships", "g.V().hasLabel('Meeting').out().label().dedup()"),
        ]
        
        for description, query in test_queries:
            try:
                print(f"\nüìä {description}:")
                result = await self.graph_client._execute_query(query)
                
                if isinstance(result, list) and len(result) > 0:
                    if len(result) == 1 and isinstance(result[0], (int, float)):
                        print(f"   Result: {result[0]}")
                    else:
                        print(f"   Found {len(result)} items:")
                        for i, item in enumerate(result[:5]):  # Show first 5
                            if isinstance(item, dict):
                                # Show key-value pairs for complex objects
                                key_items = list(item.items())[:3]  # First 3 keys
                                print(f"     {i+1}. {dict(key_items)}")
                            else:
                                print(f"     {i+1}. {item}")
                        if len(result) > 5:
                            print(f"     ... and {len(result) - 5} more")
                else:
                    print("   No results found")
                    
            except Exception as e:
                print(f"   ‚ùå Query failed: {e}")
    
    def test_rag_queries(self):
        """Test RAG system queries"""
        if not self.rag_available:
            print("‚ö†Ô∏è  RAG system not available - skipping RAG tests")
            return
            
        print("\nü§ñ Testing RAG Query System")
        print("=" * 50)
        
        try:
            import requests
            import json
            
            # Test queries for city clerk documents
            test_questions = [
                "What meetings were held recently?",
                "What resolutions were passed?", 
                "Who are the city commissioners?",
                "What contracts were approved?",
                "What ordinances were discussed?",
            ]
            
            # Test if RAG server is running
            try:
                response = requests.get("http://localhost:8080/stats", timeout=5)
                if response.status_code == 200:
                    stats = response.json()
                    print(f"üìà RAG Server Status: {stats}")
                    
                    # Test actual queries
                    for question in test_questions:
                        print(f"\n‚ùì Question: {question}")
                        try:
                            search_response = requests.post(
                                "http://localhost:8080/search",
                                json={"query": question},
                                timeout=10
                            )
                            
                            if search_response.status_code == 200:
                                result = search_response.json()
                                answer = result.get('answer', 'No answer')
                                citations = result.get('citations', [])
                                
                                print(f"   üí¨ Answer: {answer[:200]}{'...' if len(answer) > 200 else ''}")
                                print(f"   üìö Citations: {len(citations)} found")
                            else:
                                print(f"   ‚ùå Query failed: {search_response.status_code}")
                                
                        except requests.exceptions.Timeout:
                            print("   ‚è∞ Query timeout")
                        except Exception as e:
                            print(f"   ‚ùå Query error: {e}")
                            
                else:
                    print(f"‚ùå RAG server not responding: {response.status_code}")
                    
            except requests.exceptions.ConnectionError:
                print("‚ùå RAG server not running at localhost:8080")
                print("üí° To start RAG server: python3 scripts/rag_local_web_app.py")
                
        except ImportError:
            print("‚ùå Missing 'requests' package for RAG testing")
            print("üí° Install with: pip install requests")
    
    def show_manual_test_instructions(self):
        """Show instructions for manual testing"""
        print("\nüìã Manual Testing Options")
        print("=" * 50)
        
        print("\nüîç Graph Database Testing:")
        print("1. Visual Explorer:")
        print("   python3 graph_visualizer.py")
        print("   Then visit: http://localhost:8050")
        
        print("\n2. Direct Graph Queries:")
        print("   python3 scripts/check_graph_empty.py")
        print("   python3 scripts/test_graph_urls.py")
        
        print("\nü§ñ RAG System Testing:")
        print("1. Start RAG Server:")
        print("   python3 scripts/rag_local_web_app.py")
        print("   Then visit: http://localhost:8080")
        
        print("\n2. Example RAG Questions:")
        questions = [
            "What is the city budget for this year?",
            "Who are the current city commissioners?", 
            "What ordinances were passed recently?",
            "What contracts need approval?",
            "When is the next city council meeting?"
        ]
        for i, q in enumerate(questions, 1):
            print(f"   {i}. {q}")
            
        print("\nüîß System Status Commands:")
        print("   python3 -c 'from config import validate_config; validate_config()'")
        print("   python3 scripts/check_graph_empty.py")
    
    async def run_all_tests(self):
        """Run all available tests"""
        print("üß™ City Clerk Query System Tests")
        print("=" * 50)
        
        # Setup
        await self.setup_graph_client()
        self.setup_rag_client()
        
        # Run tests
        await self.test_graph_queries()
        self.test_rag_queries()
        
        # Show manual options
        self.show_manual_test_instructions()
        
        # Cleanup
        if self.graph_client:
            await self.graph_client.close()

async def main():
    """Main test runner"""
    tester = QueryTester()
    await tester.run_all_tests()

if __name__ == "__main__":
    asyncio.run(main())


================================================================================


################################################################################
# File: scripts/test_url_extraction.py
################################################################################

# File: scripts/test_url_extraction.py

#!/usr/bin/env python3
"""
Test script to verify URL extraction from agenda PDFs
"""

import sys
from pathlib import Path
sys.path.append('scripts')

from graph_stages.agenda_pdf_extractor import AgendaPDFExtractor
import json
import logging

# Set up logging
logging.basicConfig(level=logging.INFO)
log = logging.getLogger(__name__)

def test_url_extraction(pdf_path: str):
    """Test URL extraction on a single PDF."""
    pdf_file = Path(pdf_path)
    
    if not pdf_file.exists():
        log.error(f"PDF file not found: {pdf_path}")
        return
    
    log.info(f"Testing URL extraction on: {pdf_file.name}")
    
    # Initialize extractor
    extractor = AgendaPDFExtractor()
    
    # Extract agenda with URLs
    agenda_data = extractor.extract_agenda(pdf_file)
    
    # Report results
    log.info("\n" + "="*60)
    log.info("EXTRACTION RESULTS")
    log.info("="*60)
    
    # Overall statistics
    hyperlinks = agenda_data.get('hyperlinks', [])
    agenda_items = agenda_data.get('agenda_items', [])
    
    log.info(f"Total hyperlinks found: {len(hyperlinks)}")
    log.info(f"Total agenda items: {len(agenda_items)}")
    
    # Show hyperlinks
    if hyperlinks:
        log.info("\nHYPERLINKS FOUND:")
        for i, link in enumerate(hyperlinks[:10]):  # Show first 10
            log.info(f"  {i+1}. Text: {link.get('text', 'N/A')[:50]}...")
            log.info(f"     URL: {link.get('url', 'N/A')}")
            log.info(f"     Page: {link.get('page', 'N/A')}")
        
        if len(hyperlinks) > 10:
            log.info(f"  ... and {len(hyperlinks) - 10} more")
    
    # Show items with URLs
    items_with_urls = [item for item in agenda_items if item.get('urls')]
    
    if items_with_urls:
        log.info(f"\nAGENDA ITEMS WITH URLS: {len(items_with_urls)}")
        for item in items_with_urls[:5]:  # Show first 5
            log.info(f"\n  Item: {item.get('item_code')} - {item.get('document_reference')}")
            log.info(f"  Title: {item.get('title', 'N/A')[:100]}...")
            log.info(f"  URLs:")
            for url in item.get('urls', []):
                log.info(f"    - {url.get('text', 'Link')[:50]}... -> {url.get('url', 'N/A')[:50]}...")
    
    # Save detailed results
    output_file = pdf_file.parent / f"{pdf_file.stem}_url_test_results.json"
    with open(output_file, 'w') as f:
        json.dump({
            'pdf_file': pdf_file.name,
            'total_hyperlinks': len(hyperlinks),
            'total_agenda_items': len(agenda_items),
            'items_with_urls': len(items_with_urls),
            'hyperlinks': hyperlinks,
            'agenda_items_with_urls': items_with_urls
        }, f, indent=2)
    
    log.info(f"\nDetailed results saved to: {output_file}")

if __name__ == "__main__":
    if len(sys.argv) > 1:
        test_url_extraction(sys.argv[1])
    else:
        # Test with a default agenda from the actual directory structure
        test_path = "city_clerk_documents/global/City Comissions 2024/Agendas/Agenda 01.9.2024.pdf"
        if Path(test_path).exists():
            test_url_extraction(test_path)
        else:
            # Try alternative paths
            alt_paths = [
                "city_clerk_documents/global copy/City Comissions 2024/Agendas/Agenda 01.9.2024.pdf",
                Path.cwd() / "city_clerk_documents/global/City Comissions 2024/Agendas/Agenda 01.9.2024.pdf"
            ]
            
            for alt_path in alt_paths:
                if Path(alt_path).exists():
                    test_url_extraction(str(alt_path))
                    break
            else:
                log.error("Could not find agenda PDF in expected locations")
                log.error("Please provide a PDF path as argument")
                log.error("Usage: python test_url_extraction.py <path_to_pdf>")
                log.error("\nExpected locations:")
                log.error("  - city_clerk_documents/global/City Comissions 2024/Agendas/")
                log.error("  - city_clerk_documents/global copy/City Comissions 2024/Agendas/")


================================================================================


################################################################################
# File: scripts/test_graph_urls.py
################################################################################

# File: scripts/test_graph_urls.py

#!/usr/bin/env python3
"""
Quick test to verify URLs are stored in graph database
"""

import sys
import asyncio
import json
sys.path.append('scripts')

from graph_stages.cosmos_db_client import CosmosGraphClient

async def test_graph_urls():
    """Test that URLs are stored in the graph database."""
    
    # Connect to graph
    client = CosmosGraphClient()
    
    try:
        # Query for E-1 item
        query = "g.V().hasLabel('AgendaItem').has('code', 'E-1').limit(1)"
        result = await client.query_vertices(query)
        
        if result:
            node = result[0]
            print(f"‚úÖ Found E-1 node: {node.get('id')}")
            print(f"üìã Node properties: {list(node.keys())}")
            
            # Check for URLs
            if 'urls_json' in node:
                urls_json = node['urls_json'][0]['value']
                urls = json.loads(urls_json)
                print(f"üîó URLs found: {len(urls)}")
                for url in urls:
                    print(f"   - {url.get('url', 'No URL')}")
                    print(f"     Text: {url.get('text', 'No text')}")
                    print(f"     Page: {url.get('page', 'No page')}")
            else:
                print("‚ùå No urls_json property found")
                
            # Check has_urls flag
            if 'has_urls' in node:
                has_urls = node['has_urls'][0]['value']
                print(f"üè∑Ô∏è  Has URLs flag: {has_urls}")
            else:
                print("‚ùå No has_urls property found")
        else:
            print("‚ùå E-1 node not found")
            
        # Query for all agenda items with URLs
        query = "g.V().hasLabel('AgendaItem').has('has_urls', true).count()"
        count_result = await client.query_vertices(query)
        if count_result:
            count = count_result[0]
            print(f"\nüìä Total agenda items with URLs: {count}")
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
    finally:
        await client.close()

if __name__ == "__main__":
    asyncio.run(test_graph_urls())


================================================================================


################################################################################
# File: graph_clear_database.py
################################################################################

# File: graph_clear_database.py

#!/usr/bin/env python3
"""
Clear Cosmos DB Graph Database
This script will clear all vertices and edges from the graph database.
"""

import asyncio
import sys
import os
from pathlib import Path

# Add scripts directory to path
script_dir = Path(__file__).parent / 'scripts'
sys.path.append(str(script_dir))

from graph_stages.cosmos_db_client import CosmosGraphClient

async def clear_database():
    """Clear the entire graph database."""
    print('üóëÔ∏è  Clearing Cosmos DB graph database...')
    print('‚ö†Ô∏è  This will delete ALL vertices and edges!')
    
    try:
        async with CosmosGraphClient() as client:
            await client.clear_graph()
        print('‚úÖ Graph database cleared successfully!')
        return True
        
    except Exception as e:
        print(f'‚ùå Error clearing database: {e}')
        return False

if __name__ == "__main__":
    success = asyncio.run(clear_database())
    if not success:
        sys.exit(1)


================================================================================


################################################################################
# File: scripts/check_extracted_urls.py
################################################################################

# scripts/check_extracted_urls.py
import json
from pathlib import Path

extracted_file = Path("city_clerk_documents/extracted_text/Agenda 01.9.2024_extracted.json")

if extracted_file.exists():
    with open(extracted_file, 'r') as f:
        data = json.load(f)
    
    print(f"Total hyperlinks: {len(data.get('hyperlinks', []))}")
    print(f"Total agenda items: {len(data.get('agenda_items', []))}")
    
    # Check E-1 specifically
    for item in data.get('agenda_items', []):
        if item.get('item_code') == 'E-1':
            print(f"\nE-1 Item found:")
            print(f"  Title: {item.get('title', '')[:50]}...")
            print(f"  URLs: {item.get('urls', 'NO URLS FIELD')}")
            if 'urls' in item:
                print(f"  URL count: {len(item['urls'])}")
                for url in item['urls']:
                    print(f"    - {url}")
else:
    print(f"File not found: {extracted_file}")


================================================================================

