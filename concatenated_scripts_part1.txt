# Concatenated Project Code - Part 1 of 3
# Generated: 2025-05-29 10:08:58
# Root Directory: /Users/gianmariatroiani/Documents/knologiÌŠ/graph_database
================================================================================

# Directory Structure
################################################################################
â”œâ”€â”€ .gitignore (858.0B, no ext)
â”œâ”€â”€ city_clerk_db_migration.sql (2.0KB, .sql)
â”œâ”€â”€ city_clerk_documents/
â”‚   â””â”€â”€ [EXCLUDED] 4 items: .DS_Store (excluded file), global (excluded dir), json (excluded dir)
â”‚       ... and 1 more excluded items
â”œâ”€â”€ relationOPENAI.py (8.7KB, .py)
â”œâ”€â”€ requirements.txt (755.0B, .txt)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ clear_database.py (6.0KB, .py)
â”‚   â”œâ”€â”€ find_duplicates.py (5.2KB, .py)
â”‚   â”œâ”€â”€ graph_pipeline.py (19.5KB, .py)
â”‚   â”œâ”€â”€ graph_stages/
â”‚   â”‚   â”œâ”€â”€ __init__.py (202.0B, .py)
â”‚   â”‚   â”œâ”€â”€ agenda_parser.py (6.6KB, .py)
â”‚   â”‚   â”œâ”€â”€ cosmos_db_client.py (9.2KB, .py)
â”‚   â”‚   â”œâ”€â”€ entity_deduplicator.py (5.0KB, .py)
â”‚   â”‚   â”œâ”€â”€ graph_extractor.py (3.5KB, .py)
â”‚   â”‚   â””â”€â”€ relationship_builder.py (5.5KB, .py)
â”‚   â”œâ”€â”€ pipeline_modular_optimized.py (12.7KB, .py)
â”‚   â”œâ”€â”€ rag_local_web_app.py (18.4KB, .py)
â”‚   â”œâ”€â”€ stages/
â”‚   â”‚   â”œâ”€â”€ __init__.py (378.0B, .py)
â”‚   â”‚   â”œâ”€â”€ acceleration_utils.py (3.8KB, .py)
â”‚   â”‚   â”œâ”€â”€ chunk_text.py (18.6KB, .py)
â”‚   â”‚   â”œâ”€â”€ db_upsert.py (8.9KB, .py)
â”‚   â”‚   â”œâ”€â”€ embed_vectors.py (27.0KB, .py)
â”‚   â”‚   â”œâ”€â”€ extract_clean.py (21.7KB, .py)
â”‚   â”‚   â””â”€â”€ llm_enrich.py (5.9KB, .py)
â”‚   â”œâ”€â”€ test_graph_pipeline.py (1007.0B, .py)
â”‚   â”œâ”€â”€ test_vector_search.py (5.5KB, .py)
â”‚   â””â”€â”€ topic_filter_and_title.py (5.6KB, .py)
â”œâ”€â”€ [EXCLUDED] 6 items: .DS_Store (excluded file), .env (excluded file), .git (excluded dir)
    ... and 3 more excluded items


================================================================================


# File Index - Which Files Are in Which Parts
################################################################################

## Part 1 (8 files):
  - scripts/stages/embed_vectors.py
  - scripts/pipeline_modular_optimized.py
  - relationOPENAI.py
  - scripts/stages/llm_enrich.py
  - scripts/find_duplicates.py
  - scripts/graph_stages/entity_deduplicator.py
  - scripts/test_graph_pipeline.py
  - scripts/graph_stages/__init__.py

## Part 2 (7 files):
  - scripts/stages/extract_clean.py
  - scripts/rag_local_web_app.py
  - scripts/graph_stages/cosmos_db_client.py
  - scripts/graph_stages/agenda_parser.py
  - scripts/test_vector_search.py
  - scripts/stages/acceleration_utils.py
  - scripts/graph_stages/graph_extractor.py

## Part 3 (8 files):
  - scripts/graph_pipeline.py
  - scripts/stages/chunk_text.py
  - scripts/stages/db_upsert.py
  - scripts/clear_database.py
  - scripts/graph_stages/relationship_builder.py
  - scripts/topic_filter_and_title.py
  - requirements.txt
  - scripts/stages/__init__.py


================================================================================


################################################################################
# File: scripts/stages/embed_vectors.py
################################################################################

# File: scripts/stages/embed_vectors.py

#!/usr/bin/env python3
"""
Stage 7 â€” Optimized embedding with rate limiting, deduplication, and conservative batching.
"""
from __future__ import annotations
import argparse, json, logging, os, sys, time
from datetime import datetime
from typing import Any, Dict, List, Optional, Set
import asyncio
import aiohttp
from dotenv import load_dotenv
from openai import OpenAI
from supabase import create_client
from tqdm import tqdm
from tqdm.asyncio import tqdm as async_tqdm
import hashlib

# Try to import tiktoken for accurate token counting
try:
    import tiktoken
    TIKTOKEN_AVAILABLE = True
except ImportError:
    TIKTOKEN_AVAILABLE = False
    logging.warning("tiktoken not available - using conservative token estimation")

load_dotenv()
SUPABASE_URL  = os.getenv("SUPABASE_URL")
SUPABASE_KEY  = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
OPENAI_API_KEY= os.getenv("OPENAI_API_KEY")
EMBEDDING_MODEL="text-embedding-ada-002"
MODEL_TOKEN_LIMIT=8192
TOKEN_GUARD=200

# ğŸ¯ DYNAMIC BATCHING - Token limits with safety margins
MAX_BATCH_TOKENS = 7692  # Conservative limit (8192 - 500 safety margin)
MAX_CHUNK_TOKENS = 6000  # Individual chunk limit
MIN_BATCH_TOKENS = 100   # Minimum viable batch size

MAX_TOTAL_TOKENS=MAX_BATCH_TOKENS  # Use dynamic batch limit instead

# Conservative defaults for rate limiting
DEFAULT_BATCH_ROWS=200  # Reduced from 5000
DEFAULT_COMMIT_ROWS=10  # Reduced from 100
DEFAULT_MAX_CONCURRENT=3  # Reduced from 50
MAX_RETRIES=5
RETRY_DELAY=2
RATE_LIMIT_DELAY=0.3  # 300ms between API calls
MAX_CALLS_PER_MINUTE=150  # Conservative limit

openai_client=OpenAI(api_key=OPENAI_API_KEY)
logging.basicConfig(level=logging.INFO,
    format="%(asctime)s â€” %(levelname)s â€” %(message)s")
log=logging.getLogger(__name__)

# Global variables for rate limiting
call_timestamps = []
total_tokens_used = 0

class AsyncEmbedder:
    """Async embedding client with strict rate limiting and connection pooling."""
    
    def __init__(self, api_key: str, max_concurrent: int = DEFAULT_MAX_CONCURRENT):
        self.api_key = api_key
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.session = None
        self.call_count = 0
        self.start_time = time.time()
        
        # Initialize tiktoken encoder if available
        if TIKTOKEN_AVAILABLE:
            self.encoder = tiktoken.encoding_for_model(EMBEDDING_MODEL)
        else:
            self.encoder = None
    
    async def __aenter__(self):
        timeout = aiohttp.ClientTimeout(total=300)
        connector = aiohttp.TCPConnector(limit=20, limit_per_host=10)  # Reduced limits
        self.session = aiohttp.ClientSession(
            headers={"Authorization": f"Bearer {self.api_key}"},
            timeout=timeout,
            connector=connector
        )
        return self
    
    async def __aexit__(self, *args):
        await self.session.close()
    
    def count_tokens(self, text: str) -> int:
        """Count tokens accurately using tiktoken if available."""
        if self.encoder:
            return len(self.encoder.encode(text))
        else:
            # Conservative fallback
            return int(len(text.split()) * 0.75) + 50
    
    async def rate_limit_check(self):
        """Enforce rate limiting."""
        current_time = time.time()
        
        # Remove timestamps older than 1 minute
        call_timestamps[:] = [ts for ts in call_timestamps if current_time - ts < 60]
        
        # Check if we're approaching the rate limit
        if len(call_timestamps) >= MAX_CALLS_PER_MINUTE - 5:
            sleep_time = 60 - (current_time - call_timestamps[0])
            if sleep_time > 0:
                log.info(f"Rate limit protection: sleeping {sleep_time:.1f}s")
                await asyncio.sleep(sleep_time)
        
        # Always enforce minimum delay between calls
        await asyncio.sleep(RATE_LIMIT_DELAY)
        
        # Record this call
        call_timestamps.append(current_time)
    
    async def embed_batch_async(self, texts: List[str]) -> List[List[float]]:
        """Embed a batch of texts asynchronously with rate limiting and token validation."""
        global total_tokens_used
        
        async with self.semaphore:
            await self.rate_limit_check()
            
            # ğŸ›¡ï¸ FINAL TOKEN VALIDATION - Last safety check before API call
            batch_tokens = sum(self.count_tokens(text) for text in texts)
            
            if batch_tokens > MAX_BATCH_TOKENS:
                error_msg = f"ğŸš¨ CRITICAL: Batch tokens {batch_tokens} exceed limit {MAX_BATCH_TOKENS}"
                log.error(error_msg)
                raise RuntimeError(error_msg)
            
            total_tokens_used += batch_tokens
            
            log.info(f"ğŸ¯ API call: {len(texts)} texts (~{batch_tokens} tokens) - WITHIN LIMITS âœ…")
            log.info(f"ğŸ“Š Total tokens used so far: ~{total_tokens_used}")
            
            for attempt in range(MAX_RETRIES):
                try:
                    async with self.session.post(
                        "https://api.openai.com/v1/embeddings",
                        json={
                            "model": EMBEDDING_MODEL,
                            "input": texts
                        }
                    ) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            self.call_count += 1
                            log.info(f"âœ… API call successful: {len(texts)} embeddings generated")
                            return [item["embedding"] for item in data["data"]]
                        elif resp.status == 400:  # Bad request - likely token limit
                            error = await resp.text()
                            log.error(f"ğŸš¨ API error 400 (likely token limit): {error}")
                            log.error(f"ğŸš¨ Batch details: {len(texts)} texts, {batch_tokens} tokens")
                            raise RuntimeError(f"Token limit API error: {error}")
                        elif resp.status == 429:  # Rate limit error
                            error = await resp.text()
                            log.warning(f"Rate limit hit: {error}")
                            # Exponential backoff for rate limits
                            sleep_time = (2 ** attempt) * 2
                            log.info(f"Exponential backoff: sleeping {sleep_time}s")
                            await asyncio.sleep(sleep_time)
                        else:
                            error = await resp.text()
                            log.warning(f"API error {resp.status}: {error}")
                            await asyncio.sleep(RETRY_DELAY)
                except Exception as e:
                    log.warning(f"Attempt {attempt + 1} failed: {e}")
                    await asyncio.sleep(RETRY_DELAY * (attempt + 1))
            
            raise RuntimeError("Failed to embed batch after retries")

def deduplicate_chunks(chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Remove duplicate text chunks based on content hash."""
    seen_hashes: Set[str] = set()
    unique_chunks = []
    duplicates_removed = 0
    
    for chunk in chunks:
        text = chunk.get("text", "").strip()
        if not text:
            continue
            
        # Create hash of the text content
        text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()
        
        if text_hash not in seen_hashes:
            seen_hashes.add(text_hash)
            unique_chunks.append(chunk)
        else:
            duplicates_removed += 1
    
    if duplicates_removed > 0:
        log.info(f"Deduplication: removed {duplicates_removed} duplicate chunks")
    
    return unique_chunks

async def process_chunks_async(
    sb,
    chunks: List[Dict[str, Any]],
    embedder: AsyncEmbedder,
    commit_size: int = DEFAULT_COMMIT_ROWS
) -> int:
    """Process chunks with async embedding and batch updates."""
    # Deduplicate chunks first
    unique_chunks = deduplicate_chunks(chunks)
    log.info(f"ğŸ“Š CHUNK PROCESSING PROGRESS:")
    log.info(f"   ğŸ“„ Original chunks: {len(chunks)}")
    log.info(f"   ğŸ“„ Unique chunks: {len(unique_chunks)}")
    if len(chunks) != len(unique_chunks):
        log.info(f"   ğŸ”„ Duplicates removed: {len(chunks) - len(unique_chunks)}")
    
    # Create token-safe slices
    slices = safe_slices(unique_chunks, commit_size)
    total_embedded = 0
    total_slices = len(slices)
    
    if total_slices == 0:
        log.warning(f"ğŸ“Š NO SLICES TO PROCESS - All chunks were filtered out")
        return 0
    
    log.info(f"ğŸ“Š EMBEDDING SLICES PROGRESS:")
    log.info(f"   ğŸ¯ Total slices to process: {total_slices}")
    log.info(f"   ğŸ“„ Total chunks to embed: {sum(len(slice_data) for slice_data in slices)}")
    
    # Process each slice
    for i, slice_data in enumerate(slices):
        slice_num = i + 1
        slice_progress = (slice_num / total_slices * 100)
        
        log.info(f"ğŸ“Š SLICE {slice_num}/{total_slices} ({slice_progress:.1f}%):")
        log.info(f"   ğŸ“„ Processing {len(slice_data)} chunks in this slice")
        
        texts = [row["text"] for row in slice_data]
        
        try:
            # Get embeddings asynchronously
            log.info(f"   ğŸ”„ Calling OpenAI API for {len(texts)} embeddings...")
            embeddings = await embedder.embed_batch_async(texts)
            log.info(f"   âœ… Received {len(embeddings)} embeddings from API")
            
            # Update database in batch
            log.info(f"   ğŸ’¾ Updating database for {len(slice_data)} chunks...")
            update_tasks = []
            for row, emb in zip(slice_data, embeddings):
                # âœ… GUARANTEED SKIP LOGIC - Layer 3: Final update verification
                def update_with_verification(r, e):
                    # Final check before updating
                    final_check = sb.table("documents_chunks").select("embedding")\
                        .eq("id", r["id"]).execute()
                    
                    if final_check.data and final_check.data[0].get("embedding"):
                        log.info(f"ğŸ›¡ï¸ Chunk {r['id']} already has embedding - SKIPPING update")
                        return {"skipped": True}
                    
                    # Safe to update
                    return sb.table("documents_chunks")\
                        .update({"embedding": e})\
                        .eq("id", r["id"])\
                        .execute()
                
                # Use thread pool for database updates
                loop = asyncio.get_event_loop()
                task = loop.run_in_executor(None, update_with_verification, row, emb)
                update_tasks.append(task)
            
            # Wait for all updates
            log.info(f"   â³ Waiting for {len(update_tasks)} database updates...")
            results = await asyncio.gather(*update_tasks, return_exceptions=True)
            successful = sum(1 for r in results if not isinstance(r, Exception) and not (isinstance(r, dict) and r.get("skipped")))
            skipped = sum(1 for r in results if isinstance(r, dict) and r.get("skipped"))
            failed = sum(1 for r in results if isinstance(r, Exception))
            total_embedded += successful
            
            # ğŸ“Š SLICE COMPLETION PROGRESS
            log.info(f"ğŸ“Š SLICE {slice_num} COMPLETE:")
            log.info(f"   âœ… Successful updates: {successful}")
            if skipped > 0:
                log.info(f"   â­ï¸  Skipped (already embedded): {skipped}")
            if failed > 0:
                log.info(f"   âŒ Failed updates: {failed}")
            log.info(f"   ğŸ“ˆ Total embedded so far: {total_embedded}")
            
        except Exception as e:
            log.error(f"âŒ SLICE {slice_num} FAILED: {e}")
            log.error(f"   ğŸ“„ Chunks in failed slice: {len(slice_data)}")
    
    # Final summary
    log.info(f"ğŸ“Š CHUNK PROCESSING COMPLETE:")
    log.info(f"   âœ… Total chunks embedded: {total_embedded}")
    log.info(f"   ğŸ“Š Slices processed: {total_slices}")
    log.info(f"   ğŸ“ˆ Success rate: {(total_embedded/len(unique_chunks)*100):.1f}%" if unique_chunks else "0%")
    
    return total_embedded

async def main_async(batch_size: int = None, commit_size: int = None, max_concurrent: int = None):
    """Async main function for embedding."""
    global EMBEDDING_MODEL, MODEL_TOKEN_LIMIT, MAX_TOTAL_TOKENS, total_tokens_used
    
    # Use provided parameters or conservative defaults
    batch_size = batch_size or DEFAULT_BATCH_ROWS
    commit_size = commit_size or DEFAULT_COMMIT_ROWS
    max_concurrent = max_concurrent or DEFAULT_MAX_CONCURRENT
    
    log.info(f"Starting with conservative settings:")
    log.info(f"  Batch size: {batch_size}")
    log.info(f"  Commit size: {commit_size}")
    log.info(f"  Max concurrent: {max_concurrent}")
    log.info(f"  Rate limit delay: {RATE_LIMIT_DELAY}s")
    
    # ğŸ¯ Dynamic batching status
    log.info(f"ğŸ¯ DYNAMIC BATCHING ENABLED:")
    log.info(f"   Max batch tokens: {MAX_BATCH_TOKENS} (with safety margin)")
    log.info(f"   Max chunk tokens: {MAX_CHUNK_TOKENS}")
    log.info(f"   Min batch tokens: {MIN_BATCH_TOKENS}")
    log.info(f"   âœ… GUARANTEED: No token limit API errors")
    
    # âœ… Check tiktoken availability for accurate token counting
    if not TIKTOKEN_AVAILABLE:
        log.warning("âš ï¸  tiktoken not available - using conservative token estimation")
        log.warning("âš ï¸  For accurate token counting, install: pip install tiktoken")
    else:
        log.info("âœ… tiktoken available - using accurate token counting")
    
    if not OPENAI_API_KEY:
        log.error("OPENAI_API_KEY missing")
        sys.exit(1)
    
    sb = init_supabase()
    existing_count = count_processed_chunks(sb)
    log.info(f"Rows already embedded: {existing_count}")
    
    # âœ… GUARANTEED SKIP LOGIC - Status check
    total_chunks_res = sb.table("documents_chunks").select("id", count="exact")\
        .eq("chunking_strategy", "token_window").execute()
    total_chunks = total_chunks_res.count or 0
    
    pending_chunks = total_chunks - existing_count
    
    log.info("ğŸ“Š Current status:")
    log.info(f"   Chunks already embedded: {existing_count}")
    log.info(f"   Chunks needing embedding: {pending_chunks}")
    log.info(f"   Total chunks: {total_chunks}")
    
    if pending_chunks == 0:
        log.info("âœ… All chunks already have embeddings - nothing to do!")
        return
    
    async with AsyncEmbedder(OPENAI_API_KEY, max_concurrent) as embedder:
        loop, total = 0, 0
        
        # ğŸ“Š PROGRESS TRACKING - Initialize counters
        total_chunks_to_process = pending_chunks
        chunks_processed = 0
        chunks_skipped = 0
        
        log.info(f"ğŸ“Š EMBEDDING PROGRESS TRACKING INITIALIZED:")
        log.info(f"   ğŸ¯ Total chunks to embed: {total_chunks_to_process}")
        log.info(f"   ğŸ¯ Starting embedding process...")
        
        while True:
            loop += 1
            rows = fetch_unprocessed_chunks(sb, limit=batch_size, offset=0)
            
            if not rows:
                log.info("âœ¨ Done â€” no more rows.")
                break
            
            # ğŸ“Š PROGRESS: Show current status before processing
            remaining_chunks = total_chunks_to_process - chunks_processed
            progress_percent = (chunks_processed / total_chunks_to_process * 100) if total_chunks_to_process > 0 else 0
            
            log.info(f"ğŸ“Š EMBEDDING PROGRESS - Loop {loop}:")
            log.info(f"   ğŸ“„ Fetched: {len(rows)} chunks")
            log.info(f"   âœ… Processed: {chunks_processed}/{total_chunks_to_process} ({progress_percent:.1f}%)")
            log.info(f"   â³ Remaining: {remaining_chunks} chunks")
            if chunks_skipped > 0:
                log.info(f"   âš ï¸  Skipped: {chunks_skipped} oversized chunks")
            
            # Process chunks asynchronously
            embedded = await process_chunks_async(sb, rows, embedder, commit_size)
            total += embedded
            chunks_processed += embedded
            
            # Update skipped count (this will be calculated in process_chunks_async)
            current_chunk_count = count_processed_chunks(sb)
            actual_embedded_this_loop = current_chunk_count - (existing_count + chunks_processed - embedded)
            
            # ğŸ“Š PROGRESS: Show results after processing
            final_progress_percent = (chunks_processed / total_chunks_to_process * 100) if total_chunks_to_process > 0 else 0
            log.info(f"ğŸ“Š LOOP {loop} COMPLETE:")
            log.info(f"   âœ… This loop: {embedded} chunks embedded")
            log.info(f"   ğŸ“ˆ Total progress: {chunks_processed}/{total_chunks_to_process} ({final_progress_percent:.1f}%)")
            log.info(f"   ğŸ”„ API calls made: {embedder.call_count}")
            log.info(f"   ğŸ“Š Total tokens used: ~{total_tokens_used}")
            
            # Check if we're making progress or stuck
            if embedded == 0 and len(rows) > 0:
                chunks_skipped += len(rows)
                log.warning(f"âš ï¸  WARNING: No chunks embedded in this loop - all {len(rows)} chunks were skipped")
                log.warning(f"âš ï¸  Total skipped so far: {chunks_skipped} chunks")
                
                # Prevent infinite loop by limiting consecutive failed attempts
                if loop > 5 and embedded == 0:
                    log.error(f"ğŸš¨ STUCK: {loop} consecutive loops with no progress - stopping to prevent infinite loop")
                    break
    
    # Save report
    ts = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    report = {
        "timestamp": ts,
        "batch_size": batch_size,
        "commit": commit_size,
        "max_concurrent": max_concurrent,
        "total_embedded": total,
        "total_with_embeddings": count_processed_chunks(sb),
        "total_tokens_used": total_tokens_used,
        "api_calls_made": embedder.call_count if 'embedder' in locals() else 0
    }
    
    # Create reports directory if it doesn't exist
    reports_dir = "reports/embedding"
    os.makedirs(reports_dir, exist_ok=True)
    
    fname = f"{reports_dir}/supabase_embedding_report_{ts}.json"
    with open(fname, "w") as fp:
        json.dump(report, fp, indent=2)
    
    log.info(f"Report saved to {fname}")

async def main_async_cli():
    """CLI version with argument parsing."""
    global EMBEDDING_MODEL, MODEL_TOKEN_LIMIT, MAX_TOTAL_TOKENS
    
    ap = argparse.ArgumentParser()
    ap.add_argument("--batch-size", type=int, default=DEFAULT_BATCH_ROWS)
    ap.add_argument("--commit", type=int, default=DEFAULT_COMMIT_ROWS)
    ap.add_argument("--skip", type=int, default=0)
    ap.add_argument("--model", default=EMBEDDING_MODEL)
    ap.add_argument("--model-limit", type=int, default=MODEL_TOKEN_LIMIT)
    ap.add_argument("--max-concurrent", type=int, default=DEFAULT_MAX_CONCURRENT,
                   help="Maximum concurrent API calls (default: 3 for rate limiting)")
    args = ap.parse_args()
    
    EMBEDDING_MODEL = args.model
    MODEL_TOKEN_LIMIT = args.model_limit
    MAX_TOTAL_TOKENS = MODEL_TOKEN_LIMIT - TOKEN_GUARD
    
    await main_async(args.batch_size, args.commit, args.max_concurrent)

# Keep original interface for compatibility
def main() -> None:
    """Original synchronous interface."""
    asyncio.run(main_async_cli())

# Keep all existing helper functions unchanged...
def init_supabase():
    if not SUPABASE_URL or not SUPABASE_KEY:
        log.error("SUPABASE creds missing"); sys.exit(1)
    return create_client(SUPABASE_URL,SUPABASE_KEY)

def count_processed_chunks(sb)->int:
    res=sb.table("documents_chunks").select("id",count="exact").not_.is_("embedding","null").execute()
    return res.count or 0

def fetch_unprocessed_chunks(sb,*,limit:int,offset:int=0)->List[Dict[str,Any]]:
    first,last=offset,offset+limit-1
    res=sb.table("documents_chunks").select("id,text,token_start,token_end")\
        .eq("chunking_strategy","token_window").is_("embedding","null")\
        .range(first,last).execute()
    
    chunks = res.data or []
    
    if chunks:
        # âœ… GUARANTEED SKIP LOGIC - Layer 2: Double-check verification
        chunk_ids = [chunk["id"] for chunk in chunks]
        verification = sb.table("documents_chunks").select("id")\
            .in_("id", chunk_ids).not_.is_("embedding", "null").execute()
        
        already_embedded_ids = {row["id"] for row in verification.data or []}
        
        if already_embedded_ids:
            log.warning(f"ğŸ›¡ï¸ Found {len(already_embedded_ids)} chunks that already have embeddings - SKIPPING them")
            chunks = [chunk for chunk in chunks if chunk["id"] not in already_embedded_ids]
        
        log.info(f"âœ… GUARANTEED: Fetched {len(chunks)} chunks WITHOUT embeddings")
        
        if not chunks:
            log.info("âœ… All chunks already have embeddings - nothing to do!")
    
    return chunks

def generate_embedding_batch(texts:List[str])->List[List[float]]:
    attempt=0
    while attempt<MAX_RETRIES:
        try:
            resp=openai_client.embeddings.create(model=EMBEDDING_MODEL,input=texts)
            return [d.embedding for d in resp.data]
        except Exception as exc:
            attempt+=1; log.warning("Embedding batch %s/%s failed: %s",attempt,MAX_RETRIES,exc)
            time.sleep(RETRY_DELAY)
    raise RuntimeError("OpenAI embedding batch failed after retries")

def chunk_tokens(row:Dict[str,Any])->int:
    try:
        t=int(row["token_end"])-int(row["token_start"])+1
        if 0<t<=16384: return t
    except: pass
    
    # Use tiktoken if available for more accurate counting
    text = row.get("text", "")
    if TIKTOKEN_AVAILABLE:
        try:
            encoder = tiktoken.encoding_for_model(EMBEDDING_MODEL)
            return len(encoder.encode(text))
        except:
            pass
    
    # Conservative fallback
    approx=int(len(text.split())*0.75)+50  # More conservative estimate
    return min(max(1,approx),MODEL_TOKEN_LIMIT)

def safe_slices(rows:List[Dict[str,Any]],max_rows:int)->List[List[Dict[str,Any]]]:
    """ğŸ¯ DYNAMIC BATCH SIZING - Guarantees no token limit errors."""
    return dynamic_batch_slices(rows)

def dynamic_batch_slices(rows: List[Dict[str, Any]]) -> List[List[Dict[str, Any]]]:
    """
    ğŸ¯ DYNAMIC BATCH SIZING - Creates variable-sized batches that GUARANTEE no API token errors.
    
    Protection Layers:
    1. Individual chunk validation (max 6,000 tokens)
    2. Dynamic batch sizing (max 7,692 tokens total)
    3. Conservative safety margins
    4. Multiple validation checks
    """
    batches = []
    current_batch = []
    current_tokens = 0
    skipped_chunks = 0
    
    # Initialize token encoder for accurate counting
    encoder = None
    if TIKTOKEN_AVAILABLE:
        try:
            encoder = tiktoken.encoding_for_model(EMBEDDING_MODEL)
        except:
            pass
    
    def count_tokens_accurate(text: str) -> int:
        """Count tokens with maximum accuracy."""
        if encoder:
            return len(encoder.encode(text))
        else:
            # Conservative fallback estimation
            return int(len(text.split()) * 0.75) + 50
    
    log.info(f"ğŸ¯ Starting dynamic batch creation with {len(rows)} chunks")
    log.info(f"ğŸ¯ Limits: {MAX_CHUNK_TOKENS} tokens/chunk, {MAX_BATCH_TOKENS} tokens/batch")
    
    for i, row in enumerate(rows):
        # Clean text and validate
        text = (row.get("text") or "").replace("\x00", "").strip()
        if not text:
            log.warning(f"ğŸ¯ Skipping empty chunk {row.get('id', 'unknown')}")
            continue
        
        # ï¸ PROTECTION LAYER 1: Individual chunk validation
        chunk_tokens = count_tokens_accurate(text)
        
        if chunk_tokens > MAX_CHUNK_TOKENS:
            log.warning(f"ğŸ¯ Skipping oversized chunk {row.get('id', 'unknown')}: {chunk_tokens} tokens (max: {MAX_CHUNK_TOKENS})")
            skipped_chunks += 1
            continue
        
        # ğŸ›¡ï¸ PROTECTION LAYER 2: Dynamic batch sizing
        would_exceed = current_tokens + chunk_tokens > MAX_BATCH_TOKENS
        
        if would_exceed and current_batch:
            # Finalize current batch
            log.info(f"ğŸ¯ Created batch with {len(current_batch)} chunks (~{current_tokens} tokens)")
            batches.append(current_batch)
            current_batch = []
            current_tokens = 0
        
        # Add chunk to current batch
        current_batch.append({"id": row["id"], "text": text})
        current_tokens += chunk_tokens
        
        # ğŸ›¡ï¸ PROTECTION LAYER 3: Safety validation
        if current_tokens > MAX_BATCH_TOKENS:
            log.error(f"ğŸš¨ CRITICAL: Batch exceeded limit! {current_tokens} > {MAX_BATCH_TOKENS}")
            # Emergency fallback - remove last chunk and finalize batch
            if len(current_batch) > 1:
                current_batch.pop()
                current_tokens -= chunk_tokens
                log.info(f"ğŸ¯ Emergency: Created batch with {len(current_batch)} chunks (~{current_tokens} tokens)")
                batches.append(current_batch)
                current_batch = [{"id": row["id"], "text": text}]
                current_tokens = chunk_tokens
            else:
                log.error(f"ğŸš¨ Single chunk too large: {chunk_tokens} tokens")
                current_batch = []
                current_tokens = 0
                skipped_chunks += 1
    
    # Add final batch if not empty
    if current_batch and current_tokens >= MIN_BATCH_TOKENS:
        log.info(f"ğŸ¯ Created final batch with {len(current_batch)} chunks (~{current_tokens} tokens)")
        batches.append(current_batch)
    elif current_batch:
        log.warning(f"ğŸ¯ Skipping tiny final batch: {current_tokens} tokens < {MIN_BATCH_TOKENS} minimum")
    
    # ğŸ›¡ï¸ PROTECTION LAYER 4: Final validation
    total_chunks = sum(len(batch) for batch in batches)
    max_batch_tokens = max((sum(count_tokens_accurate(chunk["text"]) for chunk in batch) for batch in batches), default=0)
    
    log.info(f"ğŸ¯ DYNAMIC BATCHING COMPLETE:")
    log.info(f"   ğŸ“Š {len(batches)} batches created")
    log.info(f"   ğŸ“Š {total_chunks} chunks processed")
    log.info(f"   ğŸ“Š {skipped_chunks} chunks skipped")
    log.info(f"   ğŸ“Š Largest batch: {max_batch_tokens} tokens (limit: {MAX_BATCH_TOKENS})")
    log.info(f"   âœ… GUARANTEED: No batch exceeds {MAX_BATCH_TOKENS} tokens")
    
    if max_batch_tokens > MAX_BATCH_TOKENS:
        log.error(f"ğŸš¨ CRITICAL ERROR: Batch validation failed!")
        raise RuntimeError(f"Batch token validation failed: {max_batch_tokens} > {MAX_BATCH_TOKENS}")
    
    return batches

def embed_slice(sb,slice_rows:List[Dict[str,Any]])->int:
    embeds=generate_embedding_batch([r["text"] for r in slice_rows])
    ok=0
    for row,emb in zip(slice_rows,embeds):
        attempt=0
        while attempt<MAX_RETRIES:
            res=sb.table("documents_chunks").update({"embedding":emb}).eq("id",row["id"]).execute()
            if getattr(res,"error",None):
                attempt+=1; log.warning("Update %s failed (%s/%s): %s",row["id"],attempt,MAX_RETRIES,res.error)
                time.sleep(RETRY_DELAY)
            else: ok+=1; break
    return ok

if __name__=="__main__": 
    asyncio.run(main_async_cli())


================================================================================


################################################################################
# File: scripts/pipeline_modular_optimized.py
################################################################################

# File: scripts/pipeline_modular_optimized.py

#!/usr/bin/env python3
"""
Optimized pipeline orchestrator with full parallelization.
Maintains compatibility with original pipeline_modular.py interface.
"""
from __future__ import annotations
import argparse, logging, pathlib, random
from collections import Counter
from rich.console import Console
import asyncio
from typing import List, Optional
import multiprocessing as mp

# Import stages
from stages import extract_clean, llm_enrich, chunk_text, db_upsert, embed_vectors
from stages.acceleration_utils import hardware

# Keep existing toggles
RUN_EXTRACT    = True
RUN_LLM_ENRICH = True
RUN_CHUNK      = True
RUN_DB         = True
RUN_EMBED      = True

log = logging.getLogger("pipeline-modular-optimized")

class OptimizedPipeline:
    """Optimized pipeline with parallel processing and rate limiting."""
    
    def __init__(self, batch_size: int = 15, max_api_concurrent: int = 3):  # ğŸ›¡ï¸ Rate limited: was 50, 20
        self.batch_size = batch_size
        self.max_api_concurrent = max_api_concurrent
        self.stats = Counter()
        
        # ğŸ›¡ï¸ Log rate limiting settings
        log.info("ğŸ›¡ï¸  Rate-limited pipeline initialized:")
        log.info(f"   Batch size: {batch_size} (was 50)")
        log.info(f"   Max concurrent: {max_api_concurrent} (was 20)")
        log.info("   Target: <800K tokens/minute (safe margin)")
    
    async def process_batch(self, pdfs: List[pathlib.Path], start_doc_num: int = 1, total_docs: int = None) -> None:
        """Process a batch of PDFs through all stages with individual document progress tracking."""
        
        if total_docs is None:
            total_docs = len(pdfs)
        
        batch_size = len(pdfs)
        log.info(f"ğŸ“Š BATCH PROCESSING START:")
        log.info(f"   ğŸ“„ Documents in this batch: {batch_size}")
        log.info(f"   ğŸ¯ Document range: {start_doc_num} to {start_doc_num + batch_size - 1}")
        log.info(f"   ğŸ“ˆ Overall progress: {start_doc_num-1}/{total_docs} completed ({((start_doc_num-1)/total_docs*100):.1f}%)")
        
        # Stage 1-2: Extract & Clean (CPU-bound, use process pool)
        json_docs = []
        if RUN_EXTRACT:
            log.info(f"ğŸ”„ EXTRACTION STAGE - Processing {len(pdfs)} PDFs...")
            for i, pdf in enumerate(pdfs):
                doc_num = start_doc_num + i
                doc_progress = ((doc_num-1) / total_docs * 100)
                log.info(f"ğŸ“„ [{doc_num}/{total_docs}] ({doc_progress:.1f}%) Extracting: {pdf.name}")
            
            json_docs = await extract_clean.extract_batch_async(
                pdfs, 
                enrich_llm=False  # We'll do LLM enrichment separately
            )
            
            log.info(f"ğŸ“Š EXTRACTION STAGE COMPLETE:")
            for i, pdf in enumerate(pdfs):
                doc_num = start_doc_num + i
                log.info(f"   âœ… [{doc_num}/{total_docs}] Extracted: {pdf.name}")
        else:
            # Use existing JSON files
            json_docs = [extract_clean.json_path_for(pdf) for pdf in pdfs]
            log.info(f"ğŸ“Š EXTRACTION STAGE SKIPPED - Using existing JSON files")

        # Stage 4: LLM Enrich (I/O-bound, use async)
        if RUN_LLM_ENRICH and json_docs:
            log.info(f"ğŸ”„ ENRICHMENT STAGE - Processing {len(json_docs)} documents...")
            for i, json_doc in enumerate(json_docs):
                doc_num = start_doc_num + i
                doc_progress = ((doc_num-1) / total_docs * 100)
                doc_name = pathlib.Path(json_doc).stem.replace('_clean', '')
                log.info(f"ğŸ“„ [{doc_num}/{total_docs}] ({doc_progress:.1f}%) Enriching: {doc_name}")
            
            await llm_enrich.enrich_batch_async(
                json_docs,
                max_concurrent=self.max_api_concurrent
            )
            
            log.info(f"ğŸ“Š ENRICHMENT STAGE COMPLETE:")
            for i, json_doc in enumerate(json_docs):
                doc_num = start_doc_num + i
                doc_name = pathlib.Path(json_doc).stem.replace('_clean', '')
                log.info(f"   âœ… [{doc_num}/{total_docs}] Enriched: {doc_name}")

        # Stage 5: Chunk (CPU-bound, use process pool)
        chunks_map = {}
        if RUN_CHUNK and json_docs:
            log.info(f"ğŸ”„ CHUNKING STAGE - Processing {len(json_docs)} documents...")
            for i, json_doc in enumerate(json_docs):
                doc_num = start_doc_num + i
                doc_progress = ((doc_num-1) / total_docs * 100)
                doc_name = pathlib.Path(json_doc).stem.replace('_clean', '')
                log.info(f"ğŸ“„ [{doc_num}/{total_docs}] ({doc_progress:.1f}%) Chunking: {doc_name}")
            
            chunks_map = await chunk_text.chunk_batch_async(json_docs)
            
            log.info(f"ğŸ“Š CHUNKING STAGE COMPLETE:")
            total_chunks_created = 0
            for i, json_doc in enumerate(json_docs):
                doc_num = start_doc_num + i
                doc_name = pathlib.Path(json_doc).stem.replace('_clean', '')
                chunk_count = len(chunks_map.get(json_doc, []))
                total_chunks_created += chunk_count
                log.info(f"   âœ… [{doc_num}/{total_docs}] Chunked: {doc_name} ({chunk_count} chunks)")
            log.info(f"   ğŸ“Š Total chunks created in this batch: {total_chunks_created}")

        # Stage 6: DB Upsert (I/O-bound, use async)
        if RUN_DB and chunks_map:
            log.info(f"ğŸ”„ DATABASE UPSERT STAGE - Processing {len(chunks_map)} documents...")
            
            # Show progress before upserting
            total_chunks_to_upsert = sum(len(chunks) for chunks in chunks_map.values())
            log.info(f"ğŸ“Š DATABASE UPSERT PROGRESS:")
            log.info(f"   ğŸ’¾ Total chunks to upsert: {total_chunks_to_upsert}")
            
            for i, (json_path, chunks) in enumerate(chunks_map.items()):
                doc_num = start_doc_num + i
                doc_progress = ((doc_num-1) / total_docs * 100)
                doc_name = pathlib.Path(json_path).stem.replace('_clean', '')
                log.info(f"ğŸ“„ [{doc_num}/{total_docs}] ({doc_progress:.1f}%) Upserting: {doc_name} ({len(chunks)} chunks)")
            
            documents = [
                {
                    "json_path": json_path,
                    "chunks": chunks,
                    "do_embed": False  # We'll embed in batch later
                }
                for json_path, chunks in chunks_map.items()
            ]
            await db_upsert.upsert_batch_async(documents)
            
            log.info(f"ğŸ“Š DATABASE UPSERT STAGE COMPLETE:")
            total_chunks_upserted = 0
            for i, (json_path, chunks) in enumerate(chunks_map.items()):
                doc_num = start_doc_num + i
                doc_name = pathlib.Path(json_path).stem.replace('_clean', '')
                total_chunks_upserted += len(chunks)
                log.info(f"   âœ… [{doc_num}/{total_docs}] Upserted: {doc_name}")
            log.info(f"   ğŸ“Š Total chunks upserted: {total_chunks_upserted}")

        # Update stats
        self.stats["ok"] += len([c for c in chunks_map.values() if c])
        self.stats["fail"] += len([c for c in chunks_map.values() if not c])
        
        # Final batch summary
        end_doc_num = start_doc_num + batch_size - 1
        overall_progress = (end_doc_num / total_docs * 100)
        log.info(f"ğŸ“Š BATCH COMPLETE:")
        log.info(f"   âœ… Documents processed: {start_doc_num}-{end_doc_num}")
        log.info(f"   ğŸ“ˆ Overall progress: {end_doc_num}/{total_docs} ({overall_progress:.1f}%)")
        log.info(f"   ğŸ“Š Successful documents: {self.stats['ok']}")
        log.info(f"   âŒ Failed documents: {self.stats['fail']}")
    
    async def run(self, src: pathlib.Path, selection: str = "sequential", cap: int = 0):
        """Run the optimized pipeline."""
        Console().rule("[bold cyan]Misophonia PDF â†’ Vector pipeline (optimized)")
        
        # Get PDF list
        pdfs = [src] if src.is_file() else sorted(src.rglob("*.pdf"))
        if cap:
            pdfs = random.sample(pdfs, cap) if selection == "random" else pdfs[:cap]
        
        total_pdfs = len(pdfs)
        log.info(f"Processing {total_pdfs} PDFs in batches of {self.batch_size}")
        
        # Track PDF-level progress
        pdfs_processed = 0
        
        # Process in batches
        for i in range(0, len(pdfs), self.batch_size):
            batch = pdfs[i:i + self.batch_size]
            batch_num = i//self.batch_size + 1
            total_batches = (len(pdfs) + self.batch_size - 1)//self.batch_size
            
            # Calculate the starting document number for this batch
            start_doc_num = pdfs_processed + 1
            
            # Enhanced progress logging with both batch and PDF-level progress
            log.info(f"ğŸ“„ Processing batch {batch_num}/{total_batches} ({len(batch)} PDFs)")
            log.info(f"ğŸ“Š Overall progress: {pdfs_processed}/{total_pdfs} PDFs completed ({pdfs_processed/total_pdfs*100:.1f}%)")
            log.info(f"ğŸ”¢ Document range: {start_doc_num}-{start_doc_num + len(batch) - 1} of {total_pdfs}")
            
            await self.process_batch(batch, start_doc_num, total_pdfs)
            
            # Update PDF progress counter
            pdfs_processed += len(batch)
            
            # Log completion of this batch
            log.info(f"âœ… Batch {batch_num} complete - Total PDFs processed: {pdfs_processed}/{total_pdfs} ({pdfs_processed/total_pdfs*100:.1f}%)")
        
        # Final progress summary
        log.info(f"ğŸ‰ All PDFs processed: {pdfs_processed}/{total_pdfs} ({pdfs_processed/total_pdfs*100:.1f}%)")
        
        # Stage 7: Batch embed all at once with conservative settings
        if RUN_EMBED:
            log.info("ğŸ”„ STARTING EMBEDDING STAGE...")
            log.info("ğŸ“Š EMBEDDING STAGE PROGRESS:")
            log.info("   ğŸ¯ Running batch embedding with rate limiting...")
            log.info("   ğŸ¯ This stage will process all upserted chunks for embedding")
            
            # ğŸ›¡ï¸ Pass conservative embedding parameters (consistent with embed_vectors.py defaults)
            await embed_vectors.main_async(
                batch_size=200,     # ğŸ›¡ï¸ Rate limited: fetch 200 chunks at once (conservative default)
                commit_size=10,     # ğŸ›¡ï¸ Rate limited: 10 chunks per API call (conservative default) 
                max_concurrent=3    # ğŸ›¡ï¸ Rate limited: max 3 concurrent embedding calls (conservative default)
            )
            
            log.info("ğŸ“Š EMBEDDING STAGE COMPLETE âœ…")
        
        Console().rule("[green]Finished")
        log.info("ğŸ“Š PIPELINE COMPLETE - FINAL SUMMARY:")
        log.info("ğŸ›¡ï¸  Rate limiting successful - no API limits hit")
        log.info(f"ğŸ“Š Total documents processed: {self.stats['ok']}")
        log.info(f"ğŸ“Š Total documents failed: {self.stats['fail']}")
        log.info(f"ğŸ“Š Success rate: {(self.stats['ok']/(self.stats['ok']+self.stats['fail'])*100):.1f}%" if (self.stats['ok']+self.stats['fail']) > 0 else "100%")

def main(src: pathlib.Path, selection: str = "sequential", cap: int = 0) -> None:
    """Main entry point compatible with original pipeline_modular.py"""
    # Set multiprocessing start method for macOS
    mp.set_start_method('spawn', force=True)
    
    # Create and run pipeline with conservative rate limiting
    pipeline = OptimizedPipeline(
        batch_size=15,  # ğŸ›¡ï¸ Rate limited: process 15 PDFs at a time (was 50)
        max_api_concurrent=3  # ğŸ›¡ï¸ Rate limited: max 3 concurrent API requests (was 20)
    )
    
    # Run async pipeline
    asyncio.run(pipeline.run(src, selection, cap))

if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s â€” %(levelname)s â€” %(message)s"
    )
    
    p = argparse.ArgumentParser()
    p.add_argument("src", type=pathlib.Path,
                   default=pathlib.Path("city_clerk_documents/global"), nargs="?")
    p.add_argument("--selection", choices=["sequential", "random"],
                   default="sequential")
    p.add_argument("--cap", type=int, default=0)
    p.add_argument("--batch-size", type=int, default=15,  # ğŸ›¡ï¸ Rate limited: default to 15 (was 50)
                   help="Number of PDFs to process in parallel")
    p.add_argument("--api-concurrent", type=int, default=3,  # ğŸ›¡ï¸ Rate limited: default to 3 (was 20)
                   help="Max concurrent API calls")
    args = p.parse_args()
    
    # Override batch size if specified
    if args.batch_size:
        pipeline = OptimizedPipeline(
            batch_size=args.batch_size,
            max_api_concurrent=args.api_concurrent
        )
        asyncio.run(pipeline.run(args.src, args.selection, args.cap))
    else:
        main(args.src, args.selection, args.cap)


================================================================================


################################################################################
# File: relationOPENAI.py
################################################################################

# File: relationOPENAI.py

import os, json, time, re, hashlib, traceback
from azure.storage.blob import BlobServiceClient
from gremlin_python.driver import client, serializer
from openai import AzureOpenAI

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  TEST-MODE  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TEST_MODE, MAX_VERTICES = False, 5           # pon False cuando validado
vertex_count, early_exit = 0, False

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  CONFIG GENERAL  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BLOB_CONNECTION_STRING = (
    "DefaultEndpointsProtocol=https;"
    "AccountName=rasagptstorageaccount;"
    "AccountKey=[KEY_HERE];"
    "EndpointSuffix=core.windows.net"
)
COSMOS_ENDPOINT = "wss://aida-graph-db.gremlin.cosmos.azure.com:443"
COSMOS_KEY      = "[KEY_HERE]"
DATABASE, CONTAINER = "cgGraph", "cityClerk" 
PARTITION_KEY, PARTITION_VALUE = "partitionKey", "demo"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  NUEVO CLIENTE AZURE OPENAI  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
aoai = AzureOpenAI(
    api_key        = [KEY HERE],
    azure_endpoint = "https://aida-gpt4o.openai.azure.com",
    api_version    = "2024-02-15-preview"
)

DEPLOYMENT_NAME = "gpt-4o"          # nombre EXACTO en Deployments

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  RESTO DE CONFIG  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ENTITY_CONTAINERS = [
    "ks-entities-person","ks-entities-organization","ks-entities-location",
    "ks-entities-address","ks-entities-phone","ks-entities-email",
    "ks-entities-url","ks-entities-event","ks-entities-product",
    "ks-entities-persontype","ks-entities-ipaddress",
    "ks-entities-quantity","ks-entities-skill"
]
CONTAINER_TO_LABEL = {
    "ks-entities-person":"Person","ks-entities-organization":"Organization","ks-entities-location":"Location",
    "ks-entities-address":"Address","ks-entities-phone":"PhoneNumber","ks-entities-email":"Email",
    "ks-entities-url":"URL","ks-entities-event":"Event","ks-entities-product":"Product",
    "ks-entities-persontype":"PersonType","ks-entities-ipaddress":"IPAddress",
    "ks-entities-quantity":"Quantity","ks-entities-skill":"Skill"
}
CHUNKS_CONTAINER = "ks-chunks-debug"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  HELPERS  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ILLEGAL_ID_CHARS = re.compile(r'[\/\\?#]')
def clean_id(s: str)  -> str: return ILLEGAL_ID_CHARS.sub('_', s)
def clean_txt(s: str) -> str: return s.replace("'", "\\'").replace('"', "")
def limit_reached():
    global early_exit
    if TEST_MODE and vertex_count >= MAX_VERTICES:
        early_exit = True
        return True
    return False

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  CONEXIONES  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("ğŸ”—  Connecting â€¦")
blob = BlobServiceClient.from_connection_string(BLOB_CONNECTION_STRING)
gremlin = client.Client(
    f"{COSMOS_ENDPOINT}/gremlin","g",
    username=f"/dbs/{DATABASE}/colls/{CONTAINER}",
    password=COSMOS_KEY,
    message_serializer=serializer.GraphSONSerializersV2d0())

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  CARGA DE CHUNKS  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
chunk_text, chunk_entities = {}, {}
print("ğŸ“¥  Loading chunks â€¦")
for b in blob.get_container_client(CHUNKS_CONTAINER).list_blobs():
    if not b.name.endswith(".json"): continue
    doc = json.loads(blob.get_blob_client(CHUNKS_CONTAINER, b.name).download_blob().readall())
    raw_id = doc.get("chunkId") or doc.get("metadata_storage_path") or b.name
    cid    = clean_id(raw_id)
    chunk_text[cid] = doc.get("content") or doc.get("text") or ""
    chunk_entities[cid] = []

print("ğŸ“¥  Loading entities â€¦")
for cont in ENTITY_CONTAINERS:
    label = CONTAINER_TO_LABEL[cont]
    cc    = blob.get_container_client(cont)
    for b in cc.list_blobs():
        if not b.name.endswith(".json"): continue
        data = json.loads(cc.get_blob_client(b).download_blob().readall())
        if isinstance(data, dict): data = [data]
        for e in data:
            raw_cid = e.get("chunkId") or e.get("metadata_storage_path")
            if raw_cid and raw_cid.startswith(f"{CHUNKS_CONTAINER}/"):
                raw_cid = raw_cid[len(CHUNKS_CONTAINER)+1:]
            cid = clean_id(raw_cid)
            if cid not in chunk_entities:
                print(f"âš ï¸  Unmatched entity â†’ {raw_cid}")
                continue
            name = e.get("text") or e.get("name")
            vid  = f"{label}:{hashlib.sha1(name.encode()).hexdigest()}"
            chunk_entities[cid].append({"id":vid,"label":label,"name":name})

print(f"â¡ï¸  Prepared {len(chunk_entities)} chunks.")

PROMPT = """You are a knowledge-graph extractor.
Return only factual triples (pure JSON):
[{{"source":"<id>","relation":"<label>","target":"<id>"}}]

TEXT:
\"\"\"{chunk}\"\"\"

ENTITIES (pairs [id, name]):
{ents}
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  MAIN LOOP  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
for cid, ents in chunk_entities.items():
    if early_exit: break
    if not ents:   continue

    print(f"\nğŸš©  Chunk {cid[:60]}  ({len(ents)} entities)")
    text      = chunk_text[cid][:3000]
    ents_json = [[e["id"], e["name"]] for e in ents]

    # Llamada LLM
    # â”€â”€ LLM call + robust JSON parse â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    try:
        rsp = aoai.chat.completions.create(
            model       = DEPLOYMENT_NAME,     # "gpt-4o"
            temperature = 0.0,
            max_tokens  = 256,
            messages = [
                {"role":"system","content":
                "You are a knowledge-graph extractor. "
                "Return only factual triples in valid JSON."},
                {"role":"user","content":
                PROMPT.format(chunk=text, ents=json.dumps(ents_json))}
            ]
        )

        raw = (rsp.choices[0].message.content or "").strip()
        print("ğŸ§   RAW reply:", raw[:120].replace("\n"," ") + ("â€¦" if len(raw) > 120 else ""))

        if not raw:
            print("âš ï¸  Empty response (content filter?).")
            triples = []

        else:
            try:
                triples = json.loads(raw)
                print(f"ğŸ§   Parsed {len(triples)} triples")
            except json.JSONDecodeError as je:
                print("âš ï¸  JSONDecodeError:", je)
                print("âš ï¸  Full reply kept for manual inspection:")
                print(raw)
                triples = []

    except Exception as ex:
        print("âŒ  LLM call failed:", ex)
        triples = []


    ts = int(time.time()*1000)

    # Chunk vertex
    if not limit_reached():
        gremlin.submit(
            f"g.V('{cid}').fold().coalesce(unfold(),"
            f"addV('Chunk').property(id,'{cid}')"
            f".property('{PARTITION_KEY}','{PARTITION_VALUE}'))").all()

    # Entity vertices & MENTIONS
    for e in ents:
        if limit_reached(): break
        try:
            gremlin.submit(
                f"g.V('{e['id']}').fold().coalesce(unfold(),"
                f"addV('{e['label']}').property(id,'{e['id']}')"
                f".property('name','{clean_txt(e['name'])}')"
                f".property('{PARTITION_KEY}','{PARTITION_VALUE}'))").all()
            gremlin.submit(
                f"g.V('{cid}').coalesce("
                f"outE('MENTIONS').where(inV().hasId('{e['id']}')),"
                f"addE('MENTIONS').to(g.V('{e['id']}')).property('ts',{ts}))").all()
            vertex_count += 1
            print(f"   âœ”ï¸ {e['id']}")
        except Exception:
            print("âš ï¸  Vert/Edge error\n", traceback.format_exc())

    # Semantic edges
    if not limit_reached():
        for t in triples:
            s,r,d = t.get("source"), t.get("relation"), t.get("target")
            if not (s and r and d): continue
            try:
                gremlin.submit(
                    f"g.V('{s}').coalesce("
                    f"outE('{r}').where(inV().hasId('{d}')),"
                    f"addE('{r}').to(g.V('{d}')))").all()
                print(f"   â‡¢ {s} -[{r}]-> {d}")
            except Exception:
                print("âš ï¸  Edge error\n", traceback.format_exc())

    if early_exit:
        print(f"ğŸ›‘  Reached {MAX_VERTICES} vertices (TEST).")
        break

    time.sleep(0.05)

print("\nğŸ  Finished.")
gremlin.close()


================================================================================


################################################################################
# File: scripts/stages/llm_enrich.py
################################################################################

# File: scripts/stages/llm_enrich.py

#!/usr/bin/env python3
"""
Stage 4 â€” LLM metadata enrichment with concurrent API calls.
"""
from __future__ import annotations
import json, logging, pathlib, re, os
from textwrap import dedent
from typing import Any, Dict, List
import asyncio
from concurrent.futures import ThreadPoolExecutor
import aiofiles

# â”€â”€â”€ minimal shared helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _authors(val:Any)->List[str]:
    if val is None: return []
    if isinstance(val,list): return [str(a).strip() for a in val if a]
    return re.split(r"\s*,\s*|\s+and\s+", str(val).strip())

META_FIELDS_CORE = [
    "document_type", "title", "date", "year", "month", "day",
    "mayor", "vice_mayor", "commissioners",
    "city_attorney", "city_manager", "city_clerk", "public_works_director",
    "agenda", "keywords"
]
EXTRA_MD_FIELDS = ["peer_reviewed","open_access","license","open_access_status"]
META_FIELDS     = META_FIELDS_CORE+EXTRA_MD_FIELDS
_DEF_META_TEMPLATE = {**{k:None for k in META_FIELDS_CORE},
                      "doc_type":"scientific paper",
                      "authors":[], "keywords":[], "research_topics":[],
                      "peer_reviewed":None,"open_access":None,
                      "license":None,"open_access_status":None}

def merge_meta(*sources:Dict[str,Any])->Dict[str,Any]:
    merged=_DEF_META_TEMPLATE.copy()
    for src in sources:
        for k in META_FIELDS:
            v=src.get(k)
            if v not in (None,"",[],{}): merged[k]=v
    merged["authors"]=_authors(merged["authors"])
    merged["keywords"]=merged["keywords"] or []
    merged["research_topics"]=merged["research_topics"] or []
    return merged
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

from dotenv import load_dotenv
from openai import OpenAI
load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
MODEL          = "gpt-4.1-mini-2025-04-14"
log            = logging.getLogger(__name__)

def _first_words(txt:str,n:int=3000)->str: return " ".join(txt.split()[:n])

def _gpt(text:str)->Dict[str,Any]:
    if not OPENAI_API_KEY: return {}
    cli=OpenAI(api_key=OPENAI_API_KEY)
    prompt=dedent(f"""
        Extract all metadata fields from this city clerk document. Return ONE JSON object with these fields:
        - document_type: must be one of [Resolution, Ordinance, Proclamation, Contract, Meeting Minutes, Agenda]
        - title: the document title
        - date: full date string as found in document
        - year: numeric year (YYYY)
        - month: numeric month (1-12)
        - day: numeric day of month
        - mayor: name only (e.g., "John Smith") - single person
        - vice_mayor: name only (e.g., "Jane Doe") - single person
        - commissioners: array of commissioner names only (e.g., ["Robert Brown", "Sarah Johnson", "Michael Davis"])
        - city_attorney: name only (e.g., "Emily Wilson")
        - city_manager: name only
        - city_clerk: name only
        - public_works_director: name only
        - agenda: agenda items or meeting topics if present
        - keywords: array of relevant keywords or topics (e.g., ["budget", "zoning", "infrastructure"])
        
        Text:
        {text}
    """)
    rsp=cli.chat.completions.create(model=MODEL,temperature=0,
            messages=[{"role":"system","content":"metadata extractor"},
                      {"role":"user","content":prompt}])
    raw=rsp.choices[0].message.content
    m=re.search(r"{[\s\S]*}",raw)
    return json.loads(m.group(0) if m else "{}")

# Async version of GPT call
async def _gpt_async(text: str, semaphore: asyncio.Semaphore) -> Dict[str, Any]:
    """Async GPT call with rate limiting."""
    if not OPENAI_API_KEY:
        return {}
    
    async with semaphore:  # Rate limiting
        loop = asyncio.get_event_loop()
        # Run synchronous OpenAI call in thread pool
        return await loop.run_in_executor(None, _gpt, text)

async def enrich_async(json_path: pathlib.Path, semaphore: asyncio.Semaphore) -> None:
    """Async version of enrich."""
    # Read file asynchronously
    async with aiofiles.open(json_path, 'r') as f:
        content = await f.read()
        data = json.loads(content)
    
    # Reconstruct body text
    full = " ".join(
        el.get("text", "") for sec in data["sections"] 
        for el in sec.get("elements", [])
    )
    
    # Make async GPT call
    new_meta = await _gpt_async(_first_words(full), semaphore)
    
    # Merge metadata
    data.update(new_meta)
    
    # Write back asynchronously
    async with aiofiles.open(json_path, 'w') as f:
        await f.write(json.dumps(data, indent=2, ensure_ascii=False))
    
    log.info("âœ“ metadata enriched â†’ %s", json_path.name)

async def enrich_batch_async(
    json_paths: List[pathlib.Path],
    max_concurrent: int = 10
) -> None:
    """Enrich multiple documents concurrently with rate limiting."""
    semaphore = asyncio.Semaphore(max_concurrent)
    
    tasks = [enrich_async(path, semaphore) for path in json_paths]
    
    from tqdm.asyncio import tqdm_asyncio
    await tqdm_asyncio.gather(*tasks, desc="Enriching metadata")

# Keep original interface for compatibility
def enrich(json_path: pathlib.Path) -> None:
    """Original synchronous interface."""
    data = json.loads(json_path.read_text())
    full = " ".join(
        el.get("text", "") for sec in data["sections"] 
        for el in sec.get("elements", [])
    )
    new_meta = _gpt(_first_words(full))
    data.update(new_meta)
    json_path.write_text(json.dumps(data, indent=2, ensure_ascii=False), 'utf-8')
    log.info("âœ“ metadata enriched â†’ %s", json_path.name)

if __name__ == "__main__":
    import argparse, logging
    logging.basicConfig(level=logging.INFO, format="%(message)s")
    p=argparse.ArgumentParser(); p.add_argument("json",type=pathlib.Path)
    enrich(p.parse_args().json)


================================================================================


################################################################################
# File: scripts/find_duplicates.py
################################################################################

# File: scripts/find_duplicates.py

import os
import hashlib
import argparse
from pathlib import Path
from collections import defaultdict

def calculate_file_hash(filepath, algorithm='sha256', buffer_size=65536):
    """Calculate a hash for a file to identify duplicates."""
    hash_obj = hashlib.new(algorithm)
    
    with open(filepath, 'rb') as f:
        # Read the file in chunks to handle large files efficiently
        buffer = f.read(buffer_size)
        while buffer:
            hash_obj.update(buffer)
            buffer = f.read(buffer_size)
    
    return hash_obj.hexdigest()

def find_duplicates(directory):
    """Find duplicate files in the specified directory."""
    files_by_hash = defaultdict(list)
    duplicate_sets = []
    
    # Get all files in the directory
    target_dir = Path(directory)
    if not target_dir.exists() or not target_dir.is_dir():
        print(f"Error: '{directory}' is not a valid directory")
        return duplicate_sets
    
    print(f"Scanning directory: {directory}")
    
    # Calculate hashes for all files
    all_files = list(target_dir.glob('*'))
    total_files = len(all_files)
    
    for i, file_path in enumerate(all_files):
        if file_path.is_file():
            try:
                file_hash = calculate_file_hash(file_path)
                files_by_hash[file_hash].append(file_path)
                print(f"Processed file {i+1}/{total_files}: {file_path.name}")
            except Exception as e:
                print(f"Error processing {file_path}: {e}")
    
    # Identify duplicate sets (files with the same hash)
    for file_hash, paths in files_by_hash.items():
        if len(paths) > 1:
            duplicate_sets.append(paths)
    
    return duplicate_sets

def delete_duplicates(duplicate_sets, interactive=True):
    """Delete duplicate files, keeping only one copy of each."""
    total_deleted = 0
    total_size_saved = 0
    
    for duplicate_set in duplicate_sets:
        # Sort by name for consistent results
        duplicate_set.sort(key=lambda p: str(p))
        
        # Keep the first file, show options for the rest
        keep_file = duplicate_set[0]
        print(f"\nDuplicate set ({len(duplicate_set)} files):")
        print(f"  Keeping: {keep_file}")
        
        for i, dup_file in enumerate(duplicate_set[1:], 1):
            size = dup_file.stat().st_size
            
            if interactive:
                response = input(f"  Delete duplicate #{i}: {dup_file}? (y/n/a=all/q=quit): ").lower()
                
                if response == 'q':
                    print("Operation aborted.")
                    return total_deleted, total_size_saved
                    
                if response == 'a':
                    interactive = False
                    response = 'y'
            else:
                response = 'y'
                print(f"  Deleting duplicate #{i}: {dup_file}")
            
            if response == 'y':
                try:
                    dup_file.unlink()
                    total_deleted += 1
                    total_size_saved += size
                    print(f"  Deleted: {dup_file}")
                except Exception as e:
                    print(f"  Error deleting {dup_file}: {e}")
    
    return total_deleted, total_size_saved

def format_size(size_bytes):
    """Format file size in human-readable format."""
    if size_bytes < 1024:
        return f"{size_bytes} bytes"
    elif size_bytes < 1024 * 1024:
        return f"{size_bytes/1024:.2f} KB"
    elif size_bytes < 1024 * 1024 * 1024:
        return f"{size_bytes/(1024*1024):.2f} MB"
    else:
        return f"{size_bytes/(1024*1024*1024):.2f} GB"

def main():
    parser = argparse.ArgumentParser(description="Find and remove duplicate files")
    parser.add_argument('--directory', '-d', default='city_clerk_documents/global',
                        help="Directory to scan for duplicates (default: city_clerk_documents/global)")
    parser.add_argument('--delete', '-r', action='store_true',
                        help="Delete duplicate files")
    parser.add_argument('--auto', '-a', action='store_true',
                        help="Automatically delete all duplicates without prompting")
    
    args = parser.parse_args()
    
    # Find duplicates
    duplicate_sets = find_duplicates(args.directory)
    
    # Print summary of duplicates found
    if not duplicate_sets:
        print("\nNo duplicate files found.")
        return
    
    total_duplicates = sum(len(dups) - 1 for dups in duplicate_sets)
    print(f"\nFound {len(duplicate_sets)} sets of duplicate files ({total_duplicates} redundant files)")
    
    # Display details about each duplicate set
    for i, dups in enumerate(duplicate_sets, 1):
        size = dups[0].stat().st_size
        size_str = format_size(size)
        print(f"\nDuplicate Set #{i} - {len(dups)} files, {size_str} each:")
        for path in dups:
            print(f"  {path}")
    
    # Delete duplicates if requested
    if args.delete or args.auto:
        deleted, size_saved = delete_duplicates(duplicate_sets, not args.auto)
        print(f"\nSummary: Deleted {deleted} duplicate files, saving {format_size(size_saved)}")
    else:
        print("\nTo delete duplicates, run again with --delete or --auto flag")

if __name__ == "__main__":
    main()


================================================================================


################################################################################
# File: scripts/graph_stages/entity_deduplicator.py
################################################################################

# File: scripts/graph_stages/entity_deduplicator.py

"""
Entity Deduplication Module
==========================
Handles deduplication of persons and other entities across documents.
"""
import difflib
from typing import Dict, List, Optional, Set, Tuple
import logging

log = logging.getLogger(__name__)

class EntityDeduplicator:
    """Deduplicate entities across documents."""
    
    def __init__(self, similarity_threshold: float = 0.85):
        self.similarity_threshold = similarity_threshold
        self.person_aliases: Dict[str, str] = {}  # alias -> canonical name
        self._load_known_aliases()
    
    def _load_known_aliases(self):
        """Load known name variations for city officials."""
        # Common variations
        self.person_aliases.update({
            # Mayors
            "Vince Lago": "Vince Lago",
            "Vincent Lago": "Vince Lago",
            "Mayor Lago": "Vince Lago",
            
            # Commissioners
            "Rhonda Anderson": "Rhonda Anderson",
            "Vice Mayor Anderson": "Rhonda Anderson",
            
            # Add more as discovered...
        })
    
    def deduplicate_person_name(self, name: str, existing_names: List[str]) -> str:
        """
        Find canonical version of a person name.
        
        Args:
            name: The name to check
            existing_names: List of known canonical names
            
        Returns:
            Canonical name (either existing match or the input name)
        """
        # First check known aliases
        if name in self.person_aliases:
            return self.person_aliases[name]
        
        # Clean the name
        clean_name = self._clean_person_name(name)
        
        # Find best match among existing names
        best_match = self._find_best_match(clean_name, existing_names)
        
        if best_match:
            # Store alias for future use
            self.person_aliases[name] = best_match
            return best_match
        
        # No match found, this is a new canonical name
        return clean_name
    
    def _clean_person_name(self, name: str) -> str:
        """Clean and normalize a person name."""
        # Remove titles
        titles = [
            'Mayor', 'Vice Mayor', 'Commissioner', 'Dr.', 'Mr.', 'Mrs.', 'Ms.',
            'City Attorney', 'City Manager', 'City Clerk'
        ]
        
        clean = name
        for title in titles:
            clean = clean.replace(title, '').strip()
        
        # Remove extra whitespace
        clean = ' '.join(clean.split())
        
        return clean
    
    def _find_best_match(self, name: str, candidates: List[str]) -> Optional[str]:
        """Find best matching name from candidates."""
        if not candidates:
            return None
        
        # Try exact match first
        if name in candidates:
            return name
        
        # Try fuzzy matching
        matches = difflib.get_close_matches(
            name, 
            candidates, 
            n=1, 
            cutoff=self.similarity_threshold
        )
        
        if matches:
            return matches[0]
        
        # Try last name matching for "FirstName LastName" patterns
        name_parts = name.split()
        if len(name_parts) >= 2:
            last_name = name_parts[-1]
            for candidate in candidates:
                if candidate.endswith(last_name):
                    # Additional check: first name initial match
                    if name[0] == candidate[0]:
                        return candidate
        
        return None
    
    def merge_person_roles(self, existing_roles: List[str], new_roles: List[str]) -> List[str]:
        """Merge role lists, maintaining uniqueness and hierarchy."""
        # Role hierarchy (higher number = higher priority)
        role_priority = {
            'Mayor': 10,
            'Vice Mayor': 9,
            'Commissioner': 8,
            'City Attorney': 7,
            'City Manager': 7,
            'City Clerk': 6,
            'Public Works Director': 6,
            'Sponsor': 5,
            'Public Speaker': 4,
        }
        
        all_roles = set(existing_roles + new_roles)
        
        # Sort by priority
        sorted_roles = sorted(
            all_roles,
            key=lambda r: role_priority.get(r, 0),
            reverse=True
        )
        
        return sorted_roles

class MeetingDeduplicator:
    """Deduplicate meeting entities."""
    
    @staticmethod
    def normalize_date(date_str: str) -> str:
        """Normalize date string to consistent format."""
        # Handle various formats:
        # 6.11.2024 -> 06.11.2024
        # 06_11_2024 -> 06.11.2024
        # June 11, 2024 -> 06.11.2024
        
        import re
        
        # Replace underscores with dots
        normalized = date_str.replace('_', '.')
        
        # Handle M.DD.YYYY -> MM.DD.YYYY
        match = re.match(r'^(\d)\.(\d{2})\.(\d{4})$', normalized)
        if match:
            month, day, year = match.groups()
            normalized = f"{int(month):02d}.{day}.{year}"
        
        return normalized


================================================================================


################################################################################
# File: scripts/test_graph_pipeline.py
################################################################################

# File: scripts/test_graph_pipeline.py

#!/usr/bin/env python3
"""Test script for the graph pipeline."""
import asyncio
import logging
from pathlib import Path
from graph_pipeline import GraphPipeline

logging.basicConfig(level=logging.INFO)

async def test_single_date():
    """Test processing a single date."""
    base_dir = Path("city_clerk_documents/global/City Commissions 2024")
    
    if not base_dir.exists():
        print(f"Error: Base directory not found: {base_dir}")
        return
    
    pipeline = GraphPipeline(base_dir)
    
    # Test with a specific date
    test_date = "06.11.2024"
    
    print(f"Testing pipeline with date: {test_date}")
    await pipeline.initialize()
    
    try:
        await pipeline.process_batch(test_date)
        print("Test completed successfully!")
    except Exception as e:
        print(f"Test failed: {e}")
        import traceback
        traceback.print_exc()
    finally:
        await pipeline.cosmos_client.close()

if __name__ == "__main__":
    asyncio.run(test_single_date())


================================================================================


################################################################################
# File: scripts/graph_stages/__init__.py
################################################################################

# File: scripts/graph_stages/__init__.py

"""Graph pipeline stages for City Clerk document processing."""

__all__ = [
    "agenda_parser",
    "graph_extractor", 
    "cosmos_db_client",
    "entity_deduplicator",
    "relationship_builder"
]


================================================================================

