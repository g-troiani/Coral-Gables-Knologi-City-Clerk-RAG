# Concatenated Project Code - Part 1 of 3
# Generated: 2025-06-13 17:03:10
# Root Directory: /Users/gianmariatroiani/Documents/knologi/graph_database
================================================================================

# Directory Structure
################################################################################
├── .gitignore (939.0B, no ext)
├── city_clerk_documents/
│   └── [EXCLUDED] 8 items: .DS_Store (excluded file), extracted_markdown (excluded dir), extracted_text (excluded dir)
│       ... and 5 more excluded items
├── config.py (1.7KB, .py)
├── graph_visualizer.py (27.9KB, .py)
├── ontology_model.txt (6.6KB, .txt)
├── ontology_modelv2.txt (10.2KB, .txt)
├── repository_directory_structure.txt (5.7KB, .txt)
├── requirements.txt (1.2KB, .txt)
├── run_graph_visualizer.sh (3.6KB, .sh)
├── run_pipeline.sh (700.0B, .sh)
├── scripts/
│   ├── RAG_stages/
│   │   ├── __init__.py (382.0B, .py)
│   │   ├── acceleration_utils.py (3.8KB, .py)
│   │   ├── chunk_text.py (18.6KB, .py)
│   │   ├── db_upsert.py (8.9KB, .py)
│   │   ├── embed_vectors.py (27.0KB, .py)
│   │   ├── extract_clean.py (21.8KB, .py)
│   │   └── llm_enrich.py (6.0KB, .py)
│   ├── extract_all_to_markdown.py (8.2KB, .py)
│   ├── graph_pipeline.py (21.9KB, .py)
│   ├── graph_rag_stages/
│   │   ├── README.md (7.3KB, .md)
│   │   ├── __init__.py (551.0B, .py)
│   │   ├── common/
│   │   │   ├── __init__.py (701.0B, .py)
│   │   │   ├── config.py (4.6KB, .py)
│   │   │   ├── cosmos_client.py (9.4KB, .py)
│   │   │   ├── utils.py (7.4KB, .py)
│   │   │   ├── [EXCLUDED] 1 items: __pycache__ (excluded dir)
│   │   ├── main_pipeline.py (2.7KB, .py)
│   │   ├── phase1_preprocessing/
│   │   │   ├── __init__.py (2.4KB, .py)
│   │   │   ├── agenda_extractor.py (12.7KB, .py)
│   │   │   ├── document_linker.py (9.3KB, .py)
│   │   │   ├── pdf_extractor.py (3.5KB, .py)
│   │   │   ├── transcript_linker.py (12.5KB, .py)
│   │   │   ├── [EXCLUDED] 1 items: __pycache__ (excluded dir)
│   │   ├── phase2_building/
│   │   │   ├── __init__.py (4.1KB, .py)
│   │   │   ├── custom_graph_builder.py (10.8KB, .py)
│   │   │   ├── entity_deduplicator.py (8.7KB, .py)
│   │   │   ├── graphrag_adapter.py (10.5KB, .py)
│   │   │   ├── graphrag_indexer.py (7.5KB, .py)
│   │   │   ├── [EXCLUDED] 1 items: __pycache__ (excluded dir)
│   │   ├── phase3_querying/
│   │   │   ├── __init__.py (2.5KB, .py)
│   │   │   ├── city_clerk_query_engine.py (64.5KB, .py)
│   │   │   ├── query_engine.py (8.1KB, .py)
│   │   │   ├── query_router.py (4.3KB, .py)
│   │   │   ├── response_enhancer.py (3.4KB, .py)
│   │   │   ├── smart_query_router.py (16.1KB, .py)
│   │   │   ├── source_tracker.py (3.8KB, .py)
│   │   │   ├── structural_query_enhancer.py (14.7KB, .py)
│   │   │   ├── [EXCLUDED] 1 items: __pycache__ (excluded dir)
│   │   ├── [EXCLUDED] 1 items: __pycache__ (excluded dir)
│   ├── [EXCLUDED] 3 items: pipeline_modular_optimized.py (excluded file), rag_local_web_app.py (excluded file), supabase_clear_database.py (excluded file)
├── settings.yaml (1.2KB, .yaml)
├── temp_extraction_output/
│   └── [EXCLUDED] 1 items: debug (excluded dir)
├── ui/
│   ├── __init__.py (86.0B, .py)
│   ├── query_app.py (30.0KB, .py)
│   ├── [EXCLUDED] 1 items: __pycache__ (excluded dir)
├── [EXCLUDED] 11 items: .DS_Store (excluded file), .env (excluded file), .git (excluded dir)
    ... and 8 more excluded items


================================================================================


# File Index - Which Files Are in Which Parts
################################################################################

## Part 1 (12 files):
  - scripts/graph_rag_stages/phase3_querying/city_clerk_query_engine.py
  - scripts/graph_rag_stages/phase2_building/custom_graph_builder.py
  - scripts/graph_rag_stages/phase2_building/graphrag_adapter.py
  - scripts/extract_all_to_markdown.py
  - scripts/graph_rag_stages/phase3_querying/query_engine.py
  - scripts/graph_rag_stages/common/config.py
  - scripts/graph_rag_stages/phase3_querying/query_router.py
  - scripts/graph_rag_stages/phase3_querying/source_tracker.py
  - scripts/graph_rag_stages/main_pipeline.py
  - settings.yaml
  - scripts/graph_rag_stages/common/__init__.py
  - ui/__init__.py

## Part 2 (13 files):
  - ui/query_app.py
  - scripts/RAG_stages/chunk_text.py
  - scripts/graph_rag_stages/phase3_querying/structural_query_enhancer.py
  - scripts/graph_rag_stages/phase1_preprocessing/agenda_extractor.py
  - scripts/graph_rag_stages/common/cosmos_client.py
  - scripts/graph_rag_stages/phase2_building/entity_deduplicator.py
  - scripts/graph_rag_stages/phase2_building/graphrag_indexer.py
  - scripts/RAG_stages/llm_enrich.py
  - scripts/graph_rag_stages/phase2_building/__init__.py
  - scripts/graph_rag_stages/phase1_preprocessing/pdf_extractor.py
  - scripts/graph_rag_stages/phase3_querying/__init__.py
  - config.py
  - scripts/graph_rag_stages/__init__.py

## Part 3 (13 files):
  - scripts/RAG_stages/embed_vectors.py
  - scripts/RAG_stages/extract_clean.py
  - scripts/graph_rag_stages/phase3_querying/smart_query_router.py
  - scripts/graph_rag_stages/phase1_preprocessing/transcript_linker.py
  - scripts/graph_rag_stages/phase1_preprocessing/document_linker.py
  - scripts/RAG_stages/db_upsert.py
  - scripts/graph_rag_stages/common/utils.py
  - scripts/graph_rag_stages/README.md
  - scripts/RAG_stages/acceleration_utils.py
  - scripts/graph_rag_stages/phase3_querying/response_enhancer.py
  - scripts/graph_rag_stages/phase1_preprocessing/__init__.py
  - requirements.txt
  - scripts/RAG_stages/__init__.py


================================================================================


################################################################################
# File: scripts/graph_rag_stages/phase3_querying/city_clerk_query_engine.py
################################################################################

# File: scripts/graph_rag_stages/phase3_querying/city_clerk_query_engine.py

import subprocess
import sys
import os
import json
import re
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Set
from enum import Enum
import logging
from .smart_query_router import SmartQueryRouter, QueryIntent
from .source_tracker import SourceTracker
from .structural_query_enhancer import StructuralQueryEnhancer

logger = logging.getLogger(__name__)

class QueryType(Enum):
    LOCAL = "local"
    GLOBAL = "global"
    DRIFT = "drift"

class CityClerkQueryEngine:
    """Enhanced query engine with inline source citations."""
    
    def __init__(self, graphrag_root: Path):
        self.graphrag_root = Path(graphrag_root)
        # Check for deduplicated data and use it if available
        output_dir = self.graphrag_root / "output"
        dedup_dir = output_dir / "deduplicated"
        if dedup_dir.exists() and list(dedup_dir.glob("*.parquet")):
            self.output_dir = dedup_dir
            logger.info("Using deduplicated GraphRAG data")
            
            # Load aliases for better query matching
            import pandas as pd
            entities_df = pd.read_parquet(dedup_dir / "entities.parquet")
            if 'aliases' in entities_df.columns:
                self.entity_aliases = {}
                for idx, row in entities_df.iterrows():
                    if row.get('aliases'):
                        for alias in row['aliases'].split('|'):
                            self.entity_aliases[alias.lower()] = row['title']
        else:
            self.output_dir = output_dir
            self.entity_aliases = {}
        self.source_tracker = SourceTracker()  # New component
        
        # Initialize structural query enhancer for completeness queries
        extracted_text_dir = self.graphrag_root.parent / "city_clerk_documents" / "extracted_text"
        self.structural_enhancer = StructuralQueryEnhancer(extracted_text_dir)
        
    def _get_python_executable(self):
        """Get the correct Python executable."""
        from pathlib import Path
        
        current_file = Path(__file__)
        project_root = current_file.parent.parent.parent
        
        venv_python = project_root / "venv" / "bin" / "python3"
        if venv_python.exists():
            return str(venv_python)
        
        return sys.executable
        
    async def query(self, query: str, method: Optional[str] = None, **kwargs) -> Dict[str, Any]:
        """Execute query with source tracking and inline citations."""
        
        # Enable source tracking
        kwargs['track_sources'] = True
        
        # Route query
        if not method:
            router = SmartQueryRouter()
            route_info = router.determine_query_method(query)
            method = route_info['method']
            kwargs.update(route_info.get('params', {}))
        
        # Execute query with source tracking
        if method == 'local':
            result = await self._local_search_with_sources(query, **kwargs)
        elif method == 'global':
            result = await self._global_search_with_sources(query, **kwargs)
        elif method == 'drift':
            result = await self._drift_search_with_sources(query, **kwargs)
        else:
            raise ValueError(f"Unknown method: {method}")
        
        # Clean up any JSON artifacts from the answer
        result['answer'] = self._clean_json_artifacts(result['answer'])
        
        # Process answer to add inline citations
        result['answer'] = self._add_inline_citations(result['answer'], result['sources_used'])
        
        # Apply structural enhancement for completeness queries
        result = self.structural_enhancer.enhance_graphrag_response(query, result)
        
        return result
    
    async def _local_search_with_sources(self, query: str, **kwargs) -> Dict[str, Any]:
        """Local search with comprehensive source tracking."""
        
        # Use the existing working local search implementation
        result = await self._execute_local_query(query, kwargs)
        
        # Extract sources from the GraphRAG response
        sources_used = self._extract_sources_from_local_response(result['answer'])
        
        # Add source tracking to the result
        result['sources_used'] = sources_used
        result['data_sources'] = self._format_data_sources(sources_used)
        
        return result
    
    async def _global_search_with_sources(self, query: str, **kwargs) -> Dict[str, Any]:
        """Global search with source tracking."""
        
        # Use the existing working global search implementation
        result = await self._execute_global_query(query, kwargs)
        
        # Extract sources from the GraphRAG response
        sources_used = self._extract_sources_from_global_response(result['answer'])
        
        # Add source tracking to the result
        result['sources_used'] = sources_used
        result['data_sources'] = self._format_data_sources(sources_used)
        
        return result
    
    async def _drift_search_with_sources(self, query: str, **kwargs) -> Dict[str, Any]:
        """DRIFT search with source tracking."""
        
        # Use the existing working drift search implementation
        result = await self._execute_drift_query(query, kwargs)
        
        # Extract sources from the GraphRAG response
        sources_used = self._extract_sources_from_drift_response(result['answer'])
        
        # Add source tracking to the result
        result['sources_used'] = sources_used
        result['data_sources'] = self._format_data_sources(sources_used)
        
        return result
    
    def _clean_json_artifacts(self, answer: str) -> str:
        """Clean JSON artifacts and metadata from GraphRAG response."""
        if not answer:
            return answer
            
        import re
        import json
        
        # Remove JSON blocks that might appear in the response
        # Pattern 1: Remove standalone JSON objects
        json_pattern = r'\{[^{}]*"[^"]*":\s*[^{}]*\}'
        answer = re.sub(json_pattern, '', answer)
        
        # Pattern 2: Remove array-like structures
        array_pattern = r'\[[^\[\]]*"[^"]*"[^\[\]]*\]'
        answer = re.sub(array_pattern, '', answer)
        
        # Pattern 3: Remove configuration-like strings
        config_patterns = [
            r'"[^"]*":\s*"[^"]*"',  # "key": "value"
            r'"[^"]*":\s*\d+',      # "key": 123
            r'"[^"]*":\s*true|false', # "key": true/false
            r'"[^"]*":\s*null',     # "key": null
        ]
        
        for pattern in config_patterns:
            answer = re.sub(pattern, '', answer)
        
        # Remove metadata headers that sometimes appear
        metadata_patterns = [
            r'SUCCESS:\s*.*?\n',
            r'INFO:\s*.*?\n',
            r'DEBUG:\s*.*?\n',
            r'WARNING:\s*.*?\n',
            r'ERROR:\s*.*?\n',
            r'METADATA:\s*.*?\n',
            r'RESPONSE:\s*',
            r'QUERY:\s*.*?\n',
        ]
        
        for pattern in metadata_patterns:
            answer = re.sub(pattern, '', answer, flags=re.IGNORECASE)
        
        # Remove any lines that look like JSON structure
        lines = answer.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            # Skip lines that are purely JSON-like
            if (line.startswith('{') and line.endswith('}')) or \
               (line.startswith('[') and line.endswith(']')) or \
               (line.startswith('"') and line.endswith('"') and ':' in line):
                continue
            # Skip empty lines created by removal
            if line:
                cleaned_lines.append(line)
        
        # Rejoin and clean up extra whitespace
        cleaned_answer = '\n'.join(cleaned_lines)
        
        # Remove multiple consecutive newlines
        cleaned_answer = re.sub(r'\n\s*\n\s*\n', '\n\n', cleaned_answer)
        
        # Remove leading/trailing whitespace
        cleaned_answer = cleaned_answer.strip()
        
        return cleaned_answer
    
    def _add_inline_citations(self, answer: str, sources_used: Dict[str, Any]) -> str:
        """Add inline citations to answer text."""
        
        # Extract entity and relationship IDs for citation
        entity_ids = list(sources_used.get('entities', {}).keys())
        rel_ids = list(sources_used.get('relationships', {}).keys())
        source_ids = list(sources_used.get('sources', {}).keys())
        
        # Split answer into paragraphs
        paragraphs = answer.split('\n\n')
        cited_paragraphs = []
        
        for para in paragraphs:
            if not para.strip():
                cited_paragraphs.append(para)
                continue
            
            # Determine which sources are relevant to this paragraph
            relevant_entities = []
            relevant_rels = []
            relevant_sources = []
            
            # Simple relevance check based on entity mentions
            para_lower = para.lower()
            
            for eid, entity in sources_used.get('entities', {}).items():
                if entity['title'].lower() in para_lower or \
                   any(word in para_lower for word in entity.get('description', '').lower().split()[:10]):
                    relevant_entities.append(str(eid))
            
            for rid, rel in sources_used.get('relationships', {}).items():
                if any(word in para_lower for word in rel.get('description', '').lower().split()[:10]):
                    relevant_rels.append(str(rid))
            
            # Add generic source references
            if relevant_entities or relevant_rels:
                relevant_sources = source_ids[:3]  # Use first few sources
            
            # Build citation
            if relevant_entities or relevant_rels or relevant_sources:
                citation_parts = []
                
                if relevant_sources:
                    citation_parts.append(f"Sources ({', '.join(map(str, relevant_sources[:5]))})")
                
                if relevant_entities:
                    citation_parts.append(f"Entities ({', '.join(relevant_entities[:7])})")
                
                if relevant_rels:
                    citation_parts.append(f"Relationships ({', '.join(relevant_rels[:5])})")
                
                citation = f" Data: {'; '.join(citation_parts)}."
                cited_paragraphs.append(para + citation)
            else:
                cited_paragraphs.append(para)
        
        return '\n\n'.join(cited_paragraphs)
    
    def _format_data_sources(self, sources_used: Dict[str, Any]) -> Dict[str, List[Any]]:
        """Format sources for display."""
        return {
            'entities': list(sources_used.get('entities', {}).values()),
            'relationships': list(sources_used.get('relationships', {}).values()),
            'sources': list(sources_used.get('sources', {}).values()),
            'text_units': list(sources_used.get('text_units', {}).values())
        }
    
    def _extract_sources_from_global_response(self, response: str) -> Dict[str, Any]:
        """Extract sources from global search response with actual data lookup."""
        sources_used = {
            'entities': {},
            'relationships': {},
            'sources': {},
            'text_units': {}
        }
        
        try:
            import pandas as pd
            
            # Load GraphRAG output files
            entities_path = self.graphrag_root / "output" / "entities.parquet"
            community_reports_path = self.graphrag_root / "output" / "community_reports.parquet"
            
            entities_df = None
            reports_df = None
            
            if entities_path.exists():
                entities_df = pd.read_parquet(entities_path)
            if community_reports_path.exists():
                reports_df = pd.read_parquet(community_reports_path)
            
            # Parse community references and entity IDs from response
            import re
            entity_matches = re.findall(r'Entities\s*\(([^)]+)\)', response)
            
            for match in entity_matches:
                ids = [id.strip() for id in match.split(',') if id.strip().replace('+more', '').strip().isdigit()]
                for entity_id_str in ids:
                    entity_id = int(entity_id_str)
                    
                    # Look up actual entity data
                    if entities_df is not None and entity_id in entities_df.index:
                        entity_row = entities_df.loc[entity_id]
                        sources_used['entities'][entity_id] = {
                            'id': entity_id,
                            'title': entity_row.get('title', f'Entity {entity_id}'),
                            'type': entity_row.get('type', 'Unknown'),
                            'description': entity_row.get('description', 'No description available')[:200]
                        }
                    else:
                        # Fallback if not found
                        sources_used['entities'][entity_id] = {
                            'id': entity_id,
                            'title': f'Entity {entity_id}',
                            'type': 'Unknown',
                            'description': 'Entity not found in output'
                        }
            
            # Parse community report references
            report_matches = re.findall(r'Reports\s*\(([^)]+)\)', response)
            
            for match in report_matches:
                ids = [id.strip() for id in match.split(',') if id.strip().replace('+more', '').strip().isdigit()]
                for report_id_str in ids:
                    report_id = int(report_id_str)
                    
                    # Look up actual community report data
                    if reports_df is not None and report_id in reports_df.index:
                        report_row = reports_df.loc[report_id]
                        sources_used['sources'][report_id] = {
                            'id': report_id,
                            'title': f"Community Report #{report_id}",
                            'type': 'community_report',
                            'text_preview': report_row.get('summary', 'No summary available')[:100]
                        }
                    else:
                        # Fallback if not found
                        sources_used['sources'][report_id] = {
                            'id': report_id,
                            'title': f'Community Report {report_id}',
                            'type': 'community_report',
                            'text_preview': 'Report not found in output'
                        }
        
        except Exception as e:
            logger.error(f"Error loading GraphRAG data for global source extraction: {e}")
            import traceback
            traceback.print_exc()
        
        return sources_used
    
    def _extract_sources_from_drift_response(self, response: str) -> Dict[str, Any]:
        """Extract sources from DRIFT search response with actual data lookup."""
        sources_used = {
            'entities': {},
            'relationships': {},
            'sources': {},
            'text_units': {}
        }
        
        try:
            import pandas as pd
            
            # Load GraphRAG output files
            entities_path = self.graphrag_root / "output" / "entities.parquet"
            relationships_path = self.graphrag_root / "output" / "relationships.parquet"
            text_units_path = self.graphrag_root / "output" / "text_units.parquet"
            
            entities_df = None
            relationships_df = None
            text_units_df = None
            
            if entities_path.exists():
                entities_df = pd.read_parquet(entities_path)
            if relationships_path.exists():
                relationships_df = pd.read_parquet(relationships_path)
            if text_units_path.exists():
                text_units_df = pd.read_parquet(text_units_path)
            
            # Parse entity references from DRIFT response
            import re
            entity_matches = re.findall(r'Entities\s*\(([^)]+)\)', response)
            
            for match in entity_matches:
                ids = [id.strip() for id in match.split(',') if id.strip().replace('+more', '').strip().isdigit()]
                for entity_id_str in ids:
                    entity_id = int(entity_id_str)
                    
                    # Look up actual entity data
                    if entities_df is not None and entity_id in entities_df.index:
                        entity_row = entities_df.loc[entity_id]
                        sources_used['entities'][entity_id] = {
                            'id': entity_id,
                            'title': entity_row.get('title', f'Entity {entity_id}'),
                            'type': entity_row.get('type', 'Unknown'),
                            'description': entity_row.get('description', 'No description available')[:200]
                        }
                    else:
                        # Fallback if not found
                        sources_used['entities'][entity_id] = {
                            'id': entity_id,
                            'title': f'Entity {entity_id}',
                            'type': 'Unknown',
                            'description': 'Entity not found in output'
                        }
            
            # Parse relationship references
            rel_matches = re.findall(r'Relationships\s*\(([^)]+)\)', response)
            
            for match in rel_matches:
                ids = [id.strip() for id in match.split(',') if id.strip().replace('+more', '').strip().isdigit()]
                for rel_id_str in ids:
                    rel_id = int(rel_id_str)
                    
                    # Look up actual relationship data
                    if relationships_df is not None and rel_id in relationships_df.index:
                        rel_row = relationships_df.loc[rel_id]
                        sources_used['relationships'][rel_id] = {
                            'id': rel_id,
                            'source': rel_row.get('source', f'Relationship {rel_id}'),
                            'target': rel_row.get('target', 'Unknown'),
                            'description': rel_row.get('description', 'No description available')[:200],
                            'weight': rel_row.get('weight', 0.0)
                        }
                    else:
                        # Fallback if not found
                        sources_used['relationships'][rel_id] = {
                            'id': rel_id,
                            'source': f'Relationship {rel_id}',
                            'target': 'Unknown',
                            'description': 'Relationship not found in output',
                            'weight': 0.0
                        }
            
            # Parse source references (text units)
            source_matches = re.findall(r'Sources\s*\(([^)]+)\)', response)
            
            for match in source_matches:
                ids = [id.strip() for id in match.split(',') if id.strip().replace('+more', '').strip().isdigit()]
                for source_id_str in ids:
                    source_id = int(source_id_str)
                    
                    # Look up actual text unit data
                    if text_units_df is not None and source_id in text_units_df.index:
                        source_row = text_units_df.loc[source_id]
                        sources_used['sources'][source_id] = {
                            'id': source_id,
                            'title': source_row.get('document_ids', f'Source {source_id}'),
                            'type': 'text_unit',
                            'text_preview': source_row.get('text', 'No text available')[:100]
                        }
                    else:
                        # Fallback if not found
                        sources_used['sources'][source_id] = {
                            'id': source_id,
                            'title': f'Source {source_id}',
                            'type': 'document',
                            'text_preview': 'Source not found in output'
                        }
        
        except Exception as e:
            logger.error(f"Error loading GraphRAG data for DRIFT source extraction: {e}")
            import traceback
            traceback.print_exc()
        
        return sources_used
    
    def _extract_sources_from_local_response(self, response: str) -> Dict[str, Any]:
        """Extract sources from local search response with actual data lookup."""
        sources_used = {
            'entities': {},
            'relationships': {},
            'sources': {},
            'text_units': {}
        }
        
        try:
            import pandas as pd
            
            # Load GraphRAG output files
            entities_path = self.graphrag_root / "output" / "entities.parquet"
            relationships_path = self.graphrag_root / "output" / "relationships.parquet"
            text_units_path = self.graphrag_root / "output" / "text_units.parquet"
            
            entities_df = None
            relationships_df = None
            text_units_df = None
            
            if entities_path.exists():
                entities_df = pd.read_parquet(entities_path)
            if relationships_path.exists():
                relationships_df = pd.read_parquet(relationships_path)
            if text_units_path.exists():
                text_units_df = pd.read_parquet(text_units_path)
            
            # Parse entity references from local response
            import re
            entity_matches = re.findall(r'Entities\s*\(([^)]+)\)', response)
            
            for match in entity_matches:
                ids = [id.strip() for id in match.split(',') if id.strip().replace('+more', '').strip().isdigit()]
                for entity_id_str in ids:
                    entity_id = int(entity_id_str)
                    
                    # Look up actual entity data
                    if entities_df is not None and entity_id in entities_df.index:
                        entity_row = entities_df.loc[entity_id]
                        sources_used['entities'][entity_id] = {
                            'id': entity_id,
                            'title': entity_row.get('title', f'Entity {entity_id}'),
                            'type': entity_row.get('type', 'Unknown'),
                            'description': entity_row.get('description', 'No description available')[:200]
                        }
                    else:
                        # Fallback if not found
                        sources_used['entities'][entity_id] = {
                            'id': entity_id,
                            'title': f'Entity {entity_id}',
                            'type': 'Unknown',
                            'description': 'Entity not found in output'
                        }
            
            # Parse relationship references
            rel_matches = re.findall(r'Relationships\s*\(([^)]+)\)', response)
            
            for match in rel_matches:
                ids = [id.strip() for id in match.split(',') if id.strip().replace('+more', '').strip().isdigit()]
                for rel_id_str in ids:
                    rel_id = int(rel_id_str)
                    
                    # Look up actual relationship data
                    if relationships_df is not None and rel_id in relationships_df.index:
                        rel_row = relationships_df.loc[rel_id]
                        sources_used['relationships'][rel_id] = {
                            'id': rel_id,
                            'source': rel_row.get('source', f'Relationship {rel_id}'),
                            'target': rel_row.get('target', 'Unknown'),
                            'description': rel_row.get('description', 'No description available')[:200],
                            'weight': rel_row.get('weight', 0.0)
                        }
                    else:
                        # Fallback if not found
                        sources_used['relationships'][rel_id] = {
                            'id': rel_id,
                            'source': f'Relationship {rel_id}',
                            'target': 'Unknown',
                            'description': 'Relationship not found in output',
                            'weight': 0.0
                        }
            
            # Parse source references (text units)
            source_matches = re.findall(r'Sources\s*\(([^)]+)\)', response)
            
            for match in source_matches:
                ids = [id.strip() for id in match.split(',') if id.strip().replace('+more', '').strip().isdigit()]
                for source_id_str in ids:
                    source_id = int(source_id_str)
                    
                    # Look up actual text unit data
                    if text_units_df is not None and source_id in text_units_df.index:
                        source_row = text_units_df.loc[source_id]
                        sources_used['sources'][source_id] = {
                            'id': source_id,
                            'title': source_row.get('document_ids', f'Source {source_id}'),
                            'type': 'text_unit',
                            'text_preview': source_row.get('text', 'No text available')[:100]
                        }
                    else:
                        # Fallback if not found
                        sources_used['sources'][source_id] = {
                            'id': source_id,
                            'title': f'Source {source_id}',
                            'type': 'document',
                            'text_preview': 'Source not found in output'
                        }
            
        except Exception as e:
            logger.error(f"Error loading GraphRAG data for source extraction: {e}")
            import traceback
            traceback.print_exc()
        
        return sources_used
    

    
    async def _execute_query(self, question: str, method: str, **kwargs) -> Dict[str, Any]:
        """Execute the actual query with original functionality."""
        
        # Auto-route if method not specified
        if method is None:
            route_info = self.router.determine_query_method(question)
            method = route_info['method']
            params = route_info['params']
            intent = route_info['intent']
            
            # Log the routing decision
            logger.info(f"Query: {question}")
            logger.info(f"Routed to: {method} (intent: {intent.value})")
            
            # Check if multiple entities detected
            if 'multiple_entities' in params:
                entity_count = len(params['multiple_entities'])
                logger.info(f"Detected {entity_count} entities in query")
                logger.info(f"Query focus: {'comparison' if params.get('comparison_mode') else 'specific' if params.get('strict_entity_focus') else 'contextual'}")
        else:
            params = kwargs
            intent = None
        
        # Execute query based on method
        if method == "global":
            result = await self._execute_global_query(question, params)
        elif method == "local":
            result = await self._execute_local_query(question, params)
        elif method == "drift":
            result = await self._execute_drift_query(question, params)
        else:
            raise ValueError(f"Unknown query method: {method}")
        
        # Add routing metadata to result
        result['routing_metadata'] = {
            'detected_intent': self._get_intent_type(params),
            'community_context_enabled': params.get('include_community_context', True),
            'query_method': method,
            'entity_count': len(params.get('multiple_entities', [])) if 'multiple_entities' in params else 1 if 'entity_filter' in params else 0
        }
        
        # Extract comprehensive source information
        sources_info = self._extract_sources_from_response(
            result.get('answer', ''), 
            result.get('query_type', method)
        )
        
        # Add both the sources info and the entity chunks if this is a local search
        result['sources_info'] = sources_info
        
        # For local searches, also get the actual entity chunks that were used
        if method == "local" or result.get('query_type') == 'local':
            result['entity_chunks'] = await self._get_entity_chunks(question, params)
        
        return result
    
    async def _extract_data_sources(self, result: Dict[str, Any], method: str) -> Dict[str, Any]:
        """Extract entities, relationships, and sources from query result."""
        data_sources = {
            'entities': [],
            'relationships': [],
            'sources': [],
            'communities': [],
            'text_units': []
        }
        
        try:
            # For local search
            if method == 'local' and 'context_data' in result:
                context = result['context_data']
                
                # Extract entity IDs and details
                if 'entities' in context:
                    entities_df = context['entities']
                    data_sources['entities'] = [
                        {
                            'id': idx,
                            'title': row.get('title', 'Unknown'),
                            'type': row.get('type', 'Unknown'),
                            'description': row.get('description', '')[:100] + '...' if len(row.get('description', '')) > 100 else row.get('description', '')
                        }
                        for idx, row in entities_df.iterrows()
                    ]
                
                # Extract relationship IDs and details
                if 'relationships' in context:
                    relationships_df = context['relationships']
                    data_sources['relationships'] = [
                        {
                            'id': idx,
                            'source': row.get('source', ''),
                            'target': row.get('target', ''),
                            'description': row.get('description', '')[:100] + '...',
                            'weight': row.get('weight', 0)
                        }
                        for idx, row in relationships_df.iterrows()
                    ]
                
                # Extract source documents
                if 'sources' in context:
                    sources_df = context['sources']
                    data_sources['sources'] = [
                        {
                            'id': idx,
                            'title': row.get('title', 'Unknown'),
                            'chunk_id': row.get('chunk_id', ''),
                            'document_type': row.get('document_type', 'Unknown')
                        }
                        for idx, row in sources_df.iterrows()
                    ]
            
            # For global search
            elif method == 'global' and 'context_data' in result:
                context = result['context_data']
                
                # Extract community information
                if 'communities' in context:
                    communities = context['communities']
                    data_sources['communities'] = [
                        {
                            'id': comm.get('id', ''),
                            'title': comm.get('title', 'Community'),
                            'level': comm.get('level', 0),
                            'entity_count': len(comm.get('entities', []))
                        }
                        for comm in communities
                    ]
                
                # Extract entities from communities
                for comm in context.get('communities', []):
                    for entity_id in comm.get('entities', []):
                        # Load entity details
                        entity = await self._get_entity_by_id(entity_id)
                        if entity:
                            data_sources['entities'].append({
                                'id': entity_id,
                                'title': entity.get('title', 'Unknown'),
                                'type': entity.get('type', 'Unknown'),
                                'from_community': comm.get('id', '')
                            })
            
            # Extract text units if available
            if 'text_units' in result.get('context_data', {}):
                text_units = result['context_data']['text_units']
                data_sources['text_units'] = [
                    {
                        'id': unit.get('id', ''),
                        'chunk_id': unit.get('chunk_id', ''),
                        'document': unit.get('document', ''),
                        'text_preview': unit.get('text', '')[:100] + '...'
                    }
                    for unit in text_units[:10]  # Limit to first 10
                ]
                
        except Exception as e:
            logger.error(f"Error extracting data sources: {e}")
            import traceback
            traceback.print_exc()
        
        return data_sources
    
    async def _get_entity_by_id(self, entity_id: int):
        """Get entity details by ID."""
        try:
            entities_path = self.graphrag_root / "output" / "entities.parquet"
            if entities_path.exists():
                import pandas as pd
                entities_df = pd.read_parquet(entities_path)
                if entity_id in entities_df.index:
                    return entities_df.loc[entity_id].to_dict()
        except Exception as e:
            logger.error(f"Error loading entity {entity_id}: {e}")
        return None

    async def _extract_local_context(self, query: str, **kwargs) -> Dict[str, Any]:
        """Manually extract context data for local search."""
        context = {
            'entities': None,
            'relationships': None,
            'sources': None,
            'text_units': []
        }
        
        try:
            import pandas as pd
            
            # Load data files
            entities_path = self.graphrag_root / "output" / "entities.parquet"
            relationships_path = self.graphrag_root / "output" / "relationships.parquet"
            text_units_path = self.graphrag_root / "output" / "text_units.parquet"
            
            # Get top-k entities
            if entities_path.exists():
                entities_df = pd.read_parquet(entities_path)
                
                # Filter based on query relevance (simple keyword matching for now)
                query_terms = query.lower().split()
                relevant_entities = []
                
                for idx, entity in entities_df.iterrows():
                    title = str(entity.get('title', '')).lower()
                    description = str(entity.get('description', '')).lower()
                    
                    # Check if any query term matches
                    if any(term in title or term in description for term in query_terms):
                        relevant_entities.append(idx)
                
                # Get top-k relevant entities
                top_k = kwargs.get('top_k_entities', 10)
                context['entities'] = entities_df.loc[relevant_entities[:top_k]]
                
                # Get relationships for these entities
                if relationships_path.exists() and len(relevant_entities) > 0:
                    relationships_df = pd.read_parquet(relationships_path)
                    
                    # Filter relationships involving our entities
                    entity_set = set(relevant_entities[:top_k])
                    relevant_rels = relationships_df[
                        relationships_df['source'].isin(entity_set) | 
                        relationships_df['target'].isin(entity_set)
                    ]
                    
                    context['relationships'] = relevant_rels
            
            # Get text units
            if text_units_path.exists():
                text_units_df = pd.read_parquet(text_units_path)
                
                # Get relevant text units (simplified - in practice would use embeddings)
                relevant_units = []
                for idx, unit in text_units_df.iterrows():
                    text = str(unit.get('text', '')).lower()
                    if any(term in text for term in query.lower().split()):
                        relevant_units.append({
                            'id': idx,
                            'text': unit.get('text', ''),
                            'chunk_id': unit.get('chunk_id', ''),
                            'document': unit.get('document', '')
                        })
                
                context['text_units'] = relevant_units[:10]
            
            # Extract source documents
            if context['entities'] is not None and not context['entities'].empty:
                # Get unique source documents from entities
                sources = []
                for idx, entity in context['entities'].iterrows():
                    if 'source_document' in entity:
                        sources.append({
                            'id': len(sources),
                            'title': entity['source_document'],
                            'document_type': entity.get('document_type', 'Unknown'),
                            'chunk_id': entity.get('chunk_id', '')
                        })
                
                # Deduplicate sources
                seen = set()
                unique_sources = []
                for source in sources:
                    key = source['title']
                    if key not in seen:
                        seen.add(key)
                        unique_sources.append(source)
                
                context['sources'] = pd.DataFrame(unique_sources)
                
        except Exception as e:
            logger.error(f"Error extracting context: {e}")
            import traceback
            traceback.print_exc()
        
        return context

    def _get_intent_type(self, params: Dict) -> str:
        """Determine intent type from parameters."""
        if params.get('comparison_mode'):
            return 'comparison'
        elif params.get('strict_entity_focus'):
            return 'specific_entity'
        elif params.get('focus_on_relationships'):
            return 'relationships'
        else:
            return 'contextual'
    
    async def _execute_global_query(self, question: str, params: Dict) -> Dict[str, Any]:
        """Execute a global search query."""
        cmd = [
            self._get_python_executable(),
            "-m", "graphrag", "query",
            "--root", str(self.graphrag_root),
            "--method", "global"
        ]
        
        if "community_level" in params:
            cmd.extend(["--community-level", str(params["community_level"])])
        
        cmd.extend(["--query", question])
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        # Clean JSON artifacts from the response
        answer = self._clean_json_artifacts(result.stdout)
        
        return {
            "query": question,
            "query_type": "global",
            "answer": answer,
            "context": self._extract_context(result.stdout),
            "parameters": params
        }
    
    async def _execute_local_query(self, question: str, params: Dict) -> Dict[str, Any]:
        """Execute a local search query with available GraphRAG options."""
        
        # Handle multiple entities
        if "multiple_entities" in params:
            return await self._execute_multi_entity_query(question, params)
        
        # Single entity query - use available GraphRAG options
        cmd = [
            self._get_python_executable(),
            "-m", "graphrag", "query",
            "--root", str(self.graphrag_root),
            "--method", "local"
        ]
        
        # Use community-level to control context (available option)
        if params.get("disable_community", False):
            # Use highest community level to get most specific results
            cmd.extend(["--community-level", "3"])  
            logger.info("Using high community level (3) for specific entity query")
        else:
            # Use default community level for broader context
            cmd.extend(["--community-level", "2"])
            logger.info("Using default community level (2) for contextual query")
        
        # If we have entity filtering request, modify the query to be more specific
        if "entity_filter" in params:
            filter_info = params["entity_filter"]
            entity_type = filter_info['type'].replace('_', ' ').lower()
            entity_value = filter_info['value']
            
            if params.get("strict_entity_focus", False):
                # Make query more specific to focus on just this entity
                enhanced_question = f"Tell me specifically about {entity_type} {entity_value}. Focus only on {entity_value} and do not include information about other items."
                logger.info(f"Enhanced query for strict focus on {entity_value}")
            else:
                # Keep original query but mention the entity
                enhanced_question = f"{question} (specifically about {entity_type} {entity_value})"
                logger.info(f"Enhanced query for contextual information about {entity_value}")
            
            question = enhanced_question
        
        cmd.extend(["--query", question])
        
        logger.debug(f"Executing command: {' '.join(cmd)}")
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        answer = result.stdout
        
        # Clean JSON artifacts from the response
        answer = self._clean_json_artifacts(answer)
        
        # Post-process if strict entity focus is requested
        if params.get("strict_entity_focus", False) and "entity_filter" in params:
            answer = self._filter_to_specific_entity(answer, params["entity_filter"]["value"])
        
        return {
            "query": question,
            "query_type": "local",
            "answer": answer,
            "context": self._extract_context(answer),
            "parameters": params,
            "intent_detection": {
                "specific_entity_focus": params.get("strict_entity_focus", False),
                "community_level_used": 3 if params.get("disable_community") else 2,
                "query_enhanced": "entity_filter" in params
            }
        }
    
    async def _execute_multi_entity_query(self, question: str, params: Dict) -> Dict[str, Any]:
        """Execute queries for multiple entities using available GraphRAG options."""
        entities = params["multiple_entities"]
        all_results = []
        
        # Determine query strategy based on intent
        if params.get("aggregate_results") and params.get("strict_entity_focus"):
            # Query each entity separately with high community level
            logger.info(f"Executing separate queries for {len(entities)} entities")
            
            for entity in entities:
                cmd = [
                    self._get_python_executable(),
                    "-m", "graphrag", "query",
                    "--root", str(self.graphrag_root),
                    "--method", "local",
                    "--community-level", "3"  # High level for specific results
                ]
                
                # Create entity-specific query
                entity_type = entity['type'].replace('_', ' ').lower()
                entity_query = f"Tell me specifically about {entity_type} {entity['value']}. Focus only on {entity['value']}."
                cmd.extend(["--query", entity_query])
                
                result = subprocess.run(cmd, capture_output=True, text=True)
                all_results.append({
                    "entity": entity,
                    "answer": result.stdout
                })
            
            # Combine results
            combined_answer = self._format_multiple_entity_results(all_results, params)
            
        elif params.get("comparison_mode"):
            # Query with all entities for comparison
            logger.info(f"Executing comparison query for entities: {[e['value'] for e in entities]}")
            
            entity_values = [e['value'] for e in entities]
            comparison_query = f"Compare and contrast {' and '.join(entity_values)}. What are the similarities and differences between these items?"
            
            cmd = [
                self._get_python_executable(),
                "-m", "graphrag", "query",
                "--root", str(self.graphrag_root),
                "--method", "local",
                "--community-level", "2",  # Medium level for comparison context
                "--query", comparison_query
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True)
            combined_answer = self._format_comparison_results(result.stdout, entities)
            
        else:
            # Query for relationships between entities
            logger.info(f"Executing relationship query for entities: {[e['value'] for e in entities]}")
            
            entity_values = [e['value'] for e in entities]
            relationship_query = f"How do {' and '.join(entity_values)} relate to each other? What connections exist between these items?"
            
            cmd = [
                self._get_python_executable(),
                "-m", "graphrag", "query",
                "--root", str(self.graphrag_root),
                "--method", "local",
                "--community-level", "1",  # Lower level for broader relationships
                "--query", relationship_query
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True)
            combined_answer = result.stdout
        
        # Clean JSON artifacts from the combined answer
        combined_answer = self._clean_json_artifacts(combined_answer)
        
        return {
            "query": question,
            "query_type": "local",
            "answer": combined_answer,
            "context": self._extract_context(combined_answer),
            "parameters": params,
            "intent_detection": {
                "multi_entity_query": True,
                "entity_count": len(entities),
                "query_mode": "comparison" if params.get("comparison_mode") else "aggregate" if params.get("aggregate_results") else "relationships"
            }
        }
    
    def _format_multiple_entity_results(self, results: List[Dict], params: Dict) -> str:
        """Format results from multiple individual entity queries."""
        formatted = []
        
        formatted.append(f"Information about {len(results)} requested items:\n")
        
        for i, result in enumerate(results, 1):
            entity = result['entity']
            answer = result['answer'].strip()
            
            formatted.append(f"\n{i}. {entity['type'].replace('_', ' ').title()} {entity['value']}:")
            formatted.append("-" * 50)
            
            # Clean and format the answer
            if answer:
                # Remove any GraphRAG metadata/headers and JSON artifacts
                clean_answer = self._clean_graphrag_output(answer)
                clean_answer = self._clean_json_artifacts(clean_answer)
                formatted.append(clean_answer)
            else:
                formatted.append(f"No information found for {entity['value']}")
        
        return "\n".join(formatted)
    
    def _format_comparison_results(self, raw_answer: str, entities: List[Dict]) -> str:
        """Format comparison results to highlight differences and similarities."""
        # Clean JSON artifacts from the raw answer
        clean_answer = self._clean_json_artifacts(raw_answer)
        
        # This could be enhanced with more sophisticated formatting
        formatted = [f"Comparison of {', '.join([e['value'] for e in entities])}:\n"]
        formatted.append(clean_answer)
        
        return "\n".join(formatted)
    
    def _clean_graphrag_output(self, output: str) -> str:
        """Remove GraphRAG metadata and format output cleanly."""
        # Remove common GraphRAG headers/footers
        lines = output.split('\n')
        cleaned_lines = []
        
        for line in lines:
            # Skip metadata lines
            if line.startswith('INFO:') or line.startswith('WARNING:') or line.startswith('DEBUG:'):
                continue
            # Skip empty lines at start/end
            if not line.strip() and (not cleaned_lines or len(cleaned_lines) == len(lines) - 1):
                continue
            cleaned_lines.append(line)
        
        return '\n'.join(cleaned_lines).strip()
    
    async def _execute_drift_query(self, question: str, params: Dict) -> Dict[str, Any]:
        """Execute a DRIFT search query."""
        cmd = [
            self._get_python_executable(),
            "-m", "graphrag", "query",
            "--root", str(self.graphrag_root),
            "--method", "drift"
        ]
        
        cmd.extend(["--query", question])
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        # Clean JSON artifacts from the response
        answer = self._clean_json_artifacts(result.stdout)
        
        return {
            "query": question,
            "query_type": "drift",
            "answer": answer,
            "context": self._extract_context(result.stdout),
            "parameters": params
        }
    
    def _filter_to_specific_entity(self, response: str, target_entity: str) -> str:
        """Aggressively filter response to ONLY information about the target entity."""
        if not target_entity or not response:
            return response
        
        # Split response into sentences
        sentences = response.split('.')
        filtered_sentences = []
        
        for sentence in sentences:
            # Only keep sentences that explicitly mention the target entity
            if target_entity in sentence:
                # Check if any other entity codes are mentioned
                other_entities = re.findall(r'\b[A-Z]-\d+\b', sentence)
                other_entities = [e for e in other_entities if e != target_entity]
                
                # Only keep if no other entities are mentioned
                if not other_entities:
                    filtered_sentences.append(sentence.strip())
        
        if filtered_sentences:
            filtered_response = '. '.join(filtered_sentences) + '.'
            filtered_response = f"Information specifically about {target_entity}:\n\n{filtered_response}"
        else:
            filtered_response = f"Specific information about {target_entity} only."
        
        return filtered_response
    
    def _is_paragraph_about_target(self, paragraph: str, target: str, all_entities: set) -> bool:
        """Determine if a paragraph should be kept in filtered response."""
        if target not in paragraph:
            return False
        
        other_entities = all_entities - {target}
        if not any(entity in paragraph for entity in other_entities):
            return True
        
        target_count = paragraph.count(target)
        other_counts = sum(paragraph.count(entity) for entity in other_entities)
        
        return target_count >= other_counts
    
    def _extract_sources_from_response(self, response: str, method: str) -> Dict[str, Any]:
        """Extract and resolve all source references from GraphRAG response."""
        sources_info = {
            'entities': [],
            'reports': [],
            'raw_references': {},
            'resolved_sources': []
        }
        
        import re
        
        # Parse all reference patterns
        entities_pattern = r'Entities\s*\(([^)]+)\)'
        reports_pattern = r'Reports\s*\(([^)]+)\)'
        sources_pattern = r'Sources\s*\(([^)]+)\)'
        data_pattern = r'Data:\s*(?:Sources\s*\([^)]+\);\s*)?(?:Entities\s*\([^)]+\)|Reports\s*\([^)]+\))'
        
        # Extract all matches
        entities_matches = re.findall(entities_pattern, response)
        reports_matches = re.findall(reports_pattern, response)
        sources_matches = re.findall(sources_pattern, response)
        
        # Store raw references
        sources_info['raw_references'] = {
            'entities': entities_matches,
            'reports': reports_matches,
            'sources': sources_matches
        }
        
        # Parse entity IDs
        all_entity_ids = []
        for match in entities_matches:
            ids = [id.strip() for id in match.split(',') if id.strip().replace('+more', '').strip().isdigit()]
            all_entity_ids.extend(ids)
        
        # Parse report IDs
        all_report_ids = []
        for match in reports_matches:
            ids = [id.strip() for id in match.split(',') if id.strip().replace('+more', '').strip().isdigit()]
            all_report_ids.extend(ids)
        
        # Load and resolve entities
        if all_entity_ids:
            try:
                entities_path = self.graphrag_root / "output/entities.parquet"
                if entities_path.exists():
                    import pandas as pd
                    entities_df = pd.read_parquet(entities_path)
                    
                    for entity_id in all_entity_ids:
                        try:
                            entity_idx = int(entity_id)
                            if entity_idx in entities_df.index:
                                entity = entities_df.loc[entity_idx]
                                sources_info['entities'].append({
                                    'id': entity_idx,
                                    'title': entity['title'],
                                    'type': entity['type'],
                                    'description': entity.get('description', '')[:300]
                                })
                        except Exception as e:
                            logger.error(f"Failed to resolve entity {entity_id}: {e}")
            except Exception as e:
                logger.error(f"Failed to load entities: {e}")
        
        # Load and resolve community reports
        if all_report_ids:
            try:
                reports_path = self.graphrag_root / "output/community_reports.parquet"
                if reports_path.exists():
                    import pandas as pd
                    reports_df = pd.read_parquet(reports_path)
                    
                    for report_id in all_report_ids:
                        try:
                            report_idx = int(report_id)
                            if report_idx in reports_df.index:
                                report = reports_df.loc[report_idx]
                                sources_info['reports'].append({
                                    'id': report_idx,
                                    'title': f"Community Report #{report_idx}",
                                    'summary': report.get('summary', '')[:300] if 'summary' in report else str(report)[:300],
                                    'level': report.get('level', 'unknown') if 'level' in report else 'unknown'
                                })
                        except Exception as e:
                            logger.error(f"Failed to resolve report {report_id}: {e}")
            except Exception as e:
                logger.error(f"Failed to load reports: {e}")
        
        # Create resolved sources list combining everything
        sources_info['resolved_sources'] = sources_info['entities'] + sources_info['reports']
        
        return sources_info

    async def _get_retrieved_entities(self, query: str, params: Dict) -> List[Dict]:
        """Get the actual entities that GraphRAG retrieved for this query."""
        source_entities = []
        
        try:
            entities_path = self.graphrag_root / "output/entities.parquet"
            text_units_path = self.graphrag_root / "output/text_units.parquet"
            relationships_path = self.graphrag_root / "output/relationships.parquet"
            
            if entities_path.exists():
                import pandas as pd
                entities_df = pd.read_parquet(entities_path)
                
                # If we have an entity filter, find that specific entity
                if 'entity_filter' in params:
                    filter_info = params['entity_filter']
                    entity_value = filter_info['value']
                    entity_type = filter_info['type']
                    
                    # Find matching entities by type and value
                    matches = entities_df[
                        (entities_df['type'].str.upper() == entity_type.upper()) &
                        (entities_df['title'].str.contains(entity_value, case=False, na=False))
                    ]
                    
                    # Also find related entities through relationships
                    if relationships_path.exists() and not matches.empty:
                        relationships_df = pd.read_parquet(relationships_path)
                        
                        for _, entity in matches.iterrows():
                            entity_id = entity.name  # Index is the entity ID
                            
                            # Find all relationships involving this entity
                            related_rels = relationships_df[
                                (relationships_df['source'] == entity_id) | 
                                (relationships_df['target'] == entity_id)
                            ]
                            
                            # Add the main entity
                            source_entities.append({
                                'entity_id': entity_id,
                                'title': entity['title'],
                                'type': entity['type'],
                                'description': entity.get('description', '')[:500],
                                'is_primary': True,
                                'source_document': self._trace_entity_to_document(entity_id, entity['title'])
                            })
                            
                            # Add related entities
                            for _, rel in related_rels.iterrows():
                                other_id = rel['target'] if rel['source'] == entity_id else rel['source']
                                if other_id in entities_df.index:
                                    related_entity = entities_df.loc[other_id]
                                    source_entities.append({
                                        'entity_id': other_id,
                                        'title': related_entity['title'],
                                        'type': related_entity['type'],
                                        'description': related_entity.get('description', '')[:300],
                                        'is_primary': False,
                                        'relationship': rel['description'],
                                        'source_document': self._trace_entity_to_document(other_id, related_entity['title'])
                                    })
        
        except Exception as e:
            logger.error(f"Failed to get retrieved entities: {e}")
        
        return source_entities

    async def _get_entity_chunks(self, query: str, params: Dict) -> List[Dict]:
        """Get the actual text chunks/entities that were retrieved."""
        chunks = []
        
        try:
            # Load entities and text units
            entities_path = self.graphrag_root / "output/entities.parquet"
            text_units_path = self.graphrag_root / "output/text_units.parquet"
            
            if entities_path.exists():
                import pandas as pd
                entities_df = pd.read_parquet(entities_path)
                
                # If text units exist, load them too
                text_units_df = None
                if text_units_path.exists():
                    text_units_df = pd.read_parquet(text_units_path)
                
                # Get entities based on the query parameters
                if 'entity_filter' in params:
                    filter_info = params['entity_filter']
                    entity_value = filter_info['value']
                    entity_type = filter_info['type']
                    
                    # Find all matching entities
                    matches = entities_df[
                        (entities_df['type'].str.upper() == entity_type.upper()) & 
                        (entities_df['title'].str.contains(entity_value, case=False, na=False))
                    ]
                    
                    # Also get related entities by looking at descriptions
                    related = entities_df[
                        entities_df['description'].str.contains(entity_value, case=False, na=False)
                    ]
                    
                    all_matches = pd.concat([matches, related]).drop_duplicates()
                    
                    # Convert to chunks format
                    for _, entity in all_matches.iterrows():
                        chunk = {
                            'entity_id': entity.name,
                            'type': entity['type'],
                            'title': entity['title'],
                            'description': entity.get('description', ''),
                            'source': self._trace_entity_to_document(entity.name, entity['title'])
                        }
                        chunks.append(chunk)
        
        except Exception as e:
            logger.error(f"Failed to get entity chunks: {e}")
        
        return chunks

    def _trace_entity_to_document(self, entity_id, entity_title: str) -> Dict:
        """Trace an entity back to its source document."""
        try:
            csv_path = self.graphrag_root / "city_clerk_documents.csv"
            if csv_path.exists():
                import pandas as pd
                docs_df = pd.read_csv(csv_path)
                
                # Extract potential item code from entity title
                import re
                item_match = re.search(r'([A-Z]-\d+)', entity_title)
                doc_match = re.search(r'(\d{4}-\d+)', entity_title)
                
                # Try to find matching document
                for _, doc in docs_df.iterrows():
                    # Check if entity matches document identifiers
                    if (item_match and item_match.group(1) == doc.get('item_code')) or \
                       (doc_match and doc_match.group(1) in str(doc.get('document_number', ''))) or \
                       (entity_title.lower() in str(doc.get('title', '')).lower()):
                        return {
                            'document_id': doc['id'],
                            'title': doc.get('title', ''),
                            'type': doc.get('document_type', ''),
                            'meeting_date': doc.get('meeting_date', ''),
                            'source_file': doc.get('source_file', '')
                        }
        except Exception as e:
            logger.error(f"Failed to trace entity to document: {e}")
        
        return {}
    
    def _extract_context(self, response: str) -> List[Dict]:
        """Extract context and sources from response."""
        context = []
        # Parse response for entity references and sources
        return context

# Legacy compatibility class
class CityClerkGraphRAGQuery(CityClerkQueryEngine):
    """Legacy compatibility wrapper."""
    
    async def query(self, 
                    question: str, 
                    query_type: QueryType = QueryType.LOCAL,
                    community_level: int = 0) -> Dict[str, Any]:
        """Legacy query method for backward compatibility."""
        return await super().query(
            question=question,
            method=query_type.value,
            community_level=community_level
        )

# Example usage function
async def handle_user_query(question: str, graphrag_root: Path = None):
    """Handle user query with intelligent routing."""
    if graphrag_root is None:
        graphrag_root = Path("./graphrag_data")
    
    engine = CityClerkQueryEngine(graphrag_root)
    result = await engine.query(question)
    
    print(f"Query: {question}")
    print(f"Selected method: {result['query_type']}")
    print(f"Detected entities: {result['routing_metadata'].get('entity_count', 0)}")
    print(f"Query intent: {result['routing_metadata'].get('detected_intent', 'unknown')}")
    
    return result


================================================================================


################################################################################
# File: scripts/graph_rag_stages/phase2_building/custom_graph_builder.py
################################################################################

# File: scripts/graph_rag_stages/phase2_building/custom_graph_builder.py

"""
Custom graph builder for creating knowledge graphs in Cosmos DB.
"""

import logging
from pathlib import Path
from typing import Dict, List, Optional, Any
import asyncio
from ..common.cosmos_client import CosmosGraphClient
from ..common.config import get_config

log = logging.getLogger(__name__)


class CustomGraphBuilder:
    """Builds custom knowledge graphs in Cosmos DB from processed documents."""
    
    def __init__(self, cosmos_config: Optional[Dict] = None):
        """
        Initialize the graph builder with Cosmos DB configuration.
        
        Args:
            cosmos_config: Optional Cosmos DB configuration override
        """
        self.config = get_config()
        
        # Override with custom config if provided
        if cosmos_config:
            for key, value in cosmos_config.items():
                if hasattr(self.config, key):
                    setattr(self.config, key, value)
        
        # Initialize Cosmos client
        self.cosmos_client = CosmosGraphClient(
            endpoint=self.config.cosmos_endpoint,
            key=self.config.cosmos_key,
            database=self.config.cosmos_database,
            container=self.config.cosmos_container
        )

    async def build_graph_from_markdown(self, markdown_dir: Path) -> None:
        """
        Build knowledge graph from enriched markdown files.
        
        Args:
            markdown_dir: Directory containing enriched markdown files
        """
        log.info(f"🔗 Building custom graph from markdown files in: {markdown_dir}")
        
        # Find all markdown files
        markdown_files = list(markdown_dir.glob("*.md"))
        log.info(f"Found {len(markdown_files)} markdown files to process")
        
        if not markdown_files:
            log.warning("No markdown files found for graph building")
            return
        
        # Connect to Cosmos DB
        async with self.cosmos_client:
            # Process files and build graph
            for md_file in markdown_files:
                try:
                    await self._process_document_for_graph(md_file)
                except Exception as e:
                    log.error(f"Error processing {md_file.name} for graph: {e}")
                    continue
        
        log.info("✅ Custom graph building completed")

    async def _process_document_for_graph(self, md_file: Path) -> None:
        """
        Process a single markdown document and add its entities/relationships to the graph.
        
        Args:
            md_file: Path to markdown file
        """
        log.info(f"📄 Processing {md_file.name} for graph building")
        
        # Read the markdown content
        with open(md_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Extract document metadata from the header
        metadata = self._extract_document_metadata(content)
        
        # Create document vertex
        doc_id = self._generate_document_id(md_file, metadata)
        await self._create_document_vertex(doc_id, metadata, md_file)
        
        # Extract and create entities based on document type
        if metadata.get('document_type') == 'agenda':
            await self._process_agenda_document(doc_id, content, metadata)
        elif metadata.get('document_type') == 'verbatim_transcript':
            await self._process_transcript_document(doc_id, content, metadata)
        elif metadata.get('document_type') in ['ordinance', 'resolution']:
            await self._process_legislative_document(doc_id, content, metadata)
        else:
            await self._process_generic_document(doc_id, content, metadata)

    def _extract_document_metadata(self, content: str) -> Dict[str, Any]:
        """Extract metadata from the markdown header."""
        metadata = {}
        
        # Look for metadata section between --- markers
        if content.startswith("---"):
            try:
                _, header_section, _ = content.split("---", 2)
                
                # Parse key-value pairs from header
                for line in header_section.strip().split("\n"):
                    line = line.strip()
                    if ":" in line and line.startswith("- "):
                        # Handle format like "- Document Type: AGENDA"
                        key_value = line[2:].split(":", 1)
                        if len(key_value) == 2:
                            key = key_value[0].strip().lower().replace(" ", "_")
                            value = key_value[1].strip()
                            metadata[key] = value
            except ValueError:
                pass  # No proper header found
        
        return metadata

    async def _create_document_vertex(self, doc_id: str, metadata: Dict, md_file: Path) -> None:
        """Create a vertex for the document."""
        properties = {
            'title': metadata.get('title', md_file.stem),
            'document_type': metadata.get('document_type', 'document'),
            'source_file': md_file.name,
            'meeting_date': metadata.get('meeting_date', ''),
            'created_at': metadata.get('extraction_timestamp', ''),
        }
        
        await self.cosmos_client.create_vertex('Document', doc_id, properties)
        log.debug(f"Created document vertex: {doc_id}")

    async def _process_agenda_document(self, doc_id: str, content: str, metadata: Dict) -> None:
        """Process agenda document and create agenda-specific entities."""
        log.debug(f"Processing agenda document: {doc_id}")
        
        # Create meeting vertex
        meeting_date = metadata.get('meeting_date', 'unknown')
        if meeting_date != 'unknown':
            meeting_id = f"MEETING_{meeting_date.replace('.', '_')}"
            meeting_properties = {
                'date': meeting_date,
                'type': 'city_commission_meeting'
            }
            await self.cosmos_client.create_vertex('Meeting', meeting_id, meeting_properties)
            
            # Link document to meeting
            await self.cosmos_client.create_edge_if_not_exists(
                doc_id, meeting_id, 'DOCUMENTS'
            )
        
        # Extract agenda items from content
        agenda_items = self._extract_agenda_items_from_content(content)
        for item in agenda_items:
            await self._create_agenda_item_vertex(item, doc_id, meeting_date)

    async def _process_transcript_document(self, doc_id: str, content: str, metadata: Dict) -> None:
        """Process transcript document and create transcript-specific entities."""
        log.debug(f"Processing transcript document: {doc_id}")
        
        # Extract agenda items mentioned in transcript
        mentioned_items = self._extract_agenda_items_from_content(content)
        
        # Link transcript to agenda items
        for item_code in mentioned_items:
            item_id = f"ITEM_{item_code}_{metadata.get('meeting_date', 'unknown').replace('.', '_')}"
            await self.cosmos_client.create_edge_if_not_exists(
                doc_id, item_id, 'DISCUSSES'
            )

    async def _process_legislative_document(self, doc_id: str, content: str, metadata: Dict) -> None:
        """Process ordinance/resolution document."""
        log.debug(f"Processing legislative document: {doc_id}")
        
        # Link to agenda item if specified
        agenda_item = metadata.get('linked_agenda_item')
        if agenda_item:
            item_id = f"ITEM_{agenda_item}_{metadata.get('meeting_date', 'unknown').replace('.', '_')}"
            await self.cosmos_client.create_edge_if_not_exists(
                item_id, doc_id, 'IMPLEMENTS'
            )

    async def _process_generic_document(self, doc_id: str, content: str, metadata: Dict) -> None:
        """Process generic document."""
        log.debug(f"Processing generic document: {doc_id}")
        # For now, just ensure the document vertex exists
        pass

    def _extract_agenda_items_from_content(self, content: str) -> List[str]:
        """Extract agenda item codes from document content."""
        import re
        
        item_codes = []
        
        # Look for agenda item patterns in the content
        patterns = [
            r'AGENDA_ITEM:\s*([A-Z]-\d+)',
            r'Item\s+([A-Z]-\d+)',
            r'([A-Z]-\d+)\s*:',
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, content)
            item_codes.extend(matches)
        
        # Remove duplicates and return
        return list(set(item_codes))

    async def _create_agenda_item_vertex(self, item_code: str, doc_id: str, meeting_date: str) -> None:
        """Create a vertex for an agenda item."""
        item_id = f"ITEM_{item_code}_{meeting_date.replace('.', '_')}"
        
        properties = {
            'item_code': item_code,
            'meeting_date': meeting_date,
            'status': 'scheduled'
        }
        
        await self.cosmos_client.create_vertex('AgendaItem', item_id, properties)
        
        # Link agenda item to document
        await self.cosmos_client.create_edge_if_not_exists(
            doc_id, item_id, 'CONTAINS'
        )

    def _generate_document_id(self, md_file: Path, metadata: Dict) -> str:
        """Generate a unique document ID."""
        import hashlib
        
        # Use file path and some metadata to create unique ID
        unique_string = f"{md_file.name}_{metadata.get('document_type', 'doc')}"
        hash_part = hashlib.sha1(unique_string.encode()).hexdigest()[:8]
        
        doc_type = metadata.get('document_type', 'doc').upper()
        return f"{doc_type}_{hash_part}"

    async def clear_graph(self) -> None:
        """Clear all data from the graph (use with caution)."""
        log.warning("🗑️ Clearing entire graph database")
        
        async with self.cosmos_client:
            await self.cosmos_client.clear_graph()
        
        log.info("✅ Graph cleared")

    async def get_graph_stats(self) -> Dict[str, int]:
        """Get basic statistics about the graph."""
        stats = {
            'total_vertices': 0,
            'documents': 0,
            'agenda_items': 0,
            'meetings': 0
        }
        
        try:
            async with self.cosmos_client:
                # Count total vertices
                result = await self.cosmos_client._execute_query("g.V().count()")
                stats['total_vertices'] = result[0] if result else 0
                
                # Count by label
                for label in ['Document', 'AgendaItem', 'Meeting']:
                    result = await self.cosmos_client._execute_query(f"g.V().hasLabel('{label}').count()")
                    count = result[0] if result else 0
                    stats[label.lower() + 's'] = count
                    
        except Exception as e:
            log.error(f"Error getting graph stats: {e}")
        
        return stats


================================================================================


################################################################################
# File: scripts/graph_rag_stages/phase2_building/graphrag_adapter.py
################################################################################

# File: scripts/graph_rag_stages/phase2_building/graphrag_adapter.py

"""
Adapts the enriched markdown files into a format suitable for GraphRAG ingestion.
"""

import pandas as pd
from pathlib import Path
import logging
import json
from typing import Dict, Any, List
from ..common.utils import extract_metadata_from_header, ensure_directory_exists

log = logging.getLogger(__name__)


class GraphRAGAdapter:
    """Prepares structured data for the GraphRAG indexing pipeline."""

    def create_graphrag_input_csv(self, markdown_dir: Path, output_dir: Path) -> Path:
        """
        Scans a directory of enriched markdown files and creates the final CSV
        that will be fed into the GraphRAG index command.
        
        Args:
            markdown_dir: Directory containing enriched markdown files
            output_dir: GraphRAG working directory
            
        Returns:
            Path to the created CSV file
        """
        log.info(f"📋 Creating GraphRAG input CSV from markdown files in: {markdown_dir}")
        
        # Ensure directories exist
        ensure_directory_exists(output_dir)
        input_dir = output_dir / "input"
        ensure_directory_exists(input_dir)
        
        documents = []
        
        # Process all markdown files
        markdown_files = list(markdown_dir.glob("*.md"))
        log.info(f"Found {len(markdown_files)} markdown files to process")
        
        for md_file in markdown_files:
            try:
                with open(md_file, 'r', encoding='utf-8') as f:
                    content = f.read()

                # Extract metadata from the enriched header
                metadata = extract_metadata_from_header(content)
                
                # Create document record
                doc_record = {
                    'id': md_file.stem,
                    'text': content,  # Full content including rich metadata header
                    'title': metadata.get('title', md_file.stem.replace('_', ' ').title()),
                    'document_type': self._determine_document_type(md_file, metadata),
                    'meeting_date': metadata.get('meeting_date', ''),
                    'source_file': md_file.name
                }
                
                # Add agenda item information if available
                if 'agenda_item' in metadata:
                    doc_record['agenda_item'] = metadata['agenda_item']
                
                documents.append(doc_record)
                
            except Exception as e:
                log.error(f"Error processing {md_file.name}: {e}")
                continue

        if not documents:
            log.warning("No documents found to adapt for GraphRAG. The pipeline might fail.")
            return None

        # Create DataFrame and save as CSV
        df = pd.DataFrame(documents)
        output_csv_path = input_dir / "city_clerk_documents.csv"
        df.to_csv(output_csv_path, index=False, encoding='utf-8')
        
        log.info(f"✅ Successfully created GraphRAG input file at: {output_csv_path}")
        log.info(f"📊 Processed {len(documents)} documents:")
        
        # Log summary by document type
        type_counts = df['document_type'].value_counts()
        for doc_type, count in type_counts.items():
            log.info(f"   - {doc_type}: {count}")
        
        return output_csv_path

    def _determine_document_type(self, md_file: Path, metadata: Dict[str, Any]) -> str:
        """
        Determine document type from filename and metadata.
        
        Args:
            md_file: Path to markdown file
            metadata: Extracted metadata
            
        Returns:
            Document type string
        """
        # Check metadata first
        if 'document_type' in metadata:
            return metadata['document_type'].lower()
        
        # Check filename patterns
        filename_lower = md_file.name.lower()
        
        if 'agenda' in filename_lower:
            return 'agenda'
        elif 'verbatim' in filename_lower:
            return 'transcript'
        elif 'ordinance' in filename_lower:
            return 'ordinance'
        elif 'resolution' in filename_lower:
            return 'resolution'
        elif 'minutes' in filename_lower:
            return 'minutes'
        else:
            return 'document'

    def create_graphrag_settings(self, output_dir: Path, custom_settings: Dict[str, Any] = None) -> Path:
        """
        Create GraphRAG settings file with city clerk specific configuration.
        
        Args:
            output_dir: GraphRAG working directory
            custom_settings: Optional custom settings to override defaults
            
        Returns:
            Path to the created settings file
        """
        log.info("⚙️ Creating GraphRAG settings file")
        
        # Default settings optimized for city clerk documents
        default_settings = {
            "llm": {
                "api_key": "${OPENAI_API_KEY}",
                "type": "openai_chat",
                "model": "gpt-4",
                "max_tokens": 4000,
                "temperature": 0.0
            },
            "parallelization": {
                "stagger": 0.3,
                "num_threads": 4
            },
            "async_mode": "threaded",
            "encoding_model": "cl100k_base",
            "skip_workflows": [],
            "entity_extraction": {
                "prompt": "Given a text document about city government proceedings, identify all entities. Extract entities that represent people, organizations, locations, agenda items, ordinances, resolutions, and key concepts discussed in city meetings.",
                "entity_types": [
                    "PERSON", "ORGANIZATION", "LOCATION", 
                    "AGENDA_ITEM", "ORDINANCE", "RESOLUTION",
                    "MEETING", "COMMITTEE", "DEPARTMENT"
                ],
                "max_gleanings": 1
            },
            "summarize_descriptions": {
                "prompt": "Given one or more entities that have been identified from city government documents, provide a comprehensive description that captures their role in municipal governance.",
                "max_length": 500
            },
            "community_reports": {
                "prompt": "Write a comprehensive report about the community and its entities, focusing on municipal governance, city operations, and citizen services.",
                "max_length": 2000,
                "max_input_length": 8000
            },
            "claim_extraction": {
                "prompt": "Given a text document about city government proceedings, extract all factual claims made during meetings, including voting records, policy decisions, and citizen concerns.",
                "description": "Any claim or assertion made in city government documents",
                "max_gleanings": 1
            },
            "chunks": {
                "size": 1200,
                "overlap": 100,
                "group_by_columns": ["source_file"]
            },
            "input": {
                "type": "file",
                "file_type": "csv",
                "base_dir": "input",
                "source_column": "text",
                "timestamp_column": "meeting_date",
                "timestamp_format": "%m.%d.%Y",
                "text_column": "text",
                "title_column": "title"
            },
            "cache": {
                "type": "file",
                "base_dir": "cache"
            },
            "storage": {
                "type": "file",
                "base_dir": "output"
            },
            "reporting": {
                "type": "file",
                "base_dir": "output/reports"
            },
            "snapshots": {
                "embeddings": False,
                "transient": False
            }
        }
        
        # Merge with custom settings if provided
        if custom_settings:
            settings = self._deep_merge_dicts(default_settings, custom_settings)
        else:
            settings = default_settings
        
        # Save settings file
        settings_path = output_dir / "settings.yaml"
        
        import yaml
        with open(settings_path, 'w') as f:
            yaml.dump(settings, f, default_flow_style=False, indent=2)
        
        log.info(f"✅ GraphRAG settings saved to: {settings_path}")
        return settings_path

    def _deep_merge_dicts(self, dict1: Dict, dict2: Dict) -> Dict:
        """Deep merge two dictionaries."""
        result = dict1.copy()
        for key, value in dict2.items():
            if key in result and isinstance(result[key], dict) and isinstance(value, dict):
                result[key] = self._deep_merge_dicts(result[key], value)
            else:
                result[key] = value
        return result

    def validate_input_data(self, csv_path: Path) -> bool:
        """
        Validate the prepared CSV data for GraphRAG ingestion.
        
        Args:
            csv_path: Path to the CSV file
            
        Returns:
            True if validation passes, False otherwise
        """
        log.info(f"🔍 Validating GraphRAG input data: {csv_path}")
        
        try:
            df = pd.read_csv(csv_path)
            
            # Check required columns
            required_columns = ['id', 'text']
            missing_columns = [col for col in required_columns if col not in df.columns]
            if missing_columns:
                log.error(f"Missing required columns: {missing_columns}")
                return False
            
            # Check for empty content
            empty_texts = df['text'].isna().sum()
            if empty_texts > 0:
                log.warning(f"Found {empty_texts} documents with empty text")
            
            # Check text length distribution
            text_lengths = df['text'].str.len()
            log.info(f"📊 Text length statistics:")
            log.info(f"   - Average: {text_lengths.mean():.0f} characters")
            log.info(f"   - Median: {text_lengths.median():.0f} characters")
            log.info(f"   - Min: {text_lengths.min():.0f} characters")
            log.info(f"   - Max: {text_lengths.max():.0f} characters")
            
            # Warn about very short documents
            short_docs = (text_lengths < 100).sum()
            if short_docs > 0:
                log.warning(f"Found {short_docs} documents with less than 100 characters")
            
            log.info(f"✅ Validation completed for {len(df)} documents")
            return True
            
        except Exception as e:
            log.error(f"❌ Validation failed: {e}")
            return False


================================================================================


################################################################################
# File: scripts/extract_all_to_markdown.py
################################################################################

# File: scripts/extract_all_to_markdown.py

#!/usr/bin/env python3
"""
Extract all PDFs to markdown format for GraphRAG processing.
"""

import asyncio
from pathlib import Path
import logging
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import os

from graph_stages.pdf_extractor import PDFExtractor
from graph_stages.agenda_pdf_extractor import AgendaPDFExtractor
from graph_stages.enhanced_document_linker import EnhancedDocumentLinker
from graph_stages.verbatim_transcript_linker import VerbatimTranscriptLinker

logging.basicConfig(level=logging.INFO)
log = logging.getLogger(__name__)

async def extract_all_documents():
    """Extract all city clerk documents with parallel processing."""
    
    base_dir = Path("city_clerk_documents/global/City Comissions 2024")
    markdown_dir = Path("city_clerk_documents/extracted_markdown")
    markdown_dir.mkdir(exist_ok=True)
    
    if not base_dir.exists():
        log.error(f"❌ Base directory not found: {base_dir}")
        log.error(f"   Current working directory: {Path.cwd()}")
        return
    
    log.info(f"📁 Base directory found: {base_dir}")
    
    # Increase max workers based on system capabilities
    max_workers = min(os.cpu_count() * 2, 16)  # Increased from min(cpu_count, 8)
    
    stats = {
        'agendas': 0,
        'ordinances': 0,
        'resolutions': 0,
        'transcripts': 0,
        'errors': 0
    }
    
    # Process Agendas in parallel
    log.info(f"📋 Extracting Agendas with {max_workers} workers...")
    agenda_dir = base_dir / "Agendas"
    if agenda_dir.exists():
        log.info(f"   Found agenda directory: {agenda_dir}")
        extractor = AgendaPDFExtractor()
        agenda_pdfs = list(agenda_dir.glob("*.pdf"))
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_pdf = {
                executor.submit(process_agenda_pdf, extractor, pdf): pdf 
                for pdf in agenda_pdfs
            }
            
            for future in as_completed(future_to_pdf):
                pdf = future_to_pdf[future]
                try:
                    success = future.result()
                    if success:
                        stats['agendas'] += 1
                except Exception as e:
                    log.error(f"Failed to extract {pdf}: {e}")
                    stats['errors'] += 1
    else:
        log.warning(f"⚠️  Agenda directory not found: {agenda_dir}")
    
    # Process Ordinances and Resolutions in parallel
    log.info("📜 Extracting Ordinances and Resolutions in parallel...")
    ord_dir = base_dir / "Ordinances"
    res_dir = base_dir / "Resolutions"
    
    if ord_dir.exists() and res_dir.exists():
        linker = EnhancedDocumentLinker()
        
        # Combine ordinances and resolutions for parallel processing
        all_docs = []
        for pdf in ord_dir.rglob("*.pdf"):
            all_docs.append(('ordinance', pdf))
        for pdf in res_dir.rglob("*.pdf"):
            all_docs.append(('resolution', pdf))
        
        # Process in parallel with asyncio
        async def process_documents_batch(docs_batch):
            tasks = []
            for doc_type, pdf_path in docs_batch:
                meeting_date = extract_meeting_date_from_filename(pdf_path.name)
                if meeting_date:
                    task = process_document_async(linker, pdf_path, meeting_date, doc_type)
                    tasks.append(task)
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            return results
        
        # Process in batches to avoid overwhelming the system
        batch_size = max_workers * 2
        for i in range(0, len(all_docs), batch_size):
            batch = all_docs[i:i + batch_size]
            results = await process_documents_batch(batch)
            
            for result, (doc_type, _) in zip(results, batch):
                if isinstance(result, Exception):
                    stats['errors'] += 1
                    log.error(f"Error: {result}")
                elif result:
                    stats['ordinances' if doc_type == 'ordinance' else 'resolutions'] += 1
    else:
        log.warning(f"⚠️  Ordinances or Resolutions directory not found: {ord_dir}, {res_dir}")
    
    log.info("🎤 Extracting Verbatim Transcripts...")
    verbatim_dirs = [
        base_dir / "Verbatim Items",
        base_dir / "Verbating Items"
    ]
    
    verbatim_dir = None
    for vdir in verbatim_dirs:
        if vdir.exists():
            verbatim_dir = vdir
            break
    
    if verbatim_dir:
        log.info(f"   Found verbatim directory: {verbatim_dir}")
        transcript_linker = VerbatimTranscriptLinker()
        
        all_verb_pdfs = list(verbatim_dir.rglob("*.pdf"))
        log.info(f"   Found {len(all_verb_pdfs)} verbatim PDFs")
        
        # Process verbatim transcripts in parallel batches
        batch_size = max_workers
        for i in range(0, len(all_verb_pdfs), batch_size):
            batch = all_verb_pdfs[i:i + batch_size]
            
            tasks = []
            for pdf_path in batch:
                meeting_date = extract_meeting_date_from_verbatim(pdf_path.name)
                if meeting_date:
                    task = process_verbatim_async(transcript_linker, pdf_path, meeting_date)
                    tasks.append(task)
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in results:
                if isinstance(result, Exception):
                    stats['errors'] += 1
                    log.error(f"Error: {result}")
                elif result:
                    stats['transcripts'] += 1
    else:
        log.warning(f"⚠️  Verbatim directory not found. Tried: {verbatim_dirs}")
    
    log.info("\n📊 Extraction Summary:")
    log.info(f"   Agendas: {stats['agendas']}")
    log.info(f"   Ordinances: {stats['ordinances']}")
    log.info(f"   Resolutions: {stats['resolutions']}")
    log.info(f"   Transcripts: {stats['transcripts']}")
    log.info(f"   Errors: {stats['errors']}")
    log.info(f"   Total: {sum(stats.values()) - stats['errors']}")
    
    log.info(f"\n✅ All documents extracted to:")
    log.info(f"   JSON: city_clerk_documents/extracted_text/")
    log.info(f"   Markdown: {markdown_dir}")

def process_agenda_pdf(extractor, pdf):
    """Process single agenda PDF (for thread pool)."""
    try:
        log.info(f"   Processing: {pdf.name}")
        agenda_data = extractor.extract_agenda(pdf)
        output_path = Path("city_clerk_documents/extracted_text") / f"{pdf.stem}_extracted.json"
        extractor.save_extracted_agenda(agenda_data, output_path)
        return True
    except Exception as e:
        log.error(f"Failed to extract {pdf}: {e}")
        return False

async def process_document_async(linker, pdf_path, meeting_date, doc_type):
    """Process document asynchronously."""
    try:
        doc_info = await linker._process_document(pdf_path, meeting_date, doc_type)
        if doc_info:
            linker._save_extracted_text(pdf_path, doc_info, doc_type)
            return True
        return False
    except Exception as e:
        raise e

# Add async helper
async def process_verbatim_async(transcript_linker, pdf_path, meeting_date):
    """Process verbatim transcript asynchronously."""
    try:
        transcript_info = await transcript_linker._process_transcript(pdf_path, meeting_date)
        if transcript_info:
            transcript_linker._save_extracted_text(pdf_path, transcript_info)
            return True
        return False
    except Exception as e:
        raise e

def extract_meeting_date_from_filename(filename: str) -> str:
    """Extract meeting date from ordinance/resolution filename."""
    import re
    
    match = re.search(r'(\d{2})_(\d{2})_(\d{4})', filename)
    if match:
        month, day, year = match.groups()
        return f"{month}.{day}.{year}"
    
    return None

def extract_meeting_date_from_verbatim(filename: str) -> str:
    """Extract meeting date from verbatim transcript filename."""
    import re
    
    match = re.match(r'(\d{2})_(\d{2})_(\d{4})', filename)
    if match:
        month, day, year = match.groups()
        return f"{month}.{day}.{year}"
    
    return None

if __name__ == "__main__":
    asyncio.run(extract_all_documents())


================================================================================


################################################################################
# File: scripts/graph_rag_stages/phase3_querying/query_engine.py
################################################################################

# File: scripts/graph_rag_stages/phase3_querying/query_engine.py

"""
The core query engine that processes user requests and generates responses using GraphRAG.
"""

import logging
import subprocess
import sys
from pathlib import Path
from typing import Dict, Any, Optional, List
import json
import pandas as pd

from .query_router import QueryRouter
from .response_enhancer import ResponseEnhancer
from .source_tracker import SourceTracker

log = logging.getLogger(__name__)


class QueryEngine:
    """Handles the end-to-end process of answering user queries using GraphRAG."""

    def __init__(self, graphrag_root: Path):
        self.graphrag_root = graphrag_root
        self.output_dir = graphrag_root / "output"
        
        # Initialize components
        self.router = QueryRouter()
        self.enhancer = ResponseEnhancer()
        self.source_tracker = SourceTracker()
        
        # Determine if deduplicated data exists and should be used
        dedup_dir = self.output_dir / "deduplicated"
        self.data_root = dedup_dir if dedup_dir.exists() else self.output_dir

    def validate_graphrag_output(self) -> bool:
        """Validate that GraphRAG output exists and is usable."""
        required_files = [
            "create_final_entities.parquet",
            "create_final_relationships.parquet",
            "create_final_communities.parquet"
        ]
        
        for file_name in required_files:
            file_path = self.data_root / file_name
            if not file_path.exists():
                log.warning(f"Missing required GraphRAG output file: {file_name}")
                return False
        
        return True

    async def answer_query(self, query_text: str, method: str = "auto") -> Dict[str, Any]:
        """
        Takes a user query and returns a comprehensive answer.
        
        Args:
            query_text: The user's question
            method: Query method ('global', 'local', or 'auto')
            
        Returns:
            Dictionary containing the answer and metadata
        """
        log.info(f"🔍 Processing query: '{query_text[:100]}...'")
        
        # Reset source tracking
        self.source_tracker.reset()
        
        # 1. Route the query to determine the best method
        if method == "auto":
            routing_info = self.router.determine_query_method(query_text)
            method = routing_info['method']
            log.info(f"📍 Auto-routed to method: '{method}'")
        
        # 2. Execute the GraphRAG query
        try:
            raw_response = await self._execute_graphrag_query(query_text, method)
            if not raw_response:
                return self._create_error_response("GraphRAG query failed")
            
            # 3. Process and enhance the response
            enhanced_response = await self.enhancer.enhance_response(query_text, raw_response)
            
            # 4. Add source tracking information
            enhanced_response['sources'] = self.source_tracker.get_summary()
            enhanced_response['query_metadata'] = {
                'method_used': method,
                'graphrag_root': str(self.graphrag_root),
                'data_source': 'deduplicated' if self.data_root.name == 'deduplicated' else 'original'
            }
            
            log.info("✅ Query processed successfully")
            return enhanced_response
            
        except Exception as e:
            log.error(f"❌ Query processing failed: {e}")
            return self._create_error_response(str(e))

    async def _execute_graphrag_query(self, query_text: str, method: str) -> Optional[Dict[str, Any]]:
        """Execute GraphRAG query using subprocess."""
        log.info(f"🚀 Executing GraphRAG {method} query")
        
        # Prepare the command
        cmd = [
            sys.executable,
            "-m", "graphrag.query",
            "--root", str(self.graphrag_root),
            "--method", method,
            query_text
        ]
        
        try:
            # Execute the query
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=300  # 5 minute timeout
            )
            
            if result.returncode != 0:
                log.error(f"GraphRAG query failed: {result.stderr}")
                return None
            
            # Parse the output
            output_lines = result.stdout.strip().split('\n')
            
            # Find the response (usually the last substantial line)
            response_text = ""
            for line in reversed(output_lines):
                if line.strip() and not line.startswith('[') and not line.startswith('INFO'):
                    response_text = line.strip()
                    break
            
            if not response_text:
                response_text = result.stdout.strip()
            
            return {
                'answer': response_text,
                'method': method,
                'raw_output': result.stdout,
                'success': True
            }
            
        except subprocess.TimeoutExpired:
            log.error("GraphRAG query timed out")
            return None
        except Exception as e:
            log.error(f"Error executing GraphRAG query: {e}")
            return None

    def _create_error_response(self, error_message: str) -> Dict[str, Any]:
        """Create a standardized error response."""
        return {
            'answer': f"I apologize, but I encountered an error processing your query: {error_message}",
            'success': False,
            'error': error_message,
            'sources': {},
            'query_metadata': {
                'method_used': 'error',
                'graphrag_root': str(self.graphrag_root)
            }
        }

    def get_available_entities(self, limit: int = 100) -> List[Dict[str, Any]]:
        """Get a sample of available entities for query suggestions."""
        entities_file = self.data_root / "create_final_entities.parquet"
        
        if not entities_file.exists():
            return []
        
        try:
            df = pd.read_parquet(entities_file)
            
            # Sample entities and return relevant fields
            sample_df = df.head(limit)
            entities = []
            
            for _, row in sample_df.iterrows():
                entity = {
                    'title': row.get('title', 'Unknown'),
                    'type': row.get('type', 'Unknown'),
                    'description': str(row.get('description', ''))[:200]  # Truncate description
                }
                entities.append(entity)
            
            return entities
            
        except Exception as e:
            log.error(f"Error loading entities: {e}")
            return []

    def get_system_stats(self) -> Dict[str, Any]:
        """Get statistics about the GraphRAG system."""
        stats = {
            'graphrag_root': str(self.graphrag_root),
            'data_source': 'deduplicated' if self.data_root.name == 'deduplicated' else 'original',
            'entities_count': 0,
            'relationships_count': 0,
            'communities_count': 0,
            'system_ready': self.validate_graphrag_output()
        }
        
        # Count entities
        entities_file = self.data_root / "create_final_entities.parquet"
        if entities_file.exists():
            try:
                df = pd.read_parquet(entities_file)
                stats['entities_count'] = len(df)
            except Exception:
                pass
        
        # Count relationships
        relationships_file = self.data_root / "create_final_relationships.parquet"
        if relationships_file.exists():
            try:
                df = pd.read_parquet(relationships_file)
                stats['relationships_count'] = len(df)
            except Exception:
                pass
        
        # Count communities
        communities_file = self.data_root / "create_final_communities.parquet"
        if communities_file.exists():
            try:
                df = pd.read_parquet(communities_file)
                stats['communities_count'] = len(df)
            except Exception:
                pass
        
        return stats


================================================================================


################################################################################
# File: scripts/graph_rag_stages/common/config.py
################################################################################

# File: scripts/graph_rag_stages/common/config.py

"""
Configuration management for the unified GraphRAG pipeline.
"""

import os
from pathlib import Path
from typing import Dict, Any, Optional
import yaml
from dataclasses import dataclass
from dotenv import load_dotenv
import logging

# Load environment variables
load_dotenv()

log = logging.getLogger(__name__)

@dataclass
class Config:
    """Configuration class for the unified pipeline."""
    
    # Environment settings
    openai_api_key: str
    openai_model: str = "gpt-4"
    
    # Cosmos DB settings (for custom graph pipeline)
    cosmos_endpoint: Optional[str] = None
    cosmos_key: Optional[str] = None
    cosmos_database: str = "cgGraph"
    cosmos_container: str = "cityClerk"
    
    # File paths
    project_root: Path = None
    source_documents_dir: Path = None
    markdown_output_dir: Path = None
    graphrag_input_dir: Path = None
    
    # Pipeline settings
    chunk_size: int = 4000
    chunk_overlap: int = 200
    max_concurrent_requests: int = 5
    
    # GraphRAG specific settings
    graphrag_verbose: bool = True
    force_reindex: bool = False
    enable_entity_deduplication: bool = True
    deduplication_config: str = "conservative"
    
    def __post_init__(self):
        """Initialize paths after dataclass creation."""
        if self.project_root is None:
            self.project_root = Path(__file__).parent.parent.parent
        
        if self.source_documents_dir is None:
            self.source_documents_dir = self.project_root / "city_clerk_documents/global/City Comissions 2024"
        
        if self.markdown_output_dir is None:
            self.markdown_output_dir = self.project_root / "city_clerk_documents/extracted_markdown"
        
        if self.graphrag_input_dir is None:
            self.graphrag_input_dir = self.project_root / "graphrag_data"


def get_config(config_file: Optional[Path] = None) -> Config:
    """
    Load configuration from environment variables and optional YAML file.
    
    Args:
        config_file: Optional YAML configuration file path
        
    Returns:
        Config object with all settings
    """
    # Start with environment variables
    config_data = {
        "openai_api_key": os.getenv("OPENAI_API_KEY"),
        "openai_model": os.getenv("OPENAI_MODEL", "gpt-4"),
        "cosmos_endpoint": os.getenv("COSMOS_ENDPOINT"),
        "cosmos_key": os.getenv("COSMOS_KEY"),
        "cosmos_database": os.getenv("COSMOS_DATABASE", "cgGraph"),
        "cosmos_container": os.getenv("COSMOS_CONTAINER", "cityClerk"),
    }
    
    # Load from YAML if provided
    if config_file and config_file.exists():
        try:
            with open(config_file, 'r') as f:
                yaml_config = yaml.safe_load(f)
                config_data.update(yaml_config)
            log.info(f"Loaded configuration from {config_file}")
        except Exception as e:
            log.warning(f"Failed to load config file {config_file}: {e}")
    
    # Check for settings.yaml in project root
    project_root = Path(__file__).parent.parent.parent
    settings_file = project_root / "settings.yaml"
    if settings_file.exists() and config_file != settings_file:
        try:
            with open(settings_file, 'r') as f:
                yaml_config = yaml.safe_load(f)
                # Only update if not already set
                for key, value in yaml_config.items():
                    if key not in config_data or config_data[key] is None:
                        config_data[key] = value
            log.info(f"Loaded additional configuration from {settings_file}")
        except Exception as e:
            log.warning(f"Failed to load settings file {settings_file}: {e}")
    
    # Validate required settings
    if not config_data.get("openai_api_key"):
        raise ValueError("OPENAI_API_KEY is required but not found in environment or config file")
    
    return Config(**{k: v for k, v in config_data.items() if v is not None})


def save_config(config: Config, output_file: Path) -> None:
    """
    Save configuration to a YAML file.
    
    Args:
        config: Configuration object
        output_file: Path to output YAML file
    """
    # Convert config to dict, handling Path objects
    config_dict = {}
    for key, value in config.__dict__.items():
        if isinstance(value, Path):
            config_dict[key] = str(value)
        else:
            config_dict[key] = value
    
    try:
        with open(output_file, 'w') as f:
            yaml.dump(config_dict, f, default_flow_style=False, indent=2)
        log.info(f"Configuration saved to {output_file}")
    except Exception as e:
        log.error(f"Failed to save configuration to {output_file}: {e}")
        raise


================================================================================


################################################################################
# File: scripts/graph_rag_stages/phase3_querying/query_router.py
################################################################################

# File: scripts/graph_rag_stages/phase3_querying/query_router.py

"""
Query router that determines the best query method based on query characteristics.
"""

import logging
from typing import Dict, Any
import re

log = logging.getLogger(__name__)


class QueryRouter:
    """Routes queries to the most appropriate GraphRAG method."""
    
    def __init__(self):
        # Keywords that suggest global vs local queries
        self.global_keywords = [
            'overview', 'summary', 'trend', 'pattern', 'analysis', 'report',
            'all', 'overall', 'general', 'comprehensive', 'throughout',
            'across', 'between', 'comparison', 'statistics', 'data'
        ]
        
        self.local_keywords = [
            'specific', 'particular', 'exactly', 'precisely', 'detail',
            'what is', 'who is', 'when', 'where', 'how', 'item',
            'agenda item', 'ordinance', 'resolution', 'meeting'
        ]

    def determine_query_method(self, query_text: str) -> Dict[str, Any]:
        """
        Determine the best query method based on query characteristics.
        
        Args:
            query_text: The user's query
            
        Returns:
            Dictionary with routing information
        """
        query_lower = query_text.lower()
        
        # Score for global vs local
        global_score = 0
        local_score = 0
        
        # Check for global keywords
        for keyword in self.global_keywords:
            if keyword in query_lower:
                global_score += 1
        
        # Check for local keywords
        for keyword in self.local_keywords:
            if keyword in query_lower:
                local_score += 1
        
        # Check for specific patterns
        if self._has_specific_entity_reference(query_lower):
            local_score += 2
        
        if self._has_broad_analysis_terms(query_lower):
            global_score += 2
        
        # Determine method
        if global_score > local_score:
            method = 'global'
            confidence = min(global_score / (global_score + local_score + 1), 0.9)
            intent = 'broad_analysis'
        elif local_score > global_score:
            method = 'local'
            confidence = min(local_score / (global_score + local_score + 1), 0.9)
            intent = 'specific_lookup'
        else:
            # Default to local for balanced or unclear queries
            method = 'local'
            confidence = 0.5
            intent = 'balanced'
        
        return {
            'method': method,
            'confidence': confidence,
            'intent': intent,
            'global_score': global_score,
            'local_score': local_score,
            'reasoning': self._generate_reasoning(method, global_score, local_score)
        }

    def _has_specific_entity_reference(self, query_lower: str) -> bool:
        """Check if query references specific entities."""
        patterns = [
            r'item [a-z]-\d+',  # Agenda items like "item e-1"
            r'ordinance \d+',   # Ordinance numbers
            r'resolution \d+',  # Resolution numbers
            r'\d{2}\.\d{2}\.\d{4}',  # Specific dates
        ]
        
        for pattern in patterns:
            if re.search(pattern, query_lower):
                return True
        
        return False

    def _has_broad_analysis_terms(self, query_lower: str) -> bool:
        """Check if query contains terms suggesting broad analysis."""
        broad_terms = [
            'trends', 'patterns', 'analysis', 'overview', 'summary',
            'all meetings', 'all items', 'general', 'overall'
        ]
        
        for term in broad_terms:
            if term in query_lower:
                return True
        
        return False

    def _generate_reasoning(self, method: str, global_score: int, local_score: int) -> str:
        """Generate human-readable reasoning for the routing decision."""
        if method == 'global':
            return f"Routed to global search (score: {global_score}) - query appears to need broad analysis across multiple documents"
        elif method == 'local':
            if local_score > global_score:
                return f"Routed to local search (score: {local_score}) - query appears to seek specific information"
            else:
                return "Routed to local search (default) - query intent unclear, using targeted search"
        else:
            return "Default routing applied"


================================================================================


################################################################################
# File: scripts/graph_rag_stages/phase3_querying/source_tracker.py
################################################################################

# File: scripts/graph_rag_stages/phase3_querying/source_tracker.py

"""
Source tracking component for GraphRAG queries to provide provenance information.
"""

import logging
from typing import Dict, List, Any, Set
from pathlib import Path

log = logging.getLogger(__name__)


class SourceTracker:
    """Track sources used during GraphRAG queries for provenance."""
    
    def __init__(self):
        self.reset()
    
    def reset(self):
        """Reset tracking state."""
        self.entities_used: Dict[str, Dict[str, Any]] = {}
        self.relationships_used: Dict[str, Dict[str, Any]] = {}
        self.sources_used: Dict[str, Dict[str, Any]] = {}
        self.documents_referenced: Set[str] = set()
    
    def track_entity(self, entity_id: str, entity_data: Dict[str, Any]):
        """Track an entity being used in the query."""
        self.entities_used[entity_id] = {
            'id': entity_id,
            'title': entity_data.get('title', 'Unknown'),
            'type': entity_data.get('type', 'Unknown'),
            'description': str(entity_data.get('description', ''))[:200]  # Truncate
        }
    
    def track_relationship(self, rel_id: str, rel_data: Dict[str, Any]):
        """Track a relationship being used in the query."""
        self.relationships_used[rel_id] = {
            'id': rel_id,
            'source': rel_data.get('source', ''),
            'target': rel_data.get('target', ''),
            'description': str(rel_data.get('description', ''))[:200],
            'weight': rel_data.get('weight', 0)
        }
    
    def track_source_document(self, doc_id: str, doc_data: Dict[str, Any]):
        """Track a source document being used."""
        self.sources_used[doc_id] = {
            'id': doc_id,
            'title': doc_data.get('title', 'Unknown'),
            'type': doc_data.get('document_type', 'document'),
            'source_file': doc_data.get('source_file', '')
        }
        
        # Also add to documents referenced set
        if doc_data.get('source_file'):
            self.documents_referenced.add(doc_data['source_file'])
    
    def get_summary(self) -> Dict[str, Any]:
        """Get summary of all tracked sources."""
        return {
            'entities_count': len(self.entities_used),
            'relationships_count': len(self.relationships_used),
            'sources_count': len(self.sources_used),
            'documents_referenced': list(self.documents_referenced),
            'entities': list(self.entities_used.values()),
            'relationships': list(self.relationships_used.values()),
            'sources': list(self.sources_used.values())
        }
    
    def get_citation_text(self) -> str:
        """Generate citation text for the sources used."""
        if not self.sources_used and not self.documents_referenced:
            return "No specific sources tracked for this query."
        
        citations = []
        
        # Add document references
        if self.documents_referenced:
            doc_list = sorted(self.documents_referenced)
            citations.append(f"Documents referenced: {', '.join(doc_list[:5])}")
            if len(doc_list) > 5:
                citations.append(f"... and {len(doc_list) - 5} more documents")
        
        # Add entity information
        if self.entities_used:
            entity_count = len(self.entities_used)
            citations.append(f"Based on {entity_count} entities from the knowledge graph")
        
        return "; ".join(citations)
    
    def get_top_entities(self, limit: int = 10) -> List[Dict[str, Any]]:
        """Get the top entities used in the query."""
        entities = list(self.entities_used.values())
        return entities[:limit]
    
    def has_sources(self) -> bool:
        """Check if any sources were tracked."""
        return bool(self.entities_used or self.relationships_used or self.sources_used)


================================================================================


################################################################################
# File: scripts/graph_rag_stages/main_pipeline.py
################################################################################

# File: scripts/graph_rag_stages/main_pipeline.py

"""
Main orchestrator for the unified City Clerk GraphRAG pipeline.

This script controls the entire workflow, from data extraction to indexing and querying,
allowing major components to be enabled or disabled via boolean flags.
"""
import asyncio
from pathlib import Path
import logging
import argparse

# Import from renamed, valid package directories
from . import phase1_preprocessing as preprocessing
from . import phase2_building as building

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
log = logging.getLogger(__name__)

# --- PIPELINE CONTROL FLAGS ---
RUN_DATA_PREPROCESSING = True
# NOTE: The custom graph pipeline (Cosmos DB) is disabled by default to focus on the
# primary GraphRAG workflow. Set to True to enable it.
RUN_CUSTOM_GRAPH_PIPELINE = False
RUN_GRAPHRAG_INDEXING_PIPELINE = True

# --- SUB-COMPONENT FLAGS ---
FORCE_REINDEX = False
RUN_DEDUPLICATION = True
DEDUP_CONFIG = 'conservative'  # Options: 'conservative', 'aggressive', 'name_focused'

async def main(args):
    """Execute the unified data pipeline based on the configured flags."""
    log.info("🚀 Starting the Unified City Clerk Knowledge Graph Pipeline")
    
    project_root = Path(__file__).resolve().parent.parent
    base_source_dir = project_root / args.source_dir
    markdown_output_dir = project_root / "city_clerk_documents/extracted_markdown"
    graphrag_input_dir = project_root / "graphrag_data"

    if RUN_DATA_PREPROCESSING:
        log.info("▶️ STAGE 1: Data Pre-processing & Extraction")
        await preprocessing.run_extraction_pipeline(base_source_dir, markdown_output_dir)
        log.info("✅ STAGE 1: Completed")

    if RUN_CUSTOM_GRAPH_PIPELINE:
        log.warning("Custom graph building is preserved but requires a full implementation to run.")
        
    if RUN_GRAPHRAG_INDEXING_PIPELINE:
        log.info("▶️ STAGE 2: Building GraphRAG Index")
        await building.run_graphrag_indexing_pipeline(
            markdown_source_dir=markdown_output_dir,
            graphrag_input_dir=graphrag_input_dir,
            force_reindex=FORCE_REINDEX,
            run_deduplication=RUN_DEDUPLICATION,
            dedup_config_name=DEDUP_CONFIG
        )
        log.info("✅ STAGE 2: Completed")
    
    log.info("🎉 Unified Pipeline Run Finished.")
    log.info("To query the graph, start the UI: `python -m ui.query_app`")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Unified City Clerk GraphRAG Pipeline")
    parser.add_argument(
        '--source-dir',
        type=str,
        default="city_clerk_documents/global/City Comissions 2024",
        help="Path to the root directory containing source PDFs, relative to the project root."
    )
    args = parser.parse_args()
    asyncio.run(main(args))


================================================================================


################################################################################
# File: settings.yaml
################################################################################

# settings.yaml
# Unified configuration for the GraphRAG pipeline

llm:
  type: openai_chat
  api_key: ${OPENAI_API_KEY}
  model: gpt-4.1-mini-2025-04-14
  max_tokens: 16384
  temperature: 0.0
  # Additional parameters like organization, base_url can be added here

embeddings:
  type: openai_embedding
  api_key: ${OPENAI_API_KEY}
  model: text-embedding-3-small
  batch_size: 16

input:
  type: file
  file_type: csv
  base_dir: "." # Relative to the graphrag_data directory
  file_pattern: "city_clerk_documents.csv"
  text_column: "text"
  title_column: "title"
  id_column: "id"

storage:
  type: file
  base_dir: "./output" # Relative to the graphrag_data directory

cache:
  type: file
  base_dir: "./cache" # Relative to the graphrag_data directory

chunks:
  size: 1024
  overlap: 256
  group_by_columns: ["document_type", "meeting_date"]

entity_extraction:
  prompt: "prompts/entity_extraction.txt"
  entity_types:
    - "person"
    - "organization"
    - "location"
    - "event"
    - "document"
    - "agenda_item"
    - "ordinance"
    - "resolution"
    - "document_number"
  max_gleanings: 3

# Other pipeline sections (claim_extraction, community_reports, etc.) follow...


================================================================================


################################################################################
# File: scripts/graph_rag_stages/common/__init__.py
################################################################################

# File: scripts/graph_rag_stages/common/__init__.py

"""
Common utilities for the unified GraphRAG pipeline.

This module provides shared functionality used across all pipeline stages:
- Configuration management
- Database clients (Cosmos DB)
- Logging utilities
- File handling utilities
- LLM client setup
"""

from .config import get_config, Config
from .cosmos_client import CosmosGraphClient
from .utils import (
    get_llm_client,
    extract_metadata_from_header,
    clean_json_response,
    setup_logging,
    ensure_directory_exists
)

__all__ = [
    'get_config',
    'Config',
    'CosmosGraphClient',
    'get_llm_client',
    'extract_metadata_from_header',
    'clean_json_response',
    'setup_logging',
    'ensure_directory_exists'
]


================================================================================


################################################################################
# File: ui/__init__.py
################################################################################

# File: ui/__init__.py

"""
User Interface module for the GraphRAG query system.
"""

__all__ = ['query_app']


================================================================================

