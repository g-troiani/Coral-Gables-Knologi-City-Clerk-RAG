# Concatenated Project Code - Part 3 of 3
# Generated: 2025-05-27 22:24:03
# Root Directory: /Users/gianmariatroiani/Documents/knologiÃä/city_clerk_rag
================================================================================

# File Index - Which Files Are in Which Parts
################################################################################

## Part 1 (4 files):
  - scripts/stages/embed_vectors.py
  - scripts/stages/db_upsert.py
  - scripts/test_vector_search.py
  - scripts/stages/acceleration_utils.py

## Part 2 (5 files):
  - scripts/stages/extract_clean.py
  - scripts/rag_local_web_app.py
  - scripts/find_duplicates.py
  - scripts/topic_filter_and_title.py
  - scripts/stages/__init__.py

## Part 3 (5 files):
  - scripts/stages/chunk_text.py
  - scripts/pipeline_modular_optimized.py
  - scripts/clear_database.py
  - scripts/stages/llm_enrich.py
  - requirements.txt


================================================================================


################################################################################
# File: scripts/stages/chunk_text.py
################################################################################

# File: scripts/stages/chunk_text.py

#!/usr/bin/env python3
"""
Stage 5 ‚Äî Token-based chunking with tiktoken validation.
"""
from __future__ import annotations
import json, logging, math, pathlib, re
from typing import Dict, List, Sequence, Tuple, Any, Optional
import asyncio
from concurrent.futures import ProcessPoolExecutor
import multiprocessing as mp

# üéØ TIKTOKEN VALIDATION - Add token validation layer
try:
    import tiktoken
    TIKTOKEN_AVAILABLE = True
except ImportError:
    TIKTOKEN_AVAILABLE = False
    logging.warning("tiktoken not available - token-based chunking requires tiktoken")

# Token limits for validation (consistent with embed_vectors.py)
MAX_CHUNK_TOKENS = 6000  # Maximum tokens per chunk for embedding
MIN_CHUNK_TOKENS = 100   # Minimum viable chunk size
EMBEDDING_MODEL = "text-embedding-ada-002"

# ‚îÄ‚îÄ‚îÄ TOKEN-BASED CHUNKING PARAMETERS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
WINDOW_TOKENS = 3000   # Token-based window (was 768 words)
OVERLAP_TOKENS = 600   # Token-based overlap (20% of window)
log = logging.getLogger(__name__)

# ‚îÄ‚îÄ‚îÄ helpers to split into token windows ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def concat_tokens_by_encoding(sections: Sequence[Dict[str, Any]]) -> Tuple[List[int], List[int]]:
    """Convert sections to encoded tokens with page mapping."""
    if not TIKTOKEN_AVAILABLE:
        raise RuntimeError("tiktoken is required for token-based chunking")
    
    encoder = tiktoken.encoding_for_model(EMBEDDING_MODEL)
    all_tokens = []
    token_to_page = []
    
    for s in sections:
        # Be defensive - build text if missing
        if 'text' not in s:
            if 'elements' in s:
                text = '\n'.join(
                    el.get('text', '') for el in s['elements'] 
                    if el.get('text', '') and not el.get('text', '').startswith('self_ref=')
                )
            else:
                text = ""
            log.warning(f"Section missing 'text' field, built from elements: {s.get('section', 'untitled')}")
        else:
            text = s.get("text", "")
        
        # Encode text to tokens
        tokens = encoder.encode(text)
        all_tokens.extend(tokens)
        
        # Handle page mapping
        if 'page_number' in s:
            page_num = s['page_number']
        elif 'page_start' in s and 'page_end' in s:
            page_start = s['page_start']
            page_end = s['page_end']
            if page_start == page_end:
                page_num = page_start
            else:
                # For multi-page sections, distribute tokens across pages
                pages_span = page_end - page_start + 1
                tokens_per_page = max(1, len(tokens) // pages_span)
                for i, token in enumerate(tokens):
                    page = min(page_end, page_start + (i // tokens_per_page))
                    token_to_page.append(page)
                continue  # Skip the extend below since we handled it above
        else:
            page_num = 1
            log.warning(f"Section has no page info: {s.get('section', 'untitled')}")
        
        # Map all tokens in this section to the page (for single page sections)
        if 'page_start' not in s or s['page_start'] == s.get('page_end', s['page_start']):
            token_to_page.extend([page_num] * len(tokens))
    
    return all_tokens, token_to_page

def sliding_chunks_by_tokens(
    encoded_tokens: List[int], 
    token_to_page: List[int], 
    *, 
    window_tokens: int, 
    overlap_tokens: int
) -> List[Dict[str, Any]]:
    """Create sliding chunks based on token count."""
    if not TIKTOKEN_AVAILABLE:
        raise RuntimeError("tiktoken is required for token-based chunking")
    
    encoder = tiktoken.encoding_for_model(EMBEDDING_MODEL)
    step = max(1, window_tokens - overlap_tokens)
    chunks = []
    i = 0
    
    while i < len(encoded_tokens):
        start = i
        end = min(len(encoded_tokens), i + window_tokens)
        
        # Extract token chunk
        chunk_tokens = encoded_tokens[start:end]
        
        # Decode back to text
        chunk_text = encoder.decode(chunk_tokens)
        
        # Determine page range for this chunk
        page_start = token_to_page[start] if start < len(token_to_page) else 1
        page_end = token_to_page[end - 1] if end - 1 < len(token_to_page) else page_start
        
        chunk = {
            "chunk_index": len(chunks),
            "token_start": start,
            "token_end": end - 1,
            "page_start": page_start,
            "page_end": page_end,
            "text": chunk_text,
            "token_count": len(chunk_tokens)  # Actual token count
        }
        
        chunks.append(chunk)
        
        # Break if we've reached the end
        if end == len(encoded_tokens):
            break
            
        i += step
    
    return chunks

# Legacy word-based functions (kept for fallback if tiktoken unavailable)
def concat_tokens(sections:Sequence[Dict[str,Any]])->Tuple[List[str],List[int]]:
    tokens,page_map=[],[]
    for s in sections:
        # Be defensive - build text if missing
        if 'text' not in s:
            if 'elements' in s:
                text = '\n'.join(
                    el.get('text', '') for el in s['elements'] 
                    if el.get('text', '') and not el.get('text', '').startswith('self_ref=')
                )
            else:
                text = ""
            log.warning(f"Section missing 'text' field, built from elements: {s.get('section', 'untitled')}")
        else:
            text = s.get("text", "")
            
        words=text.split()
        tokens.extend(words)
        
        # Handle both page_number (from page_secs) and page_start/page_end (from logical_secs)
        if 'page_number' in s:
            # Single page section
            page_map.extend([s['page_number']]*len(words))
        elif 'page_start' in s and 'page_end' in s:
            # Multi-page section - distribute words across pages
            page_start = s['page_start']
            page_end = s['page_end']
            if page_start == page_end:
                # All on same page
                page_map.extend([page_start]*len(words))
            else:
                # Distribute words evenly across pages
                pages_span = page_end - page_start + 1
                words_per_page = max(1, len(words) // pages_span)
                for i, word in enumerate(words):
                    page = min(page_end, page_start + (i // words_per_page))
                    page_map.append(page)
        else:
            # Fallback to page 1
            page_map.extend([1]*len(words))
            log.warning(f"Section has no page info: {s.get('section', 'untitled')}")
            
    return tokens,page_map

def sliding_chunks(tokens:List[str],page_map:List[int],*,window:int,overlap:int)->List[Dict[str,Any]]:
    step=max(1,window-overlap)
    out,i=[],0
    SENT_END_RE=re.compile(r"[.!?]$")
    while i<len(tokens):
        start,end=i,min(len(tokens),i+window)
        while end<len(tokens) and not SENT_END_RE.search(tokens[end-1]) and end-start<window+256:
            end+=1
        out.append({"chunk_index":len(out),"token_start":start,"token_end":end-1,
                    "page_start":page_map[start],"page_end":page_map[end-1],
                    "text":" ".join(tokens[start:end])})
        if end==len(tokens): break
        i+=step
    return out
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# üéØ TIKTOKEN VALIDATION FUNCTIONS
def count_tiktoken_accurate(text: str) -> int:
    """Count tokens accurately using tiktoken."""
    if TIKTOKEN_AVAILABLE:
        try:
            encoder = tiktoken.encoding_for_model(EMBEDDING_MODEL)
            return len(encoder.encode(text))
        except:
            pass
    
    # Conservative fallback estimation
    return int(len(text.split()) * 1.5) + 50

def smart_split_chunk(chunk: Dict[str, Any], max_tokens: int = MAX_CHUNK_TOKENS) -> List[Dict[str, Any]]:
    """Split an oversized chunk into smaller token-compliant chunks."""
    text = chunk["text"]
    tokens = count_tiktoken_accurate(text)
    
    if tokens <= max_tokens:
        return [chunk]
    
    # Calculate how many sub-chunks we need
    num_splits = math.ceil(tokens / max_tokens)
    target_words_per_split = len(text.split()) // num_splits
    
    log.info(f"üéØ Splitting oversized chunk: {tokens} tokens ‚Üí {num_splits} chunks of ~{max_tokens} tokens each")
    
    words = text.split()
    sentences = re.split(r'[.!?]+', text)
    
    sub_chunks = []
    current_words = []
    current_tokens = 0
    
    # Split by sentences when possible, otherwise by words
    for sentence in sentences:
        sentence_words = sentence.strip().split()
        if not sentence_words:
            continue
            
        sentence_tokens = count_tiktoken_accurate(sentence)
        
        # If adding this sentence would exceed limit, finalize current chunk
        if current_tokens + sentence_tokens > max_tokens and current_words:
            # Create sub-chunk
            sub_text = " ".join(current_words)
            sub_chunk = {
                "chunk_index": chunk["chunk_index"],
                "sub_chunk_index": len(sub_chunks),
                "token_start": chunk["token_start"] + len(" ".join(words[:words.index(current_words[0])])),
                "token_end": chunk["token_start"] + len(" ".join(words[:words.index(current_words[-1])])) + len(current_words[-1]),
                "page_start": chunk["page_start"],
                "page_end": chunk["page_end"],
                "text": sub_text
            }
            sub_chunks.append(sub_chunk)
            current_words = []
            current_tokens = 0
        
        # Add sentence to current chunk
        current_words.extend(sentence_words)
        current_tokens += sentence_tokens
    
    # Add final sub-chunk if there's remaining content
    if current_words:
        sub_text = " ".join(current_words)
        if count_tiktoken_accurate(sub_text) >= MIN_CHUNK_TOKENS:
            sub_chunk = {
                "chunk_index": chunk["chunk_index"],
                "sub_chunk_index": len(sub_chunks),
                "token_start": chunk["token_start"],
                "token_end": chunk["token_end"],
                "page_start": chunk["page_start"],
                "page_end": chunk["page_end"],
                "text": sub_text
            }
            sub_chunks.append(sub_chunk)
    
    log.info(f"üéØ Split complete: {len(sub_chunks)} sub-chunks created")
    return sub_chunks

def validate_and_fix_chunks(chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Validate chunks against tiktoken limits and fix oversized ones."""
    if not TIKTOKEN_AVAILABLE:
        log.warning("üéØ tiktoken not available - skipping token validation")
        return chunks
    
    valid_chunks = []
    oversized_count = 0
    total_tokens_before = 0
    total_tokens_after = 0
    
    log.info(f"üéØ TIKTOKEN VALIDATION: Processing {len(chunks)} chunks")
    
    for chunk in chunks:
        tokens = count_tiktoken_accurate(chunk["text"])
        total_tokens_before += tokens
        
        if tokens <= MAX_CHUNK_TOKENS:
            # Chunk is fine, keep as-is
            valid_chunks.append(chunk)
            total_tokens_after += tokens
        else:
            # Split oversized chunk
            log.warning(f"üéØ Oversized chunk detected: {tokens} tokens (max: {MAX_CHUNK_TOKENS})")
            oversized_count += 1
            
            split_chunks = smart_split_chunk(chunk, MAX_CHUNK_TOKENS)
            valid_chunks.extend(split_chunks)
            
            # Count tokens in split chunks
            for split_chunk in split_chunks:
                total_tokens_after += count_tiktoken_accurate(split_chunk["text"])
    
    # Final validation check
    max_tokens_found = max((count_tiktoken_accurate(chunk["text"]) for chunk in valid_chunks), default=0)
    
    log.info(f"üéØ TIKTOKEN VALIDATION COMPLETE:")
    log.info(f"   üìä Original chunks: {len(chunks)}")
    log.info(f"   üìä Final chunks: {len(valid_chunks)}")
    log.info(f"   üìä Oversized chunks split: {oversized_count}")
    log.info(f"   üìä Largest chunk: {max_tokens_found} tokens (limit: {MAX_CHUNK_TOKENS})")
    log.info(f"   ‚úÖ GUARANTEED: All chunks ‚â§ {MAX_CHUNK_TOKENS} tokens")
    
    if max_tokens_found > MAX_CHUNK_TOKENS:
        log.error(f"üö® VALIDATION FAILED: Chunk still exceeds limit!")
        raise RuntimeError(f"Chunk validation failed: {max_tokens_found} > {MAX_CHUNK_TOKENS}")
    
    return valid_chunks

# Legacy constants for fallback
WINDOW  = 768
OVERLAP = int(WINDOW*0.20)           # 154-token (20 %) overlap

# Parallel chunking for multiple documents
async def chunk_batch_async(
    json_paths: List[pathlib.Path],
    max_workers: Optional[int] = None
) -> Dict[pathlib.Path, List[Dict[str, Any]]]:
    """Chunk multiple documents in parallel with token-based chunking."""
    from .acceleration_utils import hardware
    
    results = {}
    
    # üéØ Show token-based chunking status
    if TIKTOKEN_AVAILABLE:
        log.info(f"üéØ TOKEN-BASED CHUNKING ENABLED: Using {WINDOW_TOKENS} token windows with {OVERLAP_TOKENS} token overlap")
    else:
        log.warning(f"‚ö†Ô∏è  TIKTOKEN NOT AVAILABLE: Falling back to word-based chunking (install: pip install tiktoken)")
    
    # CPU-bound task - use process pool
    with hardware.get_process_pool(max_workers) as executor:
        future_to_path = {
            executor.submit(chunk, path): path 
            for path in json_paths
        }
        
        from tqdm import tqdm
        from concurrent.futures import as_completed
        
        for future in tqdm(as_completed(future_to_path), 
                          total=len(json_paths), 
                          desc="Chunking documents"):
            path = future_to_path[future]
            try:
                chunks = future.result()
                results[path] = chunks
            except Exception as exc:
                log.error(f"Failed to chunk {path.name}: {exc}")
                results[path] = []
    
    # üéØ Summary of token-based chunking results
    total_chunks = sum(len(chunks) for chunks in results.values())
    total_tokens = sum(chunk.get('token_count', count_tiktoken_accurate(chunk['text'])) 
                      for chunks in results.values() 
                      for chunk in chunks)
    log.info(f"üéØ BATCH CHUNKING COMPLETE: {total_chunks} chunks created ({total_tokens} total tokens)")
    
    return results

# Optimized chunking with better memory management
def chunk_optimized(json_path: pathlib.Path) -> List[Dict[str, Any]]:
    """Token-based optimized chunking with validation."""
    root = json_path.with_name(json_path.stem + "_chunks.json")
    if root.exists():
        existing_chunks = json.loads(root.read_text())
        # Validate existing chunks if they don't have tiktoken validation yet
        if existing_chunks and "sub_chunk_index" not in str(existing_chunks):
            log.info(f"üéØ Re-validating existing chunks in {root.name}")
            validated_chunks = validate_and_fix_chunks(existing_chunks)
            if len(validated_chunks) != len(existing_chunks):
                # Chunks were modified, save the validated version
                with open(root, 'w') as f:
                    json.dump(validated_chunks, f, indent=2, ensure_ascii=False)
                log.info(f"üéØ Updated {root.name} with validated chunks")
            return validated_chunks
        return existing_chunks
    
    # Stream large JSON files
    with open(json_path, 'r') as f:
        data = json.load(f)
    
    if "sections" not in data:
        raise RuntimeError(f"{json_path} has no 'sections' key")
    
    # Process in smaller batches to reduce memory usage
    sections = data["sections"]
    batch_size = 100  # Process 100 sections at a time
    
    if TIKTOKEN_AVAILABLE:
        # üéØ TOKEN-BASED CHUNKING - Primary approach
        log.info(f"üéØ Using token-based chunking with {WINDOW_TOKENS} token windows")
        
        all_tokens = []
        all_token_to_page = []
        
        for i in range(0, len(sections), batch_size):
            batch = sections[i:i + batch_size]
            tokens, page_map = concat_tokens_by_encoding(batch)
            all_tokens.extend(tokens)
            all_token_to_page.extend(page_map)
        
        chunks = sliding_chunks_by_tokens(
            all_tokens, 
            all_token_to_page, 
            window_tokens=WINDOW_TOKENS, 
            overlap_tokens=OVERLAP_TOKENS
        )
        
        # Token-based chunks are already guaranteed to be within limits
        log.info(f"üéØ Token-based chunking produced {len(chunks)} chunks")
        
        # Additional validation for safety
        max_tokens = max((chunk.get('token_count', count_tiktoken_accurate(chunk['text'])) for chunk in chunks), default=0)
        if max_tokens > MAX_CHUNK_TOKENS:
            log.warning(f"üéØ Unexpected: Token-based chunk exceeds limit ({max_tokens} > {MAX_CHUNK_TOKENS}), applying fallback validation")
            chunks = validate_and_fix_chunks(chunks)
    else:
        # üéØ FALLBACK: Word-based chunking when tiktoken unavailable
        log.warning(f"üéØ Falling back to word-based chunking (tiktoken unavailable)")
        
        all_tokens = []
        all_page_map = []
        
        for i in range(0, len(sections), batch_size):
            batch = sections[i:i + batch_size]
            toks, pmap = concat_tokens(batch)
            all_tokens.extend(toks)
            all_page_map.extend(pmap)
        
        chunks = sliding_chunks(all_tokens, all_page_map, window=WINDOW, overlap=OVERLAP)
        
        # Apply validation for word-based chunks
        if chunks:
            log.info(f"üéØ Applying validation to {len(chunks)} word-based chunks")
            chunks = validate_and_fix_chunks(chunks)
    
    # Write chunks
    if chunks:
        with open(root, 'w') as f:
            json.dump(chunks, f, indent=2, ensure_ascii=False)
        log.info("‚úì %s chunks ‚Üí %s", len(chunks), root.name)
        
        return chunks
    else:
        log.warning("%s ‚Äì no chunks produced; file skipped", json_path.name)
        return []

# Keep original interface
def chunk(json_path: pathlib.Path) -> List[Dict[str, Any]]:
    """Original interface maintained for compatibility."""
    return chunk_optimized(json_path)

if __name__ == "__main__":
    import argparse, logging
    logging.basicConfig(level=logging.INFO, format="%(message)s")
    p=argparse.ArgumentParser(); p.add_argument("json",type=pathlib.Path)
    chunk(p.parse_args().json)


================================================================================


################################################################################
# File: scripts/pipeline_modular_optimized.py
################################################################################

# File: scripts/pipeline_modular_optimized.py

#!/usr/bin/env python3
"""
Optimized pipeline orchestrator with full parallelization.
Maintains compatibility with original pipeline_modular.py interface.
"""
from __future__ import annotations
import argparse, logging, pathlib, random
from collections import Counter
from rich.console import Console
import asyncio
from typing import List, Optional
import multiprocessing as mp

# Import stages
from stages import extract_clean, llm_enrich, chunk_text, db_upsert, embed_vectors
from stages.acceleration_utils import hardware

# Keep existing toggles
RUN_EXTRACT    = True
RUN_LLM_ENRICH = False
RUN_CHUNK      = False
RUN_DB         = False
RUN_EMBED      = False

log = logging.getLogger("pipeline-modular-optimized")

class OptimizedPipeline:
    """Optimized pipeline with parallel processing and rate limiting."""
    
    def __init__(self, batch_size: int = 15, max_api_concurrent: int = 3):  # üõ°Ô∏è Rate limited: was 50, 20
        self.batch_size = batch_size
        self.max_api_concurrent = max_api_concurrent
        self.stats = Counter()
        
        # üõ°Ô∏è Log rate limiting settings
        log.info("üõ°Ô∏è  Rate-limited pipeline initialized:")
        log.info(f"   Batch size: {batch_size} (was 50)")
        log.info(f"   Max concurrent: {max_api_concurrent} (was 20)")
        log.info("   Target: <800K tokens/minute (safe margin)")
    
    async def process_batch(self, pdfs: List[pathlib.Path], start_doc_num: int = 1, total_docs: int = None) -> None:
        """Process a batch of PDFs through all stages with individual document progress tracking."""
        
        if total_docs is None:
            total_docs = len(pdfs)
        
        batch_size = len(pdfs)
        log.info(f"üìä BATCH PROCESSING START:")
        log.info(f"   üìÑ Documents in this batch: {batch_size}")
        log.info(f"   üéØ Document range: {start_doc_num} to {start_doc_num + batch_size - 1}")
        log.info(f"   üìà Overall progress: {start_doc_num-1}/{total_docs} completed ({((start_doc_num-1)/total_docs*100):.1f}%)")
        
        # Stage 1-2: Extract & Clean (CPU-bound, use process pool)
        json_docs = []
        if RUN_EXTRACT:
            log.info(f"üîÑ EXTRACTION STAGE - Processing {len(pdfs)} PDFs...")
            for i, pdf in enumerate(pdfs):
                doc_num = start_doc_num + i
                doc_progress = ((doc_num-1) / total_docs * 100)
                log.info(f"üìÑ [{doc_num}/{total_docs}] ({doc_progress:.1f}%) Extracting: {pdf.name}")
            
            json_docs = await extract_clean.extract_batch_async(
                pdfs, 
                enrich_llm=False  # We'll do LLM enrichment separately
            )
            
            log.info(f"üìä EXTRACTION STAGE COMPLETE:")
            for i, pdf in enumerate(pdfs):
                doc_num = start_doc_num + i
                log.info(f"   ‚úÖ [{doc_num}/{total_docs}] Extracted: {pdf.name}")
        else:
            # Use existing JSON files
            json_docs = [extract_clean.json_path_for(pdf) for pdf in pdfs]
            log.info(f"üìä EXTRACTION STAGE SKIPPED - Using existing JSON files")

        # Stage 4: LLM Enrich (I/O-bound, use async)
        if RUN_LLM_ENRICH and json_docs:
            log.info(f"üîÑ ENRICHMENT STAGE - Processing {len(json_docs)} documents...")
            for i, json_doc in enumerate(json_docs):
                doc_num = start_doc_num + i
                doc_progress = ((doc_num-1) / total_docs * 100)
                doc_name = pathlib.Path(json_doc).stem.replace('_clean', '')
                log.info(f"üìÑ [{doc_num}/{total_docs}] ({doc_progress:.1f}%) Enriching: {doc_name}")
            
            await llm_enrich.enrich_batch_async(
                json_docs,
                max_concurrent=self.max_api_concurrent
            )
            
            log.info(f"üìä ENRICHMENT STAGE COMPLETE:")
            for i, json_doc in enumerate(json_docs):
                doc_num = start_doc_num + i
                doc_name = pathlib.Path(json_doc).stem.replace('_clean', '')
                log.info(f"   ‚úÖ [{doc_num}/{total_docs}] Enriched: {doc_name}")

        # Stage 5: Chunk (CPU-bound, use process pool)
        chunks_map = {}
        if RUN_CHUNK and json_docs:
            log.info(f"üîÑ CHUNKING STAGE - Processing {len(json_docs)} documents...")
            for i, json_doc in enumerate(json_docs):
                doc_num = start_doc_num + i
                doc_progress = ((doc_num-1) / total_docs * 100)
                doc_name = pathlib.Path(json_doc).stem.replace('_clean', '')
                log.info(f"üìÑ [{doc_num}/{total_docs}] ({doc_progress:.1f}%) Chunking: {doc_name}")
            
            chunks_map = await chunk_text.chunk_batch_async(json_docs)
            
            log.info(f"üìä CHUNKING STAGE COMPLETE:")
            total_chunks_created = 0
            for i, json_doc in enumerate(json_docs):
                doc_num = start_doc_num + i
                doc_name = pathlib.Path(json_doc).stem.replace('_clean', '')
                chunk_count = len(chunks_map.get(json_doc, []))
                total_chunks_created += chunk_count
                log.info(f"   ‚úÖ [{doc_num}/{total_docs}] Chunked: {doc_name} ({chunk_count} chunks)")
            log.info(f"   üìä Total chunks created in this batch: {total_chunks_created}")

        # Stage 6: DB Upsert (I/O-bound, use async)
        if RUN_DB and chunks_map:
            log.info(f"üîÑ DATABASE UPSERT STAGE - Processing {len(chunks_map)} documents...")
            
            # Show progress before upserting
            total_chunks_to_upsert = sum(len(chunks) for chunks in chunks_map.values())
            log.info(f"üìä DATABASE UPSERT PROGRESS:")
            log.info(f"   üíæ Total chunks to upsert: {total_chunks_to_upsert}")
            
            for i, (json_path, chunks) in enumerate(chunks_map.items()):
                doc_num = start_doc_num + i
                doc_progress = ((doc_num-1) / total_docs * 100)
                doc_name = pathlib.Path(json_path).stem.replace('_clean', '')
                log.info(f"üìÑ [{doc_num}/{total_docs}] ({doc_progress:.1f}%) Upserting: {doc_name} ({len(chunks)} chunks)")
            
            documents = [
                {
                    "json_path": json_path,
                    "chunks": chunks,
                    "do_embed": False  # We'll embed in batch later
                }
                for json_path, chunks in chunks_map.items()
            ]
            await db_upsert.upsert_batch_async(documents)
            
            log.info(f"üìä DATABASE UPSERT STAGE COMPLETE:")
            total_chunks_upserted = 0
            for i, (json_path, chunks) in enumerate(chunks_map.items()):
                doc_num = start_doc_num + i
                doc_name = pathlib.Path(json_path).stem.replace('_clean', '')
                total_chunks_upserted += len(chunks)
                log.info(f"   ‚úÖ [{doc_num}/{total_docs}] Upserted: {doc_name}")
            log.info(f"   üìä Total chunks upserted: {total_chunks_upserted}")

        # Update stats
        self.stats["ok"] += len([c for c in chunks_map.values() if c])
        self.stats["fail"] += len([c for c in chunks_map.values() if not c])
        
        # Final batch summary
        end_doc_num = start_doc_num + batch_size - 1
        overall_progress = (end_doc_num / total_docs * 100)
        log.info(f"üìä BATCH COMPLETE:")
        log.info(f"   ‚úÖ Documents processed: {start_doc_num}-{end_doc_num}")
        log.info(f"   üìà Overall progress: {end_doc_num}/{total_docs} ({overall_progress:.1f}%)")
        log.info(f"   üìä Successful documents: {self.stats['ok']}")
        log.info(f"   ‚ùå Failed documents: {self.stats['fail']}")
    
    async def run(self, src: pathlib.Path, selection: str = "sequential", cap: int = 0):
        """Run the optimized pipeline."""
        Console().rule("[bold cyan]Misophonia PDF ‚Üí Vector pipeline (optimized)")
        
        # Get PDF list
        pdfs = [src] if src.is_file() else sorted(src.rglob("*.pdf"))
        if cap:
            pdfs = random.sample(pdfs, cap) if selection == "random" else pdfs[:cap]
        
        total_pdfs = len(pdfs)
        log.info(f"Processing {total_pdfs} PDFs in batches of {self.batch_size}")
        
        # Track PDF-level progress
        pdfs_processed = 0
        
        # Process in batches
        for i in range(0, len(pdfs), self.batch_size):
            batch = pdfs[i:i + self.batch_size]
            batch_num = i//self.batch_size + 1
            total_batches = (len(pdfs) + self.batch_size - 1)//self.batch_size
            
            # Calculate the starting document number for this batch
            start_doc_num = pdfs_processed + 1
            
            # Enhanced progress logging with both batch and PDF-level progress
            log.info(f"üìÑ Processing batch {batch_num}/{total_batches} ({len(batch)} PDFs)")
            log.info(f"üìä Overall progress: {pdfs_processed}/{total_pdfs} PDFs completed ({pdfs_processed/total_pdfs*100:.1f}%)")
            log.info(f"üî¢ Document range: {start_doc_num}-{start_doc_num + len(batch) - 1} of {total_pdfs}")
            
            await self.process_batch(batch, start_doc_num, total_pdfs)
            
            # Update PDF progress counter
            pdfs_processed += len(batch)
            
            # Log completion of this batch
            log.info(f"‚úÖ Batch {batch_num} complete - Total PDFs processed: {pdfs_processed}/{total_pdfs} ({pdfs_processed/total_pdfs*100:.1f}%)")
        
        # Final progress summary
        log.info(f"üéâ All PDFs processed: {pdfs_processed}/{total_pdfs} ({pdfs_processed/total_pdfs*100:.1f}%)")
        
        # Stage 7: Batch embed all at once with conservative settings
        if RUN_EMBED:
            log.info("üîÑ STARTING EMBEDDING STAGE...")
            log.info("üìä EMBEDDING STAGE PROGRESS:")
            log.info("   üéØ Running batch embedding with rate limiting...")
            log.info("   üéØ This stage will process all upserted chunks for embedding")
            
            # üõ°Ô∏è Pass conservative embedding parameters (consistent with embed_vectors.py defaults)
            await embed_vectors.main_async(
                batch_size=200,     # üõ°Ô∏è Rate limited: fetch 200 chunks at once (conservative default)
                commit_size=10,     # üõ°Ô∏è Rate limited: 10 chunks per API call (conservative default) 
                max_concurrent=3    # üõ°Ô∏è Rate limited: max 3 concurrent embedding calls (conservative default)
            )
            
            log.info("üìä EMBEDDING STAGE COMPLETE ‚úÖ")
        
        Console().rule("[green]Finished")
        log.info("üìä PIPELINE COMPLETE - FINAL SUMMARY:")
        log.info("üõ°Ô∏è  Rate limiting successful - no API limits hit")
        log.info(f"üìä Total documents processed: {self.stats['ok']}")
        log.info(f"üìä Total documents failed: {self.stats['fail']}")
        log.info(f"üìä Success rate: {(self.stats['ok']/(self.stats['ok']+self.stats['fail'])*100):.1f}%" if (self.stats['ok']+self.stats['fail']) > 0 else "100%")

def main(src: pathlib.Path, selection: str = "sequential", cap: int = 0) -> None:
    """Main entry point compatible with original pipeline_modular.py"""
    # Set multiprocessing start method for macOS
    mp.set_start_method('spawn', force=True)
    
    # Create and run pipeline with conservative rate limiting
    pipeline = OptimizedPipeline(
        batch_size=15,  # üõ°Ô∏è Rate limited: process 15 PDFs at a time (was 50)
        max_api_concurrent=3  # üõ°Ô∏è Rate limited: max 3 concurrent API requests (was 20)
    )
    
    # Run async pipeline
    asyncio.run(pipeline.run(src, selection, cap))

if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s ‚Äî %(levelname)s ‚Äî %(message)s"
    )
    
    p = argparse.ArgumentParser()
    p.add_argument("src", type=pathlib.Path,
                   default=pathlib.Path("documents/research/Global"), nargs="?")
    p.add_argument("--selection", choices=["sequential", "random"],
                   default="sequential")
    p.add_argument("--cap", type=int, default=0)
    p.add_argument("--batch-size", type=int, default=15,  # üõ°Ô∏è Rate limited: default to 15 (was 50)
                   help="Number of PDFs to process in parallel")
    p.add_argument("--api-concurrent", type=int, default=3,  # üõ°Ô∏è Rate limited: default to 3 (was 20)
                   help="Max concurrent API calls")
    args = p.parse_args()
    
    # Override batch size if specified
    if args.batch_size:
        pipeline = OptimizedPipeline(
            batch_size=args.batch_size,
            max_api_concurrent=args.api_concurrent
        )
        asyncio.run(pipeline.run(args.src, args.selection, args.cap))
    else:
        main(args.src, args.selection, args.cap)


================================================================================


################################################################################
# File: scripts/clear_database.py
################################################################################

# File: scripts/clear_database.py

#!/usr/bin/env python3
"""
Database Clear Utility
======================

Safely clears Supabase database tables for the Misophonia Research system.
This script will delete all data from:
- research_documents table
- research_chunks table

‚ö†Ô∏è  WARNING: This operation is irreversible!
"""
from __future__ import annotations
import os
import sys
import logging
from typing import Optional

from dotenv import load_dotenv
from supabase import create_client

# Load environment variables
load_dotenv()

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")

if not SUPABASE_URL or not SUPABASE_KEY:
    sys.exit("‚ùå Missing SUPABASE_URL or SUPABASE_SERVICE_ROLE_KEY environment variables")

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s ‚Äî %(levelname)s ‚Äî %(message)s"
)
log = logging.getLogger(__name__)

def init_supabase():
    """Initialize Supabase client."""
    return create_client(SUPABASE_URL, SUPABASE_KEY)

def get_table_counts(sb) -> dict:
    """Get current row counts for all tables."""
    counts = {}
    
    try:
        # Count documents
        doc_res = sb.table("research_documents").select("id", count="exact").execute()
        counts["research_documents"] = doc_res.count or 0
        
        # Count chunks
        chunk_res = sb.table("research_chunks").select("id", count="exact").execute()
        counts["research_chunks"] = chunk_res.count or 0
        
        # Count chunks with embeddings
        embedded_res = sb.table("research_chunks").select("id", count="exact").not_.is_("embedding", "null").execute()
        counts["chunks_with_embeddings"] = embedded_res.count or 0
        
    except Exception as e:
        log.error(f"Error getting table counts: {e}")
        return {}
    
    return counts

def confirm_deletion() -> bool:
    """Ask user for confirmation before deletion."""
    print("\n" + "="*60)
    print("‚ö†Ô∏è  DATABASE CLEAR WARNING")
    print("="*60)
    print("This will permanently delete ALL data from:")
    print("  ‚Ä¢ research_documents table")
    print("  ‚Ä¢ research_chunks table")
    print("  ‚Ä¢ All embeddings and metadata")
    print("\n‚ùå This operation CANNOT be undone!")
    print("="*60)
    
    response = input("\nType 'DELETE ALL DATA' to confirm (or anything else to cancel): ")
    return response.strip() == "DELETE ALL DATA"

def clear_table_batch(sb, table_name: str, batch_size: int = 1000) -> int:
    """Clear all rows from a specific table in batches to avoid timeouts."""
    log.info(f"Clearing table: {table_name} (batch size: {batch_size})")
    
    total_deleted = 0
    
    while True:
        try:
            # Get a batch of IDs to delete
            result = sb.table(table_name).select("id").limit(batch_size).execute()
            
            if not result.data or len(result.data) == 0:
                break
            
            ids_to_delete = [row["id"] for row in result.data]
            log.info(f"Deleting batch of {len(ids_to_delete)} rows from {table_name}")
            
            # Delete this batch
            delete_result = sb.table(table_name).delete().in_("id", ids_to_delete).execute()
            
            if hasattr(delete_result, 'error') and delete_result.error:
                log.error(f"Error deleting batch from {table_name}: {delete_result.error}")
                break
            
            batch_deleted = len(delete_result.data) if delete_result.data else 0
            total_deleted += batch_deleted
            log.info(f"‚úÖ Deleted {batch_deleted} rows from {table_name} (total: {total_deleted})")
            
            # If we deleted fewer than the batch size, we're done
            if batch_deleted < batch_size:
                break
                
        except Exception as e:
            log.error(f"Exception deleting batch from {table_name}: {e}")
            break
    
    log.info(f"‚úÖ Total deleted from {table_name}: {total_deleted}")
    return total_deleted

def clear_table(sb, table_name: str) -> int:
    """Clear all rows from a specific table."""
    return clear_table_batch(sb, table_name, batch_size=500)

def main():
    """Main function to clear the database."""
    print("üóëÔ∏è  Misophonia Database Clear Utility")
    print("=" * 50)
    
    # Initialize Supabase
    sb = init_supabase()
    
    # Get current counts
    print("\nüìä Current database status:")
    counts = get_table_counts(sb)
    
    if not counts:
        print("‚ùå Could not retrieve database counts. Exiting.")
        return
    
    print(f"  ‚Ä¢ Documents: {counts['research_documents']:,}")
    print(f"  ‚Ä¢ Chunks: {counts['research_chunks']:,}")
    print(f"  ‚Ä¢ Chunks with embeddings: {counts['chunks_with_embeddings']:,}")
    
    if counts['research_documents'] == 0 and counts['research_chunks'] == 0:
        print("\n‚úÖ Database is already empty!")
        return
    
    # Get confirmation
    if not confirm_deletion():
        print("\n‚úÖ Operation cancelled. Database unchanged.")
        return
    
    print("\nüóëÔ∏è  Starting database clear operation...")
    
    # Clear chunks first (has foreign key to documents)
    chunks_deleted = clear_table(sb, "research_chunks")
    
    # Clear documents
    docs_deleted = clear_table(sb, "research_documents")
    
    # Verify deletion
    print("\nüìä Verifying deletion...")
    final_counts = get_table_counts(sb)
    
    if final_counts:
        print(f"  ‚Ä¢ Documents remaining: {final_counts['research_documents']:,}")
        print(f"  ‚Ä¢ Chunks remaining: {final_counts['research_chunks']:,}")
        
        if final_counts['research_documents'] == 0 and final_counts['research_chunks'] == 0:
            print("\n‚úÖ Database successfully cleared!")
            print(f"  ‚Ä¢ Deleted {docs_deleted:,} documents")
            print(f"  ‚Ä¢ Deleted {chunks_deleted:,} chunks")
        else:
            print("\n‚ö†Ô∏è  Some data may remain in the database.")
    else:
        print("‚ùå Could not verify deletion status.")

if __name__ == "__main__":
    main()


================================================================================


################################################################################
# File: scripts/stages/llm_enrich.py
################################################################################

# File: scripts/stages/llm_enrich.py

#!/usr/bin/env python3
"""
Stage 4 ‚Äî LLM metadata enrichment with concurrent API calls.
"""
from __future__ import annotations
import json, logging, pathlib, re, os
from textwrap import dedent
from typing import Any, Dict, List
import asyncio
from concurrent.futures import ThreadPoolExecutor
import aiofiles

# ‚îÄ‚îÄ‚îÄ minimal shared helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _authors(val:Any)->List[str]:
    if val is None: return []
    if isinstance(val,list): return [str(a).strip() for a in val if a]
    return re.split(r"\s*,\s*|\s+and\s+", str(val).strip())

META_FIELDS_CORE = [
    "doc_type","title","authors","year","journal","doi","abstract",
    "keywords","research_topics",
]
EXTRA_MD_FIELDS = ["peer_reviewed","open_access","license","open_access_status"]
META_FIELDS     = META_FIELDS_CORE+EXTRA_MD_FIELDS
_DEF_META_TEMPLATE = {**{k:None for k in META_FIELDS_CORE},
                      "doc_type":"scientific paper",
                      "authors":[], "keywords":[], "research_topics":[],
                      "peer_reviewed":None,"open_access":None,
                      "license":None,"open_access_status":None}

def merge_meta(*sources:Dict[str,Any])->Dict[str,Any]:
    merged=_DEF_META_TEMPLATE.copy()
    for src in sources:
        for k in META_FIELDS:
            v=src.get(k)
            if v not in (None,"",[],{}): merged[k]=v
    merged["authors"]=_authors(merged["authors"])
    merged["keywords"]=merged["keywords"] or []
    merged["research_topics"]=merged["research_topics"] or []
    return merged
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

from dotenv import load_dotenv
from openai import OpenAI
load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
MODEL          = "gpt-4.1-mini-2025-04-14"
log            = logging.getLogger(__name__)

def _first_words(txt:str,n:int=3000)->str: return " ".join(txt.split()[:n])

def _gpt(text:str)->Dict[str,Any]:
    if not OPENAI_API_KEY: return {}
    cli=OpenAI(api_key=OPENAI_API_KEY)
    prompt=dedent(f"""
        Extract all metadata fields ({', '.join(META_FIELDS)}) from the paper
        below and return ONE JSON object.  Text:  {text}
    """)
    rsp=cli.chat.completions.create(model=MODEL,temperature=0,
            messages=[{"role":"system","content":"metadata extractor"},
                      {"role":"user","content":prompt}])
    raw=rsp.choices[0].message.content
    m=re.search(r"{[\s\S]*}",raw)
    return json.loads(m.group(0) if m else "{}")

# Async version of GPT call
async def _gpt_async(text: str, semaphore: asyncio.Semaphore) -> Dict[str, Any]:
    """Async GPT call with rate limiting."""
    if not OPENAI_API_KEY:
        return {}
    
    async with semaphore:  # Rate limiting
        loop = asyncio.get_event_loop()
        # Run synchronous OpenAI call in thread pool
        return await loop.run_in_executor(None, _gpt, text)

async def enrich_async(json_path: pathlib.Path, semaphore: asyncio.Semaphore) -> None:
    """Async version of enrich."""
    # Read file asynchronously
    async with aiofiles.open(json_path, 'r') as f:
        content = await f.read()
        data = json.loads(content)
    
    # Reconstruct body text
    full = " ".join(
        el.get("text", "") for sec in data["sections"] 
        for el in sec.get("elements", [])
    )
    
    # Make async GPT call
    new_meta = await _gpt_async(_first_words(full), semaphore)
    
    # Merge metadata
    data.update(new_meta)
    
    # Write back asynchronously
    async with aiofiles.open(json_path, 'w') as f:
        await f.write(json.dumps(data, indent=2, ensure_ascii=False))
    
    log.info("‚úì metadata enriched ‚Üí %s", json_path.name)

async def enrich_batch_async(
    json_paths: List[pathlib.Path],
    max_concurrent: int = 10
) -> None:
    """Enrich multiple documents concurrently with rate limiting."""
    semaphore = asyncio.Semaphore(max_concurrent)
    
    tasks = [enrich_async(path, semaphore) for path in json_paths]
    
    from tqdm.asyncio import tqdm_asyncio
    await tqdm_asyncio.gather(*tasks, desc="Enriching metadata")

# Keep original interface for compatibility
def enrich(json_path: pathlib.Path) -> None:
    """Original synchronous interface."""
    data = json.loads(json_path.read_text())
    full = " ".join(
        el.get("text", "") for sec in data["sections"] 
        for el in sec.get("elements", [])
    )
    new_meta = _gpt(_first_words(full))
    data.update(new_meta)
    json_path.write_text(json.dumps(data, indent=2, ensure_ascii=False), 'utf-8')
    log.info("‚úì metadata enriched ‚Üí %s", json_path.name)

if __name__ == "__main__":
    import argparse, logging
    logging.basicConfig(level=logging.INFO, format="%(message)s")
    p=argparse.ArgumentParser(); p.add_argument("json",type=pathlib.Path)
    enrich(p.parse_args().json)


================================================================================


################################################################################
# File: requirements.txt
################################################################################

################################################################################
################################################################################
# Python dependencies for Misophonia Research RAG System

# Core dependencies
openai>=1.0.0
numpy>=1.24.0
python-dotenv>=1.0.0
flask>=2.0.0
requests>=2.28.0

# PDF processing
PyPDF2>=3.0.0
unstructured>=0.10.0
pdfminer.six>=20221105

# Utilities
tqdm>=4.65.0
tabulate>=0.9.0
colorama>=0.4.6
argparse>=1.4.0

# For concurrent processing
concurrent-log-handler>=0.9.20

# Async dependencies for optimized pipeline
aiofiles>=23.0.0
aiohttp>=3.8.0

# Supabase integration
supabase~=2.0.0

# Token counting for OpenAI rate limiting
tiktoken>=0.5.0


================================================================================

