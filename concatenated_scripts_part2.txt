# Concatenated Project Code - Part 2 of 3
# Generated: 2025-06-13 17:03:10
# Root Directory: /Users/gianmariatroiani/Documents/knologi/graph_database
================================================================================

# File Index - Which Files Are in Which Parts
################################################################################

## Part 1 (12 files):
  - scripts/graph_rag_stages/phase3_querying/city_clerk_query_engine.py
  - scripts/graph_rag_stages/phase2_building/custom_graph_builder.py
  - scripts/graph_rag_stages/phase2_building/graphrag_adapter.py
  - scripts/extract_all_to_markdown.py
  - scripts/graph_rag_stages/phase3_querying/query_engine.py
  - scripts/graph_rag_stages/common/config.py
  - scripts/graph_rag_stages/phase3_querying/query_router.py
  - scripts/graph_rag_stages/phase3_querying/source_tracker.py
  - scripts/graph_rag_stages/main_pipeline.py
  - settings.yaml
  - scripts/graph_rag_stages/common/__init__.py
  - ui/__init__.py

## Part 2 (13 files):
  - ui/query_app.py
  - scripts/RAG_stages/chunk_text.py
  - scripts/graph_rag_stages/phase3_querying/structural_query_enhancer.py
  - scripts/graph_rag_stages/phase1_preprocessing/agenda_extractor.py
  - scripts/graph_rag_stages/common/cosmos_client.py
  - scripts/graph_rag_stages/phase2_building/entity_deduplicator.py
  - scripts/graph_rag_stages/phase2_building/graphrag_indexer.py
  - scripts/RAG_stages/llm_enrich.py
  - scripts/graph_rag_stages/phase2_building/__init__.py
  - scripts/graph_rag_stages/phase1_preprocessing/pdf_extractor.py
  - scripts/graph_rag_stages/phase3_querying/__init__.py
  - config.py
  - scripts/graph_rag_stages/__init__.py

## Part 3 (13 files):
  - scripts/RAG_stages/embed_vectors.py
  - scripts/RAG_stages/extract_clean.py
  - scripts/graph_rag_stages/phase3_querying/smart_query_router.py
  - scripts/graph_rag_stages/phase1_preprocessing/transcript_linker.py
  - scripts/graph_rag_stages/phase1_preprocessing/document_linker.py
  - scripts/RAG_stages/db_upsert.py
  - scripts/graph_rag_stages/common/utils.py
  - scripts/graph_rag_stages/README.md
  - scripts/RAG_stages/acceleration_utils.py
  - scripts/graph_rag_stages/phase3_querying/response_enhancer.py
  - scripts/graph_rag_stages/phase1_preprocessing/__init__.py
  - requirements.txt
  - scripts/RAG_stages/__init__.py


================================================================================


################################################################################
# File: ui/query_app.py
################################################################################

# File: ui/query_app.py

#!/usr/bin/env python3
"""
GraphRAG Query UI
A web interface for querying the City Clerk GraphRAG knowledge base.
"""

import dash
from dash import dcc, html, Input, Output, State, ctx
import dash_bootstrap_components as dbc
from dash.exceptions import PreventUpdate
import asyncio
from pathlib import Path
import sys
import json
from datetime import datetime
import logging
from typing import Dict, Any


# Add project root to path
# Handle both cases: script in root or in a subdirectory
current_file = Path(__file__).resolve()
if current_file.parent.name == "scripts" or current_file.parent.name == "ui":
    project_root = current_file.parent.parent
else:
    project_root = current_file.parent
sys.path.append(str(project_root))

from scripts.graph_rag_stages.phase3_querying import (
    CityClerkQueryEngine,
    SmartQueryRouter,
    QueryIntent
)

# Set up logging
logging.basicConfig(level=logging.INFO)
log = logging.getLogger(__name__)

# Initialize the app with a nice theme
app = dash.Dash(
    __name__, 
    external_stylesheets=[dbc.themes.BOOTSTRAP],
    title="GraphRAG City Clerk Query System"
)

# Initialize query engine and router
GRAPHRAG_ROOT = project_root / "graphrag_data"
query_engine = None
query_router = SmartQueryRouter()

# Store query history
query_history = []

# Define the layout
app.layout = dbc.Container([
    dbc.Row([
        dbc.Col([
            html.H1("üèõÔ∏è City Clerk GraphRAG Query System", className="text-center mb-4"),
            html.Hr(),
        ])
    ]),
    
    # Query Input Section
    dbc.Row([
        dbc.Col([
            dbc.Card([
                dbc.CardHeader(html.H4("üîç Enter Your Query")),
                dbc.CardBody([
                    dbc.Textarea(
                        id="query-input",
                        placeholder="Ask about agenda items, ordinances, resolutions, or city proceedings...\n\nExamples:\n- What is agenda item E-1?\n- Tell me about ordinance 2024-01\n- What are the main development themes?\n- How has zoning policy evolved?",
                        style={"height": "150px"},
                        className="mb-3"
                    ),
                    
                    dbc.Row([
                        dbc.Col([
                            html.Label("Query Method:", className="fw-bold"),
                            dbc.RadioItems(
                                id="query-method",
                                options=[
                                    {"label": "ü§ñ Auto-Select (Recommended)", "value": "auto"},
                                    {"label": "üéØ Local Search", "value": "local"},
                                    {"label": "üåê Global Search", "value": "global"},
                                    {"label": "üîÑ DRIFT Search", "value": "drift"}
                                ],
                                value="auto",
                                inline=False
                            ),
                        ], md=6),
                        
                        dbc.Col([
                            html.Label("Query Options:", className="fw-bold"),
                            dbc.Checklist(
                                id="query-options",
                                options=[
                                    {"label": "Include community context", "value": "community"},
                                    {"label": "Show routing details", "value": "routing"},
                                    {"label": "Show data sources", "value": "sources"},
                                    {"label": "Verbose results", "value": "verbose"}
                                ],
                                value=["community", "routing", "sources"],
                                inline=False
                            ),
                        ], md=6),
                    ]),
                    
                    dbc.Row([
                        dbc.Col([
                            dbc.Button(
                                "üöÄ Submit Query",
                                id="submit-query",
                                color="primary",
                                size="lg",
                                className="w-100 mt-3",
                                n_clicks=0
                            ),
                        ], md=6),
                        dbc.Col([
                            dbc.Button(
                                "üßπ Clear",
                                id="clear-all",
                                color="secondary",
                                size="lg",
                                className="w-100 mt-3",
                                n_clicks=0
                            ),
                        ], md=6),
                    ]),
                ])
            ], className="mb-4"),
        ])
    ]),
    
    # Loading indicator
    dcc.Loading(
        id="loading",
        type="default",
        children=[
            html.Div(id="loading-output")
        ]
    ),
    
    # Routing Information
    dbc.Row([
        dbc.Col([
            dbc.Collapse(
                dbc.Card([
                    dbc.CardHeader(html.H5("üéØ Query Routing Analysis")),
                    dbc.CardBody(id="routing-info")
                ], className="mb-4"),
                id="routing-collapse",
                is_open=False
            ),
        ])
    ]),
    
    # Results Section
    dbc.Row([
        dbc.Col([
            dbc.Card([
                dbc.CardHeader(html.H4("üìä Query Results")),
                dbc.CardBody(id="query-results", style={"min-height": "300px"})
            ], className="mb-4"),
        ])
    ]),
    
    # Query History
    dbc.Row([
        dbc.Col([
            dbc.Card([
                dbc.CardHeader([
                    html.H5("üìú Query History", className="d-inline"),
                    dbc.Button(
                        "Clear History",
                        id="clear-history",
                        color="danger",
                        size="sm",
                        className="float-end",
                        n_clicks=0
                    )
                ]),
                dbc.CardBody(id="query-history")
            ]),
        ])
    ]),
    
    # Hidden div for storing state
    html.Div(id="query-state", style={"display": "none"})
    
], fluid=True, className="p-4")

def create_data_sources_display(data_sources):
    """Create a formatted display of data sources used in the query."""
    
    # Handle empty or None data_sources
    if not data_sources:
        return html.Div([
            html.Hr(style={'margin': '20px 0', 'border-top': '1px solid #e0e0e0'}),
            html.Div([
                html.H4("üìä Data Sources", style={'margin-bottom': '10px', 'color': '#333'}),
                html.P("No data sources tracked for this query.", style={
                    'background-color': '#f5f5f5',
                    'padding': '10px',
                    'border-radius': '5px',
                    'color': '#666'
                })
            ])
        ])
    
    # Get lists of items
    entities = data_sources.get('entities', [])
    relationships = data_sources.get('relationships', [])
    sources = data_sources.get('sources', [])
    text_units = data_sources.get('text_units', [])
    
    # Create summary
    summary_parts = []
    
    if entities:
        entity_ids = [str(e.get('id', 'Unknown')) for e in entities[:10]]
        ids_str = ', '.join(entity_ids)
        if len(entities) > 10:
            ids_str += f", ... +{len(entities) - 10} more"
        summary_parts.append(f"Entities ({ids_str})")
    
    if relationships:
        rel_ids = [str(r.get('id', 'Unknown')) for r in relationships[:10]]
        ids_str = ', '.join(rel_ids)
        if len(relationships) > 10:
            ids_str += f", ... +{len(relationships) - 10} more"
        summary_parts.append(f"Relationships ({ids_str})")
    
    if sources:
        source_ids = [str(s.get('id', 'Unknown')) for s in sources[:10]]
        ids_str = ', '.join(source_ids)
        if len(sources) > 10:
            ids_str += f", ... +{len(sources) - 10} more"
        summary_parts.append(f"Sources ({ids_str})")
    
    summary_text = "Data: " + "; ".join(summary_parts) + "." if summary_parts else "Data: No sources tracked."
    
    # Create expandable sections
    details_sections = []
    
    # Entities section with better formatting
    if entities:
        entity_items = []
        for entity in entities[:20]:
            entity_items.append(
                html.Li([
                    html.Div([
                        html.Strong(f"[{entity.get('id', 'Unknown')}] {entity.get('title', 'Unknown')}"),
                        html.Span(f" ({entity.get('type', 'Unknown')})", 
                                style={'color': '#666', 'font-size': '0.9em'})
                    ]),
                    html.P(entity.get('description', 'No description available')[:200] + '...' 
                          if len(entity.get('description', '')) > 200 else entity.get('description', ''),
                          style={'margin': '5px 0 0 20px', 'color': '#555', 'font-size': '0.9em'})
                ], style={'margin-bottom': '10px', 'list-style': 'none'})
            )
        
        if len(entities) > 20:
            entity_items.append(
                html.Li(f"... and {len(entities) - 20} more entities", 
                       style={'font-style': 'italic', 'color': '#666'})
            )
        
        details_sections.append(
            html.Details([
                html.Summary([
                    html.Span("üìä ", style={'font-size': '1.2em'}),
                    f"Entities Used ({len(entities)})"
                ], style={'cursor': 'pointer', 'font-weight': 'bold', 'padding': '10px 0'}),
                html.Ul(entity_items, style={'padding-left': '20px', 'margin-top': '10px'})
            ], style={'margin': '15px 0', 'border-left': '3px solid #4a90e2', 'padding-left': '10px'})
        )
    
    # Relationships section
    if relationships:
        rel_items = []
        for rel in relationships[:15]:
            rel_items.append(
                html.Li([
                    html.Div([
                        html.Strong(f"[{rel.get('id', 'Unknown')}] "),
                        html.Span(f"{rel.get('source', 'Unknown')} ‚Üí {rel.get('target', 'Unknown')}", 
                                style={'color': '#2c5aa0'})
                    ]),
                    html.P([
                        html.Em(rel.get('description', 'No description')[:150] + '...' 
                               if len(rel.get('description', '')) > 150 else rel.get('description', '')),
                        html.Span(f" (weight: {rel.get('weight', 0):.2f})", 
                                style={'color': '#666', 'font-size': '0.9em'})
                    ], style={'margin': '5px 0 0 20px', 'color': '#555', 'font-size': '0.9em'})
                ], style={'margin-bottom': '10px', 'list-style': 'none'})
            )
        
        if len(relationships) > 15:
            rel_items.append(
                html.Li(f"... and {len(relationships) - 15} more relationships", 
                       style={'font-style': 'italic', 'color': '#666'})
            )
        
        details_sections.append(
            html.Details([
                html.Summary([
                    html.Span("üîó ", style={'font-size': '1.2em'}),
                    f"Relationships Used ({len(relationships)})"
                ], style={'cursor': 'pointer', 'font-weight': 'bold', 'padding': '10px 0'}),
                html.Ul(rel_items, style={'padding-left': '20px', 'margin-top': '10px'})
            ], style={'margin': '15px 0', 'border-left': '3px solid #e24a4a', 'padding-left': '10px'})
        )
    
    # Sources section
    if sources:
        source_items = []
        for source in sources[:10]:
            source_items.append(
                html.Li([
                    html.Div([
                        html.Strong(f"[{source.get('id', 'Unknown')}] {source.get('title', 'Unknown')}"),
                        html.Span(f" ({source.get('type', 'document')})", 
                                style={'color': '#666', 'font-size': '0.9em'})
                    ]),
                    html.P(source.get('text_preview', '')[:150] + '...' 
                          if len(source.get('text_preview', '')) > 150 else source.get('text_preview', ''),
                          style={'margin': '5px 0 0 20px', 'color': '#555', 'font-size': '0.9em', 'font-style': 'italic'})
                ], style={'margin-bottom': '10px', 'list-style': 'none'})
            )
        
        if len(sources) > 10:
            source_items.append(
                html.Li(f"... and {len(sources) - 10} more sources", 
                       style={'font-style': 'italic', 'color': '#666'})
            )
        
        details_sections.append(
            html.Details([
                html.Summary([
                    html.Span("üìÑ ", style={'font-size': '1.2em'}),
                    f"Source Documents ({len(sources)})"
                ], style={'cursor': 'pointer', 'font-weight': 'bold', 'padding': '10px 0'}),
                html.Ul(source_items, style={'padding-left': '20px', 'margin-top': '10px'})
            ], style={'margin': '15px 0', 'border-left': '3px solid #4ae255', 'padding-left': '10px'})
        )
    
    # Combine everything
    return html.Div([
        html.Hr(style={'margin': '20px 0', 'border-top': '1px solid #e0e0e0'}),
        html.Div([
            html.H4("üìä Data Sources", style={'margin-bottom': '15px', 'color': '#333'}),
            html.Div(summary_text, style={
                'background-color': '#f5f5f5',
                'padding': '12px',
                'border-radius': '5px',
                'font-family': 'monospace',
                'font-size': '14px',
                'color': '#333',
                'border': '1px solid #ddd'
            }),
            html.Div(details_sections, style={'margin-top': '20px'})
        ], style={
            'background-color': '#fafafa',
            'padding': '20px',
            'border-radius': '8px',
            'border': '1px solid #e0e0e0'
        })
    ])

# Callback for handling queries
@app.callback(
    [Output("query-results", "children"),
     Output("routing-info", "children"),
     Output("routing-collapse", "is_open"),
     Output("query-history", "children"),
     Output("loading-output", "children")],
    [Input("submit-query", "n_clicks"),
     Input("clear-all", "n_clicks"),
     Input("clear-history", "n_clicks")],
    [State("query-input", "value"),
     State("query-method", "value"),
     State("query-options", "value")]
)
def handle_query(submit_clicks, clear_clicks, clear_history_clicks, query_text, method, options):
    global query_history
    
    # Determine which button was clicked
    triggered = ctx.triggered_id
    
    if triggered == "clear-all":
        return "", "", False, render_query_history(), ""
    
    if triggered == "clear-history":
        query_history = []
        return dash.no_update, dash.no_update, dash.no_update, render_query_history(), ""
    
    if triggered != "submit-query" or not query_text:
        raise PreventUpdate
    
    # Initialize query engine if needed
    global query_engine
    if query_engine is None:
        try:
            query_engine = CityClerkQueryEngine(GRAPHRAG_ROOT)
        except Exception as e:
            return render_error(f"Failed to initialize query engine: {e}"), "", False, dash.no_update, ""
    
    # Show loading message
    loading_msg = html.Div([
        html.H5("üîÑ Processing your query..."),
        html.P(f"Query: {query_text[:100]}..."),
        html.P(f"Method: {method}")
    ])
    
    try:
        # Determine method
        if method == "auto":
            # Use router to determine method
            route_info = query_router.determine_query_method(query_text)
            actual_method = route_info['method']
            routing_details = route_info
        else:
            actual_method = method
            routing_details = {"method": method, "params": {}}
        
        # Run query asynchronously
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        # Add options to params
        params = routing_details.get('params', {})
        if "community" not in options:
            params['include_community_context'] = False
        
        # Run the query
        result = loop.run_until_complete(
            query_engine.query(
                query=query_text,
                method=actual_method if method != "auto" else None,
                **params
            )
        )
        
        # Extract data sources
        data_sources = result.get('data_sources', result.get('context_data', {}))
        
        # Format the main answer with proper markdown
        answer_content = dcc.Markdown(
            result.get('answer', 'No response generated.'),
            style={
                'padding': '20px',
                'backgroundColor': '#f8f9fa',
                'borderRadius': '8px',
                'lineHeight': '1.6',
                'whiteSpace': 'pre-wrap'  # Preserve formatting
            }
        )
        
        # Create data sources display if requested
        sources_display = html.Div()
        if "sources" in options:
            sources_display = create_data_sources_display(data_sources)
        
        # Combine results
        results_content = html.Div([
            html.H3("Answer:", style={'marginBottom': '15px'}),
            answer_content,
            sources_display
        ])
        
        # Add to history
        query_history.insert(0, {
            "timestamp": datetime.now(),
            "query": query_text,
            "method": actual_method,
            "auto_routed": method == "auto"
        })
        
        # Limit history to 10 items
        query_history = query_history[:10]
        
        routing_content = render_routing_info(routing_details, actual_method) if "routing" in options else ""
        show_routing = "routing" in options
        
        return results_content, routing_content, show_routing, render_query_history(), ""
        
    except Exception as e:
        log.error(f"Query failed: {e}")
        return render_error(f"Query failed: {str(e)}"), "", False, dash.no_update, ""

def render_results(result, options):
    """Render query results with all source information."""
    
    answer = result.get('answer', 'No answer available')
    sources_info = result.get('sources_info', {})
    entity_chunks = result.get('entity_chunks', [])
    metadata = result.get('routing_metadata', {})
    
    # Clean up the answer (remove metadata lines)
    if isinstance(answer, str):
        lines = answer.split('\n')
        cleaned_lines = [line for line in lines if not line.startswith(('INFO:', 'WARNING:', 'DEBUG:', 'SUCCESS:'))]
        answer = '\n'.join(cleaned_lines).strip()
    
    content = [
        html.H5("üìù Answer:", className="mb-3"),
        dcc.Markdown(answer, className="p-3 bg-light rounded"),
    ]
    
    # Show data sources if requested
    if "sources" in options and sources_info:
        content.extend([
            html.Hr(),
            html.H5("üìä Data Sources:", className="mb-3"),
            render_all_sources(sources_info, entity_chunks)
        ])
    
    # Show verbose metadata if requested
    if "verbose" in options and metadata:
        content.extend([
            html.Hr(),
            html.H6("üîç Query Metadata:"),
            html.Pre(json.dumps(metadata, indent=2), className="bg-dark text-light p-3 rounded")
        ])
    
    return html.Div(content)

def render_all_sources(sources_info, entity_chunks):
    """Render all source information comprehensively."""
    content = []
    
    # Show raw references first
    raw_refs = sources_info.get('raw_references', {})
    if any(raw_refs.values()):
        ref_text = []
        if raw_refs.get('entities'):
            ref_text.append(f"Entities: {', '.join(raw_refs['entities'])}")
        if raw_refs.get('reports'):
            ref_text.append(f"Reports: {', '.join(raw_refs['reports'])}")
        if raw_refs.get('sources'):
            ref_text.append(f"Sources: {', '.join(raw_refs['sources'])}")
        
        content.append(
            dbc.Alert([
                html.Strong("üìã References in Answer: "),
                html.Br(),
                html.Code(' | '.join(ref_text))
            ], color="info", className="mb-3")
        )
    
    # Show resolved reports (for GLOBAL search)
    reports = sources_info.get('reports', [])
    if reports:
        content.append(html.H6("üìë Community Reports Used:", className="mb-2"))
        for report in reports[:10]:  # Limit to first 10
            content.append(
                dbc.Card([
                    dbc.CardBody([
                        html.Strong(f"Report #{report['id']}"),
                        html.Span(f" (Level {report.get('level', '?')})", className="text-muted"),
                        html.P(report.get('summary', 'No summary available'), 
                               className="small text-muted mt-1 mb-0")
                    ])
                ], className="mb-2", color="light", outline=True)
            )
        if len(reports) > 10:
            content.append(html.P(f"... and {len(reports) - 10} more reports", className="text-muted"))
    
    # Show resolved entities (for LOCAL search)
    entities = sources_info.get('entities', [])
    if entities:
        content.append(html.H6("üéØ Entities Referenced:", className="mb-2 mt-3"))
        for entity in entities[:10]:  # Limit to first 10
            content.append(
                dbc.Card([
                    dbc.CardBody([
                        html.Div([
                            html.Span(entity['type'].upper(), 
                                     className=f"badge bg-{get_entity_color(entity['type'])} me-2"),
                            html.Strong(entity['title']),
                            html.Span(f" (#{entity['id']})", className="text-muted small")
                        ]),
                        html.P(entity.get('description', ''), 
                               className="small text-muted mt-1 mb-0")
                    ])
                ], className="mb-2", color="light", outline=True)
            )
        if len(entities) > 10:
            content.append(html.P(f"... and {len(entities) - 10} more entities", className="text-muted"))
    
    # Show entity chunks (the actual retrieved content)
    if entity_chunks:
        content.append(html.H6("üìÑ Retrieved Content Chunks:", className="mb-2 mt-3"))
        for chunk in entity_chunks[:5]:  # Show first 5 chunks
            source_info = chunk.get('source', {})
            content.append(
                dbc.Card([
                    dbc.CardBody([
                        html.Div([
                            html.Span(chunk['type'].upper(), 
                                     className=f"badge bg-{get_entity_color(chunk['type'])} me-2"),
                            html.Strong(chunk['title'])
                        ]),
                        html.P(chunk.get('description', '')[:200] + "..." 
                               if len(chunk.get('description', '')) > 200 
                               else chunk.get('description', ''), 
                               className="small mt-2"),
                        html.Hr(className="my-2"),
                        html.Small([
                            html.Strong("Source: "),
                            f"{source_info.get('type', 'Unknown')} - {source_info.get('meeting_date', 'N/A')}",
                            html.Br(),
                            html.Strong("File: "),
                            html.Code(source_info.get('source_file', 'Unknown'), className="small")
                        ], className="text-muted")
                    ])
                ], className="mb-2")
            )
        if len(entity_chunks) > 5:
            content.append(html.P(f"... and {len(entity_chunks) - 5} more chunks", className="text-muted"))
    
    return html.Div(content)

def get_entity_color(entity_type):
    """Get color for entity type badge."""
    color_map = {
        'AGENDA_ITEM': 'primary',
        'ORDINANCE': 'success', 
        'RESOLUTION': 'warning',
        'PERSON': 'info',
        'ORGANIZATION': 'secondary',
        'MEETING': 'danger',
        'DOCUMENT': 'dark'
    }
    return color_map.get(entity_type.upper(), 'light')

def render_entity_card(entity, highlight=False, is_related=False):
    """Render a single entity card with proper formatting."""
    
    # Determine card color based on entity type
    color_map = {
        'AGENDA_ITEM': 'primary',
        'ORDINANCE': 'success',
        'RESOLUTION': 'warning',
        'PERSON': 'info',
        'ORGANIZATION': 'secondary',
        'referenced_entity': 'danger'
    }
    
    border_color = color_map.get(entity.get('entity_type', entity.get('type', '')), 'light')
    
    card_content = [
        html.H6([
            html.Span(
                entity.get('entity_type', entity.get('type', '')).upper(), 
                className=f"badge bg-{border_color} me-2"
            ),
            entity['title'],
            html.Span(
                f" (Entity #{entity.get('id', entity.get('entity_id', ''))})",
                className="text-muted small"
            ) if entity.get('id') or entity.get('entity_id') else ""
        ]),
        html.P(
            entity.get('description', ''), 
            className="text-muted small mb-2",
            style={"maxHeight": "100px", "overflow": "auto"}
        ),
    ]
    
    # Add relationship info if this is a related entity
    if is_related and entity.get('relationship'):
        card_content.insert(1, html.P([
            html.Strong("Relationship: "),
            html.Em(entity['relationship'][:100] + "..." if len(entity['relationship']) > 100 else entity['relationship'])
        ], className="small"))
    
    # Add source document info if available
    source_doc = entity.get('source_document', {})
    if source_doc:
        card_content.append(
            html.Div([
                html.Hr(className="my-2"),
                html.Small([
                    html.Strong("Source: "),
                    f"{source_doc.get('type', 'Document')} - {source_doc.get('meeting_date', 'N/A')}",
                    html.Br(),
                    html.Strong("File: "),
                    html.Code(source_doc.get('source_file', 'Unknown'), className="small")
                ], className="text-muted")
            ])
        )
    
    return dbc.Card(
        dbc.CardBody(card_content),
        className="mb-2",
        color=border_color if highlight else None,
        outline=True,
        style={"border-width": "2px"} if highlight else {}
    )

def render_routing_info(routing_details, actual_method):
    """Render routing analysis information."""
    
    intent = routing_details.get('intent')
    params = routing_details.get('params', {})
    
    content = [
        html.P([
            html.Strong("Selected Method: "),
            html.Span(actual_method.upper(), className="badge bg-primary")
        ]),
    ]
    
    if intent:
        content.append(html.P([
            html.Strong("Detected Intent: "),
            html.Span(intent.value if hasattr(intent, 'value') else str(intent))
        ]))
    
    # Show detected entities
    if 'entity_filter' in params:
        entity = params['entity_filter']
        content.append(html.P([
            html.Strong("Primary Entity: "),
            html.Code(f"{entity['type']}: {entity['value']}")
        ]))
    
    if 'multiple_entities' in params:
        entities = params['multiple_entities']
        content.append(html.P([
            html.Strong("Detected Entities: "),
            html.Ul([
                html.Li(html.Code(f"{e['type']}: {e['value']}"))
                for e in entities
            ])
        ]))
    
    # Show key parameters
    key_params = ['top_k_entities', 'community_level', 'comparison_mode', 'strict_entity_focus']
    param_list = []
    for param in key_params:
        if param in params:
            param_list.append(html.Li(f"{param}: {params[param]}"))
    
    if param_list:
        content.append(html.Div([
            html.Strong("Parameters:"),
            html.Ul(param_list)
        ]))
    
    return html.Div(content)

def render_query_history():
    """Render the query history."""
    if not query_history:
        return html.P("No queries yet", className="text-muted")
    
    history_items = []
    for item in query_history:
        badge_color = "success" if item['auto_routed'] else "info"
        history_items.append(
            html.Li([
                html.Small(item['timestamp'].strftime("%H:%M:%S"), className="text-muted me-2"),
                html.Span(item['method'].upper(), className=f"badge bg-{badge_color} me-2"),
                html.Span(item['query'][:100] + "..." if len(item['query']) > 100 else item['query'])
            ], className="mb-2")
        )
    
    return html.Ul(history_items, className="list-unstyled")

def render_error(error_msg):
    """Render an error message."""
    return dbc.Alert([
        html.H5("‚ùå Error", className="alert-heading"),
        html.P(error_msg)
    ], color="danger")

# Add some custom CSS
app.index_string = '''
<!DOCTYPE html>
<html>
    <head>
        {%metas%}
        <title>{%title%}</title>
        {%favicon%}
        {%css%}
        <style>
            body {
                background-color: #f8f9fa;
            }
            .card {
                box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            }
            .card-header {
                background-color: #e9ecef;
            }
            pre {
                white-space: pre-wrap;
                word-wrap: break-word;
            }
        </style>
    </head>
    <body>
        {%app_entry%}
        <footer>
            {%config%}
            {%scripts%}
            {%renderer%}
        </footer>
    </body>
</html>
'''

if __name__ == "__main__":
    print("üöÄ Starting GraphRAG Query UI...")
    print(f"üìÅ GraphRAG Root: {GRAPHRAG_ROOT}")
    print("üåê Open http://localhost:8050 in your browser")
    print("Press Ctrl+C to stop")
    
    app.run(debug=True, host='0.0.0.0', port=8050)


================================================================================


################################################################################
# File: scripts/RAG_stages/chunk_text.py
################################################################################

# File: scripts/RAG_stages/chunk_text.py

#!/usr/bin/env python3
"""
Stage 5 ‚Äî Token-based chunking with tiktoken validation.
"""
from __future__ import annotations
import json, logging, math, pathlib, re
from typing import Dict, List, Sequence, Tuple, Any, Optional
import asyncio
from concurrent.futures import ProcessPoolExecutor
import multiprocessing as mp

# üéØ TIKTOKEN VALIDATION - Add token validation layer
try:
    import tiktoken
    TIKTOKEN_AVAILABLE = True
except ImportError:
    TIKTOKEN_AVAILABLE = False
    logging.warning("tiktoken not available - token-based chunking requires tiktoken")

# Token limits for validation (consistent with embed_vectors.py)
MAX_CHUNK_TOKENS = 6000  # Maximum tokens per chunk for embedding
MIN_CHUNK_TOKENS = 100   # Minimum viable chunk size
EMBEDDING_MODEL = "text-embedding-ada-002"

# ‚îÄ‚îÄ‚îÄ TOKEN-BASED CHUNKING PARAMETERS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
WINDOW_TOKENS = 3000   # Token-based window (was 768 words)
OVERLAP_TOKENS = 600   # Token-based overlap (20% of window)
log = logging.getLogger(__name__)

# ‚îÄ‚îÄ‚îÄ helpers to split into token windows ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def concat_tokens_by_encoding(sections: Sequence[Dict[str, Any]]) -> Tuple[List[int], List[int]]:
    """Convert sections to encoded tokens with page mapping."""
    if not TIKTOKEN_AVAILABLE:
        raise RuntimeError("tiktoken is required for token-based chunking")
    
    encoder = tiktoken.encoding_for_model(EMBEDDING_MODEL)
    all_tokens = []
    token_to_page = []
    
    for s in sections:
        # Be defensive - build text if missing
        if 'text' not in s:
            if 'elements' in s:
                text = '\n'.join(
                    el.get('text', '') for el in s['elements'] 
                    if el.get('text', '') and not el.get('text', '').startswith('self_ref=')
                )
            else:
                text = ""
            log.warning(f"Section missing 'text' field, built from elements: {s.get('section', 'untitled')}")
        else:
            text = s.get("text", "")
        
        # Encode text to tokens
        tokens = encoder.encode(text)
        all_tokens.extend(tokens)
        
        # Handle page mapping
        if 'page_number' in s:
            page_num = s['page_number']
        elif 'page_start' in s and 'page_end' in s:
            page_start = s['page_start']
            page_end = s['page_end']
            if page_start == page_end:
                page_num = page_start
            else:
                # For multi-page sections, distribute tokens across pages
                pages_span = page_end - page_start + 1
                tokens_per_page = max(1, len(tokens) // pages_span)
                for i, token in enumerate(tokens):
                    page = min(page_end, page_start + (i // tokens_per_page))
                    token_to_page.append(page)
                continue  # Skip the extend below since we handled it above
        else:
            page_num = 1
            log.warning(f"Section has no page info: {s.get('section', 'untitled')}")
        
        # Map all tokens in this section to the page (for single page sections)
        if 'page_start' not in s or s['page_start'] == s.get('page_end', s['page_start']):
            token_to_page.extend([page_num] * len(tokens))
    
    return all_tokens, token_to_page

def sliding_chunks_by_tokens(
    encoded_tokens: List[int], 
    token_to_page: List[int], 
    *, 
    window_tokens: int, 
    overlap_tokens: int
) -> List[Dict[str, Any]]:
    """Create sliding chunks based on token count."""
    if not TIKTOKEN_AVAILABLE:
        raise RuntimeError("tiktoken is required for token-based chunking")
    
    encoder = tiktoken.encoding_for_model(EMBEDDING_MODEL)
    step = max(1, window_tokens - overlap_tokens)
    chunks = []
    i = 0
    
    while i < len(encoded_tokens):
        start = i
        end = min(len(encoded_tokens), i + window_tokens)
        
        # Extract token chunk
        chunk_tokens = encoded_tokens[start:end]
        
        # Decode back to text
        chunk_text = encoder.decode(chunk_tokens)
        
        # Determine page range for this chunk
        page_start = token_to_page[start] if start < len(token_to_page) else 1
        page_end = token_to_page[end - 1] if end - 1 < len(token_to_page) else page_start
        
        chunk = {
            "chunk_index": len(chunks),
            "token_start": start,
            "token_end": end - 1,
            "page_start": page_start,
            "page_end": page_end,
            "text": chunk_text,
            "token_count": len(chunk_tokens)  # Actual token count
        }
        
        chunks.append(chunk)
        
        # Break if we've reached the end
        if end == len(encoded_tokens):
            break
            
        i += step
    
    return chunks

# Legacy word-based functions (kept for fallback if tiktoken unavailable)
def concat_tokens(sections:Sequence[Dict[str,Any]])->Tuple[List[str],List[int]]:
    tokens,page_map=[],[]
    for s in sections:
        # Be defensive - build text if missing
        if 'text' not in s:
            if 'elements' in s:
                text = '\n'.join(
                    el.get('text', '') for el in s['elements'] 
                    if el.get('text', '') and not el.get('text', '').startswith('self_ref=')
                )
            else:
                text = ""
            log.warning(f"Section missing 'text' field, built from elements: {s.get('section', 'untitled')}")
        else:
            text = s.get("text", "")
            
        words=text.split()
        tokens.extend(words)
        
        # Handle both page_number (from page_secs) and page_start/page_end (from logical_secs)
        if 'page_number' in s:
            # Single page section
            page_map.extend([s['page_number']]*len(words))
        elif 'page_start' in s and 'page_end' in s:
            # Multi-page section - distribute words across pages
            page_start = s['page_start']
            page_end = s['page_end']
            if page_start == page_end:
                # All on same page
                page_map.extend([page_start]*len(words))
            else:
                # Distribute words evenly across pages
                pages_span = page_end - page_start + 1
                words_per_page = max(1, len(words) // pages_span)
                for i, word in enumerate(words):
                    page = min(page_end, page_start + (i // words_per_page))
                    page_map.append(page)
        else:
            # Fallback to page 1
            page_map.extend([1]*len(words))
            log.warning(f"Section has no page info: {s.get('section', 'untitled')}")
            
    return tokens,page_map

def sliding_chunks(tokens:List[str],page_map:List[int],*,window:int,overlap:int)->List[Dict[str,Any]]:
    step=max(1,window-overlap)
    out,i=[],0
    SENT_END_RE=re.compile(r"[.!?]$")
    while i<len(tokens):
        start,end=i,min(len(tokens),i+window)
        while end<len(tokens) and not SENT_END_RE.search(tokens[end-1]) and end-start<window+256:
            end+=1
        out.append({"chunk_index":len(out),"token_start":start,"token_end":end-1,
                    "page_start":page_map[start],"page_end":page_map[end-1],
                    "text":" ".join(tokens[start:end])})
        if end==len(tokens): break
        i+=step
    return out
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# üéØ TIKTOKEN VALIDATION FUNCTIONS
def count_tiktoken_accurate(text: str) -> int:
    """Count tokens accurately using tiktoken."""
    if TIKTOKEN_AVAILABLE:
        try:
            encoder = tiktoken.encoding_for_model(EMBEDDING_MODEL)
            return len(encoder.encode(text))
        except:
            pass
    
    # Conservative fallback estimation
    return int(len(text.split()) * 1.5) + 50

def smart_split_chunk(chunk: Dict[str, Any], max_tokens: int = MAX_CHUNK_TOKENS) -> List[Dict[str, Any]]:
    """Split an oversized chunk into smaller token-compliant chunks."""
    text = chunk["text"]
    tokens = count_tiktoken_accurate(text)
    
    if tokens <= max_tokens:
        return [chunk]
    
    # Calculate how many sub-chunks we need
    num_splits = math.ceil(tokens / max_tokens)
    target_words_per_split = len(text.split()) // num_splits
    
    log.info(f"üéØ Splitting oversized chunk: {tokens} tokens ‚Üí {num_splits} chunks of ~{max_tokens} tokens each")
    
    words = text.split()
    sentences = re.split(r'[.!?]+', text)
    
    sub_chunks = []
    current_words = []
    current_tokens = 0
    
    # Split by sentences when possible, otherwise by words
    for sentence in sentences:
        sentence_words = sentence.strip().split()
        if not sentence_words:
            continue
            
        sentence_tokens = count_tiktoken_accurate(sentence)
        
        # If adding this sentence would exceed limit, finalize current chunk
        if current_tokens + sentence_tokens > max_tokens and current_words:
            # Create sub-chunk
            sub_text = " ".join(current_words)
            sub_chunk = {
                "chunk_index": chunk["chunk_index"],
                "sub_chunk_index": len(sub_chunks),
                "token_start": chunk["token_start"] + len(" ".join(words[:words.index(current_words[0])])),
                "token_end": chunk["token_start"] + len(" ".join(words[:words.index(current_words[-1])])) + len(current_words[-1]),
                "page_start": chunk["page_start"],
                "page_end": chunk["page_end"],
                "text": sub_text
            }
            sub_chunks.append(sub_chunk)
            current_words = []
            current_tokens = 0
        
        # Add sentence to current chunk
        current_words.extend(sentence_words)
        current_tokens += sentence_tokens
    
    # Add final sub-chunk if there's remaining content
    if current_words:
        sub_text = " ".join(current_words)
        if count_tiktoken_accurate(sub_text) >= MIN_CHUNK_TOKENS:
            sub_chunk = {
                "chunk_index": chunk["chunk_index"],
                "sub_chunk_index": len(sub_chunks),
                "token_start": chunk["token_start"],
                "token_end": chunk["token_end"],
                "page_start": chunk["page_start"],
                "page_end": chunk["page_end"],
                "text": sub_text
            }
            sub_chunks.append(sub_chunk)
    
    log.info(f"üéØ Split complete: {len(sub_chunks)} sub-chunks created")
    return sub_chunks

def validate_and_fix_chunks(chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Validate chunks against tiktoken limits and fix oversized ones."""
    if not TIKTOKEN_AVAILABLE:
        log.warning("üéØ tiktoken not available - skipping token validation")
        return chunks
    
    valid_chunks = []
    oversized_count = 0
    total_tokens_before = 0
    total_tokens_after = 0
    
    log.info(f"üéØ TIKTOKEN VALIDATION: Processing {len(chunks)} chunks")
    
    for chunk in chunks:
        tokens = count_tiktoken_accurate(chunk["text"])
        total_tokens_before += tokens
        
        if tokens <= MAX_CHUNK_TOKENS:
            # Chunk is fine, keep as-is
            valid_chunks.append(chunk)
            total_tokens_after += tokens
        else:
            # Split oversized chunk
            log.warning(f"üéØ Oversized chunk detected: {tokens} tokens (max: {MAX_CHUNK_TOKENS})")
            oversized_count += 1
            
            split_chunks = smart_split_chunk(chunk, MAX_CHUNK_TOKENS)
            valid_chunks.extend(split_chunks)
            
            # Count tokens in split chunks
            for split_chunk in split_chunks:
                total_tokens_after += count_tiktoken_accurate(split_chunk["text"])
    
    # Final validation check
    max_tokens_found = max((count_tiktoken_accurate(chunk["text"]) for chunk in valid_chunks), default=0)
    
    log.info(f"üéØ TIKTOKEN VALIDATION COMPLETE:")
    log.info(f"   üìä Original chunks: {len(chunks)}")
    log.info(f"   üìä Final chunks: {len(valid_chunks)}")
    log.info(f"   üìä Oversized chunks split: {oversized_count}")
    log.info(f"   üìä Largest chunk: {max_tokens_found} tokens (limit: {MAX_CHUNK_TOKENS})")
    log.info(f"   ‚úÖ GUARANTEED: All chunks ‚â§ {MAX_CHUNK_TOKENS} tokens")
    
    if max_tokens_found > MAX_CHUNK_TOKENS:
        log.error(f"üö® VALIDATION FAILED: Chunk still exceeds limit!")
        raise RuntimeError(f"Chunk validation failed: {max_tokens_found} > {MAX_CHUNK_TOKENS}")
    
    return valid_chunks

# Legacy constants for fallback
WINDOW  = 768
OVERLAP = int(WINDOW*0.20)           # 154-token (20 %) overlap

# Parallel chunking for multiple documents
async def chunk_batch_async(
    json_paths: List[pathlib.Path],
    max_workers: Optional[int] = None
) -> Dict[pathlib.Path, List[Dict[str, Any]]]:
    """Chunk multiple documents in parallel with token-based chunking."""
    from .acceleration_utils import hardware
    
    results = {}
    
    # üéØ Show token-based chunking status
    if TIKTOKEN_AVAILABLE:
        log.info(f"üéØ TOKEN-BASED CHUNKING ENABLED: Using {WINDOW_TOKENS} token windows with {OVERLAP_TOKENS} token overlap")
    else:
        log.warning(f"‚ö†Ô∏è  TIKTOKEN NOT AVAILABLE: Falling back to word-based chunking (install: pip install tiktoken)")
    
    # CPU-bound task - use process pool
    with hardware.get_process_pool(max_workers) as executor:
        future_to_path = {
            executor.submit(chunk, path): path 
            for path in json_paths
        }
        
        from tqdm import tqdm
        from concurrent.futures import as_completed
        
        for future in tqdm(as_completed(future_to_path), 
                          total=len(json_paths), 
                          desc="Chunking documents"):
            path = future_to_path[future]
            try:
                chunks = future.result()
                results[path] = chunks
            except Exception as exc:
                log.error(f"Failed to chunk {path.name}: {exc}")
                results[path] = []
    
    # üéØ Summary of token-based chunking results
    total_chunks = sum(len(chunks) for chunks in results.values())
    total_tokens = sum(chunk.get('token_count', count_tiktoken_accurate(chunk['text'])) 
                      for chunks in results.values() 
                      for chunk in chunks)
    log.info(f"üéØ BATCH CHUNKING COMPLETE: {total_chunks} chunks created ({total_tokens} total tokens)")
    
    return results

# Optimized chunking with better memory management
def chunk_optimized(json_path: pathlib.Path) -> List[Dict[str, Any]]:
    """Token-based optimized chunking with validation."""
    root = json_path.with_name(json_path.stem + "_chunks.json")
    if root.exists():
        existing_chunks = json.loads(root.read_text())
        # Validate existing chunks if they don't have tiktoken validation yet
        if existing_chunks and "sub_chunk_index" not in str(existing_chunks):
            log.info(f"üéØ Re-validating existing chunks in {root.name}")
            validated_chunks = validate_and_fix_chunks(existing_chunks)
            if len(validated_chunks) != len(existing_chunks):
                # Chunks were modified, save the validated version
                with open(root, 'w') as f:
                    json.dump(validated_chunks, f, indent=2, ensure_ascii=False)
                log.info(f"üéØ Updated {root.name} with validated chunks")
            return validated_chunks
        return existing_chunks
    
    # Stream large JSON files
    with open(json_path, 'r') as f:
        data = json.load(f)
    
    if "sections" not in data:
        raise RuntimeError(f"{json_path} has no 'sections' key")
    
    # Process in smaller batches to reduce memory usage
    sections = data["sections"]
    batch_size = 100  # Process 100 sections at a time
    
    if TIKTOKEN_AVAILABLE:
        # üéØ TOKEN-BASED CHUNKING - Primary approach
        log.info(f"üéØ Using token-based chunking with {WINDOW_TOKENS} token windows")
        
        all_tokens = []
        all_token_to_page = []
        
        for i in range(0, len(sections), batch_size):
            batch = sections[i:i + batch_size]
            tokens, page_map = concat_tokens_by_encoding(batch)
            all_tokens.extend(tokens)
            all_token_to_page.extend(page_map)
        
        chunks = sliding_chunks_by_tokens(
            all_tokens, 
            all_token_to_page, 
            window_tokens=WINDOW_TOKENS, 
            overlap_tokens=OVERLAP_TOKENS
        )
        
        # Token-based chunks are already guaranteed to be within limits
        log.info(f"üéØ Token-based chunking produced {len(chunks)} chunks")
        
        # Additional validation for safety
        max_tokens = max((chunk.get('token_count', count_tiktoken_accurate(chunk['text'])) for chunk in chunks), default=0)
        if max_tokens > MAX_CHUNK_TOKENS:
            log.warning(f"üéØ Unexpected: Token-based chunk exceeds limit ({max_tokens} > {MAX_CHUNK_TOKENS}), applying fallback validation")
            chunks = validate_and_fix_chunks(chunks)
    else:
        # üéØ FALLBACK: Word-based chunking when tiktoken unavailable
        log.warning(f"üéØ Falling back to word-based chunking (tiktoken unavailable)")
        
        all_tokens = []
        all_page_map = []
        
        for i in range(0, len(sections), batch_size):
            batch = sections[i:i + batch_size]
            toks, pmap = concat_tokens(batch)
            all_tokens.extend(toks)
            all_page_map.extend(pmap)
        
        chunks = sliding_chunks(all_tokens, all_page_map, window=WINDOW, overlap=OVERLAP)
        
        # Apply validation for word-based chunks
        if chunks:
            log.info(f"üéØ Applying validation to {len(chunks)} word-based chunks")
            chunks = validate_and_fix_chunks(chunks)
    
    # Write chunks
    if chunks:
        with open(root, 'w') as f:
            json.dump(chunks, f, indent=2, ensure_ascii=False)
        log.info("‚úì %s chunks ‚Üí %s", len(chunks), root.name)
        
        return chunks
    else:
        log.warning("%s ‚Äì no chunks produced; file skipped", json_path.name)
        return []

# Keep original interface
def chunk(json_path: pathlib.Path) -> List[Dict[str, Any]]:
    """Original interface maintained for compatibility."""
    return chunk_optimized(json_path)

if __name__ == "__main__":
    import argparse, logging
    logging.basicConfig(level=logging.INFO, format="%(message)s")
    p=argparse.ArgumentParser(); p.add_argument("json",type=pathlib.Path)
    chunk(p.parse_args().json)


================================================================================


################################################################################
# File: scripts/graph_rag_stages/phase3_querying/structural_query_enhancer.py
################################################################################

# File: scripts/graph_rag_stages/phase3_querying/structural_query_enhancer.py

#!/usr/bin/env python3
"""
Structural Query Enhancer
=========================
Enhances GraphRAG queries with structured data from extracted documents
to ensure complete and accurate responses for agenda/document structure queries.
"""

import json
import re
from pathlib import Path
from typing import Dict, List, Any, Optional
import pandas as pd
from datetime import datetime

class StructuralQueryEnhancer:
    """Enhances queries with structural data from extracted documents."""
    
    def __init__(self, extracted_text_dir: Path):
        self.extracted_text_dir = Path(extracted_text_dir)
        self._agenda_cache = {}
        self._load_agenda_structures()
    
    def _load_agenda_structures(self):
        """Load all agenda structures from extracted JSON files."""
        print("üîç Loading agenda structures from extracted documents...")
        
        # Group individual agenda items by meeting date
        date_grouped_items = {}
        
        for json_file in self.extracted_text_dir.glob("*.json"):
            try:
                with open(json_file, 'r') as f:
                    data = json.load(f)
                
                # Extract meeting date from the document
                meeting_date_raw = data.get('meeting_date', '')
                meeting_date = self._parse_meeting_date(meeting_date_raw, json_file.name)
                
                if meeting_date:
                    if meeting_date not in date_grouped_items:
                        date_grouped_items[meeting_date] = {
                            'source_files': [],
                            'agenda_items': [],
                            'doc_ids': []
                        }
                    
                    # Create agenda item from this document
                    # Handle different file structures (regular docs vs verbatim transcripts)
                    if data.get('document_type') == 'verbatim_transcript':
                        # Verbatim transcripts have item_codes as an array
                        item_codes = data.get('item_codes', [])
                        item_code = item_codes[0] if item_codes else ''
                        # Extract title from the full text or create a meaningful one
                        title = self._extract_title_from_verbatim(data.get('full_text', ''), item_code)
                    else:
                        # Regular documents have single item_code and title
                        item_code = data.get('item_code', '')
                        title = data.get('title', '')
                    
                    agenda_item = {
                        'item_code': item_code,
                        'title': title,
                        'document_type': data.get('document_type', ''),
                        'document_number': data.get('document_number', ''),
                        'full_text': data.get('full_text', ''),
                        'source_file': json_file.name
                    }
                    
                    date_grouped_items[meeting_date]['agenda_items'].append(agenda_item)
                    date_grouped_items[meeting_date]['source_files'].append(json_file.name)
                    date_grouped_items[meeting_date]['doc_ids'].append(data.get('doc_id', json_file.stem))
                    
            except Exception as e:
                print(f"‚ö†Ô∏è  Error loading {json_file}: {e}")
        
        # Convert to agenda cache format
        for meeting_date, items_data in date_grouped_items.items():
            self._agenda_cache[meeting_date] = {
                'source_files': items_data['source_files'],
                'doc_ids': items_data['doc_ids'],
                'meeting_info': {'date': meeting_date},
                'agenda_items': items_data['agenda_items'],
                'sections': [],
                'full_data': {}
            }
        
        print(f"‚úÖ Loaded {len(self._agenda_cache)} agenda structures")
        for date, data in self._agenda_cache.items():
            print(f"   üìÖ {date}: {len(data['agenda_items'])} items")
    
    def _extract_title_from_verbatim(self, full_text: str, item_code: str) -> str:
        """Extract a meaningful title from verbatim transcript text."""
        if not full_text:
            return f"Verbatim Transcript - Agenda Item {item_code}"
        
        # Try to find title patterns in the text
        import re
        
        # Look for patterns like "RE: [title]" or "Subject: [title]"
        patterns = [
            r'RE:\s*([^\n]+)',
            r'Subject:\s*([^\n]+)',
            r'SUBJECT:\s*([^\n]+)',
            r'Matter:\s*([^\n]+)',
            r'Item\s+' + re.escape(item_code) + r'[:\-\s]*([^\n]+)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, full_text, re.IGNORECASE)
            if match:
                title = match.group(1).strip()
                # Clean up the title
                title = re.sub(r'^\W+|\W+$', '', title)  # Remove leading/trailing punctuation
                if len(title) > 10:  # Only use if it's meaningful
                    return f"Verbatim: {title[:100]}"
        
        # Look for ordinance/resolution patterns in the text
        ordinance_match = re.search(r'(AN?\s+ORDINANCE[^\n]+)', full_text, re.IGNORECASE)
        if ordinance_match:
            return f"Verbatim: {ordinance_match.group(1)[:100]}"
        
        resolution_match = re.search(r'(AN?\s+RESOLUTION[^\n]+)', full_text, re.IGNORECASE)
        if resolution_match:
            return f"Verbatim: {resolution_match.group(1)[:100]}"
        
        # Look for the first meaningful line after the header
        lines = full_text.split('\n')
        for line in lines[5:15]:  # Skip header lines, check next 10 lines
            line = line.strip()
            if len(line) > 20 and not line.startswith('#') and not line.startswith('Mayor') and not line.startswith('Commissioner'):
                return f"Verbatim: {line[:100]}"
        
        # Fallback
        return f"Verbatim Transcript - Agenda Item {item_code}"
    
    def _parse_meeting_date(self, date_str: str, filename: str) -> Optional[str]:
        """Parse meeting date from various formats."""
        # Try to extract from filename first (more reliable)
        filename_match = re.search(r'(\d{2})_(\d{2})_(\d{4})', filename)
        if filename_match:
            month, day, year = filename_match.groups()
            return f"{year}-{month}-{day}"
        
        # Try to parse from date string
        date_patterns = [
            r'(\d{1,2})\.(\d{1,2})\.(\d{4})',  # DD.MM.YYYY (European format)
            r'(\d{1,2})/(\d{1,2})/(\d{4})',   # MM/DD/YYYY
            r'(\d{4})-(\d{1,2})-(\d{1,2})',   # YYYY-MM-DD
            r'January (\d{1,2}), (\d{4})',    # January 9, 2024
            r'Jan (\d{1,2}), (\d{4})'         # Jan 9, 2024
        ]
        
        for pattern in date_patterns:
            match = re.search(pattern, date_str)
            if match:
                if 'January' in pattern or 'Jan' in pattern:
                    day, year = match.groups()
                    return f"{year}-01-{day.zfill(2)}"
                elif pattern.startswith(r'(\d{4})'):
                    year, month, day = match.groups()
                    return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                elif pattern.startswith(r'(\d{1,2})\.'):
                    # DD.MM.YYYY format (European)
                    day, month, year = match.groups()
                    return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                else:
                    # MM/DD/YYYY format (American)
                    month, day, year = match.groups()
                    return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
        
        return None
    
    def is_agenda_completeness_query(self, query: str) -> bool:
        """Determine if query requires complete agenda listing."""
        completeness_patterns = [
            r'all.*items.*agenda',
            r'complete.*agenda',
            r'agenda.*items.*presented',
            r'items.*discussed.*meeting',
            r'all.*resolutions?.*ordinances?',
            r'complete.*list.*items'
        ]
        
        query_lower = query.lower()
        return any(re.search(pattern, query_lower) for pattern in completeness_patterns)
    
    def extract_date_from_query(self, query: str) -> Optional[str]:
        """Extract date from query."""
        date_patterns = [
            r'january?\s+(\d{1,2}),?\s+(\d{4})',
            r'jan\s+(\d{1,2}),?\s+(\d{4})',
            r'(\d{1,2})\.(\d{1,2})\.(\d{4})',  # DD.MM.YYYY
            r'(\d{1,2})/(\d{1,2})/(\d{4})',   # MM/DD/YYYY  
            r'(\d{4})-(\d{1,2})-(\d{1,2})'    # YYYY-MM-DD
        ]
        
        query_lower = query.lower()
        
        for pattern in date_patterns:
            match = re.search(pattern, query_lower)
            if match:
                if 'january' in pattern or 'jan' in pattern:
                    day, year = match.groups()
                    return f"{year}-01-{day.zfill(2)}"
                elif pattern.startswith(r'(\d{4})'):
                    year, month, day = match.groups()
                    return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                elif pattern.startswith(r'(\d{1,2})\.'):
                    # DD.MM.YYYY format (European)
                    day, month, year = match.groups()
                    return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                else:
                    # MM/DD/YYYY format (American)
                    month, day, year = match.groups()
                    return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
        
        return None
    
    def get_complete_agenda_items(self, date: str) -> Dict[str, Any]:
        """Get complete agenda items for a specific date."""
        if date not in self._agenda_cache:
            return {"found": False, "message": f"No agenda found for {date}"}
        
        agenda_data = self._agenda_cache[date]
        items = agenda_data['agenda_items']
        
        # Extract all ordinances and resolutions
        ordinances = []
        resolutions = []
        other_items = []
        
        for item in items:
            item_code = item.get('item_code', '')
            title = item.get('title', '')
            
            # Categorize items
            if 'ordinance' in title.lower() or item_code.startswith('ORD'):
                ordinances.append(item)
            elif 'resolution' in title.lower() or item_code.startswith('RES'):
                resolutions.append(item)
            else:
                other_items.append(item)
        
        return {
            "found": True,
            "date": date,
            "source_files": agenda_data['source_files'],
            "doc_ids": agenda_data['doc_ids'],
            "meeting_info": agenda_data['meeting_info'],
            "total_items": len(items),
            "ordinances": ordinances,
            "resolutions": resolutions,
            "other_items": other_items,
            "all_items": items
        }
    
    def enhance_graphrag_response(self, query: str, graphrag_result: Dict[str, Any]) -> Dict[str, Any]:
        """Enhance GraphRAG response with structural completeness data."""
        
        # Check if this is a completeness query
        if not self.is_agenda_completeness_query(query):
            return graphrag_result
        
        # Extract date from query
        query_date = self.extract_date_from_query(query)
        if not query_date:
            return graphrag_result
        
        # Get complete structural data
        structural_data = self.get_complete_agenda_items(query_date)
        if not structural_data["found"]:
            return graphrag_result
        
        # Create enhanced response
        enhanced_answer = self._create_enhanced_answer(
            graphrag_result.get('answer', ''),
            structural_data,
            query_date
        )
        
        # Update result
        enhanced_result = graphrag_result.copy()
        enhanced_result['answer'] = enhanced_answer
        enhanced_result['structural_enhancement'] = {
            'applied': True,
            'query_date': query_date,
            'total_items_found': structural_data['total_items'],
            'source_doc_ids': structural_data['doc_ids']
        }
        
        return enhanced_result
    
    def _create_enhanced_answer(self, original_answer: str, structural_data: Dict, query_date: str) -> str:
        """Create enhanced answer with complete structural information."""
        
        ordinances = structural_data['ordinances']
        resolutions = structural_data['resolutions']
        other_items = structural_data['other_items']
        
        enhancement = f"""

## üîç **STRUCTURAL COMPLETENESS ENHANCEMENT**
*Enhanced with graph_stages pipeline data to ensure ALL agenda items are included*

### üìÖ **Complete Agenda Items for {query_date}**

**üìä SUMMARY**: {structural_data['total_items']} total agenda items identified from {len(structural_data['doc_ids'])} source documents

"""
        
        if ordinances:
            enhancement += "### üìú **ORDINANCES**\n"
            for ord_item in ordinances:
                enhancement += f"- **{ord_item.get('item_code', 'Unknown')}**: {ord_item.get('title', 'No title')}\n"
            enhancement += "\n"
        
        if resolutions:
            enhancement += "### üìã **RESOLUTIONS**\n"
            for res_item in resolutions:
                enhancement += f"- **{res_item.get('item_code', 'Unknown')}**: {res_item.get('title', 'No title')}\n"
            enhancement += "\n"
        
        if other_items:
            enhancement += "### üìå **OTHER AGENDA ITEMS**\n"
            for other_item in other_items:
                item_code = other_item.get('item_code', 'Unknown')
                title = other_item.get('title', '').strip()
                
                # Handle empty or placeholder titles
                if not title or title in ['', '****', 'No title', 'Unknown']:
                    doc_type = other_item.get('document_type', '')
                    if doc_type == 'verbatim_transcript':
                        title = f"Verbatim Transcript Discussion"
                    else:
                        title = "Agenda Item Discussion"
                
                enhancement += f"- **{item_code}**: {title}\n"
            enhancement += "\n"
        
        enhancement += """
### üîó **Pipeline Integration Note**
This enhanced response combines:
- **GraphRAG Analysis**: Semantic understanding and context from community reports
- **graph_stages Structure**: Complete itemized agenda structure ensuring no items are missed

*This demonstrates the successful linking of both knowledge graph pipelines for comprehensive responses.*
"""
        
        return original_answer + enhancement


================================================================================


################################################################################
# File: scripts/graph_rag_stages/phase1_preprocessing/agenda_extractor.py
################################################################################

# File: scripts/graph_rag_stages/phase1_preprocessing/agenda_extractor.py

"""
Extracts structured content from agenda PDFs, including items and hyperlinks.
"""

import logging
from pathlib import Path
from typing import Dict, List, Optional
import json
import re
import fitz  # PyMuPDF for hyperlink extraction
import hashlib
import asyncio
from datetime import datetime

from .pdf_extractor import PDFExtractor
from ..common.utils import get_llm_client, clean_json_response, call_llm_with_retry

log = logging.getLogger(__name__)


class AgendaExtractor:
    """Extracts structured content from agenda PDFs using OCR, PyMuPDF, and an LLM."""

    def __init__(self, output_dir: Path):
        self.output_dir = output_dir
        self.pdf_extractor = PDFExtractor()
        self.llm_client = get_llm_client()
        self.model = "gpt-4"
        
        # Cache for extractions
        self._extraction_cache = {}

    async def extract_and_save_agenda(self, pdf_path: Path) -> None:
        """Orchestrates the full extraction and saving process for an agenda PDF."""
        log.info(f"üìÑ Extracting agenda from: {pdf_path.name}")
        
        # Check cache first
        file_hash = self._get_file_hash(pdf_path)
        if file_hash in self._extraction_cache:
            log.info(f"üìã Using cached extraction for {pdf_path.name}")
            agenda_data = self._extraction_cache[file_hash]
        else:
            # Extract text using base PDF extractor
            full_text, pages = self.pdf_extractor.extract_text_from_pdf(pdf_path)
            if not full_text:
                log.warning(f"No text extracted from {pdf_path.name}, skipping.")
                return

            # Extract structured agenda items using LLM
            items = await self._extract_agenda_items_with_llm(full_text)
            
            # Extract hyperlinks using PyMuPDF
            links = self._extract_hyperlinks_pymupdf(pdf_path)
            
            # Associate URLs with items
            self._associate_urls_with_items(items, links)
            
            # Extract meeting info
            meeting_info = self._extract_meeting_info(pdf_path, full_text)
            
            # Create agenda data structure
            agenda_data = {
                'source_file': pdf_path.name,
                'doc_id': self._generate_doc_id(pdf_path),
                'full_text': full_text,
                'agenda_items': items,
                'hyperlinks': links,
                'meeting_info': meeting_info,
                'metadata': {
                    'extraction_method': 'docling+llm+pymupdf',
                    'num_items': len(items),
                    'num_hyperlinks': len(links),
                    'extraction_timestamp': datetime.now().isoformat()
                }
            }
            
            # Cache result
            self._extraction_cache[file_hash] = agenda_data

        # Save as enriched markdown
        self._save_as_markdown(pdf_path, agenda_data)

    async def _extract_agenda_items_with_llm(self, text: str) -> List[Dict]:
        """Extract agenda items using LLM."""
        log.info("üß† Using LLM to extract agenda structure...")
        
        # Prepare the extraction prompt
        messages = [
            {
                "role": "system",
                "content": """You are an expert at extracting structured data from city council agenda documents. 
Extract ALL agenda items from the document, looking for patterns like:
- Letter-Number format (e.g., H-1, A-2, B-3)
- Letter.-Number. format (e.g., H.-1., A.-2.)
- Items with references like "23-6819"

Return a JSON array of objects with these fields:
- item_code: The item identifier (e.g., "H-1")
- title: The item title/description
- section_name: The section this item belongs to
- document_reference: Any document reference number (e.g., "23-6819")
- item_type: Type of item (ordinance, resolution, report, etc.)

Be thorough and extract ALL items, even those marked as "None"."""
            },
            {
                "role": "user", 
                "content": f"Extract agenda items from this document:\n\n{text[:15000]}"  # Limit length
            }
        ]
        
        try:
            response = await call_llm_with_retry(
                self.llm_client,
                messages,
                model=self.model,
                temperature=0.1
            )
            
            # Parse the JSON response
            items = clean_json_response(response)
            
            if not isinstance(items, list):
                log.warning("LLM returned non-list response, wrapping in list")
                items = [items] if items else []
            
            # Add canonical IDs to items
            doc_id = hashlib.sha1(text[:1000].encode()).hexdigest()[:12]
            for i, item in enumerate(items):
                item['id'] = f"ITEM_{doc_id}_{i:03d}"
                item['urls'] = []  # Initialize empty URLs list
            
            log.info(f"‚ú® Extracted {len(items)} agenda items")
            return items
            
        except Exception as e:
            log.error(f"‚ùå Failed to extract agenda items with LLM: {e}")
            return []

    def _extract_hyperlinks_pymupdf(self, pdf_path: Path) -> List[Dict]:
        """Extract hyperlinks using PyMuPDF."""
        hyperlinks = []
        
        try:
            pdf_document = fitz.open(str(pdf_path))
            
            for page_num in range(len(pdf_document)):
                page = pdf_document[page_num]
                links = page.get_links()
                
                for link in links:
                    if link.get('uri'):  # External URL
                        rect = fitz.Rect(link['from'])
                        link_text = page.get_text(clip=rect).strip()
                        link_text = ' '.join(link_text.split())
                        
                        hyperlinks.append({
                            'url': link['uri'],
                            'text': link_text or 'Click here',
                            'page': page_num + 1,
                            'rect': {
                                'x0': link['from'].x0,
                                'y0': link['from'].y0,
                                'x1': link['from'].x1,
                                'y1': link['from'].y1
                            }
                        })
            
            pdf_document.close()
            log.info(f"üîó Extracted {len(hyperlinks)} hyperlinks")
            
        except Exception as e:
            log.error(f"Failed to extract hyperlinks: {e}")
        
        return hyperlinks

    def _associate_urls_with_items(self, items: List[Dict], links: List[Dict]) -> None:
        """Associate extracted URLs with their corresponding agenda items."""
        for link in links:
            link_text = link.get('text', '').upper()
            
            # Try to match link to agenda items
            for item in items:
                item_code = item.get('item_code', '')
                if item_code and item_code in link_text:
                    item['urls'].append({
                        'url': link['url'],
                        'text': link['text'],
                        'page': link['page']
                    })
                    log.debug(f"üîó Associated URL with item {item_code}")
                    break

    def _extract_meeting_info(self, pdf_path: Path, full_text: str) -> Dict:
        """Extract meeting information from the agenda."""
        meeting_info = {
            'date': 'N/A',
            'time': 'N/A',
            'location': 'N/A'
        }
        
        # Try to extract date from filename
        date_match = re.search(r'(\d{2})\.(\d{2})\.(\d{4})', pdf_path.name)
        if date_match:
            month, day, year = date_match.groups()
            meeting_info['date'] = f"{month}.{day}.{year}"
        
        # Try to extract time and location from text
        lines = full_text.split('\n')[:50]
        for line in lines:
            line = line.strip()
            
            # Look for time patterns
            time_match = re.search(r'(\d{1,2}:\d{2}\s*[AP]M)', line, re.IGNORECASE)
            if time_match and meeting_info['time'] == 'N/A':
                meeting_info['time'] = time_match.group(1)
            
            # Look for location
            if 'city hall' in line.lower() or 'commission chamber' in line.lower():
                meeting_info['location'] = line[:100]
        
        return meeting_info

    def _save_as_markdown(self, pdf_path: Path, agenda_data: Dict) -> None:
        """Save agenda as enriched markdown for GraphRAG."""
        meeting_info = agenda_data.get('meeting_info', {})
        meeting_date = meeting_info.get('date', 'unknown')
        
        # If date not found, try to extract from filename
        if meeting_date == 'N/A' or meeting_date == 'unknown':
            filename_match = re.search(r'(\d{1,2})\.(\d{1,2})\.(\d{4})', pdf_path.name)
            if filename_match:
                month = filename_match.group(1).zfill(2)
                day = filename_match.group(2).zfill(2)
                year = filename_match.group(3)
                meeting_date = f"{month}.{day}.{year}"
        
        # Build comprehensive header
        header = self._build_agenda_header(agenda_data)
        
        # Add detailed agenda items section
        items_section = self._build_agenda_items_section(agenda_data)
        
        # Combine with full text
        full_content = header + items_section + "\n\n# FULL AGENDA TEXT\n\n" + agenda_data.get('full_text', '')
        
        # Save markdown
        meeting_date_filename = meeting_date.replace('.', '_') if meeting_date != 'unknown' else 'unknown'
        md_filename = f"agenda_{meeting_date_filename}.md"
        md_path = self.output_dir / md_filename
        
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(full_content)
        
        log.info(f"üìù Saved agenda markdown to: {md_path}")

    def _build_agenda_header(self, agenda_data: Dict) -> str:
        """Build comprehensive agenda header."""
        meeting_info = agenda_data.get('meeting_info', {})
        agenda_items = agenda_data.get('agenda_items', [])
        
        all_item_codes = [item.get('item_code', '') for item in agenda_items if item.get('item_code')]
        
        header = f"""---
DOCUMENT METADATA AND CONTEXT
=============================

**DOCUMENT IDENTIFICATION:**
- Document Type: AGENDA
- Meeting Date: {meeting_info.get('date', 'N/A')}
- Meeting Time: {meeting_info.get('time', 'N/A')}
- Meeting Location: {meeting_info.get('location', 'N/A')}

**ENTITIES IN THIS DOCUMENT:**
{self._format_agenda_entities(all_item_codes)}

**SEARCHABLE IDENTIFIERS:**
- DOCUMENT_TYPE: AGENDA
{self._format_item_identifiers(all_item_codes)}

---

"""
        return header

    def _format_agenda_entities(self, item_codes: List[str]) -> str:
        """Format agenda item entities."""
        lines = []
        for code in item_codes[:10]:
            lines.append(f"- AGENDA_ITEM: {code}")
        if len(item_codes) > 10:
            lines.append(f"- ... and {len(item_codes) - 10} more items")
        return '\n'.join(lines)

    def _format_item_identifiers(self, item_codes: List[str]) -> str:
        """Format item identifiers."""
        lines = []
        for code in item_codes:
            lines.append(f"- AGENDA_ITEM: {code}")
        return '\n'.join(lines)

    def _build_agenda_items_section(self, agenda_data: Dict) -> str:
        """Build agenda items section."""
        lines = ["## AGENDA ITEMS QUICK REFERENCE\n"]
        
        for item in agenda_data.get('agenda_items', []):
            item_code = item.get('item_code', 'UNKNOWN')
            lines.append(f"### Agenda Item {item_code}")
            lines.append(f"**Title:** {item.get('title', 'N/A')}")
            lines.append(f"**Section:** {item.get('section_name', 'N/A')}")
            if item.get('document_reference'):
                lines.append(f"**Reference:** {item.get('document_reference')}")
            lines.append(f"\n**What is Item {item_code}?**")
            lines.append(f"Item {item_code} is '{item.get('title', 'N/A')}'")
            
            # Add URLs if available
            urls = item.get('urls', [])
            if urls:
                lines.append(f"\n**Related Documents:**")
                for url in urls:
                    lines.append(f"- [{url.get('text', 'Document')}]({url.get('url')})")
            
            lines.append("")
        
        return '\n'.join(lines)

    def _generate_doc_id(self, pdf_path: Path) -> str:
        """Generate canonical document ID."""
        return f"DOC_{hashlib.sha1(str(pdf_path.absolute()).encode()).hexdigest()[:12]}"

    def _get_file_hash(self, file_path: Path) -> str:
        """Get hash of file for caching."""
        with open(file_path, 'rb') as f:
            return hashlib.md5(f.read()).hexdigest()


================================================================================


################################################################################
# File: scripts/graph_rag_stages/common/cosmos_client.py
################################################################################

# File: scripts/graph_rag_stages/common/cosmos_client.py

"""
Azure Cosmos DB Gremlin client for city clerk graph database.
Provides async operations for graph manipulation.
"""

from __future__ import annotations
import asyncio
import logging
from typing import Any, Dict, List, Optional, Union
import os
from gremlin_python.driver import client, serializer
from gremlin_python.driver.driver_remote_connection import DriverRemoteConnection
from gremlin_python.structure.graph import Graph
from dotenv import load_dotenv
import json

load_dotenv()

log = logging.getLogger(__name__)


class CosmosGraphClient:
    """Async client for Azure Cosmos DB Gremlin API."""
    
    def __init__(self, 
                 endpoint: Optional[str] = None,
                 key: Optional[str] = None,
                 database: Optional[str] = None,
                 container: Optional[str] = None,
                 partition_value: str = "demo"):
        """Initialize Cosmos DB client."""
        self.endpoint = endpoint or os.getenv("COSMOS_ENDPOINT")
        self.key = key or os.getenv("COSMOS_KEY")
        self.database = database or os.getenv("COSMOS_DATABASE", "cgGraph")
        self.container = container or os.getenv("COSMOS_CONTAINER", "cityClerk")
        self.partition_value = partition_value
        
        if not all([self.endpoint, self.key, self.database, self.container]):
            raise ValueError("Missing required Cosmos DB configuration")
        
        self._client = None
        self._loop = None
    
    async def connect(self) -> None:
        """Establish connection to Cosmos DB."""
        try:
            self._loop = asyncio.get_running_loop()
            
            self._client = client.Client(
                f"{self.endpoint}/gremlin",
                "g",
                username=f"/dbs/{self.database}/colls/{self.container}",
                password=self.key,
                message_serializer=serializer.GraphSONSerializersV2d0()
            )
            log.info(f"‚úÖ Connected to Cosmos DB: {self.database}/{self.container}")
        except Exception as e:
            log.error(f"‚ùå Failed to connect to Cosmos DB: {e}")
            raise
    
    async def _execute_query(self, query: str, bindings: Optional[Dict] = None) -> List[Any]:
        """Execute a Gremlin query asynchronously."""
        if not self._client:
            await self.connect()
        
        try:
            loop = asyncio.get_running_loop()
            
            future = loop.run_in_executor(
                None,
                lambda: self._client.submit(query, bindings or {})
            )
            callback = await future
            
            results = []
            for result in callback:
                results.extend(result)
            
            return results
        except Exception as e:
            log.error(f"Query execution failed: {query[:100]}... Error: {e}")
            raise
    
    async def clear_graph(self) -> None:
        """Clear all vertices and edges from the graph."""
        log.warning("üóëÔ∏è  Clearing entire graph...")
        try:
            await self._execute_query("g.V().drop()")
            log.info("‚úÖ Graph cleared successfully")
        except Exception as e:
            log.error(f"Failed to clear graph: {e}")
            raise
    
    async def create_vertex(self, 
                          label: str,
                          vertex_id: str,
                          properties: Dict[str, Any],
                          update_if_exists: bool = True) -> None:
        """Create a vertex with properties, optionally updating if exists."""
        
        if await self.vertex_exists(vertex_id):
            if update_if_exists:
                await self.update_vertex(vertex_id, properties)
                log.info(f"Updated existing vertex: {vertex_id}")
            else:
                log.info(f"Vertex already exists, skipping: {vertex_id}")
            return
        
        prop_chain = ""
        for key, value in properties.items():
            if value is not None:
                if isinstance(value, bool):
                    prop_chain += f".property('{key}', {str(value).lower()})"
                elif isinstance(value, (int, float)):
                    prop_chain += f".property('{key}', {value})"
                elif isinstance(value, list):
                    json_val = json.dumps(value).replace("'", "\\'")
                    prop_chain += f".property('{key}', '{json_val}')"
                else:
                    escaped_val = str(value).replace("'", "\\'").replace('"', '\\"')
                    prop_chain += f".property('{key}', '{escaped_val}')"
        
        prop_chain += f".property('partitionKey', '{self.partition_value}')"
        
        query = f"g.addV('{label}').property('id', '{vertex_id}'){prop_chain}"
        
        await self._execute_query(query)

    async def update_vertex(self, vertex_id: str, properties: Dict[str, Any]) -> None:
        """Update properties of an existing vertex."""
        prop_chain = ""
        for key, value in properties.items():
            if value is not None:
                if isinstance(value, bool):
                    prop_chain += f".property('{key}', {str(value).lower()})"
                elif isinstance(value, (int, float)):
                    prop_chain += f".property('{key}', {value})"
                elif isinstance(value, list):
                    json_val = json.dumps(value).replace("'", "\\'")
                    prop_chain += f".property('{key}', '{json_val}')"
                else:
                    escaped_val = str(value).replace("'", "\\'").replace('"', '\\"')
                    prop_chain += f".property('{key}', '{escaped_val}')"
        
        query = f"g.V('{vertex_id}'){prop_chain}"
        
        try:
            await self._execute_query(query)
            log.info(f"Updated vertex {vertex_id}")
        except Exception as e:
            log.error(f"Failed to update vertex {vertex_id}: {e}")
            raise

    async def upsert_vertex(self, 
                           label: str,
                           vertex_id: str,
                           properties: Dict[str, Any]) -> bool:
        """Create or update a vertex. Returns True if created, False if updated."""
        if await self.vertex_exists(vertex_id):
            await self.update_vertex(vertex_id, properties)
            return False
        else:
            await self.create_vertex(label, vertex_id, properties)
            return True

    async def create_edge(self,
                         from_id: str,
                         to_id: str,
                         edge_type: str,
                         properties: Optional[Dict[str, Any]] = None) -> None:
        """Create an edge between two vertices."""
        prop_chain = ""
        if properties:
            for key, value in properties.items():
                if value is not None:
                    if isinstance(value, bool):
                        prop_chain += f".property('{key}', {str(value).lower()})"
                    elif isinstance(value, (int, float)):
                        prop_chain += f".property('{key}', {value})"
                    else:
                        escaped_val = str(value).replace("'", "\\'")
                        prop_chain += f".property('{key}', '{escaped_val}')"
        
        query = f"g.V('{from_id}').addE('{edge_type}').to(g.V('{to_id}')){prop_chain}"
        
        try:
            await self._execute_query(query)
        except Exception as e:
            log.error(f"Failed to create edge {from_id} -> {to_id}: {e}")
            raise
    
    async def create_edge_if_not_exists(self,
                                       from_id: str,
                                       to_id: str,
                                       edge_type: str,
                                       properties: Optional[Dict[str, Any]] = None) -> bool:
        """Create an edge if it doesn't already exist. Returns True if created."""
        check_query = f"g.V('{from_id}').outE('{edge_type}').where(inV().hasId('{to_id}')).count()"
        
        try:
            result = await self._execute_query(check_query)
            exists = result[0] > 0 if result else False
            
            if not exists:
                await self.create_edge(from_id, to_id, edge_type, properties)
                return True
            else:
                log.debug(f"Edge already exists: {from_id} -[{edge_type}]-> {to_id}")
                return False
        except Exception as e:
            log.error(f"Failed to check/create edge: {e}")
            raise
    
    async def vertex_exists(self, vertex_id: str) -> bool:
        """Check if a vertex exists."""
        result = await self._execute_query(f"g.V('{vertex_id}').count()")
        return result[0] > 0 if result else False
    
    async def get_vertex(self, vertex_id: str) -> Optional[Dict]:
        """Get a vertex by ID."""
        result = await self._execute_query(f"g.V('{vertex_id}').valueMap(true)")
        return result[0] if result else None
    
    async def close(self) -> None:
        """Close the connection properly."""
        if self._client:
            try:
                self._client.close()
                self._client = None
                log.info("Connection closed")
            except Exception as e:
                log.warning(f"Error during client close: {e}")
    
    async def __aenter__(self):
        await self.connect()
        return self
    
    async def __aexit__(self, *args):
        await self.close()


================================================================================


################################################################################
# File: scripts/graph_rag_stages/phase2_building/entity_deduplicator.py
################################################################################

# File: scripts/graph_rag_stages/phase2_building/entity_deduplicator.py

"""
Entity deduplication for GraphRAG output to improve graph quality.
"""

import logging
from pathlib import Path
from typing import Dict, List, Optional
import pandas as pd
import asyncio
from difflib import SequenceMatcher

log = logging.getLogger(__name__)

# Configuration presets
DEDUP_CONFIGS = {
    'aggressive': {
        'similarity_threshold': 0.75,
        'preserve_agenda_items': True,
        'min_combined_score': 0.75,
        'enable_partial_name_matching': True,
        'enable_token_matching': True,
        'enable_semantic_matching': True,
        'enable_graph_structure_matching': True,
        'enable_abbreviation_matching': True,
        'enable_role_based_matching': True,
    },
    'conservative': {
        'similarity_threshold': 0.9,
        'preserve_agenda_items': True,
        'min_combined_score': 0.85,
        'enable_partial_name_matching': True,
        'enable_token_matching': True,
        'enable_semantic_matching': True,
        'enable_graph_structure_matching': True,
        'enable_abbreviation_matching': True,
        'enable_role_based_matching': False,
    },
    'name_focused': {
        'similarity_threshold': 0.8,
        'preserve_agenda_items': True,
        'min_combined_score': 0.8,
        'enable_partial_name_matching': True,
        'enable_token_matching': True,
        'enable_semantic_matching': True,
        'enable_graph_structure_matching': True,
        'enable_abbreviation_matching': True,
        'enable_role_based_matching': True,
    }
}


class EntityDeduplicator:
    """Post-processes GraphRAG entities to remove duplicates and improve quality."""
    
    def __init__(self, output_dir: Path, config_name: str = 'conservative', custom_config: Dict = None):
        """
        Initialize the deduplicator.

        Args:
            output_dir: Path to GraphRAG output directory.
            config_name: The name of the configuration preset to use ('conservative', 'aggressive', 'name_focused').
            custom_config: A dictionary to override default settings.
        """
        self.output_dir = Path(output_dir)
        self.graphrag_root = output_dir  # For compatibility
        self.dedup_dir = self.output_dir / "deduplicated"
        
        # CORRECTED: Look up config dict from name, allow overrides
        self.config = DEDUP_CONFIGS.get(config_name, DEDUP_CONFIGS['conservative']).copy()
        if custom_config:
            self.config.update(custom_config)
        
        log.info(f"EntityDeduplicator initialized with config '{config_name}' (threshold: {self.config['similarity_threshold']})")

    async def run_deduplication(self, config_name: str = None) -> None:
        """Run entity deduplication with the specified configuration."""
        # Use the provided config_name or fall back to the one from constructor
        if config_name and config_name in DEDUP_CONFIGS:
            self.config = DEDUP_CONFIGS[config_name].copy()
        
        log.info(f"üîÑ Starting entity deduplication with config: {config_name or 'constructor default'}")
        
        # Ensure output directory exists
        self.dedup_dir.mkdir(exist_ok=True)
        
        # Load GraphRAG entities
        entities_df = self._load_entities()
        if entities_df is None or len(entities_df) == 0:
            log.warning("No entities found to deduplicate")
            return
        
        log.info(f"üìä Found {len(entities_df)} entities to process")
        
        # Perform deduplication
        deduplicated_df = await self._deduplicate_entities(entities_df, self.config)
        
        # Save deduplicated results
        self._save_deduplicated_entities(deduplicated_df)
        
        # Generate deduplication report
        self._generate_deduplication_report(entities_df, deduplicated_df, config_name or 'default')
        
        log.info(f"‚úÖ Entity deduplication completed")
        log.info(f"üìà Reduced {len(entities_df)} entities to {len(deduplicated_df)} entities")

    def _load_entities(self) -> Optional[pd.DataFrame]:
        """Load entities from GraphRAG output."""
        entities_file = self.output_dir / "create_final_entities.parquet"
        
        if not entities_file.exists():
            log.error(f"Entities file not found: {entities_file}")
            return None
        
        try:
            df = pd.read_parquet(entities_file)
            log.info(f"Loaded entities with columns: {list(df.columns)}")
            return df
        except Exception as e:
            log.error(f"Error loading entities file: {e}")
            return None

    async def _deduplicate_entities(self, entities_df: pd.DataFrame, config: Dict) -> pd.DataFrame:
        """Perform entity deduplication based on configuration."""
        log.info("üîç Analyzing entities for duplicates...")
        
        # Create a copy to work with
        df = entities_df.copy()
        
        # Find duplicate groups
        duplicate_groups = self._find_duplicate_groups(df, config)
        
        log.info(f"Found {len(duplicate_groups)} duplicate groups")
        
        # Merge duplicates within each group
        entities_to_remove = set()
        for group in duplicate_groups:
            if len(group) > 1:
                # Keep the first entity as the canonical one
                canonical_entity = group[0]
                duplicates = group[1:]
                
                # Mark duplicates for removal
                entities_to_remove.update(duplicates)
        
        # Remove duplicate entities
        df_deduplicated = df[~df.index.isin(entities_to_remove)].copy()
        
        return df_deduplicated

    def _find_duplicate_groups(self, df: pd.DataFrame, config: Dict) -> List[List[int]]:
        """Find groups of duplicate entities."""
        duplicate_groups = []
        processed_indices = set()
        
        for idx in df.index:
            if idx in processed_indices:
                continue
            
            # Find all entities similar to this one
            similar_indices = self._find_similar_entities(df, idx, config)
            
            if len(similar_indices) > 1:
                duplicate_groups.append(similar_indices)
                processed_indices.update(similar_indices)
            else:
                processed_indices.add(idx)
        
        return duplicate_groups

    def _find_similar_entities(self, df: pd.DataFrame, target_idx: int, config: Dict) -> List[int]:
        """Find entities similar to the target entity."""
        target_entity = df.loc[target_idx]
        similar_indices = [target_idx]
        
        target_name = str(target_entity.get('title', '')).strip().lower()
        
        for idx in df.index:
            if idx == target_idx:
                continue
            
            entity = df.loc[idx]
            
            # Calculate similarity
            similarity = self._calculate_entity_similarity(target_entity, entity)
            
            if similarity >= config['similarity_threshold']:
                similar_indices.append(idx)
        
        return similar_indices

    def _calculate_entity_similarity(self, entity1: pd.Series, entity2: pd.Series) -> float:
        """Calculate similarity between two entities."""
        name1 = str(entity1.get('title', '')).strip().lower()
        name2 = str(entity2.get('title', '')).strip().lower()
        
        # Name similarity (most important)
        name_sim = SequenceMatcher(None, name1, name2).ratio()
        
        return name_sim

    def _save_deduplicated_entities(self, deduplicated_df: pd.DataFrame) -> None:
        """Save deduplicated entities."""
        output_path = self.dedup_dir / "create_final_entities.parquet"
        deduplicated_df.to_parquet(output_path)
        
        log.info(f"‚úÖ Deduplicated entities saved to: {output_path}")

    def _generate_deduplication_report(self, original_df: pd.DataFrame, 
                                     deduplicated_df: pd.DataFrame, config_name: str) -> None:
        """Generate a report on the deduplication process."""
        report = {
            'config_used': config_name,
            'original_entity_count': len(original_df),
            'deduplicated_entity_count': len(deduplicated_df),
            'entities_removed': len(original_df) - len(deduplicated_df),
            'reduction_percentage': ((len(original_df) - len(deduplicated_df)) / len(original_df)) * 100
        }
        
        # Save report
        import json
        report_path = self.dedup_dir / "deduplication_report.json"
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        log.info(f"üìä Deduplication report saved to: {report_path}")
        log.info(f"üìà Reduction: {report['reduction_percentage']:.1f}% ({report['entities_removed']} entities removed)")


================================================================================


################################################################################
# File: scripts/graph_rag_stages/phase2_building/graphrag_indexer.py
################################################################################

# File: scripts/graph_rag_stages/phase2_building/graphrag_indexer.py

"""
Encapsulates the execution of the Microsoft GraphRAG indexing process.
"""

import subprocess
import logging
from pathlib import Path
from typing import Optional
import os

log = logging.getLogger(__name__)


class GraphRAGIndexer:
    """Handles the execution of Microsoft GraphRAG indexing."""
    
    def __init__(self):
        pass

    def run_indexing_process(self, graphrag_root: Path, verbose: bool = True, force: bool = False) -> None:
        """
        Executes the graphrag index command as a subprocess.
        
        Args:
            graphrag_root: Root directory for GraphRAG operations
            verbose: Whether to use verbose output
            force: Whether to force re-indexing by re-initializing
        """
        log.info(f"üîÑ Starting GraphRAG indexing process at root: {graphrag_root}")
        
        # Ensure the directory exists
        graphrag_root.mkdir(parents=True, exist_ok=True)
        
        # Initialize GraphRAG if needed or if forcing
        if force or not self._is_graphrag_initialized(graphrag_root):
            log.info("üìã Initializing GraphRAG configuration...")
            self._initialize_graphrag(graphrag_root)
        
        # Create the index command
        cmd = [
            "graphrag",
            "index",
            "--root", str(graphrag_root),
        ]

        if verbose:
            cmd.append("--verbose")

        log.info(f"üöÄ Executing command: {' '.join(cmd)}")
        
        # Change to the GraphRAG directory for execution
        original_cwd = os.getcwd()
        try:
            os.chdir(graphrag_root)
            
            # Run with live output streaming to the console
            process = subprocess.Popen(
                cmd, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.STDOUT, 
                text=True,
                cwd=str(graphrag_root)
            )
            
            # Stream output in real-time
            for line in iter(process.stdout.readline, ''):
                log.info(line.strip())
            
            process.wait()

            if process.returncode != 0:
                log.error(f"‚ùå GraphRAG indexing failed with exit code {process.returncode}")
                raise RuntimeError(f"GraphRAG indexing failed with exit code {process.returncode}")
            
            log.info("‚úÖ GraphRAG indexing completed successfully")
            
        finally:
            os.chdir(original_cwd)

    def _is_graphrag_initialized(self, graphrag_root: Path) -> bool:
        """Check if GraphRAG has been initialized in the given directory."""
        settings_file = graphrag_root / "settings.yaml"
        return settings_file.exists()

    def _initialize_graphrag(self, graphrag_root: Path) -> None:
        """Initialize GraphRAG in the given directory."""
        init_cmd = [
            "graphrag",
            "init",
            "--root", str(graphrag_root)
        ]
        
        log.info(f"üîß Initializing GraphRAG: {' '.join(init_cmd)}")
        
        try:
            result = subprocess.run(
                init_cmd,
                cwd=str(graphrag_root),
                check=True,
                capture_output=True,
                text=True
            )
            
            log.info("‚úÖ GraphRAG initialization completed")
            
            # Log output if verbose
            if result.stdout:
                log.debug(f"Init stdout: {result.stdout}")
            if result.stderr:
                log.debug(f"Init stderr: {result.stderr}")
                
        except subprocess.CalledProcessError as e:
            log.error(f"‚ùå GraphRAG initialization failed: {e}")
            if e.stdout:
                log.error(f"Stdout: {e.stdout}")
            if e.stderr:
                log.error(f"Stderr: {e.stderr}")
            raise

    def check_status(self, graphrag_root: Path) -> dict:
        """
        Check the status of GraphRAG indexing for the given directory.
        
        Args:
            graphrag_root: Root directory for GraphRAG operations
            
        Returns:
            Dictionary with status information
        """
        status = {
            'initialized': self._is_graphrag_initialized(graphrag_root),
            'output_exists': False,
            'entities_count': 0,
            'relationships_count': 0,
            'communities_count': 0
        }
        
        # Check for output files
        output_dir = graphrag_root / "output"
        if output_dir.exists():
            status['output_exists'] = True
            
            # Check for specific output files and count entities
            entities_file = output_dir / "create_final_entities.parquet"
            if entities_file.exists():
                try:
                    import pandas as pd
                    entities_df = pd.read_parquet(entities_file)
                    status['entities_count'] = len(entities_df)
                except Exception as e:
                    log.warning(f"Could not read entities file: {e}")
            
            # Check relationships
            relationships_file = output_dir / "create_final_relationships.parquet"
            if relationships_file.exists():
                try:
                    import pandas as pd
                    relationships_df = pd.read_parquet(relationships_file)
                    status['relationships_count'] = len(relationships_df)
                except Exception as e:
                    log.warning(f"Could not read relationships file: {e}")
            
            # Check communities
            communities_file = output_dir / "create_final_communities.parquet"
            if communities_file.exists():
                try:
                    import pandas as pd
                    communities_df = pd.read_parquet(communities_file)
                    status['communities_count'] = len(communities_df)
                except Exception as e:
                    log.warning(f"Could not read communities file: {e}")
        
        return status

    def cleanup_cache(self, graphrag_root: Path) -> None:
        """
        Clean up GraphRAG cache to force fresh indexing.
        
        Args:
            graphrag_root: Root directory for GraphRAG operations
        """
        log.info("üßπ Cleaning up GraphRAG cache")
        
        cache_dir = graphrag_root / "cache"
        if cache_dir.exists():
            import shutil
            shutil.rmtree(cache_dir)
            log.info("‚úÖ Cache directory removed")
        else:
            log.info("‚ÑπÔ∏è No cache directory found")

    def get_indexing_logs(self, graphrag_root: Path, lines: int = 50) -> list:
        """
        Get the last N lines from GraphRAG indexing logs.
        
        Args:
            graphrag_root: Root directory for GraphRAG operations
            lines: Number of lines to retrieve
            
        Returns:
            List of log lines
        """
        logs_dir = graphrag_root / "output" / "indexing-engine"
        log_files = []
        
        if logs_dir.exists():
            log_files = list(logs_dir.glob("*.log"))
        
        if not log_files:
            return ["No log files found"]
        
        # Get the most recent log file
        latest_log = max(log_files, key=lambda f: f.stat().st_mtime)
        
        try:
            with open(latest_log, 'r') as f:
                all_lines = f.readlines()
                return all_lines[-lines:] if len(all_lines) > lines else all_lines
        except Exception as e:
            return [f"Error reading log file: {e}"]


================================================================================


################################################################################
# File: scripts/RAG_stages/llm_enrich.py
################################################################################

# File: scripts/RAG_stages/llm_enrich.py

#!/usr/bin/env python3
"""
Stage 4 ‚Äî LLM metadata enrichment with concurrent API calls.
"""
from __future__ import annotations
import json, logging, pathlib, re, os
from textwrap import dedent
from typing import Any, Dict, List
import asyncio
from concurrent.futures import ThreadPoolExecutor
import aiofiles
from groq import Groq

# ‚îÄ‚îÄ‚îÄ minimal shared helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _authors(val:Any)->List[str]:
    if val is None: return []
    if isinstance(val,list): return [str(a).strip() for a in val if a]
    return re.split(r"\s*,\s*|\s+and\s+", str(val).strip())

META_FIELDS_CORE = [
    "document_type", "title", "date", "year", "month", "day",
    "mayor", "vice_mayor", "commissioners",
    "city_attorney", "city_manager", "city_clerk", "public_works_director",
    "agenda", "keywords"
]
EXTRA_MD_FIELDS = ["peer_reviewed","open_access","license","open_access_status"]
META_FIELDS     = META_FIELDS_CORE+EXTRA_MD_FIELDS
_DEF_META_TEMPLATE = {**{k:None for k in META_FIELDS_CORE},
                      "doc_type":"scientific paper",
                      "authors":[], "keywords":[], "research_topics":[],
                      "peer_reviewed":None,"open_access":None,
                      "license":None,"open_access_status":None}

def merge_meta(*sources:Dict[str,Any])->Dict[str,Any]:
    merged=_DEF_META_TEMPLATE.copy()
    for src in sources:
        for k in META_FIELDS:
            v=src.get(k)
            if v not in (None,"",[],{}): merged[k]=v
    merged["authors"]=_authors(merged["authors"])
    merged["keywords"]=merged["keywords"] or []
    merged["research_topics"]=merged["research_topics"] or []
    return merged
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

from dotenv import load_dotenv
load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
MODEL          = "gpt-4.1-mini-2025-04-14"
log            = logging.getLogger(__name__)

def _first_words(txt:str,n:int=3000)->str: return " ".join(txt.split()[:n])

def _gpt(text:str)->Dict[str,Any]:
    if not OPENAI_API_KEY: return text
    cli=Groq()
    sys_prompt = dedent(f"""
        Extract all metadata fields from this city clerk document. Return ONE JSON object with these fields:
        - document_type: must be one of [Resolution, Ordinance, Proclamation, Contract, Meeting Minutes, Agenda]
        - title: the document title
        - date: full date string as found in document
        - year: numeric year (YYYY)
        - month: numeric month (1-12)
        - day: numeric day of month
        - mayor: name only (e.g., "John Smith") - single person
        - vice_mayor: name only (e.g., "Jane Doe") - single person
        - commissioners: array of commissioner names only (e.g., ["Robert Brown", "Sarah Johnson", "Michael Davis"])
        - city_attorney: name only (e.g., "Emily Wilson")
        - city_manager: name only
        - city_clerk: name only
        - public_works_director: name only
        - agenda: agenda items or meeting topics if present
        - keywords: array of relevant keywords or topics (e.g., ["budget", "zoning", "infrastructure"])
    """)
    rsp=cli.chat.completions.create(
        model="meta-llama/llama-4-maverick-17b-128e-instruct",
        temperature=0,
        max_completion_tokens=8192,
        top_p=1,
        stream=False,
        stop=None,
        messages=[{"role":"system","content":sys_prompt},
                  {"role":"user","content":text}])
    raw=rsp.choices[0].message.content
    m=re.search(r"{[\s\S]*}",raw)
    return json.loads(m.group(0) if m else "{}")

# Async version of GPT call
async def _gpt_async(text: str, semaphore: asyncio.Semaphore) -> Dict[str, Any]:
    """Async GPT call with rate limiting."""
    if not OPENAI_API_KEY:
        return {}
    
    async with semaphore:  # Rate limiting
        loop = asyncio.get_event_loop()
        # Run synchronous OpenAI call in thread pool
        return await loop.run_in_executor(None, _gpt, text)

async def enrich_async(json_path: pathlib.Path, semaphore: asyncio.Semaphore) -> None:
    """Async version of enrich."""
    # Read file asynchronously
    async with aiofiles.open(json_path, 'r') as f:
        content = await f.read()
        data = json.loads(content)
    
    # Reconstruct body text
    full = " ".join(
        el.get("text", "") for sec in data["sections"] 
        for el in sec.get("elements", [])
    )
    
    # Make async GPT call
    new_meta = await _gpt_async(_first_words(full), semaphore)
    
    # Merge metadata
    data.update(new_meta)
    
    # Write back asynchronously
    async with aiofiles.open(json_path, 'w') as f:
        await f.write(json.dumps(data, indent=2, ensure_ascii=False))
    
    log.info("‚úì metadata enriched ‚Üí %s", json_path.name)

async def enrich_batch_async(
    json_paths: List[pathlib.Path],
    max_concurrent: int = 10
) -> None:
    """Enrich multiple documents concurrently with rate limiting."""
    semaphore = asyncio.Semaphore(max_concurrent)
    
    tasks = [enrich_async(path, semaphore) for path in json_paths]
    
    from tqdm.asyncio import tqdm_asyncio
    await tqdm_asyncio.gather(*tasks, desc="Enriching metadata")

# Keep original interface for compatibility
def enrich(json_path: pathlib.Path) -> None:
    """Original synchronous interface."""
    data = json.loads(json_path.read_text())
    full = " ".join(
        el.get("text", "") for sec in data["sections"] 
        for el in sec.get("elements", [])
    )
    new_meta = _gpt(_first_words(full))
    data.update(new_meta)
    json_path.write_text(json.dumps(data, indent=2, ensure_ascii=False), 'utf-8')
    log.info("‚úì metadata enriched ‚Üí %s", json_path.name)

if __name__ == "__main__":
    import argparse, logging
    logging.basicConfig(level=logging.INFO, format="%(message)s")
    p=argparse.ArgumentParser(); p.add_argument("json",type=pathlib.Path)
    enrich(p.parse_args().json)


================================================================================


################################################################################
# File: scripts/graph_rag_stages/phase2_building/__init__.py
################################################################################

# File: scripts/graph_rag_stages/phase2_building/__init__.py

"""
Graph Building Module

This module handles two parallel graph building approaches:
1. Custom graph building in Cosmos DB (from original graph_stages)
2. GraphRAG indexing pipeline (from original microsoft_framework)

Components:
- Custom graph builder for Cosmos DB
- GraphRAG adapter for data preparation
- GraphRAG indexer for Microsoft GraphRAG
- Entity deduplication for enhanced results
"""

from .custom_graph_builder import CustomGraphBuilder
from .graphrag_adapter import GraphRAGAdapter
from .graphrag_indexer import GraphRAGIndexer
from .entity_deduplicator import EntityDeduplicator

import logging
from pathlib import Path
from typing import Optional

log = logging.getLogger(__name__)

async def run_custom_graph_pipeline(
    markdown_source_dir: Path,
    cosmos_config: Optional[dict] = None
) -> None:
    """
    Run the custom graph building pipeline to populate Cosmos DB.
    
    Args:
        markdown_source_dir: Directory containing enriched markdown files
        cosmos_config: Optional Cosmos DB configuration
    """
    log.info("üîó Starting Custom Graph Building Pipeline")
    
    try:
        builder = CustomGraphBuilder(cosmos_config)
        await builder.build_graph_from_markdown(markdown_source_dir)
        log.info("‚úÖ Custom graph building completed")
    except Exception as e:
        log.error(f"‚ùå Custom graph building failed: {e}")
        raise

async def run_graphrag_indexing_pipeline(
    markdown_source_dir: Path,
    graphrag_input_dir: Path,
    force_reindex: bool = False,
    run_deduplication: bool = True,
    dedup_config_name: str = 'conservative'
) -> None:
    """
    Run the GraphRAG indexing pipeline.
    
    Args:
        markdown_source_dir: Directory containing enriched markdown files
        graphrag_input_dir: GraphRAG working directory
        force_reindex: Whether to force reindexing
        run_deduplication: Whether to run entity deduplication
        dedup_config_name: Deduplication configuration to use
    """
    log.info("üìä Starting GraphRAG Indexing Pipeline")
    
    try:
        # Step 1: Prepare data for GraphRAG
        log.info("üìã Step 1: Preparing data for GraphRAG...")
        adapter = GraphRAGAdapter()
        csv_path = adapter.create_graphrag_input_csv(markdown_source_dir, graphrag_input_dir)
        
        if csv_path is None:
            raise RuntimeError("Failed to create GraphRAG input CSV")
        
        # Validate the input data
        if not adapter.validate_input_data(csv_path):
            raise RuntimeError("GraphRAG input data validation failed")
        
        # Create GraphRAG settings if they don't exist
        settings_file = graphrag_input_dir / "settings.yaml"
        if not settings_file.exists():
            log.info("üìù Creating GraphRAG settings file...")
            adapter.create_graphrag_settings(graphrag_input_dir)
        
        # Step 2: Run GraphRAG indexing
        log.info("‚öôÔ∏è Step 2: Running GraphRAG indexing...")
        indexer = GraphRAGIndexer()
        indexer.run_indexing_process(graphrag_input_dir, verbose=True, force=force_reindex)
        
        # Step 3: Entity deduplication (optional)
        if run_deduplication:
            log.info("üîÑ Step 3: Running entity deduplication...")
            deduplicator = EntityDeduplicator(graphrag_input_dir)
            await deduplicator.run_deduplication(dedup_config_name)
        else:
            log.info("‚è≠Ô∏è Skipping entity deduplication")
        
        log.info("‚úÖ GraphRAG indexing pipeline completed")
        
        # Log final statistics
        indexer_stats = indexer.check_status(graphrag_input_dir)
        log.info(f"üìä Final stats: {indexer_stats['entities_count']} entities, "
                f"{indexer_stats['relationships_count']} relationships, "
                f"{indexer_stats['communities_count']} communities")
        
    except Exception as e:
        log.error(f"‚ùå GraphRAG indexing pipeline failed: {e}")
        raise

__all__ = [
    'CustomGraphBuilder',
    'GraphRAGAdapter',
    'GraphRAGIndexer',
    'EntityDeduplicator',
    'run_custom_graph_pipeline',
    'run_graphrag_indexing_pipeline'
]


================================================================================


################################################################################
# File: scripts/graph_rag_stages/phase1_preprocessing/pdf_extractor.py
################################################################################

# File: scripts/graph_rag_stages/phase1_preprocessing/pdf_extractor.py

"""
Extract text from PDFs using Docling for accurate OCR and text extraction.
"""
from pathlib import Path
from typing import Tuple, List, Dict, Optional
import logging
import json
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import PdfPipelineOptions

log = logging.getLogger(__name__)

class PDFExtractor:
    """Extract text from PDFs using Docling for accurate OCR and text extraction."""
    
    def __init__(self, output_dir: Optional[Path] = None):
        """Initializes the PDF extractor with Docling."""
        if output_dir:
            self.output_dir = output_dir
        else:
            self.output_dir = Path.cwd() / "temp_extraction_output"
        
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.debug_dir = self.output_dir / "debug"
        self.debug_dir.mkdir(exist_ok=True)
        
        pipeline_options = PdfPipelineOptions(do_ocr=True, do_table_structure=True)
        self.converter = DocumentConverter(
            format_options={InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)}
        )

    def extract_text_from_pdf(self, pdf_path: Path) -> Tuple[str, List[Dict[str, any]]]:
        """
        Extracts text and page data from a PDF using Docling.
        """
        log.info(f"Extracting text from: {pdf_path.name}")
        
        try:
            result = self.converter.convert(str(pdf_path))
            doc = result.document
            full_text = doc.export_to_markdown() or ""

            pages = []
            if hasattr(doc, 'pages') and doc.pages:
                for page_num, page in enumerate(doc.pages, 1):
                    # Get page text using correct methods
                    page_text = ""
                    if hasattr(page, 'text'):
                        page_text = page.text
                    elif hasattr(page, 'get_text'):
                        page_text = page.get_text()
                    else:
                        # Try to extract from elements
                        page_elements = []
                        if hasattr(page, 'elements'):
                            for element in page.elements:
                                if hasattr(element, 'text'):
                                    page_elements.append(element.text)
                        page_text = "\n".join(page_elements)
                    
                    if page_text:
                        pages.append({'text': page_text, 'page_num': page_num})
            
            if not pages and full_text:
                pages = [{'text': full_text, 'page_num': 1}]

            self._save_debug_info(pdf_path, len(pages), len(full_text))

            log.info(f"‚úÖ Successfully extracted {len(pages)} pages from {pdf_path.name}")
            return full_text, pages

        except Exception as e:
            log.error(f"‚ùå Failed to extract text from {pdf_path.name}: {e}")
            return "", []

    def _save_debug_info(self, pdf_path: Path, num_pages: int, num_chars: int):
        """Saves a debug file with metadata about the extraction process."""
        debug_info = {
            'file': pdf_path.name,
            'total_pages': num_pages,
            'total_characters': num_chars,
        }
        debug_file = self.debug_dir / f"{pdf_path.stem}_extraction_debug.json"
        with open(debug_file, 'w', encoding='utf-8') as f:
            json.dump(debug_info, f, indent=2)


================================================================================


################################################################################
# File: scripts/graph_rag_stages/phase3_querying/__init__.py
################################################################################

# File: scripts/graph_rag_stages/phase3_querying/__init__.py

"""
Query and Response Module

This module handles the query processing and response generation for the unified pipeline.
It provides interfaces to both the custom graph (Cosmos DB) and GraphRAG index.

Components:
- Query router for determining the best query method
- Query engine for GraphRAG operations
- Response enhancer for improving answers
- Source tracker for provenance
- City Clerk specific implementations for UI compatibility
"""

from .query_engine import QueryEngine
from .query_router import QueryRouter
from .response_enhancer import ResponseEnhancer
from .source_tracker import SourceTracker

# City Clerk specific implementations (for UI compatibility)
from .city_clerk_query_engine import CityClerkQueryEngine
from .smart_query_router import SmartQueryRouter, QueryIntent

import logging
from pathlib import Path

log = logging.getLogger(__name__)

def setup_query_engine(graphrag_input_dir: Path) -> QueryEngine:
    """
    Setup and initialize the query engine.
    
    Args:
        graphrag_input_dir: GraphRAG working directory
        
    Returns:
        Initialized QueryEngine instance
    """
    log.info(f"üîß Setting up query engine with GraphRAG root: {graphrag_input_dir}")
    
    # Ensure the GraphRAG directory exists
    if not graphrag_input_dir.exists():
        log.warning(f"‚ö†Ô∏è GraphRAG directory does not exist: {graphrag_input_dir}")
        log.info("Creating GraphRAG directory...")
        graphrag_input_dir.mkdir(parents=True, exist_ok=True)
    
    # Initialize the query engine
    query_engine = QueryEngine(graphrag_input_dir)
    
    # Validate that GraphRAG output exists
    if query_engine.validate_graphrag_output():
        log.info("‚úÖ GraphRAG output validated, query engine ready")
        
        # Log system statistics
        stats = query_engine.get_system_stats()
        log.info(f"üìä System ready with {stats['entities_count']} entities, "
                f"{stats['relationships_count']} relationships, "
                f"{stats['communities_count']} communities")
    else:
        log.warning("‚ö†Ô∏è GraphRAG output validation failed, some features may not work")
        log.info("üí° Run the GraphRAG indexing pipeline first to generate the required output files")
    
    return query_engine

__all__ = [
    'QueryEngine',
    'QueryRouter', 
    'ResponseEnhancer',
    'SourceTracker',
    'setup_query_engine',
    # City Clerk specific classes
    'CityClerkQueryEngine',
    'SmartQueryRouter',
    'QueryIntent'
]


================================================================================


################################################################################
# File: config.py
################################################################################

# File: config.py

#!/usr/bin/env python3
"""
Configuration file for graph database visualization
Manages database credentials and connection settings
"""

import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Azure Cosmos DB Gremlin Configuration
COSMOS_ENDPOINT = os.getenv("COSMOS_ENDPOINT", "wss://aida-graph-db.gremlin.cosmos.azure.com:443")
COSMOS_KEY = os.getenv("COSMOS_KEY", "")  # This will be set from .env file
DATABASE = os.getenv("COSMOS_DATABASE", "cgGraph")
CONTAINER = os.getenv("COSMOS_CONTAINER", "cityClerk")
PARTITION_KEY = os.getenv("COSMOS_PARTITION_KEY", "partitionKey")
PARTITION_VALUE = os.getenv("COSMOS_PARTITION_VALUE", "demo")

def validate_config():
    """Validate that all required configuration is available"""
    required_vars = {
        "COSMOS_KEY": COSMOS_KEY,
        "COSMOS_ENDPOINT": COSMOS_ENDPOINT,
        "DATABASE": DATABASE,
        "CONTAINER": CONTAINER
    }
    
    missing_vars = [var for var, value in required_vars.items() if not value]
    
    if missing_vars:
        print("‚ùå Missing required configuration:")
        for var in missing_vars:
            print(f"   - {var}")
        print("\nüîß Please create a .env file with your credentials:")
        print("   COSMOS_KEY=your_actual_cosmos_key_here")
        print("   COSMOS_ENDPOINT=wss://aida-graph-db.gremlin.cosmos.azure.com:443")
        print("   COSMOS_DATABASE=cgGraph")
        print("   COSMOS_CONTAINER=cityClerk")
        return False
    
    return True

if __name__ == "__main__":
    print("üîß Configuration Check:")
    if validate_config():
        print("‚úÖ All configuration variables are set!")
    else:
        print("‚ùå Configuration incomplete!")


================================================================================


################################################################################
# File: scripts/graph_rag_stages/__init__.py
################################################################################

# File: scripts/graph_rag_stages/__init__.py

"""
Unified City Clerk GraphRAG Pipeline Package

This package contains the modular components for:
1. Data pre-processing and extraction
2. Graph building (both custom graph and GraphRAG indexing)
3. Query and response processing

Usage:
    python -m scripts.graph_rag_stages.main_pipeline
    
    Or import specific components:
    from scripts.graph_rag_stages.data_preprocessing import PDFExtractor
    from scripts.graph_rag_stages.query_and_response import QueryEngine
"""

__version__ = "1.0.0"
__author__ = "City Clerk Knowledge Graph Team"


================================================================================

