# Concatenated Project Code - Part 1 of 3
# Generated: 2025-05-30 00:16:01
# Root Directory: /Users/gianmariatroiani/Documents/knologi̊/graph_database
================================================================================

# Directory Structure
################################################################################
├── .env.backup (3.2KB, .backup)
├── .env.original (3.2KB, .original)
├── .gitignore (884.0B, no ext)
├── city_clerk_documents/
│   └── [EXCLUDED] 4 items: .DS_Store (excluded file), global (excluded dir), json (excluded dir)
│       ... and 1 more excluded items
├── clear_gremlin.py (1018.0B, .py)
├── config.py (1.7KB, .py)
├── debug_graph.py (9.2KB, .py)
├── graph_visualizer.py (22.6KB, .py)
├── relationOPENAI.py (8.7KB, .py)
├── requirements.txt (897.0B, .txt)
├── run_graph_pipeline.sh (1.3KB, .sh)
├── run_graph_visualizer.sh (3.6KB, .sh)
├── scripts/
│   ├── agenda_structure_pipeline.py (19.4KB, .py)
│   ├── clear_database.py (10.2KB, .py)
│   ├── graph_stages/
│   │   ├── __init__.py (263.0B, .py)
│   │   ├── agenda_graph_builder.py (19.8KB, .py)
│   │   ├── agenda_ontology_extractor.py (15.4KB, .py)
│   │   ├── agenda_parser.py (6.6KB, .py)
│   │   ├── agenda_pdf_extractor.py (16.2KB, .py)
│   │   ├── cosmos_db_client.py (12.3KB, .py)
│   │   └── entity_deduplicator.py (4.9KB, .py)
│   ├── pipeline_modular_optimized.py (12.7KB, .py)
│   ├── rag_local_web_app.py (18.4KB, .py)
│   └── stages/
│       ├── __init__.py (378.0B, .py)
│       ├── acceleration_utils.py (3.8KB, .py)
│       ├── chunk_text.py (18.6KB, .py)
│       ├── db_upsert.py (8.9KB, .py)
│       ├── embed_vectors.py (27.0KB, .py)
│       ├── extract_clean.py (21.7KB, .py)
│       └── llm_enrich.py (7.0KB, .py)
├── test_query.py (1.7KB, .py)
├── [EXCLUDED] 7 items: .DS_Store (excluded file), .env (excluded file), .git (excluded dir)
    ... and 4 more excluded items


================================================================================


# File Index - Which Files Are in Which Parts
################################################################################

## Part 1 (7 files):
  - scripts/stages/embed_vectors.py
  - scripts/rag_local_web_app.py
  - scripts/pipeline_modular_optimized.py
  - debug_graph.py
  - relationOPENAI.py
  - scripts/graph_stages/agenda_parser.py
  - requirements.txt

## Part 2 (9 files):
  - scripts/stages/extract_clean.py
  - scripts/stages/chunk_text.py
  - scripts/graph_stages/agenda_pdf_extractor.py
  - scripts/graph_stages/cosmos_db_client.py
  - scripts/stages/llm_enrich.py
  - scripts/stages/acceleration_utils.py
  - test_query.py
  - clear_gremlin.py
  - scripts/graph_stages/__init__.py

## Part 3 (8 files):
  - scripts/graph_stages/agenda_graph_builder.py
  - scripts/agenda_structure_pipeline.py
  - scripts/graph_stages/agenda_ontology_extractor.py
  - scripts/clear_database.py
  - scripts/stages/db_upsert.py
  - scripts/graph_stages/entity_deduplicator.py
  - config.py
  - scripts/stages/__init__.py


================================================================================


################################################################################
# File: scripts/stages/embed_vectors.py
################################################################################

# File: scripts/stages/embed_vectors.py

#!/usr/bin/env python3
"""
Stage 7 — Optimized embedding with rate limiting, deduplication, and conservative batching.
"""
from __future__ import annotations
import argparse, json, logging, os, sys, time
from datetime import datetime
from typing import Any, Dict, List, Optional, Set
import asyncio
import aiohttp
from dotenv import load_dotenv
from openai import OpenAI
from supabase import create_client
from tqdm import tqdm
from tqdm.asyncio import tqdm as async_tqdm
import hashlib

# Try to import tiktoken for accurate token counting
try:
    import tiktoken
    TIKTOKEN_AVAILABLE = True
except ImportError:
    TIKTOKEN_AVAILABLE = False
    logging.warning("tiktoken not available - using conservative token estimation")

load_dotenv()
SUPABASE_URL  = os.getenv("SUPABASE_URL")
SUPABASE_KEY  = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
OPENAI_API_KEY= os.getenv("OPENAI_API_KEY")
EMBEDDING_MODEL="text-embedding-ada-002"
MODEL_TOKEN_LIMIT=8192
TOKEN_GUARD=200

# 🎯 DYNAMIC BATCHING - Token limits with safety margins
MAX_BATCH_TOKENS = 7692  # Conservative limit (8192 - 500 safety margin)
MAX_CHUNK_TOKENS = 6000  # Individual chunk limit
MIN_BATCH_TOKENS = 100   # Minimum viable batch size

MAX_TOTAL_TOKENS=MAX_BATCH_TOKENS  # Use dynamic batch limit instead

# Conservative defaults for rate limiting
DEFAULT_BATCH_ROWS=200  # Reduced from 5000
DEFAULT_COMMIT_ROWS=10  # Reduced from 100
DEFAULT_MAX_CONCURRENT=3  # Reduced from 50
MAX_RETRIES=5
RETRY_DELAY=2
RATE_LIMIT_DELAY=0.3  # 300ms between API calls
MAX_CALLS_PER_MINUTE=150  # Conservative limit

openai_client=OpenAI(api_key=OPENAI_API_KEY)
logging.basicConfig(level=logging.INFO,
    format="%(asctime)s — %(levelname)s — %(message)s")
log=logging.getLogger(__name__)

# Global variables for rate limiting
call_timestamps = []
total_tokens_used = 0

class AsyncEmbedder:
    """Async embedding client with strict rate limiting and connection pooling."""
    
    def __init__(self, api_key: str, max_concurrent: int = DEFAULT_MAX_CONCURRENT):
        self.api_key = api_key
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.session = None
        self.call_count = 0
        self.start_time = time.time()
        
        # Initialize tiktoken encoder if available
        if TIKTOKEN_AVAILABLE:
            self.encoder = tiktoken.encoding_for_model(EMBEDDING_MODEL)
        else:
            self.encoder = None
    
    async def __aenter__(self):
        timeout = aiohttp.ClientTimeout(total=300)
        connector = aiohttp.TCPConnector(limit=20, limit_per_host=10)  # Reduced limits
        self.session = aiohttp.ClientSession(
            headers={"Authorization": f"Bearer {self.api_key}"},
            timeout=timeout,
            connector=connector
        )
        return self
    
    async def __aexit__(self, *args):
        await self.session.close()
    
    def count_tokens(self, text: str) -> int:
        """Count tokens accurately using tiktoken if available."""
        if self.encoder:
            return len(self.encoder.encode(text))
        else:
            # Conservative fallback
            return int(len(text.split()) * 0.75) + 50
    
    async def rate_limit_check(self):
        """Enforce rate limiting."""
        current_time = time.time()
        
        # Remove timestamps older than 1 minute
        call_timestamps[:] = [ts for ts in call_timestamps if current_time - ts < 60]
        
        # Check if we're approaching the rate limit
        if len(call_timestamps) >= MAX_CALLS_PER_MINUTE - 5:
            sleep_time = 60 - (current_time - call_timestamps[0])
            if sleep_time > 0:
                log.info(f"Rate limit protection: sleeping {sleep_time:.1f}s")
                await asyncio.sleep(sleep_time)
        
        # Always enforce minimum delay between calls
        await asyncio.sleep(RATE_LIMIT_DELAY)
        
        # Record this call
        call_timestamps.append(current_time)
    
    async def embed_batch_async(self, texts: List[str]) -> List[List[float]]:
        """Embed a batch of texts asynchronously with rate limiting and token validation."""
        global total_tokens_used
        
        async with self.semaphore:
            await self.rate_limit_check()
            
            # 🛡️ FINAL TOKEN VALIDATION - Last safety check before API call
            batch_tokens = sum(self.count_tokens(text) for text in texts)
            
            if batch_tokens > MAX_BATCH_TOKENS:
                error_msg = f"🚨 CRITICAL: Batch tokens {batch_tokens} exceed limit {MAX_BATCH_TOKENS}"
                log.error(error_msg)
                raise RuntimeError(error_msg)
            
            total_tokens_used += batch_tokens
            
            log.info(f"🎯 API call: {len(texts)} texts (~{batch_tokens} tokens) - WITHIN LIMITS ✅")
            log.info(f"📊 Total tokens used so far: ~{total_tokens_used}")
            
            for attempt in range(MAX_RETRIES):
                try:
                    async with self.session.post(
                        "https://api.openai.com/v1/embeddings",
                        json={
                            "model": EMBEDDING_MODEL,
                            "input": texts
                        }
                    ) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            self.call_count += 1
                            log.info(f"✅ API call successful: {len(texts)} embeddings generated")
                            return [item["embedding"] for item in data["data"]]
                        elif resp.status == 400:  # Bad request - likely token limit
                            error = await resp.text()
                            log.error(f"🚨 API error 400 (likely token limit): {error}")
                            log.error(f"🚨 Batch details: {len(texts)} texts, {batch_tokens} tokens")
                            raise RuntimeError(f"Token limit API error: {error}")
                        elif resp.status == 429:  # Rate limit error
                            error = await resp.text()
                            log.warning(f"Rate limit hit: {error}")
                            # Exponential backoff for rate limits
                            sleep_time = (2 ** attempt) * 2
                            log.info(f"Exponential backoff: sleeping {sleep_time}s")
                            await asyncio.sleep(sleep_time)
                        else:
                            error = await resp.text()
                            log.warning(f"API error {resp.status}: {error}")
                            await asyncio.sleep(RETRY_DELAY)
                except Exception as e:
                    log.warning(f"Attempt {attempt + 1} failed: {e}")
                    await asyncio.sleep(RETRY_DELAY * (attempt + 1))
            
            raise RuntimeError("Failed to embed batch after retries")

def deduplicate_chunks(chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Remove duplicate text chunks based on content hash."""
    seen_hashes: Set[str] = set()
    unique_chunks = []
    duplicates_removed = 0
    
    for chunk in chunks:
        text = chunk.get("text", "").strip()
        if not text:
            continue
            
        # Create hash of the text content
        text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()
        
        if text_hash not in seen_hashes:
            seen_hashes.add(text_hash)
            unique_chunks.append(chunk)
        else:
            duplicates_removed += 1
    
    if duplicates_removed > 0:
        log.info(f"Deduplication: removed {duplicates_removed} duplicate chunks")
    
    return unique_chunks

async def process_chunks_async(
    sb,
    chunks: List[Dict[str, Any]],
    embedder: AsyncEmbedder,
    commit_size: int = DEFAULT_COMMIT_ROWS
) -> int:
    """Process chunks with async embedding and batch updates."""
    # Deduplicate chunks first
    unique_chunks = deduplicate_chunks(chunks)
    log.info(f"📊 CHUNK PROCESSING PROGRESS:")
    log.info(f"   📄 Original chunks: {len(chunks)}")
    log.info(f"   📄 Unique chunks: {len(unique_chunks)}")
    if len(chunks) != len(unique_chunks):
        log.info(f"   🔄 Duplicates removed: {len(chunks) - len(unique_chunks)}")
    
    # Create token-safe slices
    slices = safe_slices(unique_chunks, commit_size)
    total_embedded = 0
    total_slices = len(slices)
    
    if total_slices == 0:
        log.warning(f"📊 NO SLICES TO PROCESS - All chunks were filtered out")
        return 0
    
    log.info(f"📊 EMBEDDING SLICES PROGRESS:")
    log.info(f"   🎯 Total slices to process: {total_slices}")
    log.info(f"   📄 Total chunks to embed: {sum(len(slice_data) for slice_data in slices)}")
    
    # Process each slice
    for i, slice_data in enumerate(slices):
        slice_num = i + 1
        slice_progress = (slice_num / total_slices * 100)
        
        log.info(f"📊 SLICE {slice_num}/{total_slices} ({slice_progress:.1f}%):")
        log.info(f"   📄 Processing {len(slice_data)} chunks in this slice")
        
        texts = [row["text"] for row in slice_data]
        
        try:
            # Get embeddings asynchronously
            log.info(f"   🔄 Calling OpenAI API for {len(texts)} embeddings...")
            embeddings = await embedder.embed_batch_async(texts)
            log.info(f"   ✅ Received {len(embeddings)} embeddings from API")
            
            # Update database in batch
            log.info(f"   💾 Updating database for {len(slice_data)} chunks...")
            update_tasks = []
            for row, emb in zip(slice_data, embeddings):
                # ✅ GUARANTEED SKIP LOGIC - Layer 3: Final update verification
                def update_with_verification(r, e):
                    # Final check before updating
                    final_check = sb.table("documents_chunks").select("embedding")\
                        .eq("id", r["id"]).execute()
                    
                    if final_check.data and final_check.data[0].get("embedding"):
                        log.info(f"🛡️ Chunk {r['id']} already has embedding - SKIPPING update")
                        return {"skipped": True}
                    
                    # Safe to update
                    return sb.table("documents_chunks")\
                        .update({"embedding": e})\
                        .eq("id", r["id"])\
                        .execute()
                
                # Use thread pool for database updates
                loop = asyncio.get_event_loop()
                task = loop.run_in_executor(None, update_with_verification, row, emb)
                update_tasks.append(task)
            
            # Wait for all updates
            log.info(f"   ⏳ Waiting for {len(update_tasks)} database updates...")
            results = await asyncio.gather(*update_tasks, return_exceptions=True)
            successful = sum(1 for r in results if not isinstance(r, Exception) and not (isinstance(r, dict) and r.get("skipped")))
            skipped = sum(1 for r in results if isinstance(r, dict) and r.get("skipped"))
            failed = sum(1 for r in results if isinstance(r, Exception))
            total_embedded += successful
            
            # 📊 SLICE COMPLETION PROGRESS
            log.info(f"📊 SLICE {slice_num} COMPLETE:")
            log.info(f"   ✅ Successful updates: {successful}")
            if skipped > 0:
                log.info(f"   ⏭️  Skipped (already embedded): {skipped}")
            if failed > 0:
                log.info(f"   ❌ Failed updates: {failed}")
            log.info(f"   📈 Total embedded so far: {total_embedded}")
            
        except Exception as e:
            log.error(f"❌ SLICE {slice_num} FAILED: {e}")
            log.error(f"   📄 Chunks in failed slice: {len(slice_data)}")
    
    # Final summary
    log.info(f"📊 CHUNK PROCESSING COMPLETE:")
    log.info(f"   ✅ Total chunks embedded: {total_embedded}")
    log.info(f"   📊 Slices processed: {total_slices}")
    log.info(f"   📈 Success rate: {(total_embedded/len(unique_chunks)*100):.1f}%" if unique_chunks else "0%")
    
    return total_embedded

async def main_async(batch_size: int = None, commit_size: int = None, max_concurrent: int = None):
    """Async main function for embedding."""
    global EMBEDDING_MODEL, MODEL_TOKEN_LIMIT, MAX_TOTAL_TOKENS, total_tokens_used
    
    # Use provided parameters or conservative defaults
    batch_size = batch_size or DEFAULT_BATCH_ROWS
    commit_size = commit_size or DEFAULT_COMMIT_ROWS
    max_concurrent = max_concurrent or DEFAULT_MAX_CONCURRENT
    
    log.info(f"Starting with conservative settings:")
    log.info(f"  Batch size: {batch_size}")
    log.info(f"  Commit size: {commit_size}")
    log.info(f"  Max concurrent: {max_concurrent}")
    log.info(f"  Rate limit delay: {RATE_LIMIT_DELAY}s")
    
    # 🎯 Dynamic batching status
    log.info(f"🎯 DYNAMIC BATCHING ENABLED:")
    log.info(f"   Max batch tokens: {MAX_BATCH_TOKENS} (with safety margin)")
    log.info(f"   Max chunk tokens: {MAX_CHUNK_TOKENS}")
    log.info(f"   Min batch tokens: {MIN_BATCH_TOKENS}")
    log.info(f"   ✅ GUARANTEED: No token limit API errors")
    
    # ✅ Check tiktoken availability for accurate token counting
    if not TIKTOKEN_AVAILABLE:
        log.warning("⚠️  tiktoken not available - using conservative token estimation")
        log.warning("⚠️  For accurate token counting, install: pip install tiktoken")
    else:
        log.info("✅ tiktoken available - using accurate token counting")
    
    if not OPENAI_API_KEY:
        log.error("OPENAI_API_KEY missing")
        sys.exit(1)
    
    sb = init_supabase()
    existing_count = count_processed_chunks(sb)
    log.info(f"Rows already embedded: {existing_count}")
    
    # ✅ GUARANTEED SKIP LOGIC - Status check
    total_chunks_res = sb.table("documents_chunks").select("id", count="exact")\
        .eq("chunking_strategy", "token_window").execute()
    total_chunks = total_chunks_res.count or 0
    
    pending_chunks = total_chunks - existing_count
    
    log.info("📊 Current status:")
    log.info(f"   Chunks already embedded: {existing_count}")
    log.info(f"   Chunks needing embedding: {pending_chunks}")
    log.info(f"   Total chunks: {total_chunks}")
    
    if pending_chunks == 0:
        log.info("✅ All chunks already have embeddings - nothing to do!")
        return
    
    async with AsyncEmbedder(OPENAI_API_KEY, max_concurrent) as embedder:
        loop, total = 0, 0
        
        # 📊 PROGRESS TRACKING - Initialize counters
        total_chunks_to_process = pending_chunks
        chunks_processed = 0
        chunks_skipped = 0
        
        log.info(f"📊 EMBEDDING PROGRESS TRACKING INITIALIZED:")
        log.info(f"   🎯 Total chunks to embed: {total_chunks_to_process}")
        log.info(f"   🎯 Starting embedding process...")
        
        while True:
            loop += 1
            rows = fetch_unprocessed_chunks(sb, limit=batch_size, offset=0)
            
            if not rows:
                log.info("✨ Done — no more rows.")
                break
            
            # 📊 PROGRESS: Show current status before processing
            remaining_chunks = total_chunks_to_process - chunks_processed
            progress_percent = (chunks_processed / total_chunks_to_process * 100) if total_chunks_to_process > 0 else 0
            
            log.info(f"📊 EMBEDDING PROGRESS - Loop {loop}:")
            log.info(f"   📄 Fetched: {len(rows)} chunks")
            log.info(f"   ✅ Processed: {chunks_processed}/{total_chunks_to_process} ({progress_percent:.1f}%)")
            log.info(f"   ⏳ Remaining: {remaining_chunks} chunks")
            if chunks_skipped > 0:
                log.info(f"   ⚠️  Skipped: {chunks_skipped} oversized chunks")
            
            # Process chunks asynchronously
            embedded = await process_chunks_async(sb, rows, embedder, commit_size)
            total += embedded
            chunks_processed += embedded
            
            # Update skipped count (this will be calculated in process_chunks_async)
            current_chunk_count = count_processed_chunks(sb)
            actual_embedded_this_loop = current_chunk_count - (existing_count + chunks_processed - embedded)
            
            # 📊 PROGRESS: Show results after processing
            final_progress_percent = (chunks_processed / total_chunks_to_process * 100) if total_chunks_to_process > 0 else 0
            log.info(f"📊 LOOP {loop} COMPLETE:")
            log.info(f"   ✅ This loop: {embedded} chunks embedded")
            log.info(f"   📈 Total progress: {chunks_processed}/{total_chunks_to_process} ({final_progress_percent:.1f}%)")
            log.info(f"   🔄 API calls made: {embedder.call_count}")
            log.info(f"   📊 Total tokens used: ~{total_tokens_used}")
            
            # Check if we're making progress or stuck
            if embedded == 0 and len(rows) > 0:
                chunks_skipped += len(rows)
                log.warning(f"⚠️  WARNING: No chunks embedded in this loop - all {len(rows)} chunks were skipped")
                log.warning(f"⚠️  Total skipped so far: {chunks_skipped} chunks")
                
                # Prevent infinite loop by limiting consecutive failed attempts
                if loop > 5 and embedded == 0:
                    log.error(f"🚨 STUCK: {loop} consecutive loops with no progress - stopping to prevent infinite loop")
                    break
    
    # Save report
    ts = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    report = {
        "timestamp": ts,
        "batch_size": batch_size,
        "commit": commit_size,
        "max_concurrent": max_concurrent,
        "total_embedded": total,
        "total_with_embeddings": count_processed_chunks(sb),
        "total_tokens_used": total_tokens_used,
        "api_calls_made": embedder.call_count if 'embedder' in locals() else 0
    }
    
    # Create reports directory if it doesn't exist
    reports_dir = "reports/embedding"
    os.makedirs(reports_dir, exist_ok=True)
    
    fname = f"{reports_dir}/supabase_embedding_report_{ts}.json"
    with open(fname, "w") as fp:
        json.dump(report, fp, indent=2)
    
    log.info(f"Report saved to {fname}")

async def main_async_cli():
    """CLI version with argument parsing."""
    global EMBEDDING_MODEL, MODEL_TOKEN_LIMIT, MAX_TOTAL_TOKENS
    
    ap = argparse.ArgumentParser()
    ap.add_argument("--batch-size", type=int, default=DEFAULT_BATCH_ROWS)
    ap.add_argument("--commit", type=int, default=DEFAULT_COMMIT_ROWS)
    ap.add_argument("--skip", type=int, default=0)
    ap.add_argument("--model", default=EMBEDDING_MODEL)
    ap.add_argument("--model-limit", type=int, default=MODEL_TOKEN_LIMIT)
    ap.add_argument("--max-concurrent", type=int, default=DEFAULT_MAX_CONCURRENT,
                   help="Maximum concurrent API calls (default: 3 for rate limiting)")
    args = ap.parse_args()
    
    EMBEDDING_MODEL = args.model
    MODEL_TOKEN_LIMIT = args.model_limit
    MAX_TOTAL_TOKENS = MODEL_TOKEN_LIMIT - TOKEN_GUARD
    
    await main_async(args.batch_size, args.commit, args.max_concurrent)

# Keep original interface for compatibility
def main() -> None:
    """Original synchronous interface."""
    asyncio.run(main_async_cli())

# Keep all existing helper functions unchanged...
def init_supabase():
    if not SUPABASE_URL or not SUPABASE_KEY:
        log.error("SUPABASE creds missing"); sys.exit(1)
    return create_client(SUPABASE_URL,SUPABASE_KEY)

def count_processed_chunks(sb)->int:
    res=sb.table("documents_chunks").select("id",count="exact").not_.is_("embedding","null").execute()
    return res.count or 0

def fetch_unprocessed_chunks(sb,*,limit:int,offset:int=0)->List[Dict[str,Any]]:
    first,last=offset,offset+limit-1
    res=sb.table("documents_chunks").select("id,text,token_start,token_end")\
        .eq("chunking_strategy","token_window").is_("embedding","null")\
        .range(first,last).execute()
    
    chunks = res.data or []
    
    if chunks:
        # ✅ GUARANTEED SKIP LOGIC - Layer 2: Double-check verification
        chunk_ids = [chunk["id"] for chunk in chunks]
        verification = sb.table("documents_chunks").select("id")\
            .in_("id", chunk_ids).not_.is_("embedding", "null").execute()
        
        already_embedded_ids = {row["id"] for row in verification.data or []}
        
        if already_embedded_ids:
            log.warning(f"🛡️ Found {len(already_embedded_ids)} chunks that already have embeddings - SKIPPING them")
            chunks = [chunk for chunk in chunks if chunk["id"] not in already_embedded_ids]
        
        log.info(f"✅ GUARANTEED: Fetched {len(chunks)} chunks WITHOUT embeddings")
        
        if not chunks:
            log.info("✅ All chunks already have embeddings - nothing to do!")
    
    return chunks

def generate_embedding_batch(texts:List[str])->List[List[float]]:
    attempt=0
    while attempt<MAX_RETRIES:
        try:
            resp=openai_client.embeddings.create(model=EMBEDDING_MODEL,input=texts)
            return [d.embedding for d in resp.data]
        except Exception as exc:
            attempt+=1; log.warning("Embedding batch %s/%s failed: %s",attempt,MAX_RETRIES,exc)
            time.sleep(RETRY_DELAY)
    raise RuntimeError("OpenAI embedding batch failed after retries")

def chunk_tokens(row:Dict[str,Any])->int:
    try:
        t=int(row["token_end"])-int(row["token_start"])+1
        if 0<t<=16384: return t
    except: pass
    
    # Use tiktoken if available for more accurate counting
    text = row.get("text", "")
    if TIKTOKEN_AVAILABLE:
        try:
            encoder = tiktoken.encoding_for_model(EMBEDDING_MODEL)
            return len(encoder.encode(text))
        except:
            pass
    
    # Conservative fallback
    approx=int(len(text.split())*0.75)+50  # More conservative estimate
    return min(max(1,approx),MODEL_TOKEN_LIMIT)

def safe_slices(rows:List[Dict[str,Any]],max_rows:int)->List[List[Dict[str,Any]]]:
    """🎯 DYNAMIC BATCH SIZING - Guarantees no token limit errors."""
    return dynamic_batch_slices(rows)

def dynamic_batch_slices(rows: List[Dict[str, Any]]) -> List[List[Dict[str, Any]]]:
    """
    🎯 DYNAMIC BATCH SIZING - Creates variable-sized batches that GUARANTEE no API token errors.
    
    Protection Layers:
    1. Individual chunk validation (max 6,000 tokens)
    2. Dynamic batch sizing (max 7,692 tokens total)
    3. Conservative safety margins
    4. Multiple validation checks
    """
    batches = []
    current_batch = []
    current_tokens = 0
    skipped_chunks = 0
    
    # Initialize token encoder for accurate counting
    encoder = None
    if TIKTOKEN_AVAILABLE:
        try:
            encoder = tiktoken.encoding_for_model(EMBEDDING_MODEL)
        except:
            pass
    
    def count_tokens_accurate(text: str) -> int:
        """Count tokens with maximum accuracy."""
        if encoder:
            return len(encoder.encode(text))
        else:
            # Conservative fallback estimation
            return int(len(text.split()) * 0.75) + 50
    
    log.info(f"🎯 Starting dynamic batch creation with {len(rows)} chunks")
    log.info(f"🎯 Limits: {MAX_CHUNK_TOKENS} tokens/chunk, {MAX_BATCH_TOKENS} tokens/batch")
    
    for i, row in enumerate(rows):
        # Clean text and validate
        text = (row.get("text") or "").replace("\x00", "").strip()
        if not text:
            log.warning(f"🎯 Skipping empty chunk {row.get('id', 'unknown')}")
            continue
        
        # ️ PROTECTION LAYER 1: Individual chunk validation
        chunk_tokens = count_tokens_accurate(text)
        
        if chunk_tokens > MAX_CHUNK_TOKENS:
            log.warning(f"🎯 Skipping oversized chunk {row.get('id', 'unknown')}: {chunk_tokens} tokens (max: {MAX_CHUNK_TOKENS})")
            skipped_chunks += 1
            continue
        
        # 🛡️ PROTECTION LAYER 2: Dynamic batch sizing
        would_exceed = current_tokens + chunk_tokens > MAX_BATCH_TOKENS
        
        if would_exceed and current_batch:
            # Finalize current batch
            log.info(f"🎯 Created batch with {len(current_batch)} chunks (~{current_tokens} tokens)")
            batches.append(current_batch)
            current_batch = []
            current_tokens = 0
        
        # Add chunk to current batch
        current_batch.append({"id": row["id"], "text": text})
        current_tokens += chunk_tokens
        
        # 🛡️ PROTECTION LAYER 3: Safety validation
        if current_tokens > MAX_BATCH_TOKENS:
            log.error(f"🚨 CRITICAL: Batch exceeded limit! {current_tokens} > {MAX_BATCH_TOKENS}")
            # Emergency fallback - remove last chunk and finalize batch
            if len(current_batch) > 1:
                current_batch.pop()
                current_tokens -= chunk_tokens
                log.info(f"🎯 Emergency: Created batch with {len(current_batch)} chunks (~{current_tokens} tokens)")
                batches.append(current_batch)
                current_batch = [{"id": row["id"], "text": text}]
                current_tokens = chunk_tokens
            else:
                log.error(f"🚨 Single chunk too large: {chunk_tokens} tokens")
                current_batch = []
                current_tokens = 0
                skipped_chunks += 1
    
    # Add final batch if not empty
    if current_batch and current_tokens >= MIN_BATCH_TOKENS:
        log.info(f"🎯 Created final batch with {len(current_batch)} chunks (~{current_tokens} tokens)")
        batches.append(current_batch)
    elif current_batch:
        log.warning(f"🎯 Skipping tiny final batch: {current_tokens} tokens < {MIN_BATCH_TOKENS} minimum")
    
    # 🛡️ PROTECTION LAYER 4: Final validation
    total_chunks = sum(len(batch) for batch in batches)
    max_batch_tokens = max((sum(count_tokens_accurate(chunk["text"]) for chunk in batch) for batch in batches), default=0)
    
    log.info(f"🎯 DYNAMIC BATCHING COMPLETE:")
    log.info(f"   📊 {len(batches)} batches created")
    log.info(f"   📊 {total_chunks} chunks processed")
    log.info(f"   📊 {skipped_chunks} chunks skipped")
    log.info(f"   📊 Largest batch: {max_batch_tokens} tokens (limit: {MAX_BATCH_TOKENS})")
    log.info(f"   ✅ GUARANTEED: No batch exceeds {MAX_BATCH_TOKENS} tokens")
    
    if max_batch_tokens > MAX_BATCH_TOKENS:
        log.error(f"🚨 CRITICAL ERROR: Batch validation failed!")
        raise RuntimeError(f"Batch token validation failed: {max_batch_tokens} > {MAX_BATCH_TOKENS}")
    
    return batches

def embed_slice(sb,slice_rows:List[Dict[str,Any]])->int:
    embeds=generate_embedding_batch([r["text"] for r in slice_rows])
    ok=0
    for row,emb in zip(slice_rows,embeds):
        attempt=0
        while attempt<MAX_RETRIES:
            res=sb.table("documents_chunks").update({"embedding":emb}).eq("id",row["id"]).execute()
            if getattr(res,"error",None):
                attempt+=1; log.warning("Update %s failed (%s/%s): %s",row["id"],attempt,MAX_RETRIES,res.error)
                time.sleep(RETRY_DELAY)
            else: ok+=1; break
    return ok

if __name__=="__main__": 
    asyncio.run(main_async_cli())


================================================================================


################################################################################
# File: scripts/rag_local_web_app.py
################################################################################

# File: scripts/rag_local_web_app.py

#!/usr/bin/env python3
################################################################################
################################################################################
"""
Mini Flask app that answers misophonia questions with Retrieval‑Augmented
Generation (gpt-4.1-mini-2025-04-14 + Supabase pgvector).

### Patch 2  (2025‑05‑06)
• **Embeddings** now created with **text‑embedding‑ada‑002** (1536‑D).  
• Similarity is re‑computed client‑side with a **plain cosine function** so the
  ranking no longer depends on pgvector's built‑in distance or any RPC
  threshold quirks.

The rest of the grounded‑answer logic (added in Patch 1) is unchanged.
"""
from __future__ import annotations

import logging
import math
import os
import re
from pathlib import Path        # (unused but left in to mirror original)
from typing import Dict, List
import json

from dotenv import load_dotenv
from flask import Flask, jsonify, request, make_response
from openai import OpenAI
from supabase import create_client
from flask_compress import Compress
from flask_cors import CORS

# ────────────────────────────── configuration ───────────────────────────── #

load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
SUPABASE_URL   = os.getenv("SUPABASE_URL")
SUPABASE_KEY   = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
PORT           = int(os.getenv("PORT", 8080))

if not (OPENAI_API_KEY and SUPABASE_URL and SUPABASE_KEY):
    raise SystemExit(
        "❌  Required env vars: OPENAI_API_KEY, SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY"
    )

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s — %(levelname)s — %(message)s",
)
log = logging.getLogger("rag_app")

sb = create_client(SUPABASE_URL, SUPABASE_KEY)
oa = OpenAI(api_key=OPENAI_API_KEY)

app = Flask(__name__)
CORS(app)
app.config['COMPRESS_ALGORITHM'] = 'gzip'
Compress(app)

# ────────────────────────────── helper functions ────────────────────────── #


def embed(text: str) -> List[float]:
    """
    Return OpenAI embedding vector for *text* using text‑embedding‑ada‑002.

    ada‑002 has 1536 dimensions and is inexpensive yet solid for similarity.
    """
    resp = oa.embeddings.create(
        model="text-embedding-ada-002",
        input=text[:8192],  # safety slice
    )
    return resp.data[0].embedding


def cosine_similarity(a: List[float], b: List[float]) -> float:
    """Plain cosine similarity between two equal‑length vectors."""
    dot = sum(x * y for x, y in zip(a, b))
    na = math.sqrt(sum(x * x for x in a))
    nb = math.sqrt(sum(y * y for y in b))
    return dot / (na * nb + 1e-9)


# Add in-memory embedding cache
_qcache = {}
def embed_cached(text):
    if text in _qcache: return _qcache[text]
    vec = embed(text)
    _qcache[text] = vec
    return vec


# Add regex patterns for bibliography detection
_DOI_RE   = re.compile(r'\b10\.\d{4,9}/[-._;()/:A-Z0-9]+\b', re.I)
_YEAR_RE  = re.compile(r'\b(19|20)\d{2}\b')

def looks_like_refs(text: str) -> bool:
    """
    Return True if this chunk is likely just a bibliography list:
      • more than 12 DOIs, or
      • more than 15 year mentions.
    """
    doi_count  = len(_DOI_RE.findall(text))
    year_count = len(_YEAR_RE.findall(text))
    return doi_count > 12 or year_count > 15


def semantic_search(
    query: str,
    *,
    limit: int = 8,
    threshold: float = 0.0,
) -> List[Dict]:
    """
    Retrieve candidate chunks via the pgvector RPC, then re-rank with an
    **explicit cosine similarity** so the final score is always in
    **[-100 … +100] percent**.

    Why the extra work?
    -------------------
    •  The SQL function returns a raw inner-product that can be > 1.  
       (embeddings are *not* unit-length.)  
    •  By pulling the real 1 536-D vectors and re-computing a cosine we get a
       true, bounded similarity that front-end code can safely show.

    The -100 … +100 range is produced by:  
        pct = clamp(cosine × 100, -100, 100)
    """
    # 1. Embed the query once and keep it cached
    q_vec = embed_cached(query)

    # 2. Fast ANN search in Postgres (over-fetch 4× so we can re-rank)
    rows = (
        sb.rpc(
            "match_documents_chunks",
            {
                "query_embedding": q_vec,
                "match_threshold": threshold,
                "match_count": limit * 4,
            },
        )
        .execute()
        .data
    ) or []

    # 3. Filter out bibliography-only chunks
    rows = [r for r in rows if not looks_like_refs(r["text"])]

    if not rows:
        return []

    # 4. Fetch document metadata (title, authors …) in one round-trip
    doc_ids = {r["document_id"] for r in rows}
    meta = {
        d["id"]: d
        for d in (
            sb.table("city_clerk_documents")
              .select("id,document_type,title,date,year,month,day,mayor,vice_mayor,commissioners,city_attorney,city_manager,city_clerk,public_works_director,agenda,keywords,source_pdf")
              .in_("id", list(doc_ids))
              .execute()
              .data
            or []
        )
    }

    # 5. Pull embeddings and page info once and compute **plain cosine** (no scaling)
    chunk_ids = [r["id"] for r in rows]

    emb_rows = (
        sb.table("documents_chunks")
          .select("id, embedding, page_start, page_end")
          .in_("id", chunk_ids)
          .execute()
          .data
    ) or []

    emb_map: Dict[str, List[float]] = {}
    page_map: Dict[str, Dict] = {}
    for e in emb_rows:
        raw = e["embedding"]
        if isinstance(raw, list):                    # list[Decimal]
            emb_map[e["id"]] = [float(x) for x in raw]
        elif isinstance(raw, str) and raw.startswith('['):   # TEXT  "[…]"
            emb_map[e["id"]] = [float(x) for x in raw.strip('[]').split(',')]
        
        # Store page info
        page_map[e["id"]] = {
            "page_start": e.get("page_start", 1),
            "page_end": e.get("page_end", 1)
        }

    for r in rows:
        vec = emb_map.get(r["id"])
        if vec:                                     # we now have the real vector
            cos = cosine_similarity(q_vec, vec)
            r["similarity"] = round(cos * 100, 1)   # –100…+100 % (or 0…100 %)
        else:                                       # fallback if something failed
            dist = float(r.get("similarity", 1.0))  # 0…2 cosine-distance
            r["similarity"] = round((1.0 - dist) * 100, 1)

        r["doc"] = meta.get(r["document_id"], {})
        
        # Add page info to the row
        page_info = page_map.get(r["id"], {"page_start": 1, "page_end": 1})
        r["page_start"] = page_info["page_start"]
        r["page_end"] = page_info["page_end"]

    # 6. Keep the top *limit* rows after proper re-ranking
    ranked = sorted(rows, key=lambda x: x["similarity"], reverse=True)[:limit]
    return ranked


# ──────────────────────── NEW RAG‑PROMPT HELPERS ───────────────────────── #

MAX_PROMPT_CHARS: int = 24_000  # ~6 k tokens @ 4 chars/token heuristic


def trim_chunks(chunks: List[Dict]) -> List[Dict]:
    """
    Fail‑safe guard: ensure concatenated chunk texts remain under the
    MAX_PROMPT_CHARS budget.  Keeps highest‑similarity chunks first.
    """
    sorted_chunks = sorted(chunks, key=lambda c: c.get("similarity", 0), reverse=True)
    output: List[Dict] = []
    total_chars = 0
    for c in sorted_chunks:
        chunk_len = len(c["text"])
        if total_chars + chunk_len > MAX_PROMPT_CHARS:
            break
        output.append(c)
        total_chars += chunk_len
    return output


def build_prompt(question: str, chunks: List[Dict]) -> str:
    """
    Build a structured prompt that asks GPT to:
      • answer in Markdown with short intro + numbered list of key points
      • cite inline like [1], [2] …
      • finish with a Bibliography that includes the document title and type
    """
    snippet_lines, biblio_lines = [], []
    for i, c in enumerate(chunks, 1):
        page_start = c.get('page_start', 1)
        page_end = c.get('page_end', 1)
        snippet_lines.append(
            f"[{i}] \"{c['text'].strip()}\" "
            f"(pp. {page_start}-{page_end})"
        )

        d = c["doc"]
        title = d.get("title", "Untitled Document")
        doc_type = d.get("document_type", "Document")
        date = d.get("date", "Unknown date")
        year = d.get("year", "n.d.")
        pages = f"pp. {page_start}-{page_end}"
        source_pdf = d.get("source_pdf", "")

        # City clerk document bibliography format
        biblio_lines.append(
            f"[{i}] *{title}* · {doc_type} · {date} · {pages}"
        )

    prompt_parts = [
        "You are City Clerk Assistant, a knowledgeable AI that helps with questions about city government documents, including resolutions, ordinances, proclamations, contracts, meeting minutes, and agendas.",
        "You draw on evidence from official city documents and municipal records.",
        "Your responses are clear, professional, and grounded in the provided context.",
        "====",
        "QUESTION:",
        question,
        "====",
        "CONTEXT:",
        *snippet_lines,
        "====",
        "INSTRUCTIONS:",
        "• Write your answer in **Markdown**.",
        "• Begin with a concise summary (2–3 sentences).",
        "• Then elaborate on key points using well-structured paragraphs.",
        "• Provide relevant insights about city governance, policies, or procedures.",
        "• If helpful, use lists, subheadings, or clear explanations to enhance understanding.",
        "• Use a professional and informative tone.",
        "• Cite sources inline like [1], [2] etc.",
        "• After the answer, include a 'BIBLIOGRAPHY:' section that lists each source exactly as provided below.",
        "• If none of the context answers the question, reply: \"I'm sorry, I don't have sufficient information to answer that.\"",
        "====",
        "BEGIN OUTPUT",
        "ANSWER:",
        "",  # where the model writes the main response
        "BIBLIOGRAPHY:",
        *biblio_lines,
    ]

    return '\n'.join(prompt_parts)


def extract_citations(answer: str) -> List[str]:
    """
    Parse numeric citations (e.g., "[1]", "[2]") from the answer text.
    Returns unique citation numbers in ascending order.
    """
    citations = re.findall(r"\[(\d+)\]", answer)
    return sorted(set(citations), key=int)


# ──────────────────────────────── routes ────────────────────────────────── #

@app.route("/")
def home():
    """Simple homepage for the City Clerk RAG application."""
    html = """
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>City Clerk RAG Assistant</title>
        <style>
            body { 
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
                max-width: 800px; 
                margin: 0 auto; 
                padding: 2rem;
                line-height: 1.6;
                color: #333;
            }
            .header { 
                text-align: center; 
                margin-bottom: 2rem;
                padding-bottom: 1rem;
                border-bottom: 2px solid #e0e0e0;
            }
            .search-container {
                background: #f8f9fa;
                padding: 2rem;
                border-radius: 8px;
                margin: 2rem 0;
            }
            .search-box {
                width: 100%;
                padding: 1rem;
                border: 2px solid #ddd;
                border-radius: 4px;
                font-size: 16px;
                margin-bottom: 1rem;
            }
            .search-btn {
                background: #007bff;
                color: white;
                padding: 1rem 2rem;
                border: none;
                border-radius: 4px;
                cursor: pointer;
                font-size: 16px;
            }
            .search-btn:hover { background: #0056b3; }
            .results { margin-top: 2rem; }
            .answer { 
                background: white; 
                padding: 1.5rem; 
                border-radius: 8px; 
                border-left: 4px solid #007bff;
                margin: 1rem 0;
            }
            .sources { 
                background: #f8f9fa; 
                padding: 1rem; 
                border-radius: 4px; 
                margin-top: 1rem;
                font-size: 0.9em;
            }
            .loading { color: #666; font-style: italic; }
            .error { color: #dc3545; background: #f8d7da; padding: 1rem; border-radius: 4px; }
        </style>
    </head>
    <body>
        <div class="header">
            <h1>🏛️ City Clerk RAG Assistant</h1>
            <p>Ask questions about city government documents, resolutions, ordinances, and meeting minutes</p>
        </div>
        
        <div class="search-container">
            <input type="text" id="queryInput" class="search-box" 
                   placeholder="Ask a question about city documents..." 
                   onkeypress="if(event.key==='Enter') search()">
            <button onclick="search()" class="search-btn">Search</button>
        </div>
        
        <div id="results" class="results"></div>
        
        <script>
            async function search() {
                const query = document.getElementById('queryInput').value.trim();
                if (!query) return;
                
                const resultsDiv = document.getElementById('results');
                resultsDiv.innerHTML = '<div class="loading">Searching...</div>';
                
                try {
                    const response = await fetch('/search', {
                        method: 'POST',
                        headers: { 'Content-Type': 'application/json' },
                        body: JSON.stringify({ query: query })
                    });
                    
                    const data = await response.json();
                    
                    if (data.error) {
                        resultsDiv.innerHTML = `<div class="error">Error: ${data.error}</div>`;
                        return;
                    }
                    
                    let html = `<div class="answer">${data.answer.replace(/\\n/g, '<br>')}</div>`;
                    
                    if (data.results && data.results.length > 0) {
                        html += '<div class="sources"><strong>Sources:</strong><ul>';
                        data.results.forEach((result, i) => {
                            const doc = result.doc || {};
                            const title = doc.title || 'Untitled Document';
                            const similarity = Math.round(result.similarity || 0);
                            html += `<li>${title} (${similarity}% match)</li>`;
                        });
                        html += '</ul></div>';
                    }
                    
                    resultsDiv.innerHTML = html;
                } catch (error) {
                    resultsDiv.innerHTML = `<div class="error">Error: ${error.message}</div>`;
                }
            }
        </script>
    </body>
    </html>
    """
    return html

@app.post("/search")
def search():
    payload = request.get_json(force=True, silent=True) or {}
    question = (payload.get("query") or "").strip()
    if not question:
        return jsonify({"error": "Missing 'query'"}), 400

    try:
        # Retrieve semantic matches (client‑side cosine re‑ranked)
        raw_matches = semantic_search(question, limit=int(payload.get("limit", 8)))

        if not raw_matches:
            return jsonify(
                {
                    "answer": "I'm sorry, I don't have sufficient information to answer that.",
                    "citations": [],
                    "results": [],
                }
            )

        # ──────────────────── TRIM CHUNKS TO BUDGET ──────────────────── #
        chunks = trim_chunks(raw_matches)

        # ──────────────────── BUILD PROMPT & CALL LLM ─────────────────── #
        prompt = build_prompt(question, chunks)

        completion = oa.chat.completions.create(
            model="gpt-4.1-mini-2025-04-14",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0,
        )
        answer_text: str = completion.choices[0].message.content.strip()

        # ──────────────────── EXTRACT CITATIONS ──────────────────────── #
        citations = extract_citations(answer_text)

        # Remove embedding vectors before sending back to the browser
        for m in raw_matches:
            m.pop("embedding", None)

        # ──────────────────── RETURN JSON ─────────────────────────────── #
        response = jsonify(
            {
                "answer": answer_text,
                "citations": citations,
                "results": raw_matches,
            }
        )
        response.headers['Connection'] = 'keep-alive'
        return response
    except Exception as exc:  # noqa: BLE001
        log.exception("search failed")
        return jsonify({"error": str(exc)}), 500


@app.get("/stats")
def stats():
    """Tiny ops endpoint—count total chunks."""
    resp = sb.table("documents_chunks").select("id", count="exact").execute()
    return jsonify({"total_chunks": resp.count})


# ──────────────────────────────── main ─────────────────────────────────── #

if __name__ == "__main__":
    log.info("Starting Flask on 0.0.0.0:%s …", PORT)
    app.run(host="0.0.0.0", port=PORT, debug=True)


================================================================================


################################################################################
# File: scripts/pipeline_modular_optimized.py
################################################################################

# File: scripts/pipeline_modular_optimized.py

#!/usr/bin/env python3
"""
Optimized pipeline orchestrator with full parallelization.
Maintains compatibility with original pipeline_modular.py interface.
"""
from __future__ import annotations
import argparse, logging, pathlib, random
from collections import Counter
from rich.console import Console
import asyncio
from typing import List, Optional
import multiprocessing as mp

# Import stages
from stages import extract_clean, llm_enrich, chunk_text, db_upsert, embed_vectors
from stages.acceleration_utils import hardware

# Keep existing toggles
RUN_EXTRACT    = True
RUN_LLM_ENRICH = True
RUN_CHUNK      = True
RUN_DB         = True
RUN_EMBED      = True

log = logging.getLogger("pipeline-modular-optimized")

class OptimizedPipeline:
    """Optimized pipeline with parallel processing and rate limiting."""
    
    def __init__(self, batch_size: int = 15, max_api_concurrent: int = 3):  # 🛡️ Rate limited: was 50, 20
        self.batch_size = batch_size
        self.max_api_concurrent = max_api_concurrent
        self.stats = Counter()
        
        # 🛡️ Log rate limiting settings
        log.info("🛡️  Rate-limited pipeline initialized:")
        log.info(f"   Batch size: {batch_size} (was 50)")
        log.info(f"   Max concurrent: {max_api_concurrent} (was 20)")
        log.info("   Target: <800K tokens/minute (safe margin)")
    
    async def process_batch(self, pdfs: List[pathlib.Path], start_doc_num: int = 1, total_docs: int = None) -> None:
        """Process a batch of PDFs through all stages with individual document progress tracking."""
        
        if total_docs is None:
            total_docs = len(pdfs)
        
        batch_size = len(pdfs)
        log.info(f"📊 BATCH PROCESSING START:")
        log.info(f"   📄 Documents in this batch: {batch_size}")
        log.info(f"   🎯 Document range: {start_doc_num} to {start_doc_num + batch_size - 1}")
        log.info(f"   📈 Overall progress: {start_doc_num-1}/{total_docs} completed ({((start_doc_num-1)/total_docs*100):.1f}%)")
        
        # Stage 1-2: Extract & Clean (CPU-bound, use process pool)
        json_docs = []
        if RUN_EXTRACT:
            log.info(f"🔄 EXTRACTION STAGE - Processing {len(pdfs)} PDFs...")
            for i, pdf in enumerate(pdfs):
                doc_num = start_doc_num + i
                doc_progress = ((doc_num-1) / total_docs * 100)
                log.info(f"📄 [{doc_num}/{total_docs}] ({doc_progress:.1f}%) Extracting: {pdf.name}")
            
            json_docs = await extract_clean.extract_batch_async(
                pdfs, 
                enrich_llm=False  # We'll do LLM enrichment separately
            )
            
            log.info(f"📊 EXTRACTION STAGE COMPLETE:")
            for i, pdf in enumerate(pdfs):
                doc_num = start_doc_num + i
                log.info(f"   ✅ [{doc_num}/{total_docs}] Extracted: {pdf.name}")
        else:
            # Use existing JSON files
            json_docs = [extract_clean.json_path_for(pdf) for pdf in pdfs]
            log.info(f"📊 EXTRACTION STAGE SKIPPED - Using existing JSON files")

        # Stage 4: LLM Enrich (I/O-bound, use async)
        if RUN_LLM_ENRICH and json_docs:
            log.info(f"🔄 ENRICHMENT STAGE - Processing {len(json_docs)} documents...")
            for i, json_doc in enumerate(json_docs):
                doc_num = start_doc_num + i
                doc_progress = ((doc_num-1) / total_docs * 100)
                doc_name = pathlib.Path(json_doc).stem.replace('_clean', '')
                log.info(f"📄 [{doc_num}/{total_docs}] ({doc_progress:.1f}%) Enriching: {doc_name}")
            
            await llm_enrich.enrich_batch_async(
                json_docs,
                max_concurrent=self.max_api_concurrent
            )
            
            log.info(f"📊 ENRICHMENT STAGE COMPLETE:")
            for i, json_doc in enumerate(json_docs):
                doc_num = start_doc_num + i
                doc_name = pathlib.Path(json_doc).stem.replace('_clean', '')
                log.info(f"   ✅ [{doc_num}/{total_docs}] Enriched: {doc_name}")

        # Stage 5: Chunk (CPU-bound, use process pool)
        chunks_map = {}
        if RUN_CHUNK and json_docs:
            log.info(f"🔄 CHUNKING STAGE - Processing {len(json_docs)} documents...")
            for i, json_doc in enumerate(json_docs):
                doc_num = start_doc_num + i
                doc_progress = ((doc_num-1) / total_docs * 100)
                doc_name = pathlib.Path(json_doc).stem.replace('_clean', '')
                log.info(f"📄 [{doc_num}/{total_docs}] ({doc_progress:.1f}%) Chunking: {doc_name}")
            
            chunks_map = await chunk_text.chunk_batch_async(json_docs)
            
            log.info(f"📊 CHUNKING STAGE COMPLETE:")
            total_chunks_created = 0
            for i, json_doc in enumerate(json_docs):
                doc_num = start_doc_num + i
                doc_name = pathlib.Path(json_doc).stem.replace('_clean', '')
                chunk_count = len(chunks_map.get(json_doc, []))
                total_chunks_created += chunk_count
                log.info(f"   ✅ [{doc_num}/{total_docs}] Chunked: {doc_name} ({chunk_count} chunks)")
            log.info(f"   📊 Total chunks created in this batch: {total_chunks_created}")

        # Stage 6: DB Upsert (I/O-bound, use async)
        if RUN_DB and chunks_map:
            log.info(f"🔄 DATABASE UPSERT STAGE - Processing {len(chunks_map)} documents...")
            
            # Show progress before upserting
            total_chunks_to_upsert = sum(len(chunks) for chunks in chunks_map.values())
            log.info(f"📊 DATABASE UPSERT PROGRESS:")
            log.info(f"   💾 Total chunks to upsert: {total_chunks_to_upsert}")
            
            for i, (json_path, chunks) in enumerate(chunks_map.items()):
                doc_num = start_doc_num + i
                doc_progress = ((doc_num-1) / total_docs * 100)
                doc_name = pathlib.Path(json_path).stem.replace('_clean', '')
                log.info(f"📄 [{doc_num}/{total_docs}] ({doc_progress:.1f}%) Upserting: {doc_name} ({len(chunks)} chunks)")
            
            documents = [
                {
                    "json_path": json_path,
                    "chunks": chunks,
                    "do_embed": False  # We'll embed in batch later
                }
                for json_path, chunks in chunks_map.items()
            ]
            await db_upsert.upsert_batch_async(documents)
            
            log.info(f"📊 DATABASE UPSERT STAGE COMPLETE:")
            total_chunks_upserted = 0
            for i, (json_path, chunks) in enumerate(chunks_map.items()):
                doc_num = start_doc_num + i
                doc_name = pathlib.Path(json_path).stem.replace('_clean', '')
                total_chunks_upserted += len(chunks)
                log.info(f"   ✅ [{doc_num}/{total_docs}] Upserted: {doc_name}")
            log.info(f"   📊 Total chunks upserted: {total_chunks_upserted}")

        # Update stats
        self.stats["ok"] += len([c for c in chunks_map.values() if c])
        self.stats["fail"] += len([c for c in chunks_map.values() if not c])
        
        # Final batch summary
        end_doc_num = start_doc_num + batch_size - 1
        overall_progress = (end_doc_num / total_docs * 100)
        log.info(f"📊 BATCH COMPLETE:")
        log.info(f"   ✅ Documents processed: {start_doc_num}-{end_doc_num}")
        log.info(f"   📈 Overall progress: {end_doc_num}/{total_docs} ({overall_progress:.1f}%)")
        log.info(f"   📊 Successful documents: {self.stats['ok']}")
        log.info(f"   ❌ Failed documents: {self.stats['fail']}")
    
    async def run(self, src: pathlib.Path, selection: str = "sequential", cap: int = 0):
        """Run the optimized pipeline."""
        Console().rule("[bold cyan]Misophonia PDF → Vector pipeline (optimized)")
        
        # Get PDF list
        pdfs = [src] if src.is_file() else sorted(src.rglob("*.pdf"))
        if cap:
            pdfs = random.sample(pdfs, cap) if selection == "random" else pdfs[:cap]
        
        total_pdfs = len(pdfs)
        log.info(f"Processing {total_pdfs} PDFs in batches of {self.batch_size}")
        
        # Track PDF-level progress
        pdfs_processed = 0
        
        # Process in batches
        for i in range(0, len(pdfs), self.batch_size):
            batch = pdfs[i:i + self.batch_size]
            batch_num = i//self.batch_size + 1
            total_batches = (len(pdfs) + self.batch_size - 1)//self.batch_size
            
            # Calculate the starting document number for this batch
            start_doc_num = pdfs_processed + 1
            
            # Enhanced progress logging with both batch and PDF-level progress
            log.info(f"📄 Processing batch {batch_num}/{total_batches} ({len(batch)} PDFs)")
            log.info(f"📊 Overall progress: {pdfs_processed}/{total_pdfs} PDFs completed ({pdfs_processed/total_pdfs*100:.1f}%)")
            log.info(f"🔢 Document range: {start_doc_num}-{start_doc_num + len(batch) - 1} of {total_pdfs}")
            
            await self.process_batch(batch, start_doc_num, total_pdfs)
            
            # Update PDF progress counter
            pdfs_processed += len(batch)
            
            # Log completion of this batch
            log.info(f"✅ Batch {batch_num} complete - Total PDFs processed: {pdfs_processed}/{total_pdfs} ({pdfs_processed/total_pdfs*100:.1f}%)")
        
        # Final progress summary
        log.info(f"🎉 All PDFs processed: {pdfs_processed}/{total_pdfs} ({pdfs_processed/total_pdfs*100:.1f}%)")
        
        # Stage 7: Batch embed all at once with conservative settings
        if RUN_EMBED:
            log.info("🔄 STARTING EMBEDDING STAGE...")
            log.info("📊 EMBEDDING STAGE PROGRESS:")
            log.info("   🎯 Running batch embedding with rate limiting...")
            log.info("   🎯 This stage will process all upserted chunks for embedding")
            
            # 🛡️ Pass conservative embedding parameters (consistent with embed_vectors.py defaults)
            await embed_vectors.main_async(
                batch_size=200,     # 🛡️ Rate limited: fetch 200 chunks at once (conservative default)
                commit_size=10,     # 🛡️ Rate limited: 10 chunks per API call (conservative default) 
                max_concurrent=3    # 🛡️ Rate limited: max 3 concurrent embedding calls (conservative default)
            )
            
            log.info("📊 EMBEDDING STAGE COMPLETE ✅")
        
        Console().rule("[green]Finished")
        log.info("📊 PIPELINE COMPLETE - FINAL SUMMARY:")
        log.info("🛡️  Rate limiting successful - no API limits hit")
        log.info(f"📊 Total documents processed: {self.stats['ok']}")
        log.info(f"📊 Total documents failed: {self.stats['fail']}")
        log.info(f"📊 Success rate: {(self.stats['ok']/(self.stats['ok']+self.stats['fail'])*100):.1f}%" if (self.stats['ok']+self.stats['fail']) > 0 else "100%")

def main(src: pathlib.Path, selection: str = "sequential", cap: int = 0) -> None:
    """Main entry point compatible with original pipeline_modular.py"""
    # Set multiprocessing start method for macOS
    mp.set_start_method('spawn', force=True)
    
    # Create and run pipeline with conservative rate limiting
    pipeline = OptimizedPipeline(
        batch_size=15,  # 🛡️ Rate limited: process 15 PDFs at a time (was 50)
        max_api_concurrent=3  # 🛡️ Rate limited: max 3 concurrent API requests (was 20)
    )
    
    # Run async pipeline
    asyncio.run(pipeline.run(src, selection, cap))

if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s — %(levelname)s — %(message)s"
    )
    
    p = argparse.ArgumentParser()
    p.add_argument("src", type=pathlib.Path,
                   default=pathlib.Path("city_clerk_documents/global"), nargs="?")
    p.add_argument("--selection", choices=["sequential", "random"],
                   default="sequential")
    p.add_argument("--cap", type=int, default=0)
    p.add_argument("--batch-size", type=int, default=15,  # 🛡️ Rate limited: default to 15 (was 50)
                   help="Number of PDFs to process in parallel")
    p.add_argument("--api-concurrent", type=int, default=3,  # 🛡️ Rate limited: default to 3 (was 20)
                   help="Max concurrent API calls")
    args = p.parse_args()
    
    # Override batch size if specified
    if args.batch_size:
        pipeline = OptimizedPipeline(
            batch_size=args.batch_size,
            max_api_concurrent=args.api_concurrent
        )
        asyncio.run(pipeline.run(args.src, args.selection, args.cap))
    else:
        main(args.src, args.selection, args.cap)


================================================================================


################################################################################
# File: debug_graph.py
################################################################################

# File: debug_graph.py

#!/usr/bin/env python3
"""Debug script to inspect what's actually in the Cosmos DB graph."""
import os
import asyncio
import logging
from dotenv import load_dotenv
from gremlin_python.driver import client, serializer
from gremlin_python.process.traversal import T
import json

load_dotenv()

logging.basicConfig(level=logging.INFO)
log = logging.getLogger(__name__)

COSMOS_ENDPOINT = os.getenv("COSMOS_ENDPOINT")
COSMOS_KEY = os.getenv("COSMOS_KEY")
DATABASE = os.getenv("COSMOS_DATABASE", "cgGraph")
CONTAINER = os.getenv("COSMOS_CONTAINER", "cityClerk")


class GraphDebugger:
    def __init__(self):
        self.client = None
        
    def connect(self):
        """Connect to Cosmos DB."""
        self.client = client.Client(
            f"{COSMOS_ENDPOINT}/gremlin",
            "g",
            username=f"/dbs/{DATABASE}/colls/{CONTAINER}",
            password=COSMOS_KEY,
            message_serializer=serializer.GraphSONSerializersV2d0()
        )
        log.info("Connected to Cosmos DB")
    
    def close(self):
        """Close connection."""
        if self.client:
            self.client.close()
    
    def execute_query(self, query: str):
        """Execute a query and return results."""
        try:
            # Submit the query and get the result set
            result_set = self.client.submit(query)
            # Call .all() to get all results and .result() to resolve the Future
            results = result_set.all().result()
            return results
        except Exception as e:
            log.error(f"Query failed: {e}")
            log.error(f"Query was: {query}")
            return None
    
    def debug_graph(self):
        """Run comprehensive debugging queries."""
        print("\n" + "="*60)
        print("COSMOS DB GRAPH DEBUGGING")
        print("="*60)
        
        # 1. Count all vertices
        print("\n1. VERTEX COUNTS:")
        print("-" * 30)
        total_vertices = self.execute_query("g.V().count()")
        print(f"Total vertices: {total_vertices[0] if total_vertices else 0}")
        
        # 2. Count by label
        print("\n2. VERTICES BY LABEL:")
        print("-" * 30)
        labels_query = "g.V().label().groupCount()"
        labels = self.execute_query(labels_query)
        if labels:
            for label, count in labels[0].items():
                print(f"  {label}: {count}")
        else:
            print("  No vertices found!")
        
        # 3. List all vertices with properties
        print("\n3. ALL VERTICES (first 20):")
        print("-" * 30)
        vertices = self.execute_query("g.V().limit(20).valueMap(true)")
        if vertices:
            for i, vertex in enumerate(vertices):
                print(f"\nVertex {i+1}:")
                vertex_id = str(vertex.get(T.id))
                vertex_label = vertex.get(T.label)
                print(f"  ID: {vertex_id}")
                print(f"  Label: {vertex_label}")
                
                # Extract properties properly
                for key, value in vertex.items():
                    if key not in [T.id, T.label]:
                        # Handle single values vs lists
                        if isinstance(value, list) and len(value) == 1:
                            print(f"  {key}: {value[0]}")
                        else:
                            print(f"  {key}: {value}")
        else:
            print("  No vertices found!")
        
        # 4. Count edges
        print("\n4. EDGE COUNTS:")
        print("-" * 30)
        total_edges = self.execute_query("g.E().count()")
        print(f"Total edges: {total_edges[0] if total_edges else 0}")
        
        # 5. Count edges by label
        print("\n5. EDGES BY LABEL:")
        print("-" * 30)
        edge_labels = self.execute_query("g.E().label().groupCount()")
        if edge_labels and edge_labels[0]:
            for label, count in edge_labels[0].items():
                print(f"  {label}: {count}")
        else:
            print("  No edges found!")
        
        # 6. List sample edges with their connections
        print("\n6. SAMPLE EDGES (first 10):")
        print("-" * 30)
        edges_query = """g.E().limit(10).valueMap(true)"""
        edges = self.execute_query(edges_query)
        if edges:
            for i, edge in enumerate(edges):
                edge_id = str(edge.get(T.id))
                edge_label = edge.get(T.label)
                
                # Get source and target IDs
                out_v = edge.get('OUT')
                in_v = edge.get('IN')
                
                if out_v and in_v:
                    source_id = str(out_v.get(T.id)) if isinstance(out_v, dict) else str(out_v)
                    target_id = str(in_v.get(T.id)) if isinstance(in_v, dict) else str(in_v)
                    print(f"\nEdge {i+1}:")
                    print(f"  {source_id} --[{edge_label}]--> {target_id}")
                    print(f"  Edge ID: {edge_id}")
        else:
            print("  No edges found!")
        
        # 7. Check Meeting nodes specifically
        print("\n7. MEETING NODES:")
        print("-" * 30)
        meetings = self.execute_query("g.V().hasLabel('Meeting').valueMap(true)")
        if meetings:
            for meeting in meetings:
                print(f"\nMeeting:")
                # Extract ID and label properly
                meeting_id = str(meeting.get(T.id))
                meeting_label = meeting.get(T.label)
                print(f"  ID: {meeting_id}")
                print(f"  Label: {meeting_label}")
                
                # Extract properties properly
                for key, value in meeting.items():
                    if key not in [T.id, T.label]:
                        if isinstance(value, list) and len(value) == 1:
                            print(f"  {key}: {value[0]}")
                        else:
                            print(f"  {key}: {value}")
        else:
            print("  No Meeting nodes found!")
        
        # 8. Check AgendaSection nodes
        print("\n8. AGENDA SECTIONS (first 5):")
        print("-" * 30)
        sections = self.execute_query("g.V().hasLabel('AgendaSection').limit(5).valueMap(true)")
        if sections:
            for section in sections:
                print(f"\nSection:")
                # Extract ID and label properly
                section_id = str(section.get(T.id))
                section_label = section.get(T.label)
                print(f"  ID: {section_id}")
                print(f"  Label: {section_label}")
                
                # Extract properties properly
                for key, value in section.items():
                    if key not in [T.id, T.label]:
                        if isinstance(value, list) and len(value) == 1:
                            print(f"  {key}: {value[0]}")
                        else:
                            print(f"  {key}: {value}")
        else:
            print("  No AgendaSection nodes found!")
        
        # 9. Check relationships from meetings
        print("\n9. MEETING RELATIONSHIPS:")
        print("-" * 30)
        meeting_edges_query = """g.V().hasLabel('Meeting').outE().limit(10)
                                .project('from','to','label')
                                .by(outV().id())
                                .by(inV().id())
                                .by(label())"""
        meeting_edges = self.execute_query(meeting_edges_query)
        if meeting_edges:
            for edge in meeting_edges:
                print(f"  {edge['from']} --[{edge['label']}]--> {edge['to']}")
        else:
            print("  No outgoing edges from Meeting nodes!")
        
        # 10. Test a specific meeting structure query
        print("\n10. MEETING STRUCTURE TEST:")
        print("-" * 30)
        structure_test = self.execute_query(
            "g.V().hasLabel('Meeting').as('m').out('HAS_SECTION').as('s').select('m','s').by(id()).limit(5)"
        )
        if structure_test:
            for result in structure_test:
                print(f"  Meeting {result['m']} -> Section {result['s']}")
        else:
            print("  No Meeting->Section relationships found!")
        
        # 11. Check for orphaned nodes
        print("\n11. ORPHANED NODES (no edges):")
        print("-" * 30)
        orphans_query = """g.V().where(__.both().count().is(0)).limit(10)
                          .project('id','label')
                          .by(id())
                          .by(label())"""
        orphans = self.execute_query(orphans_query)
        if orphans:
            for orphan in orphans:
                print(f"  {orphan['label']}: {orphan['id']}")
        else:
            print("  No orphaned nodes found!")
        
        # 12. Database statistics summary
        print("\n12. DATABASE SUMMARY:")
        print("-" * 30)
        vertex_count = self.execute_query("g.V().count()")
        edge_count = self.execute_query("g.E().count()")
        print(f"  Total Vertices: {vertex_count[0] if vertex_count else 0}")
        print(f"  Total Edges: {edge_count[0] if edge_count else 0}")
        
        print("\n" + "="*60)


def main():
    debugger = GraphDebugger()
    debugger.connect()
    
    try:
        debugger.debug_graph()
    finally:
        debugger.close()


if __name__ == "__main__":
    main()


================================================================================


################################################################################
# File: relationOPENAI.py
################################################################################

# File: relationOPENAI.py

import os, json, time, re, hashlib, traceback
from azure.storage.blob import BlobServiceClient
from gremlin_python.driver import client, serializer
from openai import AzureOpenAI

# ─────────────  TEST-MODE  ────────────────────────────────────
TEST_MODE, MAX_VERTICES = False, 5           # pon False cuando validado
vertex_count, early_exit = 0, False

# ─────────────  CONFIG GENERAL  ───────────────────────────────
BLOB_CONNECTION_STRING = (
    "DefaultEndpointsProtocol=https;"
    "AccountName=rasagptstorageaccount;"
    "AccountKey=[KEY_HERE];"
    "EndpointSuffix=core.windows.net"
)
COSMOS_ENDPOINT = "wss://aida-graph-db.gremlin.cosmos.azure.com:443"
COSMOS_KEY      = "[KEY_HERE]"
DATABASE, CONTAINER = "cgGraph", "cityClerk" 
PARTITION_KEY, PARTITION_VALUE = "partitionKey", "demo"

# ─────────────  NUEVO CLIENTE AZURE OPENAI  ───────────────────
aoai = AzureOpenAI(
    api_key        = [KEY HERE],
    azure_endpoint = "https://aida-gpt4o.openai.azure.com",
    api_version    = "2024-02-15-preview"
)

DEPLOYMENT_NAME = "gpt-4o"          # nombre EXACTO en Deployments

─────────────  RESTO DE CONFIG  ──────────────────────────────
ENTITY_CONTAINERS = [
    "ks-entities-person","ks-entities-organization","ks-entities-location",
    "ks-entities-address","ks-entities-phone","ks-entities-email",
    "ks-entities-url","ks-entities-event","ks-entities-product",
    "ks-entities-persontype","ks-entities-ipaddress",
    "ks-entities-quantity","ks-entities-skill"
]
CONTAINER_TO_LABEL = {
    "ks-entities-person":"Person","ks-entities-organization":"Organization","ks-entities-location":"Location",
    "ks-entities-address":"Address","ks-entities-phone":"PhoneNumber","ks-entities-email":"Email",
    "ks-entities-url":"URL","ks-entities-event":"Event","ks-entities-product":"Product",
    "ks-entities-persontype":"PersonType","ks-entities-ipaddress":"IPAddress",
    "ks-entities-quantity":"Quantity","ks-entities-skill":"Skill"
}
CHUNKS_CONTAINER = "ks-chunks-debug"

# ─────────────  HELPERS  ──────────────────────────────────────
ILLEGAL_ID_CHARS = re.compile(r'[\/\\?#]')
def clean_id(s: str)  -> str: return ILLEGAL_ID_CHARS.sub('_', s)
def clean_txt(s: str) -> str: return s.replace("'", "\\'").replace('"', "")
def limit_reached():
    global early_exit
    if TEST_MODE and vertex_count >= MAX_VERTICES:
        early_exit = True
        return True
    return False

# ─────────────  CONEXIONES  ───────────────────────────────────
print("🔗  Connecting …")
blob = BlobServiceClient.from_connection_string(BLOB_CONNECTION_STRING)
gremlin = client.Client(
    f"{COSMOS_ENDPOINT}/gremlin","g",
    username=f"/dbs/{DATABASE}/colls/{CONTAINER}",
    password=COSMOS_KEY,
    message_serializer=serializer.GraphSONSerializersV2d0())

# ─────────────  CARGA DE CHUNKS  ──────────────────────────────
chunk_text, chunk_entities = {}, {}
print("📥  Loading chunks …")
for b in blob.get_container_client(CHUNKS_CONTAINER).list_blobs():
    if not b.name.endswith(".json"): continue
    doc = json.loads(blob.get_blob_client(CHUNKS_CONTAINER, b.name).download_blob().readall())
    raw_id = doc.get("chunkId") or doc.get("metadata_storage_path") or b.name
    cid    = clean_id(raw_id)
    chunk_text[cid] = doc.get("content") or doc.get("text") or ""
    chunk_entities[cid] = []

print("📥  Loading entities …")
for cont in ENTITY_CONTAINERS:
    label = CONTAINER_TO_LABEL[cont]
    cc    = blob.get_container_client(cont)
    for b in cc.list_blobs():
        if not b.name.endswith(".json"): continue
        data = json.loads(cc.get_blob_client(b).download_blob().readall())
        if isinstance(data, dict): data = [data]
        for e in data:
            raw_cid = e.get("chunkId") or e.get("metadata_storage_path")
            if raw_cid and raw_cid.startswith(f"{CHUNKS_CONTAINER}/"):
                raw_cid = raw_cid[len(CHUNKS_CONTAINER)+1:]
            cid = clean_id(raw_cid)
            if cid not in chunk_entities:
                print(f"⚠️  Unmatched entity → {raw_cid}")
                continue
            name = e.get("text") or e.get("name")
            vid  = f"{label}:{hashlib.sha1(name.encode()).hexdigest()}"
            chunk_entities[cid].append({"id":vid,"label":label,"name":name})

print(f"➡️  Prepared {len(chunk_entities)} chunks.")

PROMPT = """You are a knowledge-graph extractor.
Return only factual triples (pure JSON):
[{{"source":"<id>","relation":"<label>","target":"<id>"}}]

TEXT:
\"\"\"{chunk}\"\"\"

ENTITIES (pairs [id, name]):
{ents}
"""

# ─────────────  MAIN LOOP  ────────────────────────────────────
for cid, ents in chunk_entities.items():
    if early_exit: break
    if not ents:   continue

    print(f"\n🚩  Chunk {cid[:60]}  ({len(ents)} entities)")
    text      = chunk_text[cid][:3000]
    ents_json = [[e["id"], e["name"]] for e in ents]

    # Llamada LLM
    # ── LLM call + robust JSON parse ───────────────────────────
    try:
        rsp = aoai.chat.completions.create(
            model       = DEPLOYMENT_NAME,     # "gpt-4o"
            temperature = 0.0,
            max_tokens  = 256,
            messages = [
                {"role":"system","content":
                "You are a knowledge-graph extractor. "
                "Return only factual triples in valid JSON."},
                {"role":"user","content":
                PROMPT.format(chunk=text, ents=json.dumps(ents_json))}
            ]
        )

        raw = (rsp.choices[0].message.content or "").strip()
        print("🧠  RAW reply:", raw[:120].replace("\n"," ") + ("…" if len(raw) > 120 else ""))

        if not raw:
            print("⚠️  Empty response (content filter?).")
            triples = []

        else:
            try:
                triples = json.loads(raw)
                print(f"🧠  Parsed {len(triples)} triples")
            except json.JSONDecodeError as je:
                print("⚠️  JSONDecodeError:", je)
                print("⚠️  Full reply kept for manual inspection:")
                print(raw)
                triples = []

    except Exception as ex:
        print("❌  LLM call failed:", ex)
        triples = []


    ts = int(time.time()*1000)

    # Chunk vertex
    if not limit_reached():
        gremlin.submit(
            f"g.V('{cid}').fold().coalesce(unfold(),"
            f"addV('Chunk').property(id,'{cid}')"
            f".property('{PARTITION_KEY}','{PARTITION_VALUE}'))").all()

    # Entity vertices & MENTIONS
    for e in ents:
        if limit_reached(): break
        try:
            gremlin.submit(
                f"g.V('{e['id']}').fold().coalesce(unfold(),"
                f"addV('{e['label']}').property(id,'{e['id']}')"
                f".property('name','{clean_txt(e['name'])}')"
                f".property('{PARTITION_KEY}','{PARTITION_VALUE}'))").all()
            gremlin.submit(
                f"g.V('{cid}').coalesce("
                f"outE('MENTIONS').where(inV().hasId('{e['id']}')),"
                f"addE('MENTIONS').to(g.V('{e['id']}')).property('ts',{ts}))").all()
            vertex_count += 1
            print(f"   ✔︎ {e['id']}")
        except Exception:
            print("⚠️  Vert/Edge error\n", traceback.format_exc())

    # Semantic edges
    if not limit_reached():
        for t in triples:
            s,r,d = t.get("source"), t.get("relation"), t.get("target")
            if not (s and r and d): continue
            try:
                gremlin.submit(
                    f"g.V('{s}').coalesce("
                    f"outE('{r}').where(inV().hasId('{d}')),"
                    f"addE('{r}').to(g.V('{d}')))").all()
                print(f"   ⇢ {s} -[{r}]-> {d}")
            except Exception:
                print("⚠️  Edge error\n", traceback.format_exc())

    if early_exit:
        print(f"🛑  Reached {MAX_VERTICES} vertices (TEST).")
        break

    time.sleep(0.05)

print("\n🏁  Finished.")
gremlin.close()


================================================================================


################################################################################
# File: scripts/graph_stages/agenda_parser.py
################################################################################

# File: scripts/graph_stages/agenda_parser.py

"""
Agenda Parser Module
===================
Extracts item codes and mappings from city commission agendas.
"""
import re
from typing import Dict, List, Optional, Tuple
import logging

log = logging.getLogger(__name__)

class AgendaItemParser:
    """Parse agenda items and extract relationships to other documents."""
    
    # Common agenda item patterns
    ITEM_PATTERNS = [
        # E-1, F-12, K-3, etc.
        re.compile(r'^([A-Z])-(\d+)\.?\s*(.+)', re.MULTILINE),
        # E1, F12 (without dash)
        re.compile(r'^([A-Z])(\d+)\.?\s*(.+)', re.MULTILINE),
        # Item E-1, Item F-12
        re.compile(r'^Item\s+([A-Z])-(\d+)\.?\s*(.+)', re.MULTILINE | re.IGNORECASE),
        # Agenda Item E-1
        re.compile(r'^Agenda\s+Item\s+([A-Z])-(\d+)\.?\s*(.+)', re.MULTILINE | re.IGNORECASE),
    ]
    
    # Document type indicators
    TYPE_INDICATORS = {
        'ordinance': ['ordinance', 'amending', 'zoning', 'code amendment'],
        'resolution': ['resolution', 'approving', 'authorizing', 'accepting'],
        'proclamation': ['proclamation', 'declaring', 'recognizing'],
        'contract': ['contract', 'agreement', 'bid', 'purchase'],
        'minutes': ['minutes', 'approval of minutes'],
    }
    
    # Document number patterns
    DOC_NUMBER_PATTERNS = [
        re.compile(r'(?:Ordinance|Ord\.?)\s+(?:No\.?\s*)?(\d{4}-\d+)', re.IGNORECASE),
        re.compile(r'(?:Resolution|Res\.?)\s+(?:No\.?\s*)?(\d{4}-\d+)', re.IGNORECASE),
        re.compile(r'(?:Contract|Agreement)\s+(?:No\.?\s*)?(\d{4}-\d+)', re.IGNORECASE),
    ]

def parse_agenda_items(agenda_data: Dict) -> Dict[str, Dict]:
    """
    Parse agenda document and extract item mappings.
    
    Returns:
        Dict mapping document numbers to their agenda items and metadata
        Example: {
            "2024-66": {
                "item_code": "E-1",
                "type": "Resolution",
                "description": "A Resolution approving...",
                "sponsor": "Commissioner Smith"
            }
        }
    """
    parser = AgendaItemParser()
    item_mappings = {}
    
    # Combine all text from sections
    full_text = ""
    for section in agenda_data.get("sections", []):
        full_text += section.get("text", "") + "\n\n"
    
    # Find all agenda items
    items = _extract_agenda_items(full_text)
    
    for item in items:
        # Extract document numbers from item description
        doc_numbers = _extract_document_numbers(item['description'])
        
        # Determine document type
        doc_type = _determine_document_type(item['description'])
        
        # Extract sponsor if present
        sponsor = _extract_sponsor(item['description'])
        
        # Map each document number to this item
        for doc_num in doc_numbers:
            item_mappings[doc_num] = {
                'item_code': item['code'],
                'type': doc_type,
                'description': item['description'][:500],  # Truncate long descriptions
                'sponsor': sponsor
            }
            
            log.info(f"Mapped {doc_num} -> {item['code']} ({doc_type})")
    
    return item_mappings

def _extract_agenda_items(text: str) -> List[Dict]:
    """Extract all agenda items from text."""
    items = []
    
    # Try each pattern
    for pattern in AgendaItemParser.ITEM_PATTERNS:
        matches = pattern.finditer(text)
        for match in matches:
            letter, number, description = match.groups()
            code = f"{letter}-{number}"
            
            # Extract full item text (until next item or section)
            start_pos = match.start()
            end_pos = _find_item_end(text, start_pos)
            full_description = text[match.start():end_pos].strip()
            
            items.append({
                'code': code,
                'letter': letter,
                'number': number,
                'description': full_description
            })
    
    # Remove duplicates and sort
    seen = set()
    unique_items = []
    for item in sorted(items, key=lambda x: (x['letter'], int(x['number']))):
        if item['code'] not in seen:
            seen.add(item['code'])
            unique_items.append(item)
    
    return unique_items

def _find_item_end(text: str, start_pos: int) -> int:
    """Find where an agenda item description ends."""
    # Look for next item pattern or section header
    next_item_pos = len(text)
    
    # Check for next item
    for pattern in AgendaItemParser.ITEM_PATTERNS:
        match = pattern.search(text, start_pos + 1)
        if match:
            next_item_pos = min(next_item_pos, match.start())
    
    # Check for section headers
    section_pattern = re.compile(r'^[A-Z][.\s]+[A-Z\s]+$', re.MULTILINE)
    section_match = section_pattern.search(text, start_pos + 1)
    if section_match:
        next_item_pos = min(next_item_pos, section_match.start())
    
    return next_item_pos

def _extract_document_numbers(text: str) -> List[str]:
    """Extract document numbers from item description."""
    numbers = []
    
    for pattern in AgendaItemParser.DOC_NUMBER_PATTERNS:
        matches = pattern.findall(text)
        numbers.extend(matches)
    
    # Also look for standalone year-number patterns
    standalone_pattern = re.compile(r'\b(\d{4}-\d+)\b')
    matches = standalone_pattern.findall(text)
    for match in matches:
        if match not in numbers:  # Avoid duplicates
            numbers.append(match)
    
    return numbers

def _determine_document_type(text: str) -> str:
    """Determine document type from description text."""
    text_lower = text.lower()
    
    # Check each type's indicators
    for doc_type, indicators in AgendaItemParser.TYPE_INDICATORS.items():
        for indicator in indicators:
            if indicator in text_lower:
                return doc_type.title()
    
    # Check explicit type mentions
    if 'ordinance' in text_lower:
        return 'Ordinance'
    elif 'resolution' in text_lower:
        return 'Resolution'
    
    return 'Document'  # Default

def _extract_sponsor(text: str) -> Optional[str]:
    """Extract sponsor name from item description."""
    # Common sponsor patterns
    patterns = [
        re.compile(r'(?:Sponsored by|Sponsor:)\s*([^,\n]+)', re.IGNORECASE),
        re.compile(r'\(([^)]+)\)$'),  # Name in parentheses at end
        re.compile(r'(?:Commissioner|Mayor|Vice Mayor)\s+([A-Za-z\s]+?)(?:\n|$)', re.IGNORECASE)
    ]
    
    for pattern in patterns:
        match = pattern.search(text)
        if match:
            sponsor = match.group(1).strip()
            # Clean up common suffixes
            sponsor = re.sub(r'\s*\)$', '', sponsor)
            return sponsor
    
    return None


================================================================================


################################################################################
# File: requirements.txt
################################################################################

################################################################################
################################################################################
# Python dependencies for Misophonia Research RAG System

# Core dependencies
openai>=1.82.0
gremlinpython>=3.7.3
aiofiles>=24.1.0
python-dotenv>=1.0.0

# Graph Visualization dependencies
dash>=2.14.0
plotly>=5.17.0
networkx>=3.2.0
dash-table>=5.0.0
dash-cytoscape>=0.3.0

# Database and API dependencies
supabase>=2.0.0

# Optional for async progress bars
tqdm

# Data processing
pandas>=2.0.0
numpy

# PDF processing
PyPDF2>=3.0.0
unstructured>=0.10.0
pdfminer.six>=20221105

# Utilities
colorama>=0.4.6

# For concurrent processing
concurrent-log-handler>=0.9.20

# Async dependencies for optimized pipeline
aiohttp>=3.8.0

# Token counting for OpenAI rate limiting
tiktoken>=0.5.0


================================================================================

