# Concatenated Project Code - Part 1 of 3
# Generated: 2025-06-05 10:54:45
# Root Directory: /Users/gianmariatroiani/Documents/knologi̊/graph_database
================================================================================

# Directory Structure
################################################################################
├── .gitignore (939.0B, no ext)
├── city_clerk_documents/
│   ├── extracted_text/
│   │   ├── Agenda 01.9.2024_docling_extracted.json (53.4KB, .json)
│   │   ├── Agenda 01.9.2024_extracted.json (53.4KB, .json)
│   │   ├── Agenda 01.9.2024_full_text.txt (16.1KB, .txt)
│   │   ├── Agenda 03.12.2024_docling_extracted.json (75.6KB, .json)
│   │   ├── Agenda 03.12.2024_extracted.json (75.6KB, .json)
│   │   └── Agenda 03.12.2024_full_text.txt (24.4KB, .txt)
│   ├── global copy/
│   │   ├── City Comissions 2024/
│   │   │   ├── Agendas/
│   │   │   │   ├── Agenda 01.23.2024.pdf (155.1KB, .pdf)
│   │   │   │   ├── Agenda 01.9.2024.pdf (151.1KB, .pdf)
│   │   │   │   ├── Agenda 02.13.2024.pdf (155.0KB, .pdf)
│   │   │   │   ├── Agenda 02.27.2024.pdf (152.9KB, .pdf)
│   │   │   │   ├── Agenda 03.12.2024.pdf (166.6KB, .pdf)
│   │   │   │   ├── Agenda 05.07.2024.pdf (167.6KB, .pdf)
│   │   │   │   ├── Agenda 05.21.2024.pdf (172.2KB, .pdf)
│   │   │   │   └── Agenda 06.11.2024.pdf (160.2KB, .pdf)
│   │   │   ├── ExportedFolderContents.zip (62.4MB, .zip)
│   │   │   ├── Ordinances/
│   │   │   │   ├── 2024/
│   │   │   │   │   ├── 2024-01 - 01_09_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-02 - 01_09_2024.pdf (15.1MB, .pdf)
│   │   │   │   │   ├── 2024-03 - 01_09_2024.pdf (11.1MB, .pdf)
│   │   │   │   │   ├── 2024-04 - 01_23_2024.pdf (2.3MB, .pdf)
│   │   │   │   │   ├── 2024-05 - 02_13_2024.pdf (2.0MB, .pdf)
│   │   │   │   │   ├── 2024-06 - 02_13_2024.pdf (1.9MB, .pdf)
│   │   │   │   │   ├── 2024-07 - 02_13_2024.pdf (6.5MB, .pdf)
│   │   │   │   │   ├── 2024-08 - 02_27_2024.pdf (2.5MB, .pdf)
│   │   │   │   │   ├── 2024-09 - 02_27_2024.pdf (1.6MB, .pdf)
│   │   │   │   │   ├── 2024-10 - 03_12_2024.pdf (3.7MB, .pdf)
│   │   │   │   │   ├── 2024-11 - 03_12_2024 - (As Amended).pdf (5.6MB, .pdf)
│   │   │   │   │   ├── 2024-11 - 03_12_2024.pdf (5.5MB, .pdf)
│   │   │   │   │   ├── 2024-12 - 03_12_2024.pdf (1.8MB, .pdf)
│   │   │   │   │   ├── 2024-13 - 03_12_2024.pdf (3.2MB, .pdf)
│   │   │   │   │   ├── 2024-14 - 05_07_2024.pdf (2.8MB, .pdf)
│   │   │   │   │   ├── 2024-15 - 05_07_2024.pdf (3.9MB, .pdf)
│   │   │   │   │   ├── 2024-16 - 05_07_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-17 - 05_07_2024.pdf (4.3MB, .pdf)
│   │   │   │   │   ├── 2024-18 - 05_21_2024.pdf (2.4MB, .pdf)
│   │   │   │   │   ├── 2024-19 - 05_21_2024.pdf (1.7MB, .pdf)
│   │   │   │   │   ├── 2024-20 - 05_21_2024.pdf (1.6MB, .pdf)
│   │   │   │   │   ├── 2024-21 - 06_11_2024.pdf (1.7MB, .pdf)
│   │   │   │   │   ├── 2024-22 - 06_11_2024.pdf (2.0MB, .pdf)
│   │   │   │   │   ├── 2024-23 - 06_11_2024.pdf (1.9MB, .pdf)
│   │   │   │   │   ├── 2024-24 - 06_11_2024.pdf (2.1MB, .pdf)
│   │   │   │   │   └── 2024-25 - 06_11_2024.pdf (1.3MB, .pdf)
│   │   │   │   ├── [EXCLUDED] 1 items: .DS_Store (excluded file)
│   │   │   ├── Resolutions/
│   │   │   │   ├── 2024/
│   │   │   │   │   ├── 2024-01 - 01_09_2024.pdf (448.9KB, .pdf)
│   │   │   │   │   ├── 2024-02 - 01_09_2024.pdf (451.9KB, .pdf)
│   │   │   │   │   ├── 2024-03 - 01_09_2024.pdf (867.3KB, .pdf)
│   │   │   │   │   ├── 2024-04 - 01_09_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-05 - 01_09_2024.pdf (936.9KB, .pdf)
│   │   │   │   │   ├── 2024-06 - 01_09_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-07 - 01_09_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-08 - 01_23_2024.pdf (457.9KB, .pdf)
│   │   │   │   │   ├── 2024-09 - 01_23_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-10 - 01_23_2024.pdf (454.8KB, .pdf)
│   │   │   │   │   ├── 2024-10 - 03_12_2024.pdf (3.7MB, .pdf)
│   │   │   │   │   ├── 2024-100 - 05_21_2024.pdf (3.0MB, .pdf)
│   │   │   │   │   ├── 2024-101 - 05_21_2024.pdf (947.4KB, .pdf)
│   │   │   │   │   ├── 2024-102 - 05_21_2024.pdf (466.0KB, .pdf)
│   │   │   │   │   ├── 2024-103 - 05_21_2024.pdf (991.8KB, .pdf)
│   │   │   │   │   ├── 2024-104 - 05_21_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-105 - 05_21_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-106 - 05_21_2024.pdf (2.4MB, .pdf)
│   │   │   │   │   ├── 2024-107 - 05_21_2024.pdf (6.0MB, .pdf)
│   │   │   │   │   ├── 2024-108 - 05_21_2024.pdf (2.1MB, .pdf)
│   │   │   │   │   ├── 2024-109 - 05_21_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-11 - 01_23_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-110 - 05_21_2024.pdf (921.5KB, .pdf)
│   │   │   │   │   ├── 2024-111 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-112 - 05_21_2024.pdf (6.2MB, .pdf)
│   │   │   │   │   ├── 2024-113 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-114 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-115 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-116 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-117 - 05_21_2024.pdf (6.4MB, .pdf)
│   │   │   │   │   ├── 2024-118 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-119 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-12 - 01_23_2024.pdf (588.3KB, .pdf)
│   │   │   │   │   ├── 2024-120 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-121 - 05_21_2024.pdf (6.4MB, .pdf)
│   │   │   │   │   ├── 2024-122 - 05_21_2024.pdf (6.8MB, .pdf)
│   │   │   │   │   ├── 2024-123 - 05_21_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-124 - 05_21_2024.pdf (11.2MB, .pdf)
│   │   │   │   │   ├── 2024-125 - 05_21_2024.pdf (776.0KB, .pdf)
│   │   │   │   │   ├── 2024-126 - 05_21_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-127 - 05_21_2024.pdf (8.1MB, .pdf)
│   │   │   │   │   ├── 2024-129 - 06_11_2024.pdf (1.3MB, .pdf)
│   │   │   │   │   ├── 2024-13 - 01_23_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-130 - 06_11_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-131 - 06_11_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-132 - 06_11_2024.pdf (458.0KB, .pdf)
│   │   │   │   │   ├── 2024-133 - 06_11_2024 -As Amended.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-134 - 06_11_2024.pdf (884.8KB, .pdf)
│   │   │   │   │   ├── 2024-135 - 06_11_2024.pdf (1022.1KB, .pdf)
│   │   │   │   │   ├── 2024-136 - 06_11_2024.pdf (537.4KB, .pdf)
│   │   │   │   │   ├── 2024-137 - 06_11_2024.pdf (1.7MB, .pdf)
│   │   │   │   │   ├── 2024-138 - 06_11_2024.pdf (1.7MB, .pdf)
│   │   │   │   │   ├── 2024-139 - 06_11_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-14 - 01_23_2024.pdf (996.6KB, .pdf)
│   │   │   │   │   ├── 2024-140 - 06_11_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-141 - 06_11_2024.pdf (13.2MB, .pdf)
│   │   │   │   │   ├── 2024-142 - 06_11_2024.pdf (790.5KB, .pdf)
│   │   │   │   │   ├── 2024-143 -06_11_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-15 - 01_23_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-16 - 01_23_2024.pdf (1.8MB, .pdf)
│   │   │   │   │   ├── 2024-17 - 01_23_2024.pdf (488.0KB, .pdf)
│   │   │   │   │   ├── 2024-18 - 01_23_2024.pdf (849.1KB, .pdf)
│   │   │   │   │   ├── 2024-19 - 02_19_2024.pdf (1019.5KB, .pdf)
│   │   │   │   │   ├── 2024-20 - 02_13_2024.pdf (824.5KB, .pdf)
│   │   │   │   │   ├── 2024-21 - 02_13_2024.pdf (502.6KB, .pdf)
│   │   │   │   │   ├── 2024-22 - 02_13_2024.pdf (450.6KB, .pdf)
│   │   │   │   │   ├── 2024-23 - 02_13_2024.pdf (447.7KB, .pdf)
│   │   │   │   │   ├── 2024-24 - 02_13_2024.pdf (464.1KB, .pdf)
│   │   │   │   │   ├── 2024-25 - 02_13_2024.pdf (458.3KB, .pdf)
│   │   │   │   │   ├── 2024-26 - 02_13_2024.pdf (465.7KB, .pdf)
│   │   │   │   │   ├── 2024-27 - 02_13_2024.pdf (754.2KB, .pdf)
│   │   │   │   │   ├── 2024-28 - 02_13_2024.pdf (761.7KB, .pdf)
│   │   │   │   │   ├── 2024-29 - 02_13_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-30 - 02_13_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-31 - 02_13_2024.pdf (936.6KB, .pdf)
│   │   │   │   │   ├── 2024-32 - 02_13_2024.pdf (1.9MB, .pdf)
│   │   │   │   │   ├── 2024-33 - 02_13_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-34 - 02_13_2024.pdf (1.9MB, .pdf)
│   │   │   │   │   ├── 2024-35 - 02_27_2024.pdf (480.7KB, .pdf)
│   │   │   │   │   ├── 2024-36 - 02_27_2024.pdf (1.3MB, .pdf)
│   │   │   │   │   ├── 2024-37 - 02_27_2024.pdf (940.3KB, .pdf)
│   │   │   │   │   ├── 2024-38 - 02_27_2024.pdf (829.6KB, .pdf)
│   │   │   │   │   ├── 2024-39 - 02_27_2024.pdf (816.5KB, .pdf)
│   │   │   │   │   ├── 2024-41 - 02_27_2024.pdf (833.1KB, .pdf)
│   │   │   │   │   ├── 2024-42 - 02_27_2024.pdf (473.4KB, .pdf)
│   │   │   │   │   ├── 2024-43 - 02_27_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-44 - 02_27_2024.pdf (1.8MB, .pdf)
│   │   │   │   │   ├── 2024-45 - 03_12_2024.pdf (1002.1KB, .pdf)
│   │   │   │   │   ├── 2024-46 - 03_12_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-47 - 03_12_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-48 - 03_12_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-49 - 03_12_2024.pdf (899.6KB, .pdf)
│   │   │   │   │   ├── 2024-50 - 03_12_2024.pdf (462.1KB, .pdf)
│   │   │   │   │   ├── 2024-51 - 03_12_2024.pdf (538.1KB, .pdf)
│   │   │   │   │   ├── 2024-52 - 03_12_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-53 - 03_12_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-54 - 03_12_2024.pdf (2.0MB, .pdf)
│   │   │   │   │   ├── 2024-55 - 03_12_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-56 - 03_12_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-57 - 03_12_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-58 - 03_12_2024.pdf (775.8KB, .pdf)
│   │   │   │   │   ├── 2024-60 - 03_12_2024.pdf (2.3MB, .pdf)
│   │   │   │   │   ├── 2024-61 - 04_16_2024.pdf (6.4MB, .pdf)
│   │   │   │   │   ├── 2024-62 - 04_16_2024.pdf (2.7MB, .pdf)
│   │   │   │   │   ├── 2024-63 -  04_16_2024.pdf (832.3KB, .pdf)
│   │   │   │   │   ├── 2024-64 - 04_16_2024.pdf (452.6KB, .pdf)
│   │   │   │   │   ├── 2024-65 - 04_16_2024.pdf (894.5KB, .pdf)
│   │   │   │   │   ├── 2024-66 - 04_16_2024.pdf (446.6KB, .pdf)
│   │   │   │   │   ├── 2024-67 - 04_16_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-68 - 04_16_2024.pdf (5.6MB, .pdf)
│   │   │   │   │   ├── 2024-69- -04_16_2024.pdf (443.1KB, .pdf)
│   │   │   │   │   ├── 2024-70 - 04_16_2024.pdf (878.9KB, .pdf)
│   │   │   │   │   ├── 2024-71 - 04_16_2024.pdf (951.8KB, .pdf)
│   │   │   │   │   ├── 2024-72 - 04_16_2024.pdf (821.7KB, .pdf)
│   │   │   │   │   ├── 2024-73 - 04_16_2024.pdf (810.8KB, .pdf)
│   │   │   │   │   ├── 2024-74 - 04_16_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-75 - 04_16_2024.pdf (1.6MB, .pdf)
│   │   │   │   │   ├── 2024-76 - 04_16_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-77 - 04_16_2024.pdf (1013.9KB, .pdf)
│   │   │   │   │   ├── 2024-78 - 04_16_2024.pdf (1007.5KB, .pdf)
│   │   │   │   │   ├── 2024-79 - 04_16_2024.pdf (997.7KB, .pdf)
│   │   │   │   │   ├── 2024-80 - 04_16_2024.pdf (525.7KB, .pdf)
│   │   │   │   │   ├── 2024-81 - 04_16_2024.pdf (923.2KB, .pdf)
│   │   │   │   │   ├── 2024-82 - 04_16_2024.pdf (473.6KB, .pdf)
│   │   │   │   │   ├── 2024-83 - 04_16_2024.pdf (915.3KB, .pdf)
│   │   │   │   │   ├── 2024-84 - 05_07_2024.pdf (992.7KB, .pdf)
│   │   │   │   │   ├── 2024-85 - 05_07_2024.pdf (2.0MB, .pdf)
│   │   │   │   │   ├── 2024-86 - 05_07_2024.pdf (503.1KB, .pdf)
│   │   │   │   │   ├── 2024-87 - 05_07_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-88 - 05_07_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-89 - 05_07_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-90 - 05_07_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-91 - 05_07_2024.pdf (1.8MB, .pdf)
│   │   │   │   │   ├── 2024-92 - 05_07_2024.pdf (452.4KB, .pdf)
│   │   │   │   │   ├── 2024-93 - 05_07_2024.pdf (1014.3KB, .pdf)
│   │   │   │   │   ├── 2024-94 - 05_05_2024.pdf (1.3MB, .pdf)
│   │   │   │   │   ├── 2024-95 - 05_07_2024.pdf (974.5KB, .pdf)
│   │   │   │   │   ├── 2024-96 - 05_07_2024.pdf (1.5MB, .pdf)
│   │   │   │   │   ├── 2024-97 - 05_07_2024.pdf (1023.3KB, .pdf)
│   │   │   │   │   ├── 2024-98 - 05_07_2024.pdf (2.2MB, .pdf)
│   │   │   │   │   └── 2024-99 - 05_07_2024.pdf (792.7KB, .pdf)
│   │   │   │   ├── [EXCLUDED] 1 items: .DS_Store (excluded file)
│   │   │   ├── Verbating Items/
│   │   │   │   ├── 2024/
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - E-4.pdf (2.4MB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - E-5.pdf (735.6KB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - E-7.pdf (306.9KB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - E-8.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - F-10.pdf (925.3KB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - F-2.pdf (288.2KB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - F-5.pdf (360.6KB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - F-6.pdf (212.8KB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - Public Comment.pdf (762.7KB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - C- Public Comment.pdf (1.3MB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - E-3 and E-4.pdf (2.4MB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - E-5.pdf (290.7KB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - F-1.pdf (855.5KB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - F-5.pdf (3.7MB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - F-7.pdf (768.6KB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - H-1.pdf (1.5MB, .pdf)
│   │   │   │   │   ├── 02_13_2024 - Meeting Minutes - Public.pdf (363.5KB, .pdf)
│   │   │   │   │   ├── 02_13_2024 - Verbatim Transcripts - F-12.pdf (7.2MB, .pdf)
│   │   │   │   │   ├── 02_13_2024 - Verbatim Transcripts - H-1.pdf (570.8KB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - E-1.pdf (4.2MB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - E-2.pdf (281.3KB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - E-3.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - F-10.pdf (2.5MB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - F-12.pdf (448.2KB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - H-1.pdf (794.2KB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - Public Comment.pdf (439.4KB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - E-1.pdf (124.3KB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - E-2.pdf (487.9KB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - E-4 and E-11.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - F-1.pdf (416.0KB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - F-2.pdf (1.5MB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - F-4 and F-5 and F-11 and F-12.pdf (1.4MB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - H-1.pdf (121.5KB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - K - Discussion Items.pdf (285.0KB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - Public Comment.pdf (412.4KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - 2-1 AND 2-2.pdf (652.3KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - E-11.pdf (365.8KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - E-2.pdf (369.8KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - E-4.pdf (410.2KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - E-5 E-6 E-7 E-8 E-9 E-10.pdf (3.6MB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-1.pdf (751.9KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-10.pdf (1.3MB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-11.pdf (2.4MB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-12.pdf (2.8MB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-15.pdf (255.0KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-5.pdf (205.0KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-8.pdf (961.1KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-9.pdf (330.1KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - Public Comment.pdf (1.5MB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - 2-1.pdf (1.7MB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - E-1.pdf (4.6MB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - E-10.pdf (128.5KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - E-11.pdf (559.6KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - E-12.pdf (156.1KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - E-8.pdf (288.5KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - E-9.pdf (263.4KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - F-1.pdf (561.1KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - F-13.pdf (151.9KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - F-16.pdf (782.8KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - F-3.pdf (352.6KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - F-9.pdf (736.0KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - H-1.pdf (455.3KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - Public Comment.pdf (1.6MB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - D-3.pdf (539.3KB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - E-10.pdf (629.7KB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - E-3.pdf (350.4KB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - F-2.pdf (1.7MB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - F-3.pdf (2.4MB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - F-4.pdf (3.4MB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - F-5.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - F-7 and F-10.pdf (1.6MB, .pdf)
│   │   │   │   │   └── 06_11_2024 - Verbatim Transcripts - Public Comment.pdf (513.4KB, .pdf)
│   │   │   │   ├── [EXCLUDED] 1 items: .DS_Store (excluded file)
│   │   │   ├── [EXCLUDED] 1 items: .DS_Store (excluded file)
│   │   ├── [EXCLUDED] 1 items: .DS_Store (excluded file)
│   ├── [EXCLUDED] 2 items: .DS_Store (excluded file), global (excluded dir)
├── config.py (1.7KB, .py)
├── graph_clear_database.py (963.0B, .py)
├── graph_visualizer.py (25.1KB, .py)
├── ontology_model.txt (6.6KB, .txt)
├── ontology_modelv2.txt (10.2KB, .txt)
├── pipeline_debug.log (81.3KB, .log)
├── repository_directory_structure.txt (5.7KB, .txt)
├── requirements.txt (1018.0B, .txt)
├── run_city_clerk_pipeline.sh (440.0B, .sh)
├── run_graph_pipeline.sh (1.3KB, .sh)
├── run_graph_visualizer.sh (3.6KB, .sh)
├── scripts/
│   ├── check_extracted_urls.py (922.0B, .py)
│   ├── check_graph_empty.py (1.8KB, .py)
│   ├── debug_visualizer.py (3.4KB, .py)
│   ├── graph_pipeline.py (21.8KB, .py)
│   ├── graph_stages/
│   │   ├── __init__.py (647.0B, .py)
│   │   ├── agenda_graph_builder.py (63.2KB, .py)
│   │   ├── agenda_ontology_extractor.py (25.7KB, .py)
│   │   ├── agenda_pdf_extractor.py (24.6KB, .py)
│   │   ├── cosmos_db_client.py (10.2KB, .py)
│   │   ├── document_linker.py (12.9KB, .py)
│   │   ├── enhanced_document_linker.py (18.3KB, .py)
│   │   ├── ontology_extractor.py (41.9KB, .py)
│   │   ├── pdf_extractor.py (6.5KB, .py)
│   │   └── verbatim_transcript_linker.py (13.9KB, .py)
│   ├── graphrag_breakdown/
│   ├── minimal_url_test.py (1.8KB, .py)
│   ├── test_e1_urls.py (748.0B, .py)
│   ├── test_graph_urls.py (2.0KB, .py)
│   ├── test_ontology_urls.py (2.3KB, .py)
│   ├── test_url_extraction.py (4.0KB, .py)
│   ├── [EXCLUDED] 4 items: pipeline_modular_optimized.py (excluded file), rag_local_web_app.py (excluded file), stages (excluded dir)
│       ... and 1 more excluded items
├── test_query_system.py (8.6KB, .py)
├── [EXCLUDED] 9 items: .DS_Store (excluded file), .env (excluded file), .git (excluded dir)
    ... and 6 more excluded items


================================================================================


# File Index - Which Files Are in Which Parts
################################################################################

## Part 1 (6 files):
  - scripts/graph_stages/enhanced_document_linker.py
  - scripts/graph_stages/pdf_extractor.py
  - scripts/test_ontology_urls.py
  - scripts/minimal_url_test.py
  - requirements.txt
  - scripts/test_e1_urls.py

## Part 2 (6 files):
  - scripts/graph_stages/verbatim_transcript_linker.py
  - test_query_system.py
  - scripts/test_url_extraction.py
  - scripts/test_graph_urls.py
  - graph_clear_database.py
  - scripts/check_extracted_urls.py

## Part 3 (6 files):
  - scripts/graph_stages/document_linker.py
  - scripts/graph_stages/cosmos_db_client.py
  - scripts/debug_visualizer.py
  - scripts/check_graph_empty.py
  - config.py
  - scripts/graph_stages/__init__.py


================================================================================


################################################################################
# File: scripts/graph_stages/enhanced_document_linker.py
################################################################################

# File: scripts/graph_stages/enhanced_document_linker.py

"""
Enhanced Document Linker
Links both ordinance and resolution documents to their corresponding agenda items.
"""

import logging
import re
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import json
from datetime import datetime
import PyPDF2
from openai import OpenAI
import os
from dotenv import load_dotenv

load_dotenv()

log = logging.getLogger('enhanced_document_linker')


class EnhancedDocumentLinker:
    """Links ordinance and resolution documents to agenda items."""
    
    def __init__(self,
                 openai_api_key: Optional[str] = None,
                 model: str = "gpt-4.1-mini-2025-04-14",
                 agenda_extraction_max_tokens: int = 32768):
        """Initialize the enhanced document linker."""
        self.api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY not found in environment")
        
        self.client = OpenAI(api_key=self.api_key)
        self.model = model
        self.agenda_extraction_max_tokens = agenda_extraction_max_tokens
    
    async def link_documents_for_meeting(self, 
                                       meeting_date: str,
                                       ordinances_dir: Path,
                                       resolutions_dir: Path) -> Dict[str, List[Dict]]:
        """Find and link all documents (ordinances AND resolutions) for a specific meeting date."""
        log.info(f"🔗 Enhanced linking: documents for meeting date: {meeting_date}")
        log.info(f"📁 Ordinances directory: {ordinances_dir}")
        log.info(f"📁 Resolutions directory: {resolutions_dir}")
        
        # Create debug directory
        debug_dir = Path("city_clerk_documents/graph_json/debug")
        debug_dir.mkdir(exist_ok=True)
        
        # Convert date format: "01.09.2024" -> "01_09_2024"
        date_underscore = meeting_date.replace(".", "_")
        
        # Initialize results
        linked_documents = {
            "ordinances": [],
            "resolutions": []
        }
        
        # Process ordinances
        if ordinances_dir.exists():
            ordinance_files = self._find_matching_files(ordinances_dir, date_underscore)
            log.info(f"📄 Found {len(ordinance_files)} ordinance files")
            
            for doc_path in ordinance_files:
                doc_info = await self._process_document(doc_path, meeting_date, "ordinance")
                if doc_info:
                    linked_documents["ordinances"].append(doc_info)
        else:
            log.warning(f"⚠️  Ordinances directory not found: {ordinances_dir}")
        
        # Process resolutions - NEW LOGIC
        if resolutions_dir.exists():
            # Check for year subdirectory first
            year = meeting_date.split('.')[-1]  # Extract year from date
            year_dir = resolutions_dir / year
            
            if year_dir.exists():
                resolution_files = self._find_matching_files(year_dir, date_underscore)
                log.info(f"📄 Found {len(resolution_files)} resolution files in {year} directory")
            else:
                # Fall back to main resolutions directory
                resolution_files = self._find_matching_files(resolutions_dir, date_underscore)
                log.info(f"📄 Found {len(resolution_files)} resolution files in main directory")
            
            for doc_path in resolution_files:
                doc_info = await self._process_document(doc_path, meeting_date, "resolution")
                if doc_info:
                    linked_documents["resolutions"].append(doc_info)
        else:
            log.warning(f"⚠️  Resolutions directory not found: {resolutions_dir}")
        
        # Save enhanced linked documents info
        with open(debug_dir / "enhanced_linked_documents.json", 'w') as f:
            json.dump(linked_documents, f, indent=2)
        
        # Log summary
        total_linked = len(linked_documents['ordinances']) + len(linked_documents['resolutions'])
        log.info(f"✅ Enhanced linking complete:")
        log.info(f"   📄 Ordinances: {len(linked_documents['ordinances'])}")
        log.info(f"   📄 Resolutions: {len(linked_documents['resolutions'])}")
        log.info(f"   📄 Total linked: {total_linked}")
        
        # Save detailed report
        self._generate_linking_report(meeting_date, linked_documents, debug_dir)
        
        return linked_documents
    
    def _find_matching_files(self, directory: Path, date_pattern: str) -> List[Path]:
        """Find all PDF files matching the date pattern."""
        # Pattern: YYYY-## - MM_DD_YYYY.pdf
        pattern = f"*{date_pattern}.pdf"
        matching_files = list(directory.glob(pattern))
        
        # Also try without spaces in case filenames vary
        pattern2 = f"*{date_pattern}*.pdf"
        additional_files = [f for f in directory.glob(pattern2) if f not in matching_files]
        matching_files.extend(additional_files)
        
        # Also check for variations in date format
        # Some files might use dashes instead of underscores
        date_dash = date_pattern.replace("_", "-")
        pattern3 = f"*{date_dash}*.pdf"
        more_files = [f for f in directory.glob(pattern3) if f not in matching_files]
        matching_files.extend(more_files)
        
        return sorted(matching_files)
    
    async def _process_document(self, doc_path: Path, meeting_date: str, doc_type: str) -> Optional[Dict[str, Any]]:
        """Process a single document to extract agenda item reference."""
        try:
            # Extract document number from filename
            doc_match = re.match(r'^(\d{4}-\d{2,3})', doc_path.name)
            if not doc_match:
                log.warning(f"Could not parse document number from {doc_path.name}")
                return None
            
            document_number = doc_match.group(1)
            
            # Extract text from PDF
            text = self._extract_pdf_text(doc_path)
            if not text:
                log.warning(f"No text extracted from {doc_path.name}")
                return None
            
            # Extract agenda item code using LLM
            item_code = await self._extract_agenda_item_code(text, document_number, doc_type)
            
            # Extract additional metadata
            parsed_data = self._parse_document_metadata(text, doc_type)
            
            doc_info = {
                "path": str(doc_path),
                "filename": doc_path.name,
                "document_number": document_number,
                "item_code": item_code,
                "document_type": doc_type.capitalize(),
                "title": self._extract_title(text, doc_type),
                "parsed_data": parsed_data
            }
            
            log.info(f"📄 Processed {doc_type} {doc_path.name}: Item {item_code or 'NOT_FOUND'}")
            return doc_info
            
        except Exception as e:
            log.error(f"Error processing {doc_path.name}: {e}")
            return None
    
    def _extract_pdf_text(self, pdf_path: Path) -> str:
        """Extract text from PDF file."""
        try:
            with open(pdf_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                text_parts = []
                
                # Extract text from all pages
                for page in reader.pages:
                    text = page.extract_text()
                    if text:
                        text_parts.append(text)
                
                return "\n".join(text_parts)
        except Exception as e:
            log.error(f"Failed to extract text from {pdf_path.name}: {e}")
            return ""
    
    async def _extract_agenda_item_code(self, text: str, document_number: str, doc_type: str) -> Optional[str]:
        """Extract agenda item code from document text using LLM."""
        debug_dir = Path("city_clerk_documents/graph_json/debug")
        debug_dir.mkdir(exist_ok=True)
        
        # Try regex patterns first for better accuracy
        patterns = [
            r'Item\s+([A-Z]\.-?\d+\.?)',  # Item D.-1.
            r'Agenda\s+Item[:\s]+([A-Z]\.-?\d+\.?)',  # Agenda Item: D.-1.
            r'Section\s+([A-Z])[,\s]+Item\s+(\d+)',  # Section D, Item 1
            r'consent\s+agenda.*item\s+([A-Z]\.-?\d+\.?)',  # Consent Agenda ... Item D.-1.
            r'\b([A-Z]\.-\d+\.?)\s+\d{2}-\d{4}',  # D.-1. 23-6830 pattern
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                if len(match.groups()) == 2:  # Section X, Item Y format
                    code = f"{match.group(1)}-{match.group(2)}"
                else:
                    code = match.group(1)
                normalized_code = self._normalize_item_code(code)
                log.info(f"✅ Found agenda item code via regex for {document_number}: {normalized_code}")
                return normalized_code
        
        # Customize prompt based on document type
        if doc_type == "resolution":
            doc_type_text = "resolution"
            typical_sections = "F items (e.g., F-1, F-2, F-3)"
        else:
            doc_type_text = "ordinance"
            typical_sections = "E items (e.g., E-1, E-2, E-3)"
        
        prompt = f"""You are analyzing a City of Coral Gables {doc_type_text} document (Document #{document_number}).

Your task is to find the AGENDA ITEM CODE referenced in this document.

CRITICAL INSTRUCTIONS:
1. Search the ENTIRE document for agenda item references
2. Return ONLY the code in this format: AGENDA_ITEM: [code]
3. The code should be ONLY the letter and number (e.g., E-2, F-10, H-1)
4. Do NOT include any explanations, reasoning, or additional text
5. If no agenda item is found, return: AGENDA_ITEM: NOT_FOUND

Examples of valid responses:
- AGENDA_ITEM: E-2
- AGENDA_ITEM: F-10
- AGENDA_ITEM: H-1
- AGENDA_ITEM: NOT_FOUND

DO NOT RETURN ANYTHING ELSE. NO EXPLANATIONS.

Full document text:
{text}"""
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": f"You are a precise data extractor for {doc_type_text} documents. Find and extract only the agenda item code. Search the ENTIRE document thoroughly."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_tokens=self.agenda_extraction_max_tokens
            )
            
            raw_response = response.choices[0].message.content.strip()
            
            # Save raw LLM response for debugging
            with open(debug_dir / f"llm_response_{doc_type}_{document_number}_raw.txt", 'w', encoding='utf-8') as f:
                f.write(raw_response)
            
            # Parse response directly (no qwen parsing needed)
            result = raw_response
            
            # Save cleaned response
            with open(debug_dir / f"llm_response_{doc_type}_{document_number}_cleaned.txt", 'w', encoding='utf-8') as f:
                f.write(result)
            
            # Parse the response
            if "AGENDA_ITEM:" in result:
                # Extract just the code part, stopping at first space or newline after the code
                parts = result.split("AGENDA_ITEM:")[1].strip()
                
                # Extract just the code pattern (letter-number)
                code_match = re.match(r'^([A-Z]-?\d+)', parts)
                if code_match:
                    code = code_match.group(1)
                    if code != "NOT_FOUND":
                        code = self._normalize_item_code(code)
                        log.info(f"✅ Found agenda item code for {document_number}: {code}")
                        return code
                elif parts.startswith("NOT_FOUND"):
                    log.warning(f"❌ LLM could not find agenda item in {document_number}")
                else:
                    # Try to extract code from a messy response
                    code_pattern = r'\b([A-Z]-?\d+)\b'
                    match = re.search(code_pattern, parts)
                    if match:
                        code = self._normalize_item_code(match.group(1))
                        log.info(f"✅ Extracted agenda item code for {document_number}: {code} (from messy response)")
                        return code
                    log.error(f"❌ Could not parse item code from response: {parts[:100]}")
            else:
                log.error(f"❌ Invalid LLM response format for {document_number}: {result[:100]}")
            
            return None
            
        except Exception as e:
            log.error(f"Failed to extract agenda item for {doc_type} {document_number}: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _normalize_item_code(self, code: str) -> str:
        """Normalize item code to consistent format."""
        if not code:
            return code
        
        # Remove trailing dots and spaces
        code = code.rstrip('. ')
        
        # Remove dots between letter and dash: "E.-1" -> "E-1"
        code = re.sub(r'([A-Z])\.(-)', r'\1\2', code)
        
        # Handle cases without dash: "E.1" -> "E-1"
        code = re.sub(r'([A-Z])\.(\d)', r'\1-\2', code)
        
        # Remove any remaining dots
        code = code.replace('.', '')
        
        # Ensure we have a dash between letter and number
        code = re.sub(r'([A-Z])(\d)', r'\1-\2', code)
        
        return code
    
    def _extract_title(self, text: str, doc_type: str) -> str:
        """Extract document title from text."""
        # Look for "AN ORDINANCE" or "A RESOLUTION" pattern
        if doc_type == "resolution":
            pattern = r'(A\s+RESOLUTION[^.]+\.)'
        else:
            pattern = r'(AN?\s+ORDINANCE[^.]+\.)'
            
        title_match = re.search(pattern, text[:2000], re.IGNORECASE)
        if title_match:
            return title_match.group(1).strip()
        
        # Fallback to first substantive line
        lines = text.split('\n')
        for line in lines[:20]:
            if len(line) > 20 and not line.isdigit():
                return line.strip()[:200]
        
        return f"Untitled {doc_type.capitalize()}"
    
    def _parse_document_metadata(self, text: str, doc_type: str) -> Dict[str, Any]:
        """Parse additional metadata from document."""
        metadata = {
            "document_type": doc_type
        }
        
        # Extract date passed
        date_match = re.search(r'day\s+of\s+(\w+),?\s+(\d{4})', text)
        if date_match:
            metadata["date_passed"] = date_match.group(0)
        
        # Extract vote information
        vote_match = re.search(r'PASSED\s+AND\s+ADOPTED.*?(\d+).*?(\d+)', text, re.IGNORECASE | re.DOTALL)
        if vote_match:
            metadata["vote_details"] = {
                "ayes": vote_match.group(1),
                "nays": vote_match.group(2) if len(vote_match.groups()) > 1 else "0"
            }
        
        # Extract motion information
        motion_match = re.search(r'motion\s+(?:was\s+)?made\s+by\s+([^,]+)', text, re.IGNORECASE)
        if motion_match:
            metadata["motion"] = {"moved_by": motion_match.group(1).strip()}
        
        # Extract mayor signature
        mayor_match = re.search(r'Mayor[:\s]+([^\n]+)', text[-1000:])
        if mayor_match:
            metadata["signatories"] = {"mayor": mayor_match.group(1).strip()}
        
        # Resolution-specific metadata
        if doc_type == "resolution":
            # Look for resolution-specific patterns
            purpose_match = re.search(r'(?:WHEREAS|PURPOSE)[:\s]+([^.]+)', text, re.IGNORECASE)
            if purpose_match:
                metadata["purpose"] = purpose_match.group(1).strip()
        
        return metadata
    
    def _generate_linking_report(self, meeting_date: str, linked_documents: Dict, debug_dir: Path):
        """Generate a detailed report of the linking process."""
        report = {
            "meeting_date": meeting_date,
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total_ordinances": len(linked_documents["ordinances"]),
                "total_resolutions": len(linked_documents["resolutions"]),
                "ordinances_with_items": len([d for d in linked_documents["ordinances"] if d.get("item_code")]),
                "resolutions_with_items": len([d for d in linked_documents["resolutions"] if d.get("item_code")]),
                "unlinked_ordinances": len([d for d in linked_documents["ordinances"] if not d.get("item_code")]),
                "unlinked_resolutions": len([d for d in linked_documents["resolutions"] if not d.get("item_code")])
            },
            "details": {
                "ordinances": [
                    {
                        "document_number": d["document_number"],
                        "item_code": d.get("item_code", "NOT_FOUND"),
                        "title": d.get("title", "")[:100]
                    }
                    for d in linked_documents["ordinances"]
                ],
                "resolutions": [
                    {
                        "document_number": d["document_number"],
                        "item_code": d.get("item_code", "NOT_FOUND"),
                        "title": d.get("title", "")[:100]
                    }
                    for d in linked_documents["resolutions"]
                ]
            }
        }
        
        # FIXED: Use double quotes for f-string
        report_filename = f"linking_report_{meeting_date.replace('.', '_')}.json"
        report_path = debug_dir / report_filename
        
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        log.info(f"📊 Linking report saved to: {report_path}")
    
    # Add backward compatibility method
    async def link_documents_for_meeting_legacy(self, 
                                               meeting_date: str,
                                               documents_dir: Path) -> Dict[str, List[Dict]]:
        """Legacy method for backward compatibility - ordinances only."""
        return await self.link_documents_for_meeting(
            meeting_date,
            documents_dir,  # Ordinances directory
            Path("dummy")   # No resolutions directory
        )


================================================================================


################################################################################
# File: scripts/graph_stages/pdf_extractor.py
################################################################################

# scripts/graph_stages/pdf_extractor.py

import os
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import logging
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.datamodel.base_models import InputFormat, DocumentStream
from docling.datamodel.pipeline_options import PdfPipelineOptions
import json
from io import BytesIO

# Set up logging
logging.basicConfig(level=logging.INFO)
log = logging.getLogger(__name__)

class PDFExtractor:
    """Extract text from PDFs using Docling for accurate OCR and text extraction."""
    
    def __init__(self, pdf_dir: Path, output_dir: Optional[Path] = None):
        """Initialize the PDF extractor with Docling."""
        self.pdf_dir = Path(pdf_dir)
        self.output_dir = output_dir or Path("city_clerk_documents/extracted_text")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Create debug directory
        self.debug_dir = self.output_dir / "debug"
        self.debug_dir.mkdir(exist_ok=True)
        
        # Initialize Docling converter with OCR enabled
        # Configure for better OCR and structure detection
        pipeline_options = PdfPipelineOptions()
        pipeline_options.do_ocr = True  # Enable OCR for better text extraction
        pipeline_options.do_table_structure = True  # Better table extraction
        
        self.converter = DocumentConverter(
            format_options={
                InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
            }
        )
    
    def extract_text_from_pdf(self, pdf_path: Path) -> Tuple[str, List[Dict[str, str]]]:
        """
        Extract text from PDF using Docling with OCR support.
        
        Returns:
            Tuple of (full_text, pages) where pages is a list of dicts with 'text' and 'page_num'
        """
        log.info(f"📄 Extracting text from: {pdf_path.name}")
        
        # Convert PDF with Docling - just pass the path directly
        result = self.converter.convert(str(pdf_path))
        
        # Get the document
        doc = result.document
        
        # Get the markdown representation which preserves structure better
        full_markdown = doc.export_to_markdown()
        
        # Extract pages if available
        pages = []
        
        # Try to extract page-level content from the document structure
        if hasattr(doc, 'pages') and doc.pages:
            for page_num, page in enumerate(doc.pages, 1):
                # Get page text
                page_text = ""
                if hasattr(page, 'text'):
                    page_text = page.text
                elif hasattr(page, 'get_text'):
                    page_text = page.get_text()
                else:
                    # Try to extract from elements
                    page_elements = []
                    if hasattr(page, 'elements'):
                        for element in page.elements:
                            if hasattr(element, 'text'):
                                page_elements.append(element.text)
                    page_text = "\n".join(page_elements)
                
                if page_text:
                    pages.append({
                        'text': page_text,
                        'page_num': page_num
                    })
        
        # If we couldn't extract pages, create one page with all content
        if not pages and full_markdown:
            pages = [{
                'text': full_markdown,
                'page_num': 1
            }]
        
        # Use markdown as the full text (better structure preservation)
        full_text = full_markdown if full_markdown else ""
        
        # Save debug information
        debug_info = {
            'file': pdf_path.name,
            'total_pages': len(pages),
            'total_characters': len(full_text),
            'extraction_method': 'docling',
            'has_markdown': bool(full_markdown),
            'doc_attributes': list(dir(doc)) if doc else []
        }
        
        debug_file = self.debug_dir / f"{pdf_path.stem}_extraction_debug.json"
        with open(debug_file, 'w', encoding='utf-8') as f:
            json.dump(debug_info, f, indent=2)
        
        # Save the full extracted text for inspection
        text_file = self.debug_dir / f"{pdf_path.stem}_full_text.txt"
        with open(text_file, 'w', encoding='utf-8') as f:
            f.write(full_text)
        
        # Save markdown version separately for debugging
        if full_markdown:
            markdown_file = self.debug_dir / f"{pdf_path.stem}_markdown.md"
            with open(markdown_file, 'w', encoding='utf-8') as f:
                f.write(full_markdown)
        
        log.info(f"✅ Extracted {len(pages)} pages, {len(full_text)} characters")
        
        return full_text, pages
    
    def extract_all_pdfs(self) -> Dict[str, Dict[str, any]]:
        """Extract text from all PDFs in the directory."""
        pdf_files = list(self.pdf_dir.glob("*.pdf"))
        log.info(f"📚 Found {len(pdf_files)} PDF files to process")
        
        extracted_data = {}
        
        for pdf_path in pdf_files:
            try:
                full_text, pages = self.extract_text_from_pdf(pdf_path)
                
                extracted_data[pdf_path.stem] = {
                    'full_text': full_text,
                    'pages': pages,
                    'metadata': {
                        'filename': pdf_path.name,
                        'num_pages': len(pages),
                        'total_chars': len(full_text),
                        'extraction_method': 'docling'
                    }
                }
                
                # Save extracted text
                output_file = self.output_dir / f"{pdf_path.stem}_extracted.json"
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(extracted_data[pdf_path.stem], f, indent=2, ensure_ascii=False)
                
                log.info(f"✅ Saved extracted text to: {output_file}")
                
            except Exception as e:
                log.error(f"❌ Failed to process {pdf_path.name}: {e}")
                import traceback
                traceback.print_exc()
                # No fallback - if Docling fails, we fail
                raise
        
        return extracted_data

# Add this to requirements.txt:
# unstructured[pdf]==0.10.30
# pytesseract>=0.3.10
# pdf2image>=1.16.3
# poppler-utils (system dependency - install with: brew install poppler)


================================================================================


################################################################################
# File: scripts/test_ontology_urls.py
################################################################################

# File: scripts/test_ontology_urls.py

#!/usr/bin/env python3
"""
Test to verify ontology extractor preserves URLs from pre-extracted data
"""

import sys
from pathlib import Path
sys.path.append('scripts')

from graph_stages.ontology_extractor import OntologyExtractor
import json

def test_ontology_url_preservation():
    """Test that ontology extractor preserves URLs from extracted data."""
    
    # Load the extracted JSON with URLs
    extracted_file = Path("city_clerk_documents/extracted_text/Agenda 01.9.2024_extracted.json")
    
    if not extracted_file.exists():
        print("❌ Extracted file not found. Run URL extraction test first.")
        return False
    
    with open(extracted_file, 'r') as f:
        extracted_data = json.load(f)
    
    agenda_items = extracted_data.get('agenda_items', [])
    
    # Find E-1 in the extracted data
    e1_item = None
    for item in agenda_items:
        if item.get('item_code') == 'E-1':
            e1_item = item
            break
    
    if not e1_item:
        print("❌ E-1 not found in extracted data")
        return False
    
    print(f"✅ Found E-1 in extracted data with {len(e1_item.get('urls', []))} URLs")
    
    # Now test the ontology extractor
    agenda_file = Path("city_clerk_documents/global/City Comissions 2024/Agendas/Agenda 01.9.2024.pdf")
    extractor = OntologyExtractor()
    
    ontology_data = extractor.extract_ontology(agenda_file)
    
    # Find E-1 in the ontology data
    e1_ontology = None
    for section in ontology_data.get('sections', []):
        for item in section.get('items', []):
            if item.get('item_code') == 'E-1':
                e1_ontology = item
                break
        if e1_ontology:
            break
    
    if not e1_ontology:
        print("❌ E-1 not found in ontology data")
        return False
    
    urls_in_ontology = e1_ontology.get('urls', [])
    print(f"✅ Found E-1 in ontology data with {len(urls_in_ontology)} URLs")
    
    if len(urls_in_ontology) > 0:
        print("🎉 SUCCESS: URLs are being preserved through ontology extraction!")
        print(f"   URLs: {urls_in_ontology}")
        return True
    else:
        print("❌ FAIL: URLs are being lost in ontology extraction")
        return False

if __name__ == "__main__":
    print("Testing ontology URL preservation...")
    success = test_ontology_url_preservation()
    sys.exit(0 if success else 1)


================================================================================


################################################################################
# File: scripts/minimal_url_test.py
################################################################################

# File: scripts/minimal_url_test.py

#!/usr/bin/env python3
"""
Minimal test to verify PyMuPDF can extract URLs from agenda PDFs
Run this first to ensure PyMuPDF is working correctly
"""

import fitz  # PyMuPDF
from pathlib import Path
import sys

def test_pymupdf_urls(pdf_path):
    """Quick test of PyMuPDF URL extraction."""
    print(f"\nTesting PyMuPDF on: {pdf_path}")
    print("-" * 60)
    
    try:
        # Open PDF
        doc = fitz.open(pdf_path)
        print(f"✅ Opened PDF with {len(doc)} pages")
        
        # Check each page for links
        total_links = 0
        for page_num in range(len(doc)):
            page = doc[page_num]
            links = page.get_links()
            
            if links:
                print(f"\n📄 Page {page_num + 1}: Found {len(links)} links")
                for i, link in enumerate(links[:3]):  # Show first 3
                    if link.get('uri'):
                        rect = fitz.Rect(link['from'])
                        text = page.get_text(clip=rect).strip()
                        print(f"   Link {i+1}: {text[:30]}... → {link['uri'][:50]}...")
                        total_links += 1
        
        doc.close()
        
        print(f"\n✅ Total URLs found: {total_links}")
        return total_links > 0
        
    except Exception as e:
        print(f"❌ Error: {e}")
        return False

if __name__ == "__main__":
    # Default test path
    test_path = "city_clerk_documents/global/City Comissions 2024/Agendas/Agenda 01.9.2024.pdf"
    
    if len(sys.argv) > 1:
        test_path = sys.argv[1]
    
    if Path(test_path).exists():
        success = test_pymupdf_urls(test_path)
        print(f"\n{'✅ SUCCESS' if success else '❌ FAILED'}: PyMuPDF URL extraction")
    else:
        print(f"❌ File not found: {test_path}")
        print("Usage: python minimal_url_test.py [path_to_pdf]")


================================================================================


################################################################################
# File: requirements.txt
################################################################################

################################################################################
################################################################################
# Python dependencies for Misophonia Research RAG System

# Core dependencies
openai>=1.82.0
groq>=0.4.1
gremlinpython>=3.7.3
aiofiles>=24.1.0
python-dotenv>=1.0.0

# Graph Visualization dependencies
dash>=2.14.0
plotly>=5.17.0
networkx>=3.2.0
dash-table>=5.0.0
dash-cytoscape>=0.3.0

# Database and API dependencies
supabase>=2.0.0

# Optional for async progress bars
tqdm

# Data processing
pandas>=2.0.0
numpy

# PDF processing
PyPDF2>=3.0.0
unstructured[pdf]==0.10.30
pytesseract>=0.3.10
pdf2image>=1.16.3
docling
pdfminer.six>=20221105

# PDF processing with hyperlink support
PyMuPDF>=1.23.0

# Utilities
colorama>=0.4.6

# For concurrent processing
concurrent-log-handler>=0.9.20

# Async dependencies for optimized pipeline
aiohttp>=3.8.0

# Token counting for OpenAI rate limiting
tiktoken>=0.5.0


================================================================================


################################################################################
# File: scripts/test_e1_urls.py
################################################################################

# File: scripts/test_e1_urls.py

#!/usr/bin/env python3
import sys, json
sys.path.append('scripts')
from graph_stages.ontology_extractor import OntologyExtractor
from pathlib import Path

# Test the ontology extractor
extractor = OntologyExtractor()
result = extractor.extract_ontology(Path('city_clerk_documents/global/City Comissions 2024/Agendas/Agenda 01.9.2024.pdf'))

# Find E-1 and check its URLs
found = False
for section in result.get('sections', []):
    for item in section.get('items', []):
        if item.get('item_code') == 'E-1':
            urls = item.get('urls', [])
            print(f'✅ E-1 found with {len(urls)} URLs: {urls}')
            found = True
            break
    if found:
        break

if not found:
    print('❌ E-1 not found in ontology')


================================================================================

