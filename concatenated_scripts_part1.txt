# Concatenated Project Code - Part 1 of 3
# Generated: 2025-06-03 11:11:21
# Root Directory: /Users/gianmariatroiani/Documents/knologiÌŠ/graph_database
================================================================================

# Directory Structure
################################################################################
â”œâ”€â”€ .gitignore (939.0B, no ext)
â”œâ”€â”€ city_clerk_documents/
â”‚   â”œâ”€â”€ global copy/
â”‚   â”‚   â”œâ”€â”€ City Comissions 2024/
â”‚   â”‚   â”‚   â”œâ”€â”€ Agendas/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Agenda 01.23.2024.pdf (155.1KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Agenda 01.9.2024.pdf (151.1KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Agenda 02.13.2024.pdf (155.0KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Agenda 02.27.2024.pdf (152.9KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Agenda 03.12.2024.pdf (166.6KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Agenda 05.07.2024.pdf (167.6KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Agenda 05.21.2024.pdf (172.2KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Agenda 06.11.2024.pdf (160.2KB, .pdf)
â”‚   â”‚   â”‚   â”œâ”€â”€ ExportedFolderContents.zip (62.4MB, .zip)
â”‚   â”‚   â”‚   â”œâ”€â”€ Ordinances/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024/
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-01 - 01_09_2024.pdf (1.2MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-02 - 01_09_2024.pdf (15.1MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-03 - 01_09_2024.pdf (11.1MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-04 - 01_23_2024.pdf (2.3MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-05 - 02_13_2024.pdf (2.0MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-06 - 02_13_2024.pdf (1.9MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-07 - 02_13_2024.pdf (6.5MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-08 - 02_27_2024.pdf (2.5MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-09 - 02_27_2024.pdf (1.6MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-10 - 03_12_2024.pdf (3.7MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-11 - 03_12_2024 - (As Amended).pdf (5.6MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-11 - 03_12_2024.pdf (5.5MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-12 - 03_12_2024.pdf (1.8MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-13 - 03_12_2024.pdf (3.2MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-14 - 05_07_2024.pdf (2.8MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-15 - 05_07_2024.pdf (3.9MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-16 - 05_07_2024.pdf (1.1MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-17 - 05_07_2024.pdf (4.3MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-18 - 05_21_2024.pdf (2.4MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-19 - 05_21_2024.pdf (1.7MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-20 - 05_21_2024.pdf (1.6MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-21 - 06_11_2024.pdf (1.7MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-22 - 06_11_2024.pdf (2.0MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-23 - 06_11_2024.pdf (1.9MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-24 - 06_11_2024.pdf (2.1MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ 2024-25 - 06_11_2024.pdf (1.3MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ [EXCLUDED] 1 items: .DS_Store (excluded file)
â”‚   â”‚   â”‚   â”œâ”€â”€ Resolutions/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024/
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-01 - 01_09_2024.pdf (448.9KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-02 - 01_09_2024.pdf (451.9KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-03 - 01_09_2024.pdf (867.3KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-04 - 01_09_2024.pdf (1.0MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-05 - 01_09_2024.pdf (936.9KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-06 - 01_09_2024.pdf (1.0MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-07 - 01_09_2024.pdf (1.0MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-08 - 01_23_2024.pdf (457.9KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-09 - 01_23_2024.pdf (1.2MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-10 - 01_23_2024.pdf (454.8KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-10 - 03_12_2024.pdf (3.7MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-100 - 05_21_2024.pdf (3.0MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-101 - 05_21_2024.pdf (947.4KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-102 - 05_21_2024.pdf (466.0KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-103 - 05_21_2024.pdf (991.8KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-104 - 05_21_2024.pdf (1.1MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-105 - 05_21_2024.pdf (1.0MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-106 - 05_21_2024.pdf (2.4MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-107 - 05_21_2024.pdf (6.0MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-108 - 05_21_2024.pdf (2.1MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-109 - 05_21_2024.pdf (1.1MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-11 - 01_23_2024.pdf (1.1MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-110 - 05_21_2024.pdf (921.5KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-111 - 05_21_2024.pdf (6.3MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-112 - 05_21_2024.pdf (6.2MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-113 - 05_21_2024.pdf (6.3MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-114 - 05_21_2024.pdf (6.3MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-115 - 05_21_2024.pdf (6.3MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-116 - 05_21_2024.pdf (6.3MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-117 - 05_21_2024.pdf (6.4MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-118 - 05_21_2024.pdf (6.3MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-119 - 05_21_2024.pdf (6.3MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-12 - 01_23_2024.pdf (588.3KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-120 - 05_21_2024.pdf (6.3MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-121 - 05_21_2024.pdf (6.4MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-122 - 05_21_2024.pdf (6.8MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-123 - 05_21_2024.pdf (1.2MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-124 - 05_21_2024.pdf (11.2MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-125 - 05_21_2024.pdf (776.0KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-126 - 05_21_2024.pdf (1.0MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-127 - 05_21_2024.pdf (8.1MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-129 - 06_11_2024.pdf (1.3MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-13 - 01_23_2024.pdf (1.2MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-130 - 06_11_2024.pdf (1.2MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-131 - 06_11_2024.pdf (1.2MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-132 - 06_11_2024.pdf (458.0KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-133 - 06_11_2024 -As Amended.pdf (1.2MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-134 - 06_11_2024.pdf (884.8KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-135 - 06_11_2024.pdf (1022.1KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-136 - 06_11_2024.pdf (537.4KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-137 - 06_11_2024.pdf (1.7MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-138 - 06_11_2024.pdf (1.7MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-139 - 06_11_2024.pdf (1.2MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-14 - 01_23_2024.pdf (996.6KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-140 - 06_11_2024.pdf (1.2MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-141 - 06_11_2024.pdf (13.2MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-142 - 06_11_2024.pdf (790.5KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-143 -06_11_2024.pdf (1.0MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-15 - 01_23_2024.pdf (1.0MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-16 - 01_23_2024.pdf (1.8MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-17 - 01_23_2024.pdf (488.0KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-18 - 01_23_2024.pdf (849.1KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-19 - 02_19_2024.pdf (1019.5KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-20 - 02_13_2024.pdf (824.5KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-21 - 02_13_2024.pdf (502.6KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-22 - 02_13_2024.pdf (450.6KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-23 - 02_13_2024.pdf (447.7KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-24 - 02_13_2024.pdf (464.1KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-25 - 02_13_2024.pdf (458.3KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-26 - 02_13_2024.pdf (465.7KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-27 - 02_13_2024.pdf (754.2KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-28 - 02_13_2024.pdf (761.7KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-29 - 02_13_2024.pdf (1.2MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-30 - 02_13_2024.pdf (1.2MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-31 - 02_13_2024.pdf (936.6KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-32 - 02_13_2024.pdf (1.9MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-33 - 02_13_2024.pdf (1.1MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-34 - 02_13_2024.pdf (1.9MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-35 - 02_27_2024.pdf (480.7KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-36 - 02_27_2024.pdf (1.3MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-37 - 02_27_2024.pdf (940.3KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-38 - 02_27_2024.pdf (829.6KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-39 - 02_27_2024.pdf (816.5KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-41 - 02_27_2024.pdf (833.1KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-42 - 02_27_2024.pdf (473.4KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-43 - 02_27_2024.pdf (1.0MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-44 - 02_27_2024.pdf (1.8MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-45 - 03_12_2024.pdf (1002.1KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-46 - 03_12_2024.pdf (1.0MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-47 - 03_12_2024.pdf (1.2MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-48 - 03_12_2024.pdf (1.2MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-49 - 03_12_2024.pdf (899.6KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-50 - 03_12_2024.pdf (462.1KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-51 - 03_12_2024.pdf (538.1KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-52 - 03_12_2024.pdf (1.1MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-53 - 03_12_2024.pdf (1.1MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-54 - 03_12_2024.pdf (2.0MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-55 - 03_12_2024.pdf (1.1MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-56 - 03_12_2024.pdf (1.1MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-57 - 03_12_2024.pdf (1.2MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-58 - 03_12_2024.pdf (775.8KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-60 - 03_12_2024.pdf (2.3MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-61 - 04_16_2024.pdf (6.4MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-62 - 04_16_2024.pdf (2.7MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-63 -  04_16_2024.pdf (832.3KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-64 - 04_16_2024.pdf (452.6KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-65 - 04_16_2024.pdf (894.5KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-66 - 04_16_2024.pdf (446.6KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-67 - 04_16_2024.pdf (1.0MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-68 - 04_16_2024.pdf (5.6MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-69- -04_16_2024.pdf (443.1KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-70 - 04_16_2024.pdf (878.9KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-71 - 04_16_2024.pdf (951.8KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-72 - 04_16_2024.pdf (821.7KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-73 - 04_16_2024.pdf (810.8KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-74 - 04_16_2024.pdf (1.1MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-75 - 04_16_2024.pdf (1.6MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-76 - 04_16_2024.pdf (1.2MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-77 - 04_16_2024.pdf (1013.9KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-78 - 04_16_2024.pdf (1007.5KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-79 - 04_16_2024.pdf (997.7KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-80 - 04_16_2024.pdf (525.7KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-81 - 04_16_2024.pdf (923.2KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-82 - 04_16_2024.pdf (473.6KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-83 - 04_16_2024.pdf (915.3KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-84 - 05_07_2024.pdf (992.7KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-85 - 05_07_2024.pdf (2.0MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-86 - 05_07_2024.pdf (503.1KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-87 - 05_07_2024.pdf (1.2MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-88 - 05_07_2024.pdf (1.1MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-89 - 05_07_2024.pdf (1.2MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-90 - 05_07_2024.pdf (1.1MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-91 - 05_07_2024.pdf (1.8MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-92 - 05_07_2024.pdf (452.4KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-93 - 05_07_2024.pdf (1014.3KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-94 - 05_05_2024.pdf (1.3MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-95 - 05_07_2024.pdf (974.5KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-96 - 05_07_2024.pdf (1.5MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-97 - 05_07_2024.pdf (1023.3KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024-98 - 05_07_2024.pdf (2.2MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ 2024-99 - 05_07_2024.pdf (792.7KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ [EXCLUDED] 1 items: .DS_Store (excluded file)
â”‚   â”‚   â”‚   â”œâ”€â”€ Verbating Items/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 2024/
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 01_09_2024 - Verbatim Transcripts - E-4.pdf (2.4MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 01_09_2024 - Verbatim Transcripts - E-5.pdf (735.6KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 01_09_2024 - Verbatim Transcripts - E-7.pdf (306.9KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 01_09_2024 - Verbatim Transcripts - E-8.pdf (1.1MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 01_09_2024 - Verbatim Transcripts - F-10.pdf (925.3KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 01_09_2024 - Verbatim Transcripts - F-2.pdf (288.2KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 01_09_2024 - Verbatim Transcripts - F-5.pdf (360.6KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 01_09_2024 - Verbatim Transcripts - F-6.pdf (212.8KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 01_09_2024 - Verbatim Transcripts - Public Comment.pdf (762.7KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 01_23_2024 - Verbatim Transcripts - C- Public Comment.pdf (1.3MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 01_23_2024 - Verbatim Transcripts - E-3 and E-4.pdf (2.4MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 01_23_2024 - Verbatim Transcripts - E-5.pdf (290.7KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 01_23_2024 - Verbatim Transcripts - F-1.pdf (855.5KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 01_23_2024 - Verbatim Transcripts - F-5.pdf (3.7MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 01_23_2024 - Verbatim Transcripts - F-7.pdf (768.6KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 01_23_2024 - Verbatim Transcripts - H-1.pdf (1.5MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 02_13_2024 - Meeting Minutes - Public.pdf (363.5KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 02_13_2024 - Verbatim Transcripts - F-12.pdf (7.2MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 02_13_2024 - Verbatim Transcripts - H-1.pdf (570.8KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 02_27_2024 - Verbatim Transcripts - E-1.pdf (4.2MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 02_27_2024 - Verbatim Transcripts - E-2.pdf (281.3KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 02_27_2024 - Verbatim Transcripts - E-3.pdf (1.2MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 02_27_2024 - Verbatim Transcripts - F-10.pdf (2.5MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 02_27_2024 - Verbatim Transcripts - F-12.pdf (448.2KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 02_27_2024 - Verbatim Transcripts - H-1.pdf (794.2KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 02_27_2024 - Verbatim Transcripts - Public Comment.pdf (439.4KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 03_12_2024 - Verbatim Transcripts - E-1.pdf (124.3KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 03_12_2024 - Verbatim Transcripts - E-2.pdf (487.9KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 03_12_2024 - Verbatim Transcripts - E-4 and E-11.pdf (1.1MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 03_12_2024 - Verbatim Transcripts - F-1.pdf (416.0KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 03_12_2024 - Verbatim Transcripts - F-2.pdf (1.5MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 03_12_2024 - Verbatim Transcripts - F-4 and F-5 and F-11 and F-12.pdf (1.4MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 03_12_2024 - Verbatim Transcripts - H-1.pdf (121.5KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 03_12_2024 - Verbatim Transcripts - K - Discussion Items.pdf (285.0KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 03_12_2024 - Verbatim Transcripts - Public Comment.pdf (412.4KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 04_16_2024 - Verbatim Transcripts - 2-1 AND 2-2.pdf (652.3KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 04_16_2024 - Verbatim Transcripts - E-11.pdf (365.8KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 04_16_2024 - Verbatim Transcripts - E-2.pdf (369.8KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 04_16_2024 - Verbatim Transcripts - E-4.pdf (410.2KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 04_16_2024 - Verbatim Transcripts - E-5 E-6 E-7 E-8 E-9 E-10.pdf (3.6MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 04_16_2024 - Verbatim Transcripts - F-1.pdf (751.9KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 04_16_2024 - Verbatim Transcripts - F-10.pdf (1.3MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 04_16_2024 - Verbatim Transcripts - F-11.pdf (2.4MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 04_16_2024 - Verbatim Transcripts - F-12.pdf (2.8MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 04_16_2024 - Verbatim Transcripts - F-15.pdf (255.0KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 04_16_2024 - Verbatim Transcripts - F-5.pdf (205.0KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 04_16_2024 - Verbatim Transcripts - F-8.pdf (961.1KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 04_16_2024 - Verbatim Transcripts - F-9.pdf (330.1KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 04_16_2024 - Verbatim Transcripts - Public Comment.pdf (1.5MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 05_07_2024 - Verbatim Transcripts - 2-1.pdf (1.7MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 05_07_2024 - Verbatim Transcripts - E-1.pdf (4.6MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 05_07_2024 - Verbatim Transcripts - E-10.pdf (128.5KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 05_07_2024 - Verbatim Transcripts - E-11.pdf (559.6KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 05_07_2024 - Verbatim Transcripts - E-12.pdf (156.1KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 05_07_2024 - Verbatim Transcripts - E-8.pdf (288.5KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 05_07_2024 - Verbatim Transcripts - E-9.pdf (263.4KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 05_07_2024 - Verbatim Transcripts - F-1.pdf (561.1KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 05_07_2024 - Verbatim Transcripts - F-13.pdf (151.9KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 05_07_2024 - Verbatim Transcripts - F-16.pdf (782.8KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 05_07_2024 - Verbatim Transcripts - F-3.pdf (352.6KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 05_07_2024 - Verbatim Transcripts - F-9.pdf (736.0KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 05_07_2024 - Verbatim Transcripts - H-1.pdf (455.3KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 05_07_2024 - Verbatim Transcripts - Public Comment.pdf (1.6MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 06_11_2024 - Verbatim Transcripts - D-3.pdf (539.3KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 06_11_2024 - Verbatim Transcripts - E-10.pdf (629.7KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 06_11_2024 - Verbatim Transcripts - E-3.pdf (350.4KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 06_11_2024 - Verbatim Transcripts - F-2.pdf (1.7MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 06_11_2024 - Verbatim Transcripts - F-3.pdf (2.4MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 06_11_2024 - Verbatim Transcripts - F-4.pdf (3.4MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 06_11_2024 - Verbatim Transcripts - F-5.pdf (1.0MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ 06_11_2024 - Verbatim Transcripts - F-7 and F-10.pdf (1.6MB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ 06_11_2024 - Verbatim Transcripts - Public Comment.pdf (513.4KB, .pdf)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ [EXCLUDED] 1 items: .DS_Store (excluded file)
â”‚   â”‚   â”‚   â”œâ”€â”€ [EXCLUDED] 1 items: .DS_Store (excluded file)
â”‚   â”‚   â”œâ”€â”€ [EXCLUDED] 1 items: .DS_Store (excluded file)
â”‚   â”œâ”€â”€ graph_json/
â”‚   â”‚   â”œâ”€â”€ Agenda 01.9.2024_docling_extracted.json (22.8KB, .json)
â”‚   â”‚   â”œâ”€â”€ Agenda 01.9.2024_extracted.json (22.8KB, .json)
â”‚   â”‚   â”œâ”€â”€ Agenda 01.9.2024_full_text.txt (16.1KB, .txt)
â”‚   â”‚   â”œâ”€â”€ Agenda 01.9.2024_ontology.json (9.6KB, .json)
â”‚   â”‚   â”œâ”€â”€ debug/
â”‚   â”‚   â”‚   â”œâ”€â”€ all_ordinance_files.txt (170.0B, .txt)
â”‚   â”‚   â”‚   â”œâ”€â”€ document_search_info.txt (174.0B, .txt)
â”‚   â”‚   â”‚   â”œâ”€â”€ enhanced_linked_documents.json (4.0KB, .json)
â”‚   â”‚   â”‚   â”œâ”€â”€ linked_documents.json (1014.0B, .json)
â”‚   â”‚   â”‚   â”œâ”€â”€ linking_report_01_09_2024.json (2.6KB, .json)
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_input_2024-01.txt (3.7KB, .txt)
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_input_2024-02.txt (49.8KB, .txt)
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_input_2024-03.txt (26.1KB, .txt)
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_response_2024-01_cleaned.txt (1.5KB, .txt)
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_response_2024-01_raw.txt (1.5KB, .txt)
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_response_2024-02_cleaned.txt (1.2KB, .txt)
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_response_2024-02_raw.txt (1.3KB, .txt)
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_response_2024-03_cleaned.txt (1.2KB, .txt)
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_response_2024-03_raw.txt (1.2KB, .txt)
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_response_ordinance_2024-01_cleaned.txt (1.3KB, .txt)
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_response_ordinance_2024-01_raw.txt (1.3KB, .txt)
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_response_ordinance_2024-02_cleaned.txt (1.2KB, .txt)
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_response_ordinance_2024-02_raw.txt (1.2KB, .txt)
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_response_ordinance_2024-03_cleaned.txt (1.3KB, .txt)
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_response_ordinance_2024-03_raw.txt (1.3KB, .txt)
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_response_resolution_2024-01_cleaned.txt (1.3KB, .txt)
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_response_resolution_2024-01_raw.txt (1.3KB, .txt)
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_response_resolution_2024-02_cleaned.txt (1.5KB, .txt)
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_response_resolution_2024-02_raw.txt (1.5KB, .txt)
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_response_resolution_2024-03_cleaned.txt (1.3KB, .txt)
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_response_resolution_2024-03_raw.txt (1.4KB, .txt)
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_response_resolution_2024-04_cleaned.txt (3.7KB, .txt)
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_response_resolution_2024-04_raw.txt (3.7KB, .txt)
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_response_resolution_2024-05_cleaned.txt (1.8KB, .txt)
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_response_resolution_2024-05_raw.txt (1.8KB, .txt)
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_response_resolution_2024-06_cleaned.txt (3.5KB, .txt)
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_response_resolution_2024-06_raw.txt (3.5KB, .txt)
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_response_resolution_2024-07_cleaned.txt (2.5KB, .txt)
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_response_resolution_2024-07_raw.txt (2.5KB, .txt)
â”‚   â”‚   â”‚   â”œâ”€â”€ matched_documents.txt (123.0B, .txt)
â”‚   â”‚   â”‚   â””â”€â”€ resolution_debug_report.json (6.1KB, .json)
â”‚   â”‚   â”œâ”€â”€ pipeline_report_20250602_203056.json (5.8KB, .json)
â”‚   â”‚   â”œâ”€â”€ pipeline_report_20250603_140212.json (1.4KB, .json)
â”‚   â”‚   â””â”€â”€ pipeline_report_20250603_150924.json (1.4KB, .json)
â”‚   â”œâ”€â”€ [EXCLUDED] 2 items: .DS_Store (excluded file), global (excluded dir)
â”œâ”€â”€ config.py (1.7KB, .py)
â”œâ”€â”€ debug/
â”œâ”€â”€ graph_clear_database.py (963.0B, .py)
â”œâ”€â”€ graph_visualizer.py (23.6KB, .py)
â”œâ”€â”€ ontology_model.txt (6.6KB, .txt)
â”œâ”€â”€ ontology_modelv2.txt (10.2KB, .txt)
â”œâ”€â”€ repository_directory_structure.txt (5.7KB, .txt)
â”œâ”€â”€ requirements.txt (1018.0B, .txt)
â”œâ”€â”€ run_city_clerk_pipeline.sh (440.0B, .sh)
â”œâ”€â”€ run_graph_pipeline.sh (1.3KB, .sh)
â”œâ”€â”€ run_graph_visualizer.sh (3.6KB, .sh)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ check_pipeline_setup.py (3.1KB, .py)
â”‚   â”œâ”€â”€ clear_database.py (1.6KB, .py)
â”‚   â”œâ”€â”€ clear_database_sync.py (1.6KB, .py)
â”‚   â”œâ”€â”€ clear_meeting_data.py (1.8KB, .py)
â”‚   â”œâ”€â”€ debug_agenda_items.py (2.4KB, .py)
â”‚   â”œâ”€â”€ debug_missing_resolutions.py (14.5KB, .py)
â”‚   â”œâ”€â”€ debug_ordinance_2024_02.py (3.5KB, .py)
â”‚   â”œâ”€â”€ debug_pipeline_output.py (1.5KB, .py)
â”‚   â”œâ”€â”€ find_duplicates.py (5.2KB, .py)
â”‚   â”œâ”€â”€ graph_pipeline.py (18.1KB, .py)
â”‚   â”œâ”€â”€ graph_stages/
â”‚   â”‚   â”œâ”€â”€ __init__.py (550.0B, .py)
â”‚   â”‚   â”œâ”€â”€ agenda_graph_builder.py (39.5KB, .py)
â”‚   â”‚   â”œâ”€â”€ agenda_ontology_extractor.py (24.4KB, .py)
â”‚   â”‚   â”œâ”€â”€ agenda_pdf_extractor.py (13.4KB, .py)
â”‚   â”‚   â”œâ”€â”€ cosmos_db_client.py (10.2KB, .py)
â”‚   â”‚   â”œâ”€â”€ document_linker.py (13.7KB, .py)
â”‚   â”‚   â”œâ”€â”€ enhanced_document_linker.py (17.1KB, .py)
â”‚   â”‚   â”œâ”€â”€ ontology_extractor.py (25.2KB, .py)
â”‚   â”‚   â””â”€â”€ pdf_extractor.py (6.5KB, .py)
â”‚   â”œâ”€â”€ pipeline_modular_optimized.py (12.7KB, .py)
â”‚   â”œâ”€â”€ process_resolutions.py (6.5KB, .py)
â”‚   â”œâ”€â”€ rag_local_web_app.py (18.4KB, .py)
â”‚   â”œâ”€â”€ stages/
â”‚   â”‚   â”œâ”€â”€ __init__.py (378.0B, .py)
â”‚   â”‚   â”œâ”€â”€ acceleration_utils.py (3.8KB, .py)
â”‚   â”‚   â”œâ”€â”€ chunk_text.py (18.6KB, .py)
â”‚   â”‚   â”œâ”€â”€ db_upsert.py (8.9KB, .py)
â”‚   â”‚   â”œâ”€â”€ embed_vectors.py (27.0KB, .py)
â”‚   â”‚   â”œâ”€â”€ extract_clean.py (21.7KB, .py)
â”‚   â”‚   â””â”€â”€ llm_enrich.py (5.9KB, .py)
â”‚   â”œâ”€â”€ supabase_clear_database.py (6.0KB, .py)
â”‚   â”œâ”€â”€ test_graph_pipeline.py (2.1KB, .py)
â”‚   â”œâ”€â”€ test_vector_search.py (5.5KB, .py)
â”‚   â”œâ”€â”€ topic_filter_and_title.py (5.6KB, .py)
â”‚   â”œâ”€â”€ [EXCLUDED] 1 items: .DS_Store (excluded file)
â”œâ”€â”€ [EXCLUDED] 6 items: .DS_Store (excluded file), .env (excluded file), .git (excluded dir)
    ... and 3 more excluded items


================================================================================


# File Index - Which Files Are in Which Parts
################################################################################

## Part 1 (12 files):
  - scripts/graph_stages/agenda_graph_builder.py
  - scripts/stages/chunk_text.py
  - scripts/graph_stages/enhanced_document_linker.py
  - scripts/graph_stages/agenda_pdf_extractor.py
  - scripts/stages/db_upsert.py
  - city_clerk_documents/graph_json/debug/resolution_debug_report.json
  - scripts/find_duplicates.py
  - city_clerk_documents/graph_json/debug/enhanced_linked_documents.json
  - scripts/check_pipeline_setup.py
  - config.py
  - scripts/debug_pipeline_output.py
  - requirements.txt

## Part 2 (14 files):
  - scripts/stages/embed_vectors.py
  - scripts/stages/extract_clean.py
  - scripts/graph_pipeline.py
  - scripts/debug_missing_resolutions.py
  - scripts/pipeline_modular_optimized.py
  - scripts/graph_stages/pdf_extractor.py
  - scripts/stages/llm_enrich.py
  - scripts/topic_filter_and_title.py
  - scripts/debug_ordinance_2024_02.py
  - scripts/debug_agenda_items.py
  - scripts/test_graph_pipeline.py
  - scripts/clear_database.py
  - graph_clear_database.py
  - scripts/graph_stages/__init__.py

## Part 3 (14 files):
  - scripts/graph_stages/ontology_extractor.py
  - scripts/graph_stages/agenda_ontology_extractor.py
  - scripts/rag_local_web_app.py
  - scripts/graph_stages/document_linker.py
  - scripts/graph_stages/cosmos_db_client.py
  - scripts/process_resolutions.py
  - scripts/supabase_clear_database.py
  - scripts/test_vector_search.py
  - scripts/stages/acceleration_utils.py
  - city_clerk_documents/graph_json/debug/linking_report_01_09_2024.json
  - scripts/clear_meeting_data.py
  - scripts/clear_database_sync.py
  - city_clerk_documents/graph_json/debug/linked_documents.json
  - scripts/stages/__init__.py


================================================================================


################################################################################
# File: scripts/graph_stages/agenda_graph_builder.py
################################################################################

# File: scripts/graph_stages/agenda_graph_builder.py

"""
Enhanced Agenda Graph Builder - RICH VERSION
Builds comprehensive graph representation from LLM-extracted agenda ontology.
"""
import logging
from typing import Dict, List, Optional
from pathlib import Path
import hashlib
import json
import calendar
import re

from .cosmos_db_client import CosmosGraphClient

log = logging.getLogger('pipeline_debug.graph_builder')


class AgendaGraphBuilder:
    """Build comprehensive graph representation from rich agenda ontology."""
    
    def __init__(self, cosmos_client: CosmosGraphClient, upsert_mode: bool = True):
        self.cosmos = cosmos_client
        self.upsert_mode = upsert_mode
        self.entity_id_cache = {}  # Cache for entity IDs
        self.partition_value = 'demo'  # Partition value property
        
        # Track statistics
        self.stats = {
            'nodes_created': 0,
            'nodes_updated': 0,
            'edges_created': 0,
            'edges_skipped': 0
        }
    
    @staticmethod
    def normalize_item_code(code: str) -> str:
        """Normalize item codes to consistent format for matching with ordinances."""
        if not code:
            return code
        
        # Remove trailing dots: "E.-1." -> "E.-1"
        code = code.rstrip('.')
        
        # Remove dots between letter and dash: "E.-1" -> "E-1"
        code = re.sub(r'([A-Z])\.(-)', r'\1\2', code)
        
        # Also handle cases without dash: "E.1" -> "E-1"
        code = re.sub(r'([A-Z])\.(\d)', r'\1-\2', code)
        
        # Ensure we have a dash between letter and number
        code = re.sub(r'([A-Z])(\d)', r'\1-\2', code)
        
        return code
    
    @staticmethod
    def ensure_us_date_format(date_str: str) -> str:
        """Ensure date is in US format MM-DD-YYYY with dashes."""
        # Handle different input formats
        if '.' in date_str:
            # Format: 01.23.2024 -> 01-23-2024
            return date_str.replace('.', '-')
        elif '/' in date_str:
            # Format: 01/23/2024 -> 01-23-2024
            return date_str.replace('/', '-')
        elif re.match(r'^\d{4}-\d{2}-\d{2}$', date_str):
            # ISO format: 2024-01-23 -> 01-23-2024
            parts = date_str.split('-')
            return f"{parts[1]}-{parts[2]}-{parts[0]}"
        else:
            # Already in correct format or unknown
            return date_str

    async def build_graph(self, ontology_file: Path, linked_docs: Optional[Dict] = None, upsert: bool = True) -> Dict:
        """Build graph from ontology file - main entry point."""
        # Load ontology
        with open(ontology_file, 'r', encoding='utf-8') as f:
            ontology = json.load(f)
        
        return await self.build_graph_from_ontology(ontology, ontology_file, linked_docs)

    async def build_graph_from_ontology(self, ontology: Dict, source_path: Path, linked_docs: Optional[Dict] = None) -> Dict:
        """Build comprehensive graph representation from rich ontology."""
        log.info(f"ðŸ”¨ Starting enhanced graph build for {source_path.name}")
        log.info(f"ðŸ”§ Upsert mode: {'ENABLED' if self.upsert_mode else 'DISABLED'}")
        
        # Reset statistics
        self.stats = {
            'nodes_created': 0,
            'nodes_updated': 0,
            'edges_created': 0,
            'edges_skipped': 0
        }
        
        try:
            graph_data = {
                'nodes': {},
                'edges': [],
                'statistics': {}
            }
            
            # CRITICAL: Ensure meeting date is in US format
            meeting_date_original = ontology['meeting_date']
            meeting_date_us = self.ensure_us_date_format(meeting_date_original)
            meeting_info = ontology['meeting_info']
            
            log.info(f"ðŸ“… Meeting date: {meeting_date_original} -> {meeting_date_us}")
            
            # 1. Create Meeting node as the root
            meeting_id = f"meeting-{meeting_date_us}"
            await self._create_meeting_node(meeting_date_us, meeting_info, source_path.name)
            log.info(f"âœ… Created meeting node: {meeting_id}")
            
            # 1.5 Create Date node and link to meeting
            try:
                date_id = await self._create_date_node(meeting_date_original, meeting_id)
                graph_data['nodes'][date_id] = {
                    'type': 'Date',
                    'date': meeting_date_original
                }
            except Exception as e:
                log.error(f"Failed to create date node: {e}")
            
            graph_data['nodes'][meeting_id] = {
                'type': 'Meeting',
                'date': meeting_date_us,
                'info': meeting_info
            }
            
            # 2. Create nodes for officials present  
            await self._create_official_nodes(meeting_info, meeting_id)
            
            # 3. Process sections and agenda items
            section_count = 0
            item_count = 0
            
            sections = ontology.get('sections', [])
            log.info(f"ðŸ“‘ Processing {len(sections)} sections")
            
            for section_idx, section in enumerate(sections):
                try:
                    section_count += 1
                    section_id = f"section-{meeting_date_us}-{section_idx}"
                    
                    # Create Section node
                    await self._create_section_node(section_id, section, section_idx)
                    log.info(f"âœ… Created section {section_idx}: {section.get('section_name', 'Unknown')}")
                    
                    graph_data['nodes'][section_id] = {
                        'type': 'Section',
                        'name': section['section_name'],
                        'order': section_idx
                    }
                    
                    # Link section to meeting
                    if await self.cosmos.create_edge_if_not_exists(
                        from_id=meeting_id,
                        to_id=section_id,
                        edge_type='HAS_SECTION',
                        properties={'order': section_idx}
                    ):
                        self.stats['edges_created'] += 1
                    else:
                        self.stats['edges_skipped'] += 1
                    
                    # Process items in section
                    previous_item_id = None
                    items = section.get('items', [])
                    
                    for item_idx, item in enumerate(items):
                        try:
                            if not item.get('item_code'):
                                log.warning(f"Skipping item without code in section {section['section_name']}")
                                continue
                                
                            item_count += 1
                            # Normalize the item code
                            normalized_code = self.normalize_item_code(item['item_code'])
                            # Use US date format for item ID
                            item_id = f"item-{meeting_date_us}-{normalized_code}"
                            
                            log.info(f"Creating item: {item_id} (from code: {item['item_code']})")
                            
                            # Create enhanced AgendaItem node
                            await self._create_enhanced_agenda_item_node(item_id, item, section)
                            
                            graph_data['nodes'][item_id] = {
                                'type': 'AgendaItem',
                                'code': normalized_code,
                                'original_code': item['item_code'],
                                'title': item.get('title', 'Unknown')
                            }
                            
                            # Link item to section
                            if await self.cosmos.create_edge_if_not_exists(
                                from_id=section_id,
                                to_id=item_id,
                                edge_type='CONTAINS_ITEM',
                                properties={'order': item_idx}
                            ):
                                self.stats['edges_created'] += 1
                            else:
                                self.stats['edges_skipped'] += 1
                            
                            # Create sequential relationships
                            if previous_item_id:
                                if await self.cosmos.create_edge_if_not_exists(
                                    from_id=previous_item_id,
                                    to_id=item_id,
                                    edge_type='FOLLOWS',
                                    properties={'sequence': item_idx}
                                ):
                                    self.stats['edges_created'] += 1
                                else:
                                    self.stats['edges_skipped'] += 1
                            
                            previous_item_id = item_id
                            
                            # Create rich relationships for this item
                            await self._create_item_relationships(item, item_id, meeting_date_us)
                            
                            # Create URL nodes and relationships
                            await self._create_url_relationships(item, item_id)
                                
                        except Exception as e:
                            log.error(f"Failed to process item {item.get('item_code', 'unknown')}: {e}")
                            
                except Exception as e:
                    log.error(f"Failed to process section {section.get('section_name', 'unknown')}: {e}")
            
            # 4. Create entity nodes from extracted entities
            entity_count = await self._create_entity_nodes(ontology.get('entities', []), meeting_id)
            
            # 5. Create relationships from ontology
            relationship_count = 0
            for rel in ontology.get('relationships', []):
                try:
                    await self._create_ontology_relationship(rel, meeting_date_us)
                    relationship_count += 1
                except Exception as e:
                    log.error(f"Failed to create relationship: {e}")
            
            # 6. Process linked documents if available
            if linked_docs:
                await self.process_linked_documents(linked_docs, meeting_id, meeting_date_us)
            
            # Update statistics
            graph_data['statistics'] = {
                'sections': section_count,
                'items': item_count, 
                'entities': entity_count,
                'relationships': relationship_count,
                'meeting_date': meeting_date_us
            }
            
            log.info(f"ðŸŽ‰ Enhanced graph build complete for {source_path.name}")
            log.info(f"   ðŸ“Š Statistics:")
            log.info(f"      - Nodes created: {self.stats['nodes_created']}")
            log.info(f"      - Nodes updated: {self.stats['nodes_updated']}")
            log.info(f"      - Edges created: {self.stats['edges_created']}")
            log.info(f"      - Edges skipped: {self.stats['edges_skipped']}")
            log.info(f"   - Sections: {section_count}")
            log.info(f"   - Items: {item_count}")
            log.info(f"   - Entities: {entity_count}")
            log.info(f"   - Relationships: {relationship_count}")
            
            return graph_data
            
        except Exception as e:
            log.error(f"CRITICAL ERROR in build_graph_from_ontology: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    async def _create_meeting_node(self, meeting_date: str, meeting_info: Dict, source_file: str = None) -> str:
        """Create or update Meeting node with comprehensive metadata."""
        meeting_id = f"meeting-{meeting_date}"
        
        # Handle location - could be string or dict
        location = meeting_info.get('location', 'City Commission Chambers')
        if isinstance(location, dict):
            location_str = f"{location.get('name', 'City Commission Chambers')}"
            if location.get('address'):
                location_str += f" - {location['address']}"
        else:
            location_str = str(location) if location else "City Commission Chambers"
        
        properties = {
            'nodeType': 'Meeting',
            'date': meeting_date,
            'type': meeting_info.get('type', 'Regular Meeting'),
            'time': meeting_info.get('time', '5:30 PM'),
            'location': location_str
        }
        
        if source_file:
            properties['source_file'] = source_file
        
        # Use upsert instead of create
        if self.upsert_mode:
            created = await self.cosmos.upsert_vertex('Meeting', meeting_id, properties)
            if created:
                self.stats['nodes_created'] += 1
                log.info(f"âœ… Created Meeting node: {meeting_id}")
            else:
                self.stats['nodes_updated'] += 1
                log.info(f"ðŸ“ Updated Meeting node: {meeting_id}")
        else:
            await self.cosmos.create_vertex('Meeting', meeting_id, properties)
            self.stats['nodes_created'] += 1
            log.info(f"âœ… Created Meeting node: {meeting_id}")
        
        return meeting_id
    
    async def _create_date_node(self, date_str: str, meeting_id: str) -> str:
        """Create a Date node and link it to the meeting."""
        from datetime import datetime
        
        # Parse date from MM.DD.YYYY format
        parts = date_str.split('.')
        if len(parts) != 3:
            log.error(f"Invalid date format: {date_str}")
            return None
            
        month, day, year = int(parts[0]), int(parts[1]), int(parts[2])
        
        # Create consistent date ID in ISO format
        date_id = f"date-{year:04d}-{month:02d}-{day:02d}"
        
        # Check if date already exists
        if await self.cosmos.vertex_exists(date_id):
            log.info(f"Date {date_id} already exists")
            # Still create the relationship
            await self.cosmos.create_edge(
                from_id=meeting_id,
                to_id=date_id,
                edge_type='OCCURRED_ON',
                properties={'primary_date': True}
            )
            return date_id
        
        # Get day of week
        date_obj = datetime(year, month, day)
        day_of_week = date_obj.strftime('%A')
        
        # Create date node
        properties = {
            'nodeType': 'Date',
            'full_date': date_str,
            'year': year,
            'month': month,
            'day': day,
            'quarter': (month - 1) // 3 + 1,
            'month_name': calendar.month_name[month],
            'day_of_week': day_of_week,
            'iso_date': f'{year:04d}-{month:02d}-{day:02d}'
        }
        
        await self.cosmos.create_vertex('Date', date_id, properties)
        log.info(f"âœ… Created Date node: {date_id}")
        
        # Create relationship: Meeting -> OCCURRED_ON -> Date
        await self.cosmos.create_edge(
            from_id=meeting_id,
            to_id=date_id,
            edge_type='OCCURRED_ON',
            properties={'primary_date': True}
        )
        
        return date_id
    
    async def _create_section_node(self, section_id: str, section: Dict, order: int) -> str:
        """Create or update Section node."""
        properties = {
            'nodeType': 'Section',
            'title': section.get('section_name', 'Unknown'),
            'type': section.get('section_type', 'OTHER'),
            'description': section.get('description', ''),
            'order': order
        }
        
        if self.upsert_mode:
            created = await self.cosmos.upsert_vertex('Section', section_id, properties)
            if created:
                self.stats['nodes_created'] += 1
            else:
                self.stats['nodes_updated'] += 1
        else:
            if await self.cosmos.vertex_exists(section_id):
                log.info(f"Section {section_id} already exists, skipping creation")
                return section_id
            await self.cosmos.create_vertex('Section', section_id, properties)
            self.stats['nodes_created'] += 1
        
        return section_id
    
    async def _create_enhanced_agenda_item_node(self, item_id: str, item: Dict, section: Dict) -> str:
        """Create or update AgendaItem node with rich metadata from LLM extraction."""
        # Store both original and normalized codes
        original_code = item.get('item_code', '')
        normalized_code = self.normalize_item_code(original_code)
        
        properties = {
            'nodeType': 'AgendaItem',
            'code': normalized_code,
            'original_code': original_code,
            'title': item.get('title', 'Unknown'),
            'type': item.get('item_type', 'Item'),
            'section': section.get('section_name', 'Unknown'),
            'section_type': section.get('section_type', 'other')
        }
        
        # Add enhanced details from LLM extraction
        if item.get('description'):
            properties['description'] = item['description'][:500]  # Limit length
        
        if item.get('document_reference'):
            properties['document_reference'] = item['document_reference']
        
        # Add sponsors as JSON array
        if item.get('sponsors'):
            properties['sponsors_json'] = json.dumps(item['sponsors'])
        
        # Add departments as JSON array  
        if item.get('departments'):
            properties['departments_json'] = json.dumps(item['departments'])
        
        # Add actions as JSON array
        if item.get('actions'):
            properties['actions_json'] = json.dumps(item['actions'])
        
        # Add stakeholders as JSON array
        if item.get('stakeholders'):
            properties['stakeholders_json'] = json.dumps(item['stakeholders'])
        
        # Add URLs as JSON array
        if item.get('urls'):
            properties['urls_json'] = json.dumps(item['urls'])
            properties['has_urls'] = True
        
        if self.upsert_mode:
            created = await self.cosmos.upsert_vertex('AgendaItem', item_id, properties)
            if created:
                self.stats['nodes_created'] += 1
            else:
                self.stats['nodes_updated'] += 1
        else:
            await self.cosmos.create_vertex('AgendaItem', item_id, properties)
            self.stats['nodes_created'] += 1
        
        return item_id
    
    async def _create_official_nodes(self, meeting_info: Dict, meeting_id: str):
        """Create nodes for city officials and link to meeting."""
        officials = meeting_info.get('officials', {})
        commissioners = meeting_info.get('commissioners', [])
        
        # Create official nodes
        for role, name in officials.items():
            if name and name != 'null':
                person_id = await self._ensure_person_node(name, role.replace('_', ' ').title())
                await self.cosmos.create_edge_if_not_exists(
                    from_id=person_id,
                    to_id=meeting_id,
                    edge_type='ATTENDED',
                    properties={'role': role.replace('_', ' ').title()}
                )
        
        # Create commissioner nodes
        for idx, commissioner in enumerate(commissioners):
            if commissioner and commissioner != 'null':
                person_id = await self._ensure_person_node(commissioner, 'Commissioner')
                await self.cosmos.create_edge_if_not_exists(
                    from_id=person_id,
                    to_id=meeting_id,
                    edge_type='ATTENDED',
                    properties={'role': 'Commissioner', 'seat': idx + 1}
                )
    
    async def _create_entity_nodes(self, entities: List[Dict], meeting_id: str) -> int:
        """Create nodes for all extracted entities."""
        entity_count = 0
        
        for entity in entities:
            try:
                entity_type = entity.get('type', 'unknown')
                entity_name = entity.get('name', '')
                entity_role = entity.get('role', '')
                entity_context = entity.get('context', '')
                
                if not entity_name:
                    continue
                
                if entity_type == 'person':
                    person_id = await self._ensure_person_node(entity_name, entity_role)
                    await self.cosmos.create_edge_if_not_exists(
                        from_id=person_id,
                        to_id=meeting_id,
                        edge_type='MENTIONED_IN',
                        properties={
                            'context': entity_context[:100],
                            'role': entity_role
                        }
                    )
                    entity_count += 1
                    
                elif entity_type == 'organization':
                    org_id = await self._ensure_organization_node(entity_name, entity_role)
                    await self.cosmos.create_edge_if_not_exists(
                        from_id=org_id,
                        to_id=meeting_id,
                        edge_type='MENTIONED_IN',
                        properties={
                            'context': entity_context[:100],
                            'org_type': entity_role
                        }
                    )
                    entity_count += 1
                    
                elif entity_type == 'department':
                    dept_id = await self._ensure_department_node(entity_name)
                    await self.cosmos.create_edge_if_not_exists(
                        from_id=dept_id,
                        to_id=meeting_id,
                        edge_type='INVOLVED_IN',
                        properties={'context': entity_context[:100]}
                    )
                    entity_count += 1
                    
                elif entity_type == 'location':
                    loc_id = await self._ensure_location_node(entity_name, entity_context)
                    await self.cosmos.create_edge_if_not_exists(
                        from_id=loc_id,
                        to_id=meeting_id,
                        edge_type='REFERENCED_IN',
                        properties={'context': entity_context[:100]}
                    )
                    entity_count += 1
                    
            except Exception as e:
                log.error(f"Failed to create entity node for {entity}: {e}")
        
        return entity_count
    
    async def _create_item_relationships(self, item: Dict, item_id: str, meeting_date: str):
        """Create rich relationships for agenda items."""
        
        # Sponsor relationships
        for sponsor in item.get('sponsors', []):
            try:
                person_id = await self._ensure_person_node(sponsor, 'Commissioner')
                await self.cosmos.create_edge_if_not_exists(
                    from_id=person_id,
                    to_id=item_id,
                    edge_type='SPONSORS',
                    properties={'role': 'sponsor'}
                )
                self.stats['edges_created'] += 1
            except Exception as e:
                log.error(f"Failed to create sponsor relationship: {e}")
        
        # Department relationships
        for dept in item.get('departments', []):
            try:
                dept_id = await self._ensure_department_node(dept)
                await self.cosmos.create_edge_if_not_exists(
                    from_id=dept_id,
                    to_id=item_id,
                    edge_type='RESPONSIBLE_FOR',
                    properties={'role': 'responsible_department'}
                )
                self.stats['edges_created'] += 1
            except Exception as e:
                log.error(f"Failed to create department relationship: {e}")
        
        # Stakeholder relationships  
        for stakeholder in item.get('stakeholders', []):
            try:
                org_id = await self._ensure_organization_node(stakeholder, 'Stakeholder')
                await self.cosmos.create_edge_if_not_exists(
                    from_id=org_id,
                    to_id=item_id,
                    edge_type='INVOLVED_IN',
                    properties={'role': 'stakeholder'}
                )
                self.stats['edges_created'] += 1
            except Exception as e:
                log.error(f"Failed to create stakeholder relationship: {e}")
        
        # Action relationships
        for action in item.get('actions', []):
            try:
                action_id = await self._ensure_action_node(action)
                await self.cosmos.create_edge_if_not_exists(
                    from_id=item_id,
                    to_id=action_id,
                    edge_type='REQUIRES_ACTION',
                    properties={'action_type': action}
                )
                self.stats['edges_created'] += 1
            except Exception as e:
                log.error(f"Failed to create action relationship: {e}")
    
    async def _create_url_relationships(self, item: Dict, item_id: str):
        """Create URL nodes and link to agenda items."""
        for url in item.get('urls', []):
            try:
                url_id = await self._ensure_url_node(url)
                await self.cosmos.create_edge_if_not_exists(
                    from_id=item_id,
                    to_id=url_id,
                    edge_type='HAS_URL',
                    properties={'url_type': 'document_link'}
                )
                self.stats['edges_created'] += 1
            except Exception as e:
                log.error(f"Failed to create URL relationship: {e}")
    
    async def _create_ontology_relationship(self, rel: Dict, meeting_date: str):
        """Create relationship from ontology data."""
        try:
            source = rel.get('source', '')
            target = rel.get('target', '')
            relationship = rel.get('relationship', '')
            source_type = rel.get('source_type', '')
            target_type = rel.get('target_type', '')
            
            # Determine source and target IDs based on type
            if source_type == 'person':
                source_id = await self._ensure_person_node(source, 'Participant')
            elif source_type == 'department':
                source_id = await self._ensure_department_node(source)
            elif source_type == 'organization':
                source_id = await self._ensure_organization_node(source, 'Organization')
            else:
                log.warning(f"Unknown source type: {source_type}")
                return
            
            if target_type == 'agenda_item':
                # Normalize target agenda item code
                normalized_target = self.normalize_item_code(target)
                target_id = f"item-{meeting_date}-{normalized_target}"
            else:
                log.warning(f"Unknown target type: {target_type}")
                return
            
            # Create the relationship
            await self.cosmos.create_edge_if_not_exists(
                from_id=source_id,
                to_id=target_id,
                edge_type=relationship.upper(),
                properties={
                    'source_type': source_type,
                    'target_type': target_type
                }
            )
            
        except Exception as e:
            log.error(f"Failed to create ontology relationship: {e}")
    
    async def _ensure_person_node(self, name: str, role: str) -> str:
        """Create or retrieve person node with upsert support."""
        clean_name = name.strip()
        # Clean the ID by removing invalid characters
        cleaned_id_part = clean_name.lower().replace(' ', '-').replace('.', '').replace("'", '').replace('"', '').replace('/', '-')
        person_id = f"person-{cleaned_id_part}"
        
        # Check cache first
        if person_id in self.entity_id_cache:
            return person_id
        
        # Check if exists in database
        if await self.cosmos.vertex_exists(person_id):
            self.entity_id_cache[person_id] = True
            return person_id
        
        # Create new person
        properties = {
            'nodeType': 'Person',
            'name': clean_name,
            'roles': role
        }
        
        if self.upsert_mode:
            created = await self.cosmos.upsert_vertex('Person', person_id, properties)
            if created:
                self.stats['nodes_created'] += 1
            else:
                self.stats['nodes_updated'] += 1
        else:
            if not await self.cosmos.vertex_exists(person_id):
                await self.cosmos.create_vertex('Person', person_id, properties)
                self.stats['nodes_created'] += 1
        
        self.entity_id_cache[person_id] = True
        return person_id
    
    async def _ensure_organization_node(self, name: str, org_type: str) -> str:
        """Create or retrieve organization node."""
        # Clean the ID
        cleaned_org_name = name.lower().replace(' ', '-').replace('.', '').replace("'", '').replace('"', '').replace('/', '-').replace(',', '')
        org_id = f"org-{cleaned_org_name}"
        
        if org_id in self.entity_id_cache:
            return org_id
        
        if await self.cosmos.vertex_exists(org_id):
            self.entity_id_cache[org_id] = True
            return org_id
        
        properties = {
            'nodeType': 'Organization',
            'name': name,
            'type': org_type
        }
        
        await self.cosmos.create_vertex('Organization', org_id, properties)
        self.entity_id_cache[org_id] = True
        self.stats['nodes_created'] += 1
        return org_id
    
    async def _ensure_department_node(self, name: str) -> str:
        """Create or retrieve department node."""
        cleaned_dept_name = name.lower().replace(' ', '-').replace('.', '').replace("'", '').replace('"', '').replace('/', '-')
        dept_id = f"dept-{cleaned_dept_name}"
        
        if dept_id in self.entity_id_cache:
            return dept_id
        
        if await self.cosmos.vertex_exists(dept_id):
            self.entity_id_cache[dept_id] = True
            return dept_id
        
        properties = {
            'nodeType': 'Department', 
            'name': name,
            'type': 'CityDepartment'
        }
        
        await self.cosmos.create_vertex('Department', dept_id, properties)
        self.entity_id_cache[dept_id] = True
        self.stats['nodes_created'] += 1
        return dept_id
    
    async def _ensure_location_node(self, name: str, context: str = '') -> str:
        """Create or retrieve location node."""
        cleaned_loc_name = name.lower().replace(' ', '-').replace('.', '').replace("'", '').replace('"', '').replace('/', '-').replace(',', '')
        loc_id = f"location-{cleaned_loc_name}"
        
        if loc_id in self.entity_id_cache:
            return loc_id
        
        if await self.cosmos.vertex_exists(loc_id):
            self.entity_id_cache[loc_id] = True
            return loc_id
        
        properties = {
            'nodeType': 'Location',
            'name': name,
            'context': context[:200] if context else '',
            'type': 'Location'
        }
        
        await self.cosmos.create_vertex('Location', loc_id, properties)
        self.entity_id_cache[loc_id] = True
        self.stats['nodes_created'] += 1
        return loc_id
    
    async def _ensure_action_node(self, action: str) -> str:
        """Create or retrieve action node."""
        cleaned_action = action.lower().replace(' ', '-').replace('.', '')
        action_id = f"action-{cleaned_action}"
        
        if action_id in self.entity_id_cache:
            return action_id
        
        if await self.cosmos.vertex_exists(action_id):
            self.entity_id_cache[action_id] = True
            return action_id
        
        properties = {
            'nodeType': 'Action',
            'name': action,
            'type': 'RequiredAction'
        }
        
        await self.cosmos.create_vertex('Action', action_id, properties)
        self.entity_id_cache[action_id] = True
        self.stats['nodes_created'] += 1
        return action_id
    
    async def _ensure_url_node(self, url: str) -> str:
        """Create or retrieve URL node."""
        url_hash = hashlib.md5(url.encode()).hexdigest()[:12]
        url_id = f"url-{url_hash}"
        
        if url_id in self.entity_id_cache:
            return url_id
        
        if await self.cosmos.vertex_exists(url_id):
            self.entity_id_cache[url_id] = True
            return url_id
        
        properties = {
            'nodeType': 'URL',
            'url': url,
            'domain': url.split('/')[2] if '://' in url else 'unknown',
            'type': 'Hyperlink'
        }
        
        await self.cosmos.create_vertex('URL', url_id, properties)
        self.entity_id_cache[url_id] = True
        self.stats['nodes_created'] += 1
        return url_id

    async def process_linked_documents(self, linked_docs: Dict, meeting_id: str, meeting_date: str):
        """Process and create nodes for linked documents."""
        log.info("ðŸ“„ Processing linked documents...")
        
        created_count = 0
        missing_items = []
        
        for doc_type, docs in linked_docs.items():
            if not docs:
                continue
                
            log.info(f"\n   ðŸ“‚ Processing {len(docs)} {doc_type}")
            
            for doc in docs:
                # Use the singular form for logging
                doc_type_singular = doc_type[:-1] if doc_type.endswith('s') else doc_type
                
                if doc_type in ['ordinances', 'resolutions']:
                    log.info(f"\n   Processing {doc_type_singular} {doc.get('document_number', 'unknown')}")
                    log.info(f"      Item code: {doc.get('item_code', 'MISSING')}")
                    
                    # Create document node
                    doc_id = await self._create_document_node(doc, doc_type, meeting_date)
                    
                    if doc_id:
                        created_count += 1
                        log.info(f"      âœ… Created document node: {doc_id}")
                        
                        # Link to meeting
                        await self.cosmos.create_edge(
                            from_id=doc_id,
                            to_id=meeting_id,
                            edge_type='PRESENTED_AT',
                            properties={'date': meeting_date}
                        )
                        
                        # Try to link to agenda item if item_code exists
                        item_code = doc.get('item_code')
                        if item_code:
                            normalized_code = self.normalize_item_code(item_code)
                            item_id = f"item-{meeting_date}-{normalized_code}"
                            
                            # Check if agenda item exists
                            if await self.cosmos.vertex_exists(item_id):
                                await self.cosmos.create_edge(
                                    from_id=item_id,
                                    to_id=doc_id,
                                    edge_type='REFERENCES_DOCUMENT',
                                    properties={'document_type': doc_type_singular}
                                )
                                log.info(f"      ðŸ”— Linked to agenda item: {item_id}")
                            else:
                                log.warning(f"      âŒ Agenda item not found: {item_id}")
                                missing_items.append({
                                    'document_number': doc.get('document_number'),
                                    'item_code': item_code,
                                    'normalized_code': normalized_code,
                                    'expected_item_id': item_id,
                                    'document_type': doc_type_singular
                                })
                        else:
                            log.warning(f"      âš ï¸  No item_code found for {doc.get('document_number')}")
        
        log.info(f"ðŸ“„ Document processing complete: {created_count} documents created")
        if missing_items:
            log.warning(f"âš ï¸  {len(missing_items)} documents could not be linked to agenda items")
        
        return missing_items

    async def _create_document_node(self, doc_info: Dict, doc_type: str, meeting_date: str) -> str:
        """Create or update an Ordinance or Resolution node."""
        doc_number = doc_info.get('document_number', 'unknown')
        
        # Use the document type from doc_info if available, otherwise use the passed type
        actual_doc_type = doc_info.get('document_type', doc_type)
        
        # Ensure consistency in ID generation
        if actual_doc_type.lower() == 'resolution':
            doc_id = f"resolution-{doc_number}"
            node_type = 'Resolution'
        else:
            doc_id = f"ordinance-{doc_number}"
            node_type = 'Ordinance'
        
        # Get full title without truncation
        title = doc_info.get('title', '')
        if not title and doc_info.get('parsed_data', {}).get('title'):
            title = doc_info['parsed_data']['title']
        
        if title is None:
            title = f"Untitled {actual_doc_type.capitalize()} {doc_number}"
            log.warning(f"No title found for {actual_doc_type} {doc_number}, using default")
        
        properties = {
            'nodeType': node_type,
            'document_number': doc_number,
            'full_title': title,
            'title': title[:200] if len(title) > 200 else title,
            'document_type': actual_doc_type.capitalize(),
            'meeting_date': meeting_date
        }
        
        # Add parsed metadata
        parsed_data = doc_info.get('parsed_data', {})
        
        if parsed_data.get('date_passed'):
            properties['date_passed'] = parsed_data['date_passed']
        
        if parsed_data.get('agenda_item'):
            properties['agenda_item'] = parsed_data['agenda_item']
        
        # Add vote details as JSON
        if parsed_data.get('vote_details'):
            properties['vote_details'] = json.dumps(parsed_data['vote_details'])
        
        # Add signatories
        if parsed_data.get('signatories', {}).get('mayor'):
            properties['mayor_signature'] = parsed_data['signatories']['mayor']
        
        if self.upsert_mode:
            created = await self.cosmos.upsert_vertex(node_type, doc_id, properties)
            if created:
                self.stats['nodes_created'] += 1
                log.info(f"âœ… Created document node: {doc_id}")
            else:
                self.stats['nodes_updated'] += 1
                log.info(f"ðŸ“ Updated document node: {doc_id}")
        else:
            await self.cosmos.create_vertex(node_type, doc_id, properties)
            self.stats['nodes_created'] += 1
        
        # Create edges for sponsors
        if parsed_data.get('motion', {}).get('moved_by'):
            person_id = await self._ensure_person_node(
                parsed_data['motion']['moved_by'], 
                'Commissioner'
            )
            if await self.cosmos.create_edge_if_not_exists(person_id, doc_id, 'MOVED'):
                self.stats['edges_created'] += 1
            else:
                self.stats['edges_skipped'] += 1
        
        return doc_id


================================================================================


################################################################################
# File: scripts/stages/chunk_text.py
################################################################################

# File: scripts/stages/chunk_text.py

#!/usr/bin/env python3
"""
Stage 5 â€” Token-based chunking with tiktoken validation.
"""
from __future__ import annotations
import json, logging, math, pathlib, re
from typing import Dict, List, Sequence, Tuple, Any, Optional
import asyncio
from concurrent.futures import ProcessPoolExecutor
import multiprocessing as mp

# ðŸŽ¯ TIKTOKEN VALIDATION - Add token validation layer
try:
    import tiktoken
    TIKTOKEN_AVAILABLE = True
except ImportError:
    TIKTOKEN_AVAILABLE = False
    logging.warning("tiktoken not available - token-based chunking requires tiktoken")

# Token limits for validation (consistent with embed_vectors.py)
MAX_CHUNK_TOKENS = 6000  # Maximum tokens per chunk for embedding
MIN_CHUNK_TOKENS = 100   # Minimum viable chunk size
EMBEDDING_MODEL = "text-embedding-ada-002"

# â”€â”€â”€ TOKEN-BASED CHUNKING PARAMETERS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
WINDOW_TOKENS = 3000   # Token-based window (was 768 words)
OVERLAP_TOKENS = 600   # Token-based overlap (20% of window)
log = logging.getLogger(__name__)

# â”€â”€â”€ helpers to split into token windows â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def concat_tokens_by_encoding(sections: Sequence[Dict[str, Any]]) -> Tuple[List[int], List[int]]:
    """Convert sections to encoded tokens with page mapping."""
    if not TIKTOKEN_AVAILABLE:
        raise RuntimeError("tiktoken is required for token-based chunking")
    
    encoder = tiktoken.encoding_for_model(EMBEDDING_MODEL)
    all_tokens = []
    token_to_page = []
    
    for s in sections:
        # Be defensive - build text if missing
        if 'text' not in s:
            if 'elements' in s:
                text = '\n'.join(
                    el.get('text', '') for el in s['elements'] 
                    if el.get('text', '') and not el.get('text', '').startswith('self_ref=')
                )
            else:
                text = ""
            log.warning(f"Section missing 'text' field, built from elements: {s.get('section', 'untitled')}")
        else:
            text = s.get("text", "")
        
        # Encode text to tokens
        tokens = encoder.encode(text)
        all_tokens.extend(tokens)
        
        # Handle page mapping
        if 'page_number' in s:
            page_num = s['page_number']
        elif 'page_start' in s and 'page_end' in s:
            page_start = s['page_start']
            page_end = s['page_end']
            if page_start == page_end:
                page_num = page_start
            else:
                # For multi-page sections, distribute tokens across pages
                pages_span = page_end - page_start + 1
                tokens_per_page = max(1, len(tokens) // pages_span)
                for i, token in enumerate(tokens):
                    page = min(page_end, page_start + (i // tokens_per_page))
                    token_to_page.append(page)
                continue  # Skip the extend below since we handled it above
        else:
            page_num = 1
            log.warning(f"Section has no page info: {s.get('section', 'untitled')}")
        
        # Map all tokens in this section to the page (for single page sections)
        if 'page_start' not in s or s['page_start'] == s.get('page_end', s['page_start']):
            token_to_page.extend([page_num] * len(tokens))
    
    return all_tokens, token_to_page

def sliding_chunks_by_tokens(
    encoded_tokens: List[int], 
    token_to_page: List[int], 
    *, 
    window_tokens: int, 
    overlap_tokens: int
) -> List[Dict[str, Any]]:
    """Create sliding chunks based on token count."""
    if not TIKTOKEN_AVAILABLE:
        raise RuntimeError("tiktoken is required for token-based chunking")
    
    encoder = tiktoken.encoding_for_model(EMBEDDING_MODEL)
    step = max(1, window_tokens - overlap_tokens)
    chunks = []
    i = 0
    
    while i < len(encoded_tokens):
        start = i
        end = min(len(encoded_tokens), i + window_tokens)
        
        # Extract token chunk
        chunk_tokens = encoded_tokens[start:end]
        
        # Decode back to text
        chunk_text = encoder.decode(chunk_tokens)
        
        # Determine page range for this chunk
        page_start = token_to_page[start] if start < len(token_to_page) else 1
        page_end = token_to_page[end - 1] if end - 1 < len(token_to_page) else page_start
        
        chunk = {
            "chunk_index": len(chunks),
            "token_start": start,
            "token_end": end - 1,
            "page_start": page_start,
            "page_end": page_end,
            "text": chunk_text,
            "token_count": len(chunk_tokens)  # Actual token count
        }
        
        chunks.append(chunk)
        
        # Break if we've reached the end
        if end == len(encoded_tokens):
            break
            
        i += step
    
    return chunks

# Legacy word-based functions (kept for fallback if tiktoken unavailable)
def concat_tokens(sections:Sequence[Dict[str,Any]])->Tuple[List[str],List[int]]:
    tokens,page_map=[],[]
    for s in sections:
        # Be defensive - build text if missing
        if 'text' not in s:
            if 'elements' in s:
                text = '\n'.join(
                    el.get('text', '') for el in s['elements'] 
                    if el.get('text', '') and not el.get('text', '').startswith('self_ref=')
                )
            else:
                text = ""
            log.warning(f"Section missing 'text' field, built from elements: {s.get('section', 'untitled')}")
        else:
            text = s.get("text", "")
            
        words=text.split()
        tokens.extend(words)
        
        # Handle both page_number (from page_secs) and page_start/page_end (from logical_secs)
        if 'page_number' in s:
            # Single page section
            page_map.extend([s['page_number']]*len(words))
        elif 'page_start' in s and 'page_end' in s:
            # Multi-page section - distribute words across pages
            page_start = s['page_start']
            page_end = s['page_end']
            if page_start == page_end:
                # All on same page
                page_map.extend([page_start]*len(words))
            else:
                # Distribute words evenly across pages
                pages_span = page_end - page_start + 1
                words_per_page = max(1, len(words) // pages_span)
                for i, word in enumerate(words):
                    page = min(page_end, page_start + (i // words_per_page))
                    page_map.append(page)
        else:
            # Fallback to page 1
            page_map.extend([1]*len(words))
            log.warning(f"Section has no page info: {s.get('section', 'untitled')}")
            
    return tokens,page_map

def sliding_chunks(tokens:List[str],page_map:List[int],*,window:int,overlap:int)->List[Dict[str,Any]]:
    step=max(1,window-overlap)
    out,i=[],0
    SENT_END_RE=re.compile(r"[.!?]$")
    while i<len(tokens):
        start,end=i,min(len(tokens),i+window)
        while end<len(tokens) and not SENT_END_RE.search(tokens[end-1]) and end-start<window+256:
            end+=1
        out.append({"chunk_index":len(out),"token_start":start,"token_end":end-1,
                    "page_start":page_map[start],"page_end":page_map[end-1],
                    "text":" ".join(tokens[start:end])})
        if end==len(tokens): break
        i+=step
    return out
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# ðŸŽ¯ TIKTOKEN VALIDATION FUNCTIONS
def count_tiktoken_accurate(text: str) -> int:
    """Count tokens accurately using tiktoken."""
    if TIKTOKEN_AVAILABLE:
        try:
            encoder = tiktoken.encoding_for_model(EMBEDDING_MODEL)
            return len(encoder.encode(text))
        except:
            pass
    
    # Conservative fallback estimation
    return int(len(text.split()) * 1.5) + 50

def smart_split_chunk(chunk: Dict[str, Any], max_tokens: int = MAX_CHUNK_TOKENS) -> List[Dict[str, Any]]:
    """Split an oversized chunk into smaller token-compliant chunks."""
    text = chunk["text"]
    tokens = count_tiktoken_accurate(text)
    
    if tokens <= max_tokens:
        return [chunk]
    
    # Calculate how many sub-chunks we need
    num_splits = math.ceil(tokens / max_tokens)
    target_words_per_split = len(text.split()) // num_splits
    
    log.info(f"ðŸŽ¯ Splitting oversized chunk: {tokens} tokens â†’ {num_splits} chunks of ~{max_tokens} tokens each")
    
    words = text.split()
    sentences = re.split(r'[.!?]+', text)
    
    sub_chunks = []
    current_words = []
    current_tokens = 0
    
    # Split by sentences when possible, otherwise by words
    for sentence in sentences:
        sentence_words = sentence.strip().split()
        if not sentence_words:
            continue
            
        sentence_tokens = count_tiktoken_accurate(sentence)
        
        # If adding this sentence would exceed limit, finalize current chunk
        if current_tokens + sentence_tokens > max_tokens and current_words:
            # Create sub-chunk
            sub_text = " ".join(current_words)
            sub_chunk = {
                "chunk_index": chunk["chunk_index"],
                "sub_chunk_index": len(sub_chunks),
                "token_start": chunk["token_start"] + len(" ".join(words[:words.index(current_words[0])])),
                "token_end": chunk["token_start"] + len(" ".join(words[:words.index(current_words[-1])])) + len(current_words[-1]),
                "page_start": chunk["page_start"],
                "page_end": chunk["page_end"],
                "text": sub_text
            }
            sub_chunks.append(sub_chunk)
            current_words = []
            current_tokens = 0
        
        # Add sentence to current chunk
        current_words.extend(sentence_words)
        current_tokens += sentence_tokens
    
    # Add final sub-chunk if there's remaining content
    if current_words:
        sub_text = " ".join(current_words)
        if count_tiktoken_accurate(sub_text) >= MIN_CHUNK_TOKENS:
            sub_chunk = {
                "chunk_index": chunk["chunk_index"],
                "sub_chunk_index": len(sub_chunks),
                "token_start": chunk["token_start"],
                "token_end": chunk["token_end"],
                "page_start": chunk["page_start"],
                "page_end": chunk["page_end"],
                "text": sub_text
            }
            sub_chunks.append(sub_chunk)
    
    log.info(f"ðŸŽ¯ Split complete: {len(sub_chunks)} sub-chunks created")
    return sub_chunks

def validate_and_fix_chunks(chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Validate chunks against tiktoken limits and fix oversized ones."""
    if not TIKTOKEN_AVAILABLE:
        log.warning("ðŸŽ¯ tiktoken not available - skipping token validation")
        return chunks
    
    valid_chunks = []
    oversized_count = 0
    total_tokens_before = 0
    total_tokens_after = 0
    
    log.info(f"ðŸŽ¯ TIKTOKEN VALIDATION: Processing {len(chunks)} chunks")
    
    for chunk in chunks:
        tokens = count_tiktoken_accurate(chunk["text"])
        total_tokens_before += tokens
        
        if tokens <= MAX_CHUNK_TOKENS:
            # Chunk is fine, keep as-is
            valid_chunks.append(chunk)
            total_tokens_after += tokens
        else:
            # Split oversized chunk
            log.warning(f"ðŸŽ¯ Oversized chunk detected: {tokens} tokens (max: {MAX_CHUNK_TOKENS})")
            oversized_count += 1
            
            split_chunks = smart_split_chunk(chunk, MAX_CHUNK_TOKENS)
            valid_chunks.extend(split_chunks)
            
            # Count tokens in split chunks
            for split_chunk in split_chunks:
                total_tokens_after += count_tiktoken_accurate(split_chunk["text"])
    
    # Final validation check
    max_tokens_found = max((count_tiktoken_accurate(chunk["text"]) for chunk in valid_chunks), default=0)
    
    log.info(f"ðŸŽ¯ TIKTOKEN VALIDATION COMPLETE:")
    log.info(f"   ðŸ“Š Original chunks: {len(chunks)}")
    log.info(f"   ðŸ“Š Final chunks: {len(valid_chunks)}")
    log.info(f"   ðŸ“Š Oversized chunks split: {oversized_count}")
    log.info(f"   ðŸ“Š Largest chunk: {max_tokens_found} tokens (limit: {MAX_CHUNK_TOKENS})")
    log.info(f"   âœ… GUARANTEED: All chunks â‰¤ {MAX_CHUNK_TOKENS} tokens")
    
    if max_tokens_found > MAX_CHUNK_TOKENS:
        log.error(f"ðŸš¨ VALIDATION FAILED: Chunk still exceeds limit!")
        raise RuntimeError(f"Chunk validation failed: {max_tokens_found} > {MAX_CHUNK_TOKENS}")
    
    return valid_chunks

# Legacy constants for fallback
WINDOW  = 768
OVERLAP = int(WINDOW*0.20)           # 154-token (20 %) overlap

# Parallel chunking for multiple documents
async def chunk_batch_async(
    json_paths: List[pathlib.Path],
    max_workers: Optional[int] = None
) -> Dict[pathlib.Path, List[Dict[str, Any]]]:
    """Chunk multiple documents in parallel with token-based chunking."""
    from .acceleration_utils import hardware
    
    results = {}
    
    # ðŸŽ¯ Show token-based chunking status
    if TIKTOKEN_AVAILABLE:
        log.info(f"ðŸŽ¯ TOKEN-BASED CHUNKING ENABLED: Using {WINDOW_TOKENS} token windows with {OVERLAP_TOKENS} token overlap")
    else:
        log.warning(f"âš ï¸  TIKTOKEN NOT AVAILABLE: Falling back to word-based chunking (install: pip install tiktoken)")
    
    # CPU-bound task - use process pool
    with hardware.get_process_pool(max_workers) as executor:
        future_to_path = {
            executor.submit(chunk, path): path 
            for path in json_paths
        }
        
        from tqdm import tqdm
        from concurrent.futures import as_completed
        
        for future in tqdm(as_completed(future_to_path), 
                          total=len(json_paths), 
                          desc="Chunking documents"):
            path = future_to_path[future]
            try:
                chunks = future.result()
                results[path] = chunks
            except Exception as exc:
                log.error(f"Failed to chunk {path.name}: {exc}")
                results[path] = []
    
    # ðŸŽ¯ Summary of token-based chunking results
    total_chunks = sum(len(chunks) for chunks in results.values())
    total_tokens = sum(chunk.get('token_count', count_tiktoken_accurate(chunk['text'])) 
                      for chunks in results.values() 
                      for chunk in chunks)
    log.info(f"ðŸŽ¯ BATCH CHUNKING COMPLETE: {total_chunks} chunks created ({total_tokens} total tokens)")
    
    return results

# Optimized chunking with better memory management
def chunk_optimized(json_path: pathlib.Path) -> List[Dict[str, Any]]:
    """Token-based optimized chunking with validation."""
    root = json_path.with_name(json_path.stem + "_chunks.json")
    if root.exists():
        existing_chunks = json.loads(root.read_text())
        # Validate existing chunks if they don't have tiktoken validation yet
        if existing_chunks and "sub_chunk_index" not in str(existing_chunks):
            log.info(f"ðŸŽ¯ Re-validating existing chunks in {root.name}")
            validated_chunks = validate_and_fix_chunks(existing_chunks)
            if len(validated_chunks) != len(existing_chunks):
                # Chunks were modified, save the validated version
                with open(root, 'w') as f:
                    json.dump(validated_chunks, f, indent=2, ensure_ascii=False)
                log.info(f"ðŸŽ¯ Updated {root.name} with validated chunks")
            return validated_chunks
        return existing_chunks
    
    # Stream large JSON files
    with open(json_path, 'r') as f:
        data = json.load(f)
    
    if "sections" not in data:
        raise RuntimeError(f"{json_path} has no 'sections' key")
    
    # Process in smaller batches to reduce memory usage
    sections = data["sections"]
    batch_size = 100  # Process 100 sections at a time
    
    if TIKTOKEN_AVAILABLE:
        # ðŸŽ¯ TOKEN-BASED CHUNKING - Primary approach
        log.info(f"ðŸŽ¯ Using token-based chunking with {WINDOW_TOKENS} token windows")
        
        all_tokens = []
        all_token_to_page = []
        
        for i in range(0, len(sections), batch_size):
            batch = sections[i:i + batch_size]
            tokens, page_map = concat_tokens_by_encoding(batch)
            all_tokens.extend(tokens)
            all_token_to_page.extend(page_map)
        
        chunks = sliding_chunks_by_tokens(
            all_tokens, 
            all_token_to_page, 
            window_tokens=WINDOW_TOKENS, 
            overlap_tokens=OVERLAP_TOKENS
        )
        
        # Token-based chunks are already guaranteed to be within limits
        log.info(f"ðŸŽ¯ Token-based chunking produced {len(chunks)} chunks")
        
        # Additional validation for safety
        max_tokens = max((chunk.get('token_count', count_tiktoken_accurate(chunk['text'])) for chunk in chunks), default=0)
        if max_tokens > MAX_CHUNK_TOKENS:
            log.warning(f"ðŸŽ¯ Unexpected: Token-based chunk exceeds limit ({max_tokens} > {MAX_CHUNK_TOKENS}), applying fallback validation")
            chunks = validate_and_fix_chunks(chunks)
    else:
        # ðŸŽ¯ FALLBACK: Word-based chunking when tiktoken unavailable
        log.warning(f"ðŸŽ¯ Falling back to word-based chunking (tiktoken unavailable)")
        
        all_tokens = []
        all_page_map = []
        
        for i in range(0, len(sections), batch_size):
            batch = sections[i:i + batch_size]
            toks, pmap = concat_tokens(batch)
            all_tokens.extend(toks)
            all_page_map.extend(pmap)
        
        chunks = sliding_chunks(all_tokens, all_page_map, window=WINDOW, overlap=OVERLAP)
        
        # Apply validation for word-based chunks
        if chunks:
            log.info(f"ðŸŽ¯ Applying validation to {len(chunks)} word-based chunks")
            chunks = validate_and_fix_chunks(chunks)
    
    # Write chunks
    if chunks:
        with open(root, 'w') as f:
            json.dump(chunks, f, indent=2, ensure_ascii=False)
        log.info("âœ“ %s chunks â†’ %s", len(chunks), root.name)
        
        return chunks
    else:
        log.warning("%s â€“ no chunks produced; file skipped", json_path.name)
        return []

# Keep original interface
def chunk(json_path: pathlib.Path) -> List[Dict[str, Any]]:
    """Original interface maintained for compatibility."""
    return chunk_optimized(json_path)

if __name__ == "__main__":
    import argparse, logging
    logging.basicConfig(level=logging.INFO, format="%(message)s")
    p=argparse.ArgumentParser(); p.add_argument("json",type=pathlib.Path)
    chunk(p.parse_args().json)


================================================================================


################################################################################
# File: scripts/graph_stages/enhanced_document_linker.py
################################################################################

# File: scripts/graph_stages/enhanced_document_linker.py

"""
Enhanced Document Linker
Links both ordinance and resolution documents to their corresponding agenda items.
"""

import logging
import re
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import json
from datetime import datetime
import PyPDF2
from groq import Groq
import os
from dotenv import load_dotenv

load_dotenv()

log = logging.getLogger('enhanced_document_linker')


class EnhancedDocumentLinker:
    """Links ordinance and resolution documents to agenda items."""
    
    def __init__(self,
                 groq_api_key: Optional[str] = None,
                 model: str = "qwen-qwq-32b",
                 agenda_extraction_max_tokens: int = 100000):
        """Initialize the enhanced document linker."""
        self.api_key = groq_api_key or os.getenv("GROQ_API_KEY")
        if not self.api_key:
            raise ValueError("GROQ_API_KEY not found in environment")
        
        self.client = Groq(api_key=self.api_key)
        self.model = model
        self.agenda_extraction_max_tokens = agenda_extraction_max_tokens
    
    def _parse_qwen_response(self, response_text: str) -> str:
        """Parse qwen response to extract content outside thinking tags."""
        thinking_pattern = r'<thinking>.*?</thinking>'
        cleaned_text = re.sub(thinking_pattern, '', response_text, flags=re.DOTALL)
        cleaned_text = re.sub(r'<[^>]+>', '', cleaned_text)
        return cleaned_text.strip()
    
    async def link_documents_for_meeting(self, 
                                       meeting_date: str,
                                       ordinances_dir: Path,
                                       resolutions_dir: Path) -> Dict[str, List[Dict]]:
        """Find and link all documents (ordinances AND resolutions) for a specific meeting date."""
        log.info(f"ðŸ”— Enhanced linking: documents for meeting date: {meeting_date}")
        log.info(f"ðŸ“ Ordinances directory: {ordinances_dir}")
        log.info(f"ðŸ“ Resolutions directory: {resolutions_dir}")
        
        # Create debug directory
        debug_dir = Path("city_clerk_documents/graph_json/debug")
        debug_dir.mkdir(exist_ok=True)
        
        # Convert date format: "01.09.2024" -> "01_09_2024"
        date_underscore = meeting_date.replace(".", "_")
        
        # Initialize results
        linked_documents = {
            "ordinances": [],
            "resolutions": []
        }
        
        # Process ordinances
        if ordinances_dir.exists():
            ordinance_files = self._find_matching_files(ordinances_dir, date_underscore)
            log.info(f"ðŸ“„ Found {len(ordinance_files)} ordinance files")
            
            for doc_path in ordinance_files:
                doc_info = await self._process_document(doc_path, meeting_date, "ordinance")
                if doc_info:
                    linked_documents["ordinances"].append(doc_info)
        else:
            log.warning(f"âš ï¸  Ordinances directory not found: {ordinances_dir}")
        
        # Process resolutions - NEW LOGIC
        if resolutions_dir.exists():
            # Check for year subdirectory first
            year = meeting_date.split('.')[-1]  # Extract year from date
            year_dir = resolutions_dir / year
            
            if year_dir.exists():
                resolution_files = self._find_matching_files(year_dir, date_underscore)
                log.info(f"ðŸ“„ Found {len(resolution_files)} resolution files in {year} directory")
            else:
                # Fall back to main resolutions directory
                resolution_files = self._find_matching_files(resolutions_dir, date_underscore)
                log.info(f"ðŸ“„ Found {len(resolution_files)} resolution files in main directory")
            
            for doc_path in resolution_files:
                doc_info = await self._process_document(doc_path, meeting_date, "resolution")
                if doc_info:
                    linked_documents["resolutions"].append(doc_info)
        else:
            log.warning(f"âš ï¸  Resolutions directory not found: {resolutions_dir}")
        
        # Save enhanced linked documents info
        with open(debug_dir / "enhanced_linked_documents.json", 'w') as f:
            json.dump(linked_documents, f, indent=2)
        
        # Log summary
        total_linked = len(linked_documents['ordinances']) + len(linked_documents['resolutions'])
        log.info(f"âœ… Enhanced linking complete:")
        log.info(f"   ðŸ“„ Ordinances: {len(linked_documents['ordinances'])}")
        log.info(f"   ðŸ“„ Resolutions: {len(linked_documents['resolutions'])}")
        log.info(f"   ðŸ“„ Total linked: {total_linked}")
        
        # Save detailed report
        self._generate_linking_report(meeting_date, linked_documents, debug_dir)
        
        return linked_documents
    
    def _find_matching_files(self, directory: Path, date_pattern: str) -> List[Path]:
        """Find all PDF files matching the date pattern."""
        # Pattern: YYYY-## - MM_DD_YYYY.pdf
        pattern = f"*{date_pattern}.pdf"
        matching_files = list(directory.glob(pattern))
        
        # Also try without spaces in case filenames vary
        pattern2 = f"*{date_pattern}*.pdf"
        additional_files = [f for f in directory.glob(pattern2) if f not in matching_files]
        matching_files.extend(additional_files)
        
        # Also check for variations in date format
        # Some files might use dashes instead of underscores
        date_dash = date_pattern.replace("_", "-")
        pattern3 = f"*{date_dash}*.pdf"
        more_files = [f for f in directory.glob(pattern3) if f not in matching_files]
        matching_files.extend(more_files)
        
        return sorted(matching_files)
    
    async def _process_document(self, doc_path: Path, meeting_date: str, doc_type: str) -> Optional[Dict[str, Any]]:
        """Process a single document to extract agenda item reference."""
        try:
            # Extract document number from filename
            doc_match = re.match(r'^(\d{4}-\d{2,3})', doc_path.name)
            if not doc_match:
                log.warning(f"Could not parse document number from {doc_path.name}")
                return None
            
            document_number = doc_match.group(1)
            
            # Extract text from PDF
            text = self._extract_pdf_text(doc_path)
            if not text:
                log.warning(f"No text extracted from {doc_path.name}")
                return None
            
            # Extract agenda item code using LLM
            item_code = await self._extract_agenda_item_code(text, document_number, doc_type)
            
            # Extract additional metadata
            parsed_data = self._parse_document_metadata(text, doc_type)
            
            doc_info = {
                "path": str(doc_path),
                "filename": doc_path.name,
                "document_number": document_number,
                "item_code": item_code,
                "document_type": doc_type.capitalize(),
                "title": self._extract_title(text, doc_type),
                "parsed_data": parsed_data
            }
            
            log.info(f"ðŸ“„ Processed {doc_type} {doc_path.name}: Item {item_code or 'NOT_FOUND'}")
            return doc_info
            
        except Exception as e:
            log.error(f"Error processing {doc_path.name}: {e}")
            return None
    
    def _extract_pdf_text(self, pdf_path: Path) -> str:
        """Extract text from PDF file."""
        try:
            with open(pdf_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                text_parts = []
                
                # Extract text from all pages
                for page in reader.pages:
                    text = page.extract_text()
                    if text:
                        text_parts.append(text)
                
                return "\n".join(text_parts)
        except Exception as e:
            log.error(f"Failed to extract text from {pdf_path.name}: {e}")
            return ""
    
    async def _extract_agenda_item_code(self, text: str, document_number: str, doc_type: str) -> Optional[str]:
        """Extract agenda item code from document text using LLM."""
        debug_dir = Path("city_clerk_documents/graph_json/debug")
        debug_dir.mkdir(exist_ok=True)
        
        # Customize prompt based on document type
        if doc_type == "resolution":
            doc_type_text = "resolution"
            typical_sections = "F items (e.g., F-1, F-2, F-3)"
        else:
            doc_type_text = "ordinance"
            typical_sections = "E items (e.g., E-1, E-2, E-3)"
        
        prompt = f"""You are analyzing a City of Coral Gables {doc_type_text} document (Document #{document_number}).

Your task is to find the AGENDA ITEM CODE referenced in this document.

IMPORTANT: The agenda item can appear ANYWHERE in the document - on page 3, at the end, or anywhere else. Search the ENTIRE document carefully.

For {doc_type_text}s, agenda items typically appear as {typical_sections}.

The agenda item typically appears in formats like:
- (Agenda Item: F-1)
- Agenda Item: F-3)
- (Agenda Item F-1)
- Item F-3
- F.-3. (with periods and dots)
- F.-2. (with dots)
- F-2 (without dots)
- Item F-2

Full document text:
{text}

Respond in this EXACT format:
AGENDA_ITEM: [code] or AGENDA_ITEM: NOT_FOUND

Important: Return the code as it appears (e.g., F-2, not F.-2.)

Examples:
- If you find "(Agenda Item: F-2)" â†’ respond: AGENDA_ITEM: F-2
- If you find "Item F-2" â†’ respond: AGENDA_ITEM: F-2
- If you find "F.-3." â†’ respond: AGENDA_ITEM: F-3
- If no agenda item found â†’ respond: AGENDA_ITEM: NOT_FOUND"""
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": f"You are a precise data extractor for {doc_type_text} documents. Find and extract only the agenda item code. Search the ENTIRE document thoroughly."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_tokens=self.agenda_extraction_max_tokens
            )
            
            raw_response = response.choices[0].message.content.strip()
            
            # Save raw LLM response for debugging
            with open(debug_dir / f"llm_response_{doc_type}_{document_number}_raw.txt", 'w', encoding='utf-8') as f:
                f.write(raw_response)
            
            # Parse response
            result = self._parse_qwen_response(raw_response)
            
            # Save cleaned response
            with open(debug_dir / f"llm_response_{doc_type}_{document_number}_cleaned.txt", 'w', encoding='utf-8') as f:
                f.write(result)
            
            # Parse the response
            if "AGENDA_ITEM:" in result:
                code = result.split("AGENDA_ITEM:")[1].strip()
                if code != "NOT_FOUND":
                    code = self._normalize_item_code(code)
                    log.info(f"âœ… Found agenda item code for {doc_type} {document_number}: {code}")
                    return code
                else:
                    log.warning(f"âŒ LLM could not find agenda item in {doc_type} {document_number}")
            else:
                log.error(f"âŒ Invalid LLM response format for {doc_type} {document_number}: {result[:100]}")
            
            return None
            
        except Exception as e:
            log.error(f"Failed to extract agenda item for {doc_type} {document_number}: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _normalize_item_code(self, code: str) -> str:
        """Normalize item code to consistent format."""
        if not code:
            return code
        
        # Remove trailing dots and spaces
        code = code.rstrip('. ')
        
        # Remove dots between letter and dash: "E.-1" -> "E-1"
        code = re.sub(r'([A-Z])\.(-)', r'\1\2', code)
        
        # Handle cases without dash: "E.1" -> "E-1"
        code = re.sub(r'([A-Z])\.(\d)', r'\1-\2', code)
        
        # Remove any remaining dots
        code = code.replace('.', '')
        
        # Ensure we have a dash between letter and number
        code = re.sub(r'([A-Z])(\d)', r'\1-\2', code)
        
        return code
    
    def _extract_title(self, text: str, doc_type: str) -> str:
        """Extract document title from text."""
        # Look for "AN ORDINANCE" or "A RESOLUTION" pattern
        if doc_type == "resolution":
            pattern = r'(A\s+RESOLUTION[^.]+\.)'
        else:
            pattern = r'(AN?\s+ORDINANCE[^.]+\.)'
            
        title_match = re.search(pattern, text[:2000], re.IGNORECASE)
        if title_match:
            return title_match.group(1).strip()
        
        # Fallback to first substantive line
        lines = text.split('\n')
        for line in lines[:20]:
            if len(line) > 20 and not line.isdigit():
                return line.strip()[:200]
        
        return f"Untitled {doc_type.capitalize()}"
    
    def _parse_document_metadata(self, text: str, doc_type: str) -> Dict[str, Any]:
        """Parse additional metadata from document."""
        metadata = {
            "document_type": doc_type
        }
        
        # Extract date passed
        date_match = re.search(r'day\s+of\s+(\w+),?\s+(\d{4})', text)
        if date_match:
            metadata["date_passed"] = date_match.group(0)
        
        # Extract vote information
        vote_match = re.search(r'PASSED\s+AND\s+ADOPTED.*?(\d+).*?(\d+)', text, re.IGNORECASE | re.DOTALL)
        if vote_match:
            metadata["vote_details"] = {
                "ayes": vote_match.group(1),
                "nays": vote_match.group(2) if len(vote_match.groups()) > 1 else "0"
            }
        
        # Extract motion information
        motion_match = re.search(r'motion\s+(?:was\s+)?made\s+by\s+([^,]+)', text, re.IGNORECASE)
        if motion_match:
            metadata["motion"] = {"moved_by": motion_match.group(1).strip()}
        
        # Extract mayor signature
        mayor_match = re.search(r'Mayor[:\s]+([^\n]+)', text[-1000:])
        if mayor_match:
            metadata["signatories"] = {"mayor": mayor_match.group(1).strip()}
        
        # Resolution-specific metadata
        if doc_type == "resolution":
            # Look for resolution-specific patterns
            purpose_match = re.search(r'(?:WHEREAS|PURPOSE)[:\s]+([^.]+)', text, re.IGNORECASE)
            if purpose_match:
                metadata["purpose"] = purpose_match.group(1).strip()
        
        return metadata
    
    def _generate_linking_report(self, meeting_date: str, linked_documents: Dict, debug_dir: Path):
        """Generate a detailed report of the linking process."""
        report = {
            "meeting_date": meeting_date,
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total_ordinances": len(linked_documents["ordinances"]),
                "total_resolutions": len(linked_documents["resolutions"]),
                "ordinances_with_items": len([d for d in linked_documents["ordinances"] if d.get("item_code")]),
                "resolutions_with_items": len([d for d in linked_documents["resolutions"] if d.get("item_code")]),
                "unlinked_ordinances": len([d for d in linked_documents["ordinances"] if not d.get("item_code")]),
                "unlinked_resolutions": len([d for d in linked_documents["resolutions"] if not d.get("item_code")])
            },
            "details": {
                "ordinances": [
                    {
                        "document_number": d["document_number"],
                        "item_code": d.get("item_code", "NOT_FOUND"),
                        "title": d.get("title", "")[:100]
                    }
                    for d in linked_documents["ordinances"]
                ],
                "resolutions": [
                    {
                        "document_number": d["document_number"],
                        "item_code": d.get("item_code", "NOT_FOUND"),
                        "title": d.get("title", "")[:100]
                    }
                    for d in linked_documents["resolutions"]
                ]
            }
        }
        
        # FIXED: Use double quotes for f-string
        report_filename = f"linking_report_{meeting_date.replace('.', '_')}.json"
        report_path = debug_dir / report_filename
        
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        log.info(f"ðŸ“Š Linking report saved to: {report_path}")
    
    # Add backward compatibility method
    async def link_documents_for_meeting_legacy(self, 
                                               meeting_date: str,
                                               documents_dir: Path) -> Dict[str, List[Dict]]:
        """Legacy method for backward compatibility - ordinances only."""
        return await self.link_documents_for_meeting(
            meeting_date,
            documents_dir,  # Ordinances directory
            Path("dummy")   # No resolutions directory
        )


================================================================================


################################################################################
# File: scripts/graph_stages/agenda_pdf_extractor.py
################################################################################

# scripts/graph_stages/agenda_pdf_extractor.py
"""
PDF Extractor for City Clerk Agenda Documents
Extracts text, structure, and hyperlinks from agenda PDFs.
"""

import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import json
import re
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import PdfPipelineOptions
from groq import Groq
import os

log = logging.getLogger(__name__)


class AgendaPDFExtractor:
    """Extract structured content from agenda PDFs using Docling and LLM."""
    
    def __init__(self, output_dir: Optional[Path] = None):
        """Initialize the agenda PDF extractor."""
        self.output_dir = output_dir or Path("city_clerk_documents/extracted_text")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize Docling converter with OCR enabled
        pipeline_options = PdfPipelineOptions()
        pipeline_options.do_ocr = True  # Enable OCR for better text extraction
        pipeline_options.do_table_structure = True  # Better table extraction
        
        self.converter = DocumentConverter(
            format_options={
                InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
            }
        )
        
        # Initialize Groq client for LLM extraction
        self.client = Groq(api_key=os.getenv("GROQ_API_KEY"))
        self.model = "qwen-qwq-32b"
    
    def extract_agenda(self, pdf_path: Path) -> Dict[str, any]:
        """Extract agenda content from PDF using Docling + LLM."""
        log.info(f"ðŸ“„ Extracting agenda from {pdf_path.name}")
        
        # Convert with Docling - pass path directly
        result = self.converter.convert(str(pdf_path))
        
        # Get the document
        doc = result.document
        
        # Get full text and markdown
        full_text = doc.export_to_markdown() or ""
        
        # Use LLM to extract structured agenda items
        log.info("ðŸ§  Using LLM to extract agenda structure...")
        extracted_items = self._extract_agenda_items_with_llm(full_text)
        
        # Build sections from extracted items
        sections = self._build_sections_from_items(extracted_items, full_text)
        
        # Extract hyperlinks if available
        hyperlinks = self._extract_hyperlinks(doc)
        
        # Create agenda data structure with both raw and structured data
        agenda_data = {
            'source_file': pdf_path.name,
            'full_text': full_text,
            'sections': sections,
            'agenda_items': extracted_items,  # Add structured items
            'hyperlinks': hyperlinks,
            'metadata': {
                'extraction_method': 'docling+llm',
                'num_sections': len(sections),
                'num_items': len(extracted_items),
                'num_hyperlinks': len(hyperlinks)
            }
        }
        
        # IMPORTANT: Save the extracted data with the filename expected by ontology extractor
        # The ontology extractor looks for "{pdf_stem}_extracted.json"
        output_file = self.output_dir / f"{pdf_path.stem}_extracted.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(agenda_data, f, indent=2, ensure_ascii=False)
        
        # Also save debug output
        debug_file = self.output_dir / f"{pdf_path.stem}_docling_extracted.json"
        with open(debug_file, 'w', encoding='utf-8') as f:
            json.dump(agenda_data, f, indent=2, ensure_ascii=False)
        
        # Also save just the full text for debugging
        text_file = self.output_dir / f"{pdf_path.stem}_full_text.txt"
        with open(text_file, 'w', encoding='utf-8') as f:
            f.write(full_text)
        
        log.info(f"âœ… Extraction complete: {len(sections)} sections, {len(extracted_items)} items, {len(hyperlinks)} hyperlinks")
        log.info(f"âœ… Saved extracted data to: {output_file}")
        
        return agenda_data
    
    def _extract_agenda_items_with_llm(self, text: str) -> List[Dict[str, any]]:
        """Use LLM to extract agenda items from the text."""
        # Split text into chunks if too long
        max_chars = 30000
        chunks = []
        
        if len(text) > max_chars:
            # Split by lines to avoid breaking mid-sentence
            lines = text.split('\n')
            current_chunk = []
            current_length = 0
            
            for line in lines:
                if current_length + len(line) > max_chars and current_chunk:
                    chunks.append('\n'.join(current_chunk))
                    current_chunk = [line]
                    current_length = len(line)
                else:
                    current_chunk.append(line)
                    current_length += len(line) + 1
            
            if current_chunk:
                chunks.append('\n'.join(current_chunk))
        else:
            chunks = [text]
        
        all_items = []
        
        for i, chunk in enumerate(chunks):
            log.info(f"Processing chunk {i+1}/{len(chunks)}")
            
            prompt = """Extract ALL agenda items from this city council agenda document. Look for items with these EXACT formats:

- E.-1. 23-6723 (ordinances - with periods)
- F.-1. 23-6762 (city commission items - with periods)  
- H.-1. 23-6819 (city manager items - with periods)
- D.-1. 23-6830 (consent agenda items - with periods)

The format is: ## LETTER.-NUMBER. REFERENCE-NUMBER

For EACH item found, extract:
1. item_code: Just the letter-number part (e.g., "E-1", "F-10", "H-3") - REMOVE the periods
2. document_reference: The reference number (e.g., "23-6723")  
3. title: The full title/description that follows
4. item_type: "Ordinance" for E items, "Resolution" for F items, "Other" for everything else

IMPORTANT: Look for ALL items including E.-1., E.-2., E.-3., F.-1., F.-2., etc.

Return ONLY a valid JSON array in this format:
[
  {
    "item_code": "E-1",
    "document_reference": "23-6723", 
    "title": "An Ordinance of the City Commission...",
    "item_type": "Ordinance"
  }
]

Document text:
""" + chunk
            
            try:
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "You are an expert at extracting structured data from city government agenda documents. Return only valid JSON."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.1
                )
                
                response_text = response.choices[0].message.content.strip()
                
                # Clean up response to ensure valid JSON
                if response_text.startswith('```json'):
                    response_text = response_text.replace('```json', '').replace('```', '')
                elif response_text.startswith('```'):
                    response_text = response_text.replace('```', '')
                
                response_text = response_text.strip()
                
                # Try to parse JSON
                try:
                    data = json.loads(response_text)
                    if isinstance(data, dict) and 'items' in data:
                        items = data['items']
                    elif isinstance(data, list):
                        items = data
                    else:
                        log.warning(f"Unexpected LLM response format: {type(data)}")
                        items = []
                        
                    all_items.extend(items)
                    log.info(f"Extracted {len(items)} items from chunk {i+1}")
                    
                except json.JSONDecodeError as e:
                    log.error(f"Failed to parse JSON from chunk {i+1}: {e}")
                    log.error(f"Raw response: {response_text[:200]}...")
                    # Try manual extraction as fallback
                    manual_items = self._manual_extract_items(chunk)
                    all_items.extend(manual_items)
                    log.info(f"Manual fallback extracted {len(manual_items)} items")
                    
            except Exception as e:
                log.error(f"LLM extraction failed for chunk {i+1}: {e}")
                # Fallback to manual extraction
                manual_items = self._manual_extract_items(chunk)
                all_items.extend(manual_items)
                log.info(f"Manual fallback extracted {len(manual_items)} items")
        
        # Deduplicate items by item_code
        seen_codes = set()
        unique_items = []
        for item in all_items:
            if item.get('item_code') and item['item_code'] not in seen_codes:
                seen_codes.add(item['item_code'])
                unique_items.append(item)
        
        log.info(f"Total unique items extracted: {len(unique_items)}")
        return unique_items
    
    def _manual_extract_items(self, text: str) -> List[Dict[str, any]]:
        """Manually extract agenda items using regex patterns."""
        items = []
        
        # Pattern to match agenda items in markdown format: ## E.-1. 23-6723
        # Also handle cases without markdown headers
        patterns = [
            # Markdown header format: ## E.-1. 23-6723
            r'^##\s*([A-Z])\.-(\d+)\.\s+(\d{2}-\d{4,5})\s*$',
            # Direct format: E.-1. 23-6723  
            r'^([A-Z])\.-(\d+)\.\s+(\d{2}-\d{4,5})\s*$',
            # Table format: | E.-1. | 23-6723 |
            r'^\|\s*([A-Z])\.-(\d+)\.\s*\|\s*(\d{2}-\d{4,5})\s*\|'
        ]
        
        lines = text.split('\n')
        
        for i, line in enumerate(lines):
            line = line.strip()
            
            for pattern in patterns:
                match = re.match(pattern, line)
                if match:
                    letter = match.group(1)
                    number = match.group(2)
                    doc_ref = match.group(3)
                    
                    # Get title from subsequent lines
                    title_lines = []
                    for j in range(i + 1, min(i + 5, len(lines))):
                        next_line = lines[j].strip()
                        if next_line and not re.match(r'^##\s*[A-Z]\.-\d+\.', next_line):
                            title_lines.append(next_line)
                        else:
                            break
                    
                    title = ' '.join(title_lines) if title_lines else f"{letter}-{number}"
                    
                    # Clean up title - remove markdown formatting
                    title = re.sub(r'^[-\*\#\|]+\s*', '', title)
                    title = title.replace('|', '').strip()
                    
                    # Determine item type
                    if letter == 'E':
                        item_type = "Ordinance"
                    elif letter == 'F':
                        item_type = "Resolution"
                    else:
                        item_type = "Other"
                    
                    items.append({
                        "item_code": f"{letter}-{number}",
                        "document_reference": doc_ref,
                        "title": title[:500],  # Limit title length
                        "item_type": item_type
                    })
                    break
        
        log.info(f"Manual regex extraction found {len(items)} items")
        return items
    
    def _build_sections_from_items(self, items: List[Dict], full_text: str) -> List[Dict[str, str]]:
        """Build sections structure from extracted items."""
        if not items:
            # If no items found, return the full document as one section
            return [{
                'title': 'Full Document',
                'text': full_text
            }]
        
        # Group items into sections
        sections = []
        
        # Create agenda items section
        agenda_section_text = []
        for item in items:
            item_text = f"{item['item_code']} - {item['document_reference']}\n{item['title']}\n"
            agenda_section_text.append(item_text)
        
        sections.append({
            'title': 'AGENDA ITEMS',
            'text': '\n'.join(agenda_section_text)
        })
        
        return sections
    
    def _extract_hyperlinks(self, doc) -> Dict[str, Dict[str, any]]:
        """Extract hyperlinks from the document."""
        hyperlinks = {}
        
        # Try to extract links from document structure
        if hasattr(doc, 'links'):
            for link in doc.links:
                if hasattr(link, 'text') and hasattr(link, 'url'):
                    hyperlinks[link.text] = {
                        'url': link.url,
                        'page': getattr(link, 'page', 0)
                    }
        
        # Try to extract from markdown if links are preserved there
        if hasattr(doc, 'export_to_markdown'):
            markdown = doc.export_to_markdown()
            # Extract markdown links pattern [text](url)
            link_pattern = r'\[([^\]]+)\]\(([^)]+)\)'
            for match in re.finditer(link_pattern, markdown):
                text, url = match.groups()
                if text and url:
                    hyperlinks[text] = {
                        'url': url,
                        'page': 0  # We don't have page info from markdown
                    }
        
        return hyperlinks


================================================================================


################################################################################
# File: scripts/stages/db_upsert.py
################################################################################

# File: scripts/stages/db_upsert.py

#!/usr/bin/env python3
"""
Stage 6 â€” Optimized database operations with batching and connection pooling.
"""
from __future__ import annotations
import json, logging, os, pathlib, sys
from datetime import datetime
from typing import Any, Dict, List, Sequence, Optional
import asyncio
from concurrent.futures import ThreadPoolExecutor
import threading

from dotenv import load_dotenv
from supabase import create_client
from tqdm import tqdm

load_dotenv()
SUPABASE_URL  = os.getenv("SUPABASE_URL")
SUPABASE_KEY  = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
OPENAI_API_KEY= os.getenv("OPENAI_API_KEY")

META_FIELDS = [
    "document_type", "title", "date", "year", "month", "day",
    "mayor", "vice_mayor", "commissioners",
    "city_attorney", "city_manager", "city_clerk", "public_works_director",
    "agenda", "keywords"
]

log = logging.getLogger(__name__)

# Thread-safe connection pool
class SupabasePool:
    """Thread-safe Supabase client pool."""
    def __init__(self, size: int = 10):
        self.size = size
        self._clients = []
        self._lock = threading.Lock()
        self._initialized = False
    
    def get(self):
        """Get a client from the pool."""
        with self._lock:
            if not self._initialized:
                self._initialize()
            # Round-robin selection
            import random
            return self._clients[random.randint(0, self.size - 1)]
    
    def _initialize(self):
        """Initialize client pool."""
        for _ in range(self.size):
            self._clients.append(init_supabase())
        self._initialized = True

# Global pool
_sb_pool = SupabasePool()

# â”€â”€â”€ Supabase & sanitiser helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def scrub_nuls(obj:Any)->Any:
    if isinstance(obj,str):  return obj.replace("\x00","")
    if isinstance(obj,list): return [scrub_nuls(x) for x in obj]
    if isinstance(obj,dict): return {k:scrub_nuls(v) for k,v in obj.items()}
    return obj

def init_supabase():
    if not (SUPABASE_URL and SUPABASE_KEY):
        sys.exit("â›”  SUPABASE env vars missing")
    return create_client(SUPABASE_URL,SUPABASE_KEY)

def upsert_document(sb,meta:Dict[str,Any])->str:
    meta = scrub_nuls(meta)
    doc_type = meta.get("document_type")
    date = meta.get("date")
    title = meta.get("title")
    if doc_type and date and title:
        existing = (
            sb.table("city_clerk_documents")
            .select("id")
            .eq("document_type", doc_type)
            .eq("date", date)
            .eq("title", title)
            .limit(1)
            .execute()
            .data
        )
        if existing:
            doc_id = existing[0]["id"]
            sb.table("city_clerk_documents").update(meta).eq("id", doc_id).execute()
            return doc_id
    res = sb.table("city_clerk_documents").insert(meta).execute()
    if hasattr(res, "error") and res.error:
        raise RuntimeError(f"Document insert failed: {res.error}")
    return res.data[0]["id"]

# Optimized batch operations
def insert_chunks_optimized(sb, doc_id: str, chunks: Sequence[Dict[str, Any]], 
                          src_json: pathlib.Path, batch_size: int = 1000):
    """Insert chunks with larger batches for better performance."""
    ts = datetime.utcnow().isoformat()
    inserted_ids = []
    
    # Prepare all rows
    rows = [
        {
            "document_id": doc_id,
            "chunk_index": ch["chunk_index"],
            "token_start": ch["token_start"],
            "token_end": ch["token_end"],
            "page_start": ch["page_start"],
            "page_end": ch["page_end"],
            "text": scrub_nuls(ch["text"]),
            "metadata": scrub_nuls(ch.get("metadata", {})),
            "chunking_strategy": "token_window",
            "source_file": str(src_json),
            "created_at": ts,
        }
        for ch in chunks
    ]
    
    # Insert in larger batches
    for i in range(0, len(rows), batch_size):
        batch = rows[i:i + batch_size]
        try:
            res = sb.table("documents_chunks").insert(batch).execute()
            if hasattr(res, "error") and res.error:
                log.error("Batch insert failed: %s", res.error)
                # Fall back to smaller batches
                for j in range(0, len(batch), 100):
                    mini_batch = batch[j:j + 100]
                    mini_res = sb.table("documents_chunks").insert(mini_batch).execute()
                    if hasattr(mini_res, "data"):
                        inserted_ids.extend([r["id"] for r in mini_res.data])
            else:
                inserted_ids.extend([r["id"] for r in res.data])
        except Exception as e:
            log.error(f"Failed to insert batch {i//batch_size + 1}: {e}")
            # Try individual inserts as last resort
            for row in batch:
                try:
                    single_res = sb.table("documents_chunks").insert(row).execute()
                    if hasattr(single_res, "data") and single_res.data:
                        inserted_ids.append(single_res.data[0]["id"])
                except:
                    pass
    
    return inserted_ids

def insert_chunks(sb,doc_id:str,chunks:Sequence[Dict[str,Any]],src_json:pathlib.Path):
    """Original interface using optimized implementation."""
    return insert_chunks_optimized(sb, doc_id, chunks, src_json, batch_size=500)

async def upsert_batch_async(
    documents: List[Dict[str, Any]],
    max_concurrent: int = 5
) -> None:
    """Upsert multiple documents concurrently."""
    semaphore = asyncio.Semaphore(max_concurrent)
    
    async def upsert_one(doc_data):
        async with semaphore:
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(
                None,
                lambda: upsert(
                    doc_data["json_path"],
                    doc_data["chunks"],
                    do_embed=doc_data.get("do_embed", False)
                )
            )
    
    tasks = [upsert_one(doc) for doc in documents]
    
    from tqdm.asyncio import tqdm_asyncio
    await tqdm_asyncio.gather(*tasks, desc="Upserting to database")

# Keep original interface but use optimized implementation
def upsert(json_doc: pathlib.Path, chunks: List[Dict[str, Any]] | None,
           *, do_embed: bool = False) -> None:
    """Original interface with optimized implementation."""
    sb = _sb_pool.get()  # Use connection from pool
    
    data = json.loads(json_doc.read_text())
    row = {k: data.get(k) for k in META_FIELDS} | {
        "source_pdf": data.get("source_pdf", str(json_doc))
    }
    
    # Ensure commissioners is a list
    if "commissioners" in row:
        if isinstance(row["commissioners"], str):
            row["commissioners"] = [row["commissioners"]]
        elif not isinstance(row["commissioners"], list):
            row["commissioners"] = []
    else:
        row["commissioners"] = []
    
    # Ensure keywords is a list
    if "keywords" in row:
        if not isinstance(row["keywords"], list):
            row["keywords"] = []
    else:
        row["keywords"] = []
    
    # Convert agenda from array to text if needed
    if "agenda" in row:
        if isinstance(row["agenda"], list):
            row["agenda"] = "; ".join(str(item) for item in row["agenda"] if item)
        elif row["agenda"] is None:
            row["agenda"] = None
        else:
            row["agenda"] = str(row["agenda"])
    
    # Ensure all text fields are strings or None
    text_fields = ["document_type", "title", "date", "mayor", "vice_mayor", 
                   "city_attorney", "city_manager", "city_clerk", "public_works_director"]
    for field in text_fields:
        if field in row and row[field] is not None:
            row[field] = str(row[field])
    
    # Ensure numeric fields are integers or None
    numeric_fields = ["year", "month", "day"]
    for field in numeric_fields:
        if field in row and row[field] is not None:
            try:
                row[field] = int(row[field])
            except (ValueError, TypeError):
                row[field] = None
    
    doc_id = upsert_document(sb, row)
    
    if not chunks:
        log.info("No chunks to insert for %s", json_doc.stem)
        return
    
    # Use optimized batch insert
    inserted_ids = insert_chunks_optimized(sb, doc_id, chunks, json_doc)
    log.info("â†‘ %s chunks inserted for %s", len(chunks), json_doc.stem)
    
    if do_embed and inserted_ids:
        from stages import embed_vectors
        embed_vectors.main()

if __name__ == "__main__":
    import argparse
    logging.basicConfig(level=logging.INFO, format="%(message)s")
    p=argparse.ArgumentParser()
    p.add_argument("json",type=pathlib.Path)
    p.add_argument("--chunks",type=pathlib.Path)
    p.add_argument("--embed",action="store_true")
    args=p.parse_args()
    
    chunks = None
    if args.chunks and args.chunks.exists():
        chunks = json.loads(args.chunks.read_text())
    
    upsert(args.json, chunks, do_embed=args.embed)


================================================================================


################################################################################
# File: city_clerk_documents/graph_json/debug/resolution_debug_report.json
################################################################################

{
  "timestamp": "2025-06-03T11:09:38.783304",
  "target_resolutions": [
    "2024-04",
    "2024-05",
    "2024-06",
    "2024-07"
  ],
  "meeting_date": "01.09.2024",
  "findings": {
    "filesystem": {
      "total_files": 7,
      "target_resolutions_found": {
        "2024-04": [
          "city_clerk_documents/global/City Comissions 2024/Resolutions/2024/2024-04 - 01_09_2024.pdf"
        ],
        "2024-05": [
          "city_clerk_documents/global/City Comissions 2024/Resolutions/2024/2024-05 - 01_09_2024.pdf"
        ],
        "2024-06": [
          "city_clerk_documents/global/City Comissions 2024/Resolutions/2024/2024-06 - 01_09_2024.pdf"
        ],
        "2024-07": [
          "city_clerk_documents/global/City Comissions 2024/Resolutions/2024/2024-07 - 01_09_2024.pdf"
        ]
      },
      "sample_filenames": [
        "2024-04 - 01_09_2024.pdf",
        "2024-05 - 01_09_2024.pdf",
        "2024-01 - 01_09_2024.pdf",
        "2024-06 - 01_09_2024.pdf",
        "2024-02 - 01_09_2024.pdf",
        "2024-07 - 01_09_2024.pdf",
        "2024-03 - 01_09_2024.pdf"
      ]
    },
    "agenda_extraction": {
      "error": "No ontology file found"
    },
    "document_linking": {
      "total_resolutions_linked": 7,
      "target_linking": {
        "2024-04": {
          "found": true,
          "item_code": "E-9 But wait, the user's examples show that even if there's a period or other characters, we remove them Wait, the user's examples: \"F-3\" becomes F-3 So in this case, the code here is written as E-9 with a closing parenthesis, but the code itself is E-9 So the answer should be E-9 \n\nWait, but the user might have a typo? Let me check again The exact line in the document is \"AgendaItem: E-9)\" The closing parenthesis is part of the line, but the code is E-9 So the code is E-9 Therefore, the answer is",
          "title": "A\nRESOLUTION NO."
        },
        "2024-05": {
          "found": true,
          "item_code": "F-10",
          "title": "A\nRESOLUTION NO."
        },
        "2024-06": {
          "found": true,
          "item_code": "H-1 But wait, the user's examples included F items, but the document here has H-1 The user's instruction says to return whatever is present, so that's correct \n\nAlternatively, maybe I made a mistake Let me check again The user's examples included F-2, F-3, etc, but the actual document here has H-1 The user's instruction says to extract it as found So the answer is H-1 But the user might have intended F, but the document says H-1 So the correct answer is H-1 \n\nWait, but the user's note says \"for resolutions, agenda items typically appear as F items (eg, F-1, F-2, F-3)\" But in this case, the document has H-1 Maybe that's a mistake, but according to the instructions, I should report what's there So the answer is H-1 \n\nAlternatively, maybe I missed another instance Let me check again Scanning through the entire text again, the only mention is \"AgendaIem: H-1)\" with the typo So that's the only occurrence Therefore, the correct code is H-1 \n\nSo the final answer should be",
          "title": "A\nRESOLUTION NO."
        },
        "2024-07": {
          "found": true,
          "item_code": "H-2",
          "title": "A\nRESOLUTION NO."
        }
      },
      "all_linked": [
        {
          "num": "2024-01",
          "code": "D-1"
        },
        {
          "num": "2024-02",
          "code": "D-2"
        },
        {
          "num": "2024-03",
          "code": "F-11"
        },
        {
          "num": "2024-04",
          "code": "E-9 But wait, the user's examples show that even if there's a period or other characters, we remove them Wait, the user's examples: \"F-3\" becomes F-3 So in this case, the code here is written as E-9 with a closing parenthesis, but the code itself is E-9 So the answer should be E-9 \n\nWait, but the user might have a typo? Let me check again The exact line in the document is \"AgendaItem: E-9)\" The closing parenthesis is part of the line, but the code is E-9 So the code is E-9 Therefore, the answer is"
        },
        {
          "num": "2024-05",
          "code": "F-10"
        },
        {
          "num": "2024-06",
          "code": "H-1 But wait, the user's examples included F items, but the document here has H-1 The user's instruction says to return whatever is present, so that's correct \n\nAlternatively, maybe I made a mistake Let me check again The user's examples included F-2, F-3, etc, but the actual document here has H-1 The user's instruction says to extract it as found So the answer is H-1 But the user might have intended F, but the document says H-1 So the correct answer is H-1 \n\nWait, but the user's note says \"for resolutions, agenda items typically appear as F items (eg, F-1, F-2, F-3)\" But in this case, the document has H-1 Maybe that's a mistake, but according to the instructions, I should report what's there So the answer is H-1 \n\nAlternatively, maybe I missed another instance Let me check again Scanning through the entire text again, the only mention is \"AgendaIem: H-1)\" with the typo So that's the only occurrence Therefore, the correct code is H-1 \n\nSo the final answer should be"
        },
        {
          "num": "2024-07",
          "code": "H-2"
        }
      ]
    },
    "graph_state": {
      "target_nodes": {
        "2024-04": {
          "exists": true,
          "properties": {
            "id": "resolution-2024-04",
            "label": "Resolution",
            "nodeType": [
              "Resolution"
            ],
            "document_number": [
              "2024-04"
            ],
            "full_title": [
              "A\nRESOLUTION NO."
            ],
            "title": [
              "A\nRESOLUTION NO."
            ],
            "document_type": [
              "Resolution"
            ],
            "meeting_date": [
              "01-09-2024"
            ],
            "partitionKey": [
              "demo"
            ]
          },
          "edge_count": 1
        },
        "2024-05": {
          "exists": false
        },
        "2024-06": {
          "exists": false
        },
        "2024-07": {
          "exists": false
        }
      },
      "date_format_used": "01-09-2024"
    }
  }
}


================================================================================


################################################################################
# File: scripts/find_duplicates.py
################################################################################

# File: scripts/find_duplicates.py

import os
import hashlib
import argparse
from pathlib import Path
from collections import defaultdict

def calculate_file_hash(filepath, algorithm='sha256', buffer_size=65536):
    """Calculate a hash for a file to identify duplicates."""
    hash_obj = hashlib.new(algorithm)
    
    with open(filepath, 'rb') as f:
        # Read the file in chunks to handle large files efficiently
        buffer = f.read(buffer_size)
        while buffer:
            hash_obj.update(buffer)
            buffer = f.read(buffer_size)
    
    return hash_obj.hexdigest()

def find_duplicates(directory):
    """Find duplicate files in the specified directory."""
    files_by_hash = defaultdict(list)
    duplicate_sets = []
    
    # Get all files in the directory
    target_dir = Path(directory)
    if not target_dir.exists() or not target_dir.is_dir():
        print(f"Error: '{directory}' is not a valid directory")
        return duplicate_sets
    
    print(f"Scanning directory: {directory}")
    
    # Calculate hashes for all files
    all_files = list(target_dir.glob('*'))
    total_files = len(all_files)
    
    for i, file_path in enumerate(all_files):
        if file_path.is_file():
            try:
                file_hash = calculate_file_hash(file_path)
                files_by_hash[file_hash].append(file_path)
                print(f"Processed file {i+1}/{total_files}: {file_path.name}")
            except Exception as e:
                print(f"Error processing {file_path}: {e}")
    
    # Identify duplicate sets (files with the same hash)
    for file_hash, paths in files_by_hash.items():
        if len(paths) > 1:
            duplicate_sets.append(paths)
    
    return duplicate_sets

def delete_duplicates(duplicate_sets, interactive=True):
    """Delete duplicate files, keeping only one copy of each."""
    total_deleted = 0
    total_size_saved = 0
    
    for duplicate_set in duplicate_sets:
        # Sort by name for consistent results
        duplicate_set.sort(key=lambda p: str(p))
        
        # Keep the first file, show options for the rest
        keep_file = duplicate_set[0]
        print(f"\nDuplicate set ({len(duplicate_set)} files):")
        print(f"  Keeping: {keep_file}")
        
        for i, dup_file in enumerate(duplicate_set[1:], 1):
            size = dup_file.stat().st_size
            
            if interactive:
                response = input(f"  Delete duplicate #{i}: {dup_file}? (y/n/a=all/q=quit): ").lower()
                
                if response == 'q':
                    print("Operation aborted.")
                    return total_deleted, total_size_saved
                    
                if response == 'a':
                    interactive = False
                    response = 'y'
            else:
                response = 'y'
                print(f"  Deleting duplicate #{i}: {dup_file}")
            
            if response == 'y':
                try:
                    dup_file.unlink()
                    total_deleted += 1
                    total_size_saved += size
                    print(f"  Deleted: {dup_file}")
                except Exception as e:
                    print(f"  Error deleting {dup_file}: {e}")
    
    return total_deleted, total_size_saved

def format_size(size_bytes):
    """Format file size in human-readable format."""
    if size_bytes < 1024:
        return f"{size_bytes} bytes"
    elif size_bytes < 1024 * 1024:
        return f"{size_bytes/1024:.2f} KB"
    elif size_bytes < 1024 * 1024 * 1024:
        return f"{size_bytes/(1024*1024):.2f} MB"
    else:
        return f"{size_bytes/(1024*1024*1024):.2f} GB"

def main():
    parser = argparse.ArgumentParser(description="Find and remove duplicate files")
    parser.add_argument('--directory', '-d', default='city_clerk_documents/global',
                        help="Directory to scan for duplicates (default: city_clerk_documents/global)")
    parser.add_argument('--delete', '-r', action='store_true',
                        help="Delete duplicate files")
    parser.add_argument('--auto', '-a', action='store_true',
                        help="Automatically delete all duplicates without prompting")
    
    args = parser.parse_args()
    
    # Find duplicates
    duplicate_sets = find_duplicates(args.directory)
    
    # Print summary of duplicates found
    if not duplicate_sets:
        print("\nNo duplicate files found.")
        return
    
    total_duplicates = sum(len(dups) - 1 for dups in duplicate_sets)
    print(f"\nFound {len(duplicate_sets)} sets of duplicate files ({total_duplicates} redundant files)")
    
    # Display details about each duplicate set
    for i, dups in enumerate(duplicate_sets, 1):
        size = dups[0].stat().st_size
        size_str = format_size(size)
        print(f"\nDuplicate Set #{i} - {len(dups)} files, {size_str} each:")
        for path in dups:
            print(f"  {path}")
    
    # Delete duplicates if requested
    if args.delete or args.auto:
        deleted, size_saved = delete_duplicates(duplicate_sets, not args.auto)
        print(f"\nSummary: Deleted {deleted} duplicate files, saving {format_size(size_saved)}")
    else:
        print("\nTo delete duplicates, run again with --delete or --auto flag")

if __name__ == "__main__":
    main()


================================================================================


################################################################################
# File: city_clerk_documents/graph_json/debug/enhanced_linked_documents.json
################################################################################

{
  "ordinances": [],
  "resolutions": [
    {
      "path": "city_clerk_documents/global/City Comissions 2024/Resolutions/2024/2024-01 - 01_09_2024.pdf",
      "filename": "2024-01 - 01_09_2024.pdf",
      "document_number": "2024-01",
      "item_code": "D-1",
      "document_type": "Resolution",
      "title": "A\nRESOLUTION NO.",
      "parsed_data": {
        "document_type": "resolution"
      }
    },
    {
      "path": "city_clerk_documents/global/City Comissions 2024/Resolutions/2024/2024-02 - 01_09_2024.pdf",
      "filename": "2024-02 - 01_09_2024.pdf",
      "document_number": "2024-02",
      "item_code": "D-2",
      "document_type": "Resolution",
      "title": "A\nRESOLUTION NO.",
      "parsed_data": {
        "document_type": "resolution"
      }
    },
    {
      "path": "city_clerk_documents/global/City Comissions 2024/Resolutions/2024/2024-03 - 01_09_2024.pdf",
      "filename": "2024-03 - 01_09_2024.pdf",
      "document_number": "2024-03",
      "item_code": "F-11",
      "document_type": "Resolution",
      "title": "A\nRESOLUTION NO.",
      "parsed_data": {
        "document_type": "resolution"
      }
    },
    {
      "path": "city_clerk_documents/global/City Comissions 2024/Resolutions/2024/2024-04 - 01_09_2024.pdf",
      "filename": "2024-04 - 01_09_2024.pdf",
      "document_number": "2024-04",
      "item_code": "E-9 But wait, the user's examples show that even if there's a period or other characters, we remove them Wait, the user's examples: \"F-3\" becomes F-3 So in this case, the code here is written as E-9 with a closing parenthesis, but the code itself is E-9 So the answer should be E-9 \n\nWait, but the user might have a typo? Let me check again The exact line in the document is \"AgendaItem: E-9)\" The closing parenthesis is part of the line, but the code is E-9 So the code is E-9 Therefore, the answer is",
      "document_type": "Resolution",
      "title": "A\nRESOLUTION NO.",
      "parsed_data": {
        "document_type": "resolution"
      }
    },
    {
      "path": "city_clerk_documents/global/City Comissions 2024/Resolutions/2024/2024-05 - 01_09_2024.pdf",
      "filename": "2024-05 - 01_09_2024.pdf",
      "document_number": "2024-05",
      "item_code": "F-10",
      "document_type": "Resolution",
      "title": "A\nRESOLUTION NO.",
      "parsed_data": {
        "document_type": "resolution"
      }
    },
    {
      "path": "city_clerk_documents/global/City Comissions 2024/Resolutions/2024/2024-06 - 01_09_2024.pdf",
      "filename": "2024-06 - 01_09_2024.pdf",
      "document_number": "2024-06",
      "item_code": "H-1 But wait, the user's examples included F items, but the document here has H-1 The user's instruction says to return whatever is present, so that's correct \n\nAlternatively, maybe I made a mistake Let me check again The user's examples included F-2, F-3, etc, but the actual document here has H-1 The user's instruction says to extract it as found So the answer is H-1 But the user might have intended F, but the document says H-1 So the correct answer is H-1 \n\nWait, but the user's note says \"for resolutions, agenda items typically appear as F items (eg, F-1, F-2, F-3)\" But in this case, the document has H-1 Maybe that's a mistake, but according to the instructions, I should report what's there So the answer is H-1 \n\nAlternatively, maybe I missed another instance Let me check again Scanning through the entire text again, the only mention is \"AgendaIem: H-1)\" with the typo So that's the only occurrence Therefore, the correct code is H-1 \n\nSo the final answer should be",
      "document_type": "Resolution",
      "title": "A\nRESOLUTION NO.",
      "parsed_data": {
        "document_type": "resolution"
      }
    },
    {
      "path": "city_clerk_documents/global/City Comissions 2024/Resolutions/2024/2024-07 - 01_09_2024.pdf",
      "filename": "2024-07 - 01_09_2024.pdf",
      "document_number": "2024-07",
      "item_code": "H-2",
      "document_type": "Resolution",
      "title": "A\nRESOLUTION NO.",
      "parsed_data": {
        "document_type": "resolution"
      }
    }
  ]
}


================================================================================


################################################################################
# File: scripts/check_pipeline_setup.py
################################################################################

# File: scripts/check_pipeline_setup.py

"""
Check City Clerk Pipeline Setup
"""
from pathlib import Path
import os

def check_setup():
    """Check if the pipeline environment is properly set up."""
    print("ðŸ” Checking City Clerk Pipeline Setup\n")
    
    # Check current directory
    cwd = Path.cwd()
    print(f"ðŸ“‚ Current directory: {cwd}")
    print(f"ðŸ“‚ Script location: {Path(__file__).parent}")
    
    # Check for agenda directory
    print("\nðŸ“ Checking for agenda files:")
    possible_dirs = [
        Path("city_clerk_documents/global"),
        Path("../city_clerk_documents/global"),
        Path("../../city_clerk_documents/global"),
        cwd / "city_clerk_documents" / "global"
    ]
    
    found_dir = None
    for dir_path in possible_dirs:
        abs_path = dir_path.absolute()
        exists = dir_path.exists()
        print(f"   {dir_path} -> {abs_path}")
        print(f"   Exists: {exists}")
        
        if exists:
            files = list(dir_path.glob("*.pdf"))
            agenda_files = list(dir_path.glob("*genda*.pdf"))
            print(f"   Total PDFs: {len(files)}")
            print(f"   Agenda PDFs: {len(agenda_files)}")
            
            if agenda_files:
                print(f"   Found agenda files:")
                for f in agenda_files[:5]:
                    print(f"      - {f.name}")
                found_dir = dir_path
                break
        print()
    
    if found_dir:
        print(f"âœ… Found agenda directory: {found_dir}")
        print(f"\nðŸ’¡ Run the pipeline with:")
        print(f"   python scripts/graph_pipeline.py --agenda-dir '{found_dir}'")
    else:
        print("âŒ Could not find agenda directory!")
        print("\nðŸ’¡ Please ensure:")
        print("   1. You have the city_clerk_documents/global directory")
        print("   2. It contains PDF files with 'Agenda' in the name")
        print("   3. You're running from the correct directory")
    
    # Check environment variables
    print("\nðŸ”‘ Checking environment variables:")
    env_vars = ['COSMOS_ENDPOINT', 'COSMOS_KEY', 'GROQ_API_KEY']
    all_set = True
    for var in env_vars:
        value = os.getenv(var)
        if value:
            print(f"   âœ… {var}: {'*' * 10} (set)")
        else:
            print(f"   âŒ {var}: NOT SET")
            all_set = False
    
    if not all_set:
        print("\nðŸ’¡ Create a .env file with the missing variables")
    
    # Check Python imports
    print("\nðŸ“¦ Checking Python imports:")
    try:
        import gremlin_python
        print("   âœ… gremlin_python")
    except ImportError:
        print("   âŒ gremlin_python - run: pip install gremlinpython")
    
    try:
        import groq
        print("   âœ… groq")
    except ImportError:
        print("   âŒ groq - run: pip install groq")
    
    try:
        import fitz
        print("   âœ… PyMuPDF (fitz)")
    except ImportError:
        print("   âŒ PyMuPDF - run: pip install PyMuPDF")
    
    try:
        import unstructured
        print("   âœ… unstructured")
    except ImportError:
        print("   âŒ unstructured - run: pip install unstructured")

if __name__ == "__main__":
    check_setup()


================================================================================


################################################################################
# File: config.py
################################################################################

# File: config.py

#!/usr/bin/env python3
"""
Configuration file for graph database visualization
Manages database credentials and connection settings
"""

import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Azure Cosmos DB Gremlin Configuration
COSMOS_ENDPOINT = os.getenv("COSMOS_ENDPOINT", "wss://aida-graph-db.gremlin.cosmos.azure.com:443")
COSMOS_KEY = os.getenv("COSMOS_KEY", "")  # This will be set from .env file
DATABASE = os.getenv("COSMOS_DATABASE", "cgGraph")
CONTAINER = os.getenv("COSMOS_CONTAINER", "cityClerk")
PARTITION_KEY = os.getenv("COSMOS_PARTITION_KEY", "partitionKey")
PARTITION_VALUE = os.getenv("COSMOS_PARTITION_VALUE", "demo")

def validate_config():
    """Validate that all required configuration is available"""
    required_vars = {
        "COSMOS_KEY": COSMOS_KEY,
        "COSMOS_ENDPOINT": COSMOS_ENDPOINT,
        "DATABASE": DATABASE,
        "CONTAINER": CONTAINER
    }
    
    missing_vars = [var for var, value in required_vars.items() if not value]
    
    if missing_vars:
        print("âŒ Missing required configuration:")
        for var in missing_vars:
            print(f"   - {var}")
        print("\nðŸ”§ Please create a .env file with your credentials:")
        print("   COSMOS_KEY=your_actual_cosmos_key_here")
        print("   COSMOS_ENDPOINT=wss://aida-graph-db.gremlin.cosmos.azure.com:443")
        print("   COSMOS_DATABASE=cgGraph")
        print("   COSMOS_CONTAINER=cityClerk")
        return False
    
    return True

if __name__ == "__main__":
    print("ðŸ”§ Configuration Check:")
    if validate_config():
        print("âœ… All configuration variables are set!")
    else:
        print("âŒ Configuration incomplete!")


================================================================================


################################################################################
# File: scripts/debug_pipeline_output.py
################################################################################

# File: scripts/debug_pipeline_output.py

"""
Debug Pipeline Output Viewer
Shows the contents of all debug files generated by the pipeline.
"""
from pathlib import Path
import json

def view_debug_output():
    """Display all debug output files."""
    debug_dir = Path("city_clerk_documents/graph_json/debug")
    
    if not debug_dir.exists():
        print("âŒ No debug directory found. Run the pipeline first.")
        return
    
    print("ðŸ” City Clerk Pipeline Debug Output")
    print("=" * 60)
    
    # List all files in debug directory
    debug_files = sorted(debug_dir.glob("*"))
    
    for file_path in debug_files:
        print(f"\nðŸ“„ {file_path.name}")
        print("-" * 40)
        
        if file_path.suffix == '.json':
            try:
                with open(file_path, 'r') as f:
                    data = json.load(f)
                print(json.dumps(data, indent=2)[:1000])
                if len(json.dumps(data)) > 1000:
                    print("... (truncated)")
            except:
                print("Error reading JSON file")
        else:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                print(content[:1000])
                if len(content) > 1000:
                    print("... (truncated)")
            except:
                print("Error reading file")
    
    print("\n" + "=" * 60)
    print("âœ… Debug output review complete")

if __name__ == "__main__":
    view_debug_output()


================================================================================


################################################################################
# File: requirements.txt
################################################################################

################################################################################
################################################################################
# Python dependencies for Misophonia Research RAG System

# Core dependencies
openai>=1.82.0
groq>=0.4.1
gremlinpython>=3.7.3
aiofiles>=24.1.0
python-dotenv>=1.0.0

# Graph Visualization dependencies
dash>=2.14.0
plotly>=5.17.0
networkx>=3.2.0
dash-table>=5.0.0
dash-cytoscape>=0.3.0

# Database and API dependencies
supabase>=2.0.0

# Optional for async progress bars
tqdm

# Data processing
pandas>=2.0.0
numpy

# PDF processing
PyPDF2>=3.0.0
unstructured[pdf]==0.10.30
pytesseract>=0.3.10
pdf2image>=1.16.3
docling
pdfminer.six>=20221105

# PDF processing with hyperlink support
PyMuPDF>=1.23.0

# Utilities
colorama>=0.4.6

# For concurrent processing
concurrent-log-handler>=0.9.20

# Async dependencies for optimized pipeline
aiohttp>=3.8.0

# Token counting for OpenAI rate limiting
tiktoken>=0.5.0


================================================================================

