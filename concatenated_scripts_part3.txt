# Concatenated Project Code - Part 3 of 3
# Generated: 2025-06-03 15:41:20
# Root Directory: /Users/gianmariatroiani/Documents/knologi̊/graph_database
================================================================================

# File Index - Which Files Are in Which Parts
################################################################################

## Part 1 (9 files):
  - scripts/graph_stages/agenda_graph_builder.py
  - scripts/stages/extract_clean.py
  - scripts/graph_stages/verbatim_transcript_linker.py
  - scripts/graph_stages/document_linker.py
  - debug/extracted_items.json
  - scripts/graph_stages/pdf_extractor.py
  - scripts/stages/acceleration_utils.py
  - graph_clear_database.py
  - debug/meeting_info_parsed.json

## Part 2 (11 files):
  - scripts/graph_stages/ontology_extractor.py
  - scripts/graph_pipeline.py
  - scripts/graph_stages/enhanced_document_linker.py
  - scripts/graph_stages/agenda_pdf_extractor.py
  - scripts/graph_stages/cosmos_db_client.py
  - scripts/stages/db_upsert.py
  - scripts/stages/llm_enrich.py
  - config.py
  - city_clerk_documents/graph_json/debug/linking_report_01_09_2024.json
  - scripts/graph_stages/__init__.py
  - scripts/stages/__init__.py

## Part 3 (9 files):
  - scripts/stages/embed_vectors.py
  - scripts/graph_stages/agenda_ontology_extractor.py
  - scripts/stages/chunk_text.py
  - scripts/rag_local_web_app.py
  - scripts/pipeline_modular_optimized.py
  - city_clerk_documents/graph_json/debug/verbatim/verbatim_linking_report_01_09_2024.json
  - scripts/supabase_clear_database.py
  - city_clerk_documents/graph_json/debug/enhanced_linked_documents.json
  - requirements.txt


================================================================================


################################################################################
# File: scripts/stages/embed_vectors.py
################################################################################

# File: scripts/stages/embed_vectors.py

#!/usr/bin/env python3
"""
Stage 7 — Optimized embedding with rate limiting, deduplication, and conservative batching.
"""
from __future__ import annotations
import argparse, json, logging, os, sys, time
from datetime import datetime
from typing import Any, Dict, List, Optional, Set
import asyncio
import aiohttp
from dotenv import load_dotenv
from openai import OpenAI
from supabase import create_client
from tqdm import tqdm
from tqdm.asyncio import tqdm as async_tqdm
import hashlib

# Try to import tiktoken for accurate token counting
try:
    import tiktoken
    TIKTOKEN_AVAILABLE = True
except ImportError:
    TIKTOKEN_AVAILABLE = False
    logging.warning("tiktoken not available - using conservative token estimation")

load_dotenv()
SUPABASE_URL  = os.getenv("SUPABASE_URL")
SUPABASE_KEY  = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
OPENAI_API_KEY= os.getenv("OPENAI_API_KEY")
EMBEDDING_MODEL="text-embedding-ada-002"
MODEL_TOKEN_LIMIT=8192
TOKEN_GUARD=200

# 🎯 DYNAMIC BATCHING - Token limits with safety margins
MAX_BATCH_TOKENS = 7692  # Conservative limit (8192 - 500 safety margin)
MAX_CHUNK_TOKENS = 6000  # Individual chunk limit
MIN_BATCH_TOKENS = 100   # Minimum viable batch size

MAX_TOTAL_TOKENS=MAX_BATCH_TOKENS  # Use dynamic batch limit instead

# Conservative defaults for rate limiting
DEFAULT_BATCH_ROWS=200  # Reduced from 5000
DEFAULT_COMMIT_ROWS=10  # Reduced from 100
DEFAULT_MAX_CONCURRENT=3  # Reduced from 50
MAX_RETRIES=5
RETRY_DELAY=2
RATE_LIMIT_DELAY=0.3  # 300ms between API calls
MAX_CALLS_PER_MINUTE=150  # Conservative limit

openai_client=OpenAI(api_key=OPENAI_API_KEY)
logging.basicConfig(level=logging.INFO,
    format="%(asctime)s — %(levelname)s — %(message)s")
log=logging.getLogger(__name__)

# Global variables for rate limiting
call_timestamps = []
total_tokens_used = 0

class AsyncEmbedder:
    """Async embedding client with strict rate limiting and connection pooling."""
    
    def __init__(self, api_key: str, max_concurrent: int = DEFAULT_MAX_CONCURRENT):
        self.api_key = api_key
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.session = None
        self.call_count = 0
        self.start_time = time.time()
        
        # Initialize tiktoken encoder if available
        if TIKTOKEN_AVAILABLE:
            self.encoder = tiktoken.encoding_for_model(EMBEDDING_MODEL)
        else:
            self.encoder = None
    
    async def __aenter__(self):
        timeout = aiohttp.ClientTimeout(total=300)
        connector = aiohttp.TCPConnector(limit=20, limit_per_host=10)  # Reduced limits
        self.session = aiohttp.ClientSession(
            headers={"Authorization": f"Bearer {self.api_key}"},
            timeout=timeout,
            connector=connector
        )
        return self
    
    async def __aexit__(self, *args):
        await self.session.close()
    
    def count_tokens(self, text: str) -> int:
        """Count tokens accurately using tiktoken if available."""
        if self.encoder:
            return len(self.encoder.encode(text))
        else:
            # Conservative fallback
            return int(len(text.split()) * 0.75) + 50
    
    async def rate_limit_check(self):
        """Enforce rate limiting."""
        current_time = time.time()
        
        # Remove timestamps older than 1 minute
        call_timestamps[:] = [ts for ts in call_timestamps if current_time - ts < 60]
        
        # Check if we're approaching the rate limit
        if len(call_timestamps) >= MAX_CALLS_PER_MINUTE - 5:
            sleep_time = 60 - (current_time - call_timestamps[0])
            if sleep_time > 0:
                log.info(f"Rate limit protection: sleeping {sleep_time:.1f}s")
                await asyncio.sleep(sleep_time)
        
        # Always enforce minimum delay between calls
        await asyncio.sleep(RATE_LIMIT_DELAY)
        
        # Record this call
        call_timestamps.append(current_time)
    
    async def embed_batch_async(self, texts: List[str]) -> List[List[float]]:
        """Embed a batch of texts asynchronously with rate limiting and token validation."""
        global total_tokens_used
        
        async with self.semaphore:
            await self.rate_limit_check()
            
            # 🛡️ FINAL TOKEN VALIDATION - Last safety check before API call
            batch_tokens = sum(self.count_tokens(text) for text in texts)
            
            if batch_tokens > MAX_BATCH_TOKENS:
                error_msg = f"🚨 CRITICAL: Batch tokens {batch_tokens} exceed limit {MAX_BATCH_TOKENS}"
                log.error(error_msg)
                raise RuntimeError(error_msg)
            
            total_tokens_used += batch_tokens
            
            log.info(f"🎯 API call: {len(texts)} texts (~{batch_tokens} tokens) - WITHIN LIMITS ✅")
            log.info(f"📊 Total tokens used so far: ~{total_tokens_used}")
            
            for attempt in range(MAX_RETRIES):
                try:
                    async with self.session.post(
                        "https://api.openai.com/v1/embeddings",
                        json={
                            "model": EMBEDDING_MODEL,
                            "input": texts
                        }
                    ) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            self.call_count += 1
                            log.info(f"✅ API call successful: {len(texts)} embeddings generated")
                            return [item["embedding"] for item in data["data"]]
                        elif resp.status == 400:  # Bad request - likely token limit
                            error = await resp.text()
                            log.error(f"🚨 API error 400 (likely token limit): {error}")
                            log.error(f"🚨 Batch details: {len(texts)} texts, {batch_tokens} tokens")
                            raise RuntimeError(f"Token limit API error: {error}")
                        elif resp.status == 429:  # Rate limit error
                            error = await resp.text()
                            log.warning(f"Rate limit hit: {error}")
                            # Exponential backoff for rate limits
                            sleep_time = (2 ** attempt) * 2
                            log.info(f"Exponential backoff: sleeping {sleep_time}s")
                            await asyncio.sleep(sleep_time)
                        else:
                            error = await resp.text()
                            log.warning(f"API error {resp.status}: {error}")
                            await asyncio.sleep(RETRY_DELAY)
                except Exception as e:
                    log.warning(f"Attempt {attempt + 1} failed: {e}")
                    await asyncio.sleep(RETRY_DELAY * (attempt + 1))
            
            raise RuntimeError("Failed to embed batch after retries")

def deduplicate_chunks(chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Remove duplicate text chunks based on content hash."""
    seen_hashes: Set[str] = set()
    unique_chunks = []
    duplicates_removed = 0
    
    for chunk in chunks:
        text = chunk.get("text", "").strip()
        if not text:
            continue
            
        # Create hash of the text content
        text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()
        
        if text_hash not in seen_hashes:
            seen_hashes.add(text_hash)
            unique_chunks.append(chunk)
        else:
            duplicates_removed += 1
    
    if duplicates_removed > 0:
        log.info(f"Deduplication: removed {duplicates_removed} duplicate chunks")
    
    return unique_chunks

async def process_chunks_async(
    sb,
    chunks: List[Dict[str, Any]],
    embedder: AsyncEmbedder,
    commit_size: int = DEFAULT_COMMIT_ROWS
) -> int:
    """Process chunks with async embedding and batch updates."""
    # Deduplicate chunks first
    unique_chunks = deduplicate_chunks(chunks)
    log.info(f"📊 CHUNK PROCESSING PROGRESS:")
    log.info(f"   📄 Original chunks: {len(chunks)}")
    log.info(f"   📄 Unique chunks: {len(unique_chunks)}")
    if len(chunks) != len(unique_chunks):
        log.info(f"   🔄 Duplicates removed: {len(chunks) - len(unique_chunks)}")
    
    # Create token-safe slices
    slices = safe_slices(unique_chunks, commit_size)
    total_embedded = 0
    total_slices = len(slices)
    
    if total_slices == 0:
        log.warning(f"📊 NO SLICES TO PROCESS - All chunks were filtered out")
        return 0
    
    log.info(f"📊 EMBEDDING SLICES PROGRESS:")
    log.info(f"   🎯 Total slices to process: {total_slices}")
    log.info(f"   📄 Total chunks to embed: {sum(len(slice_data) for slice_data in slices)}")
    
    # Process each slice
    for i, slice_data in enumerate(slices):
        slice_num = i + 1
        slice_progress = (slice_num / total_slices * 100)
        
        log.info(f"📊 SLICE {slice_num}/{total_slices} ({slice_progress:.1f}%):")
        log.info(f"   📄 Processing {len(slice_data)} chunks in this slice")
        
        texts = [row["text"] for row in slice_data]
        
        try:
            # Get embeddings asynchronously
            log.info(f"   🔄 Calling OpenAI API for {len(texts)} embeddings...")
            embeddings = await embedder.embed_batch_async(texts)
            log.info(f"   ✅ Received {len(embeddings)} embeddings from API")
            
            # Update database in batch
            log.info(f"   💾 Updating database for {len(slice_data)} chunks...")
            update_tasks = []
            for row, emb in zip(slice_data, embeddings):
                # ✅ GUARANTEED SKIP LOGIC - Layer 3: Final update verification
                def update_with_verification(r, e):
                    # Final check before updating
                    final_check = sb.table("documents_chunks").select("embedding")\
                        .eq("id", r["id"]).execute()
                    
                    if final_check.data and final_check.data[0].get("embedding"):
                        log.info(f"🛡️ Chunk {r['id']} already has embedding - SKIPPING update")
                        return {"skipped": True}
                    
                    # Safe to update
                    return sb.table("documents_chunks")\
                        .update({"embedding": e})\
                        .eq("id", r["id"])\
                        .execute()
                
                # Use thread pool for database updates
                loop = asyncio.get_event_loop()
                task = loop.run_in_executor(None, update_with_verification, row, emb)
                update_tasks.append(task)
            
            # Wait for all updates
            log.info(f"   ⏳ Waiting for {len(update_tasks)} database updates...")
            results = await asyncio.gather(*update_tasks, return_exceptions=True)
            successful = sum(1 for r in results if not isinstance(r, Exception) and not (isinstance(r, dict) and r.get("skipped")))
            skipped = sum(1 for r in results if isinstance(r, dict) and r.get("skipped"))
            failed = sum(1 for r in results if isinstance(r, Exception))
            total_embedded += successful
            
            # 📊 SLICE COMPLETION PROGRESS
            log.info(f"📊 SLICE {slice_num} COMPLETE:")
            log.info(f"   ✅ Successful updates: {successful}")
            if skipped > 0:
                log.info(f"   ⏭️  Skipped (already embedded): {skipped}")
            if failed > 0:
                log.info(f"   ❌ Failed updates: {failed}")
            log.info(f"   📈 Total embedded so far: {total_embedded}")
            
        except Exception as e:
            log.error(f"❌ SLICE {slice_num} FAILED: {e}")
            log.error(f"   📄 Chunks in failed slice: {len(slice_data)}")
    
    # Final summary
    log.info(f"📊 CHUNK PROCESSING COMPLETE:")
    log.info(f"   ✅ Total chunks embedded: {total_embedded}")
    log.info(f"   📊 Slices processed: {total_slices}")
    log.info(f"   📈 Success rate: {(total_embedded/len(unique_chunks)*100):.1f}%" if unique_chunks else "0%")
    
    return total_embedded

async def main_async(batch_size: int = None, commit_size: int = None, max_concurrent: int = None):
    """Async main function for embedding."""
    global EMBEDDING_MODEL, MODEL_TOKEN_LIMIT, MAX_TOTAL_TOKENS, total_tokens_used
    
    # Use provided parameters or conservative defaults
    batch_size = batch_size or DEFAULT_BATCH_ROWS
    commit_size = commit_size or DEFAULT_COMMIT_ROWS
    max_concurrent = max_concurrent or DEFAULT_MAX_CONCURRENT
    
    log.info(f"Starting with conservative settings:")
    log.info(f"  Batch size: {batch_size}")
    log.info(f"  Commit size: {commit_size}")
    log.info(f"  Max concurrent: {max_concurrent}")
    log.info(f"  Rate limit delay: {RATE_LIMIT_DELAY}s")
    
    # 🎯 Dynamic batching status
    log.info(f"🎯 DYNAMIC BATCHING ENABLED:")
    log.info(f"   Max batch tokens: {MAX_BATCH_TOKENS} (with safety margin)")
    log.info(f"   Max chunk tokens: {MAX_CHUNK_TOKENS}")
    log.info(f"   Min batch tokens: {MIN_BATCH_TOKENS}")
    log.info(f"   ✅ GUARANTEED: No token limit API errors")
    
    # ✅ Check tiktoken availability for accurate token counting
    if not TIKTOKEN_AVAILABLE:
        log.warning("⚠️  tiktoken not available - using conservative token estimation")
        log.warning("⚠️  For accurate token counting, install: pip install tiktoken")
    else:
        log.info("✅ tiktoken available - using accurate token counting")
    
    if not OPENAI_API_KEY:
        log.error("OPENAI_API_KEY missing")
        sys.exit(1)
    
    sb = init_supabase()
    existing_count = count_processed_chunks(sb)
    log.info(f"Rows already embedded: {existing_count}")
    
    # ✅ GUARANTEED SKIP LOGIC - Status check
    total_chunks_res = sb.table("documents_chunks").select("id", count="exact")\
        .eq("chunking_strategy", "token_window").execute()
    total_chunks = total_chunks_res.count or 0
    
    pending_chunks = total_chunks - existing_count
    
    log.info("📊 Current status:")
    log.info(f"   Chunks already embedded: {existing_count}")
    log.info(f"   Chunks needing embedding: {pending_chunks}")
    log.info(f"   Total chunks: {total_chunks}")
    
    if pending_chunks == 0:
        log.info("✅ All chunks already have embeddings - nothing to do!")
        return
    
    async with AsyncEmbedder(OPENAI_API_KEY, max_concurrent) as embedder:
        loop, total = 0, 0
        
        # 📊 PROGRESS TRACKING - Initialize counters
        total_chunks_to_process = pending_chunks
        chunks_processed = 0
        chunks_skipped = 0
        
        log.info(f"📊 EMBEDDING PROGRESS TRACKING INITIALIZED:")
        log.info(f"   🎯 Total chunks to embed: {total_chunks_to_process}")
        log.info(f"   🎯 Starting embedding process...")
        
        while True:
            loop += 1
            rows = fetch_unprocessed_chunks(sb, limit=batch_size, offset=0)
            
            if not rows:
                log.info("✨ Done — no more rows.")
                break
            
            # 📊 PROGRESS: Show current status before processing
            remaining_chunks = total_chunks_to_process - chunks_processed
            progress_percent = (chunks_processed / total_chunks_to_process * 100) if total_chunks_to_process > 0 else 0
            
            log.info(f"📊 EMBEDDING PROGRESS - Loop {loop}:")
            log.info(f"   📄 Fetched: {len(rows)} chunks")
            log.info(f"   ✅ Processed: {chunks_processed}/{total_chunks_to_process} ({progress_percent:.1f}%)")
            log.info(f"   ⏳ Remaining: {remaining_chunks} chunks")
            if chunks_skipped > 0:
                log.info(f"   ⚠️  Skipped: {chunks_skipped} oversized chunks")
            
            # Process chunks asynchronously
            embedded = await process_chunks_async(sb, rows, embedder, commit_size)
            total += embedded
            chunks_processed += embedded
            
            # Update skipped count (this will be calculated in process_chunks_async)
            current_chunk_count = count_processed_chunks(sb)
            actual_embedded_this_loop = current_chunk_count - (existing_count + chunks_processed - embedded)
            
            # 📊 PROGRESS: Show results after processing
            final_progress_percent = (chunks_processed / total_chunks_to_process * 100) if total_chunks_to_process > 0 else 0
            log.info(f"📊 LOOP {loop} COMPLETE:")
            log.info(f"   ✅ This loop: {embedded} chunks embedded")
            log.info(f"   📈 Total progress: {chunks_processed}/{total_chunks_to_process} ({final_progress_percent:.1f}%)")
            log.info(f"   🔄 API calls made: {embedder.call_count}")
            log.info(f"   📊 Total tokens used: ~{total_tokens_used}")
            
            # Check if we're making progress or stuck
            if embedded == 0 and len(rows) > 0:
                chunks_skipped += len(rows)
                log.warning(f"⚠️  WARNING: No chunks embedded in this loop - all {len(rows)} chunks were skipped")
                log.warning(f"⚠️  Total skipped so far: {chunks_skipped} chunks")
                
                # Prevent infinite loop by limiting consecutive failed attempts
                if loop > 5 and embedded == 0:
                    log.error(f"🚨 STUCK: {loop} consecutive loops with no progress - stopping to prevent infinite loop")
                    break
    
    # Save report
    ts = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    report = {
        "timestamp": ts,
        "batch_size": batch_size,
        "commit": commit_size,
        "max_concurrent": max_concurrent,
        "total_embedded": total,
        "total_with_embeddings": count_processed_chunks(sb),
        "total_tokens_used": total_tokens_used,
        "api_calls_made": embedder.call_count if 'embedder' in locals() else 0
    }
    
    # Create reports directory if it doesn't exist
    reports_dir = "reports/embedding"
    os.makedirs(reports_dir, exist_ok=True)
    
    fname = f"{reports_dir}/supabase_embedding_report_{ts}.json"
    with open(fname, "w") as fp:
        json.dump(report, fp, indent=2)
    
    log.info(f"Report saved to {fname}")

async def main_async_cli():
    """CLI version with argument parsing."""
    global EMBEDDING_MODEL, MODEL_TOKEN_LIMIT, MAX_TOTAL_TOKENS
    
    ap = argparse.ArgumentParser()
    ap.add_argument("--batch-size", type=int, default=DEFAULT_BATCH_ROWS)
    ap.add_argument("--commit", type=int, default=DEFAULT_COMMIT_ROWS)
    ap.add_argument("--skip", type=int, default=0)
    ap.add_argument("--model", default=EMBEDDING_MODEL)
    ap.add_argument("--model-limit", type=int, default=MODEL_TOKEN_LIMIT)
    ap.add_argument("--max-concurrent", type=int, default=DEFAULT_MAX_CONCURRENT,
                   help="Maximum concurrent API calls (default: 3 for rate limiting)")
    args = ap.parse_args()
    
    EMBEDDING_MODEL = args.model
    MODEL_TOKEN_LIMIT = args.model_limit
    MAX_TOTAL_TOKENS = MODEL_TOKEN_LIMIT - TOKEN_GUARD
    
    await main_async(args.batch_size, args.commit, args.max_concurrent)

# Keep original interface for compatibility
def main() -> None:
    """Original synchronous interface."""
    asyncio.run(main_async_cli())

# Keep all existing helper functions unchanged...
def init_supabase():
    if not SUPABASE_URL or not SUPABASE_KEY:
        log.error("SUPABASE creds missing"); sys.exit(1)
    return create_client(SUPABASE_URL,SUPABASE_KEY)

def count_processed_chunks(sb)->int:
    res=sb.table("documents_chunks").select("id",count="exact").not_.is_("embedding","null").execute()
    return res.count or 0

def fetch_unprocessed_chunks(sb,*,limit:int,offset:int=0)->List[Dict[str,Any]]:
    first,last=offset,offset+limit-1
    res=sb.table("documents_chunks").select("id,text,token_start,token_end")\
        .eq("chunking_strategy","token_window").is_("embedding","null")\
        .range(first,last).execute()
    
    chunks = res.data or []
    
    if chunks:
        # ✅ GUARANTEED SKIP LOGIC - Layer 2: Double-check verification
        chunk_ids = [chunk["id"] for chunk in chunks]
        verification = sb.table("documents_chunks").select("id")\
            .in_("id", chunk_ids).not_.is_("embedding", "null").execute()
        
        already_embedded_ids = {row["id"] for row in verification.data or []}
        
        if already_embedded_ids:
            log.warning(f"🛡️ Found {len(already_embedded_ids)} chunks that already have embeddings - SKIPPING them")
            chunks = [chunk for chunk in chunks if chunk["id"] not in already_embedded_ids]
        
        log.info(f"✅ GUARANTEED: Fetched {len(chunks)} chunks WITHOUT embeddings")
        
        if not chunks:
            log.info("✅ All chunks already have embeddings - nothing to do!")
    
    return chunks

def generate_embedding_batch(texts:List[str])->List[List[float]]:
    attempt=0
    while attempt<MAX_RETRIES:
        try:
            resp=openai_client.embeddings.create(model=EMBEDDING_MODEL,input=texts)
            return [d.embedding for d in resp.data]
        except Exception as exc:
            attempt+=1; log.warning("Embedding batch %s/%s failed: %s",attempt,MAX_RETRIES,exc)
            time.sleep(RETRY_DELAY)
    raise RuntimeError("OpenAI embedding batch failed after retries")

def chunk_tokens(row:Dict[str,Any])->int:
    try:
        t=int(row["token_end"])-int(row["token_start"])+1
        if 0<t<=16384: return t
    except: pass
    
    # Use tiktoken if available for more accurate counting
    text = row.get("text", "")
    if TIKTOKEN_AVAILABLE:
        try:
            encoder = tiktoken.encoding_for_model(EMBEDDING_MODEL)
            return len(encoder.encode(text))
        except:
            pass
    
    # Conservative fallback
    approx=int(len(text.split())*0.75)+50  # More conservative estimate
    return min(max(1,approx),MODEL_TOKEN_LIMIT)

def safe_slices(rows:List[Dict[str,Any]],max_rows:int)->List[List[Dict[str,Any]]]:
    """🎯 DYNAMIC BATCH SIZING - Guarantees no token limit errors."""
    return dynamic_batch_slices(rows)

def dynamic_batch_slices(rows: List[Dict[str, Any]]) -> List[List[Dict[str, Any]]]:
    """
    🎯 DYNAMIC BATCH SIZING - Creates variable-sized batches that GUARANTEE no API token errors.
    
    Protection Layers:
    1. Individual chunk validation (max 6,000 tokens)
    2. Dynamic batch sizing (max 7,692 tokens total)
    3. Conservative safety margins
    4. Multiple validation checks
    """
    batches = []
    current_batch = []
    current_tokens = 0
    skipped_chunks = 0
    
    # Initialize token encoder for accurate counting
    encoder = None
    if TIKTOKEN_AVAILABLE:
        try:
            encoder = tiktoken.encoding_for_model(EMBEDDING_MODEL)
        except:
            pass
    
    def count_tokens_accurate(text: str) -> int:
        """Count tokens with maximum accuracy."""
        if encoder:
            return len(encoder.encode(text))
        else:
            # Conservative fallback estimation
            return int(len(text.split()) * 0.75) + 50
    
    log.info(f"🎯 Starting dynamic batch creation with {len(rows)} chunks")
    log.info(f"🎯 Limits: {MAX_CHUNK_TOKENS} tokens/chunk, {MAX_BATCH_TOKENS} tokens/batch")
    
    for i, row in enumerate(rows):
        # Clean text and validate
        text = (row.get("text") or "").replace("\x00", "").strip()
        if not text:
            log.warning(f"🎯 Skipping empty chunk {row.get('id', 'unknown')}")
            continue
        
        # ️ PROTECTION LAYER 1: Individual chunk validation
        chunk_tokens = count_tokens_accurate(text)
        
        if chunk_tokens > MAX_CHUNK_TOKENS:
            log.warning(f"🎯 Skipping oversized chunk {row.get('id', 'unknown')}: {chunk_tokens} tokens (max: {MAX_CHUNK_TOKENS})")
            skipped_chunks += 1
            continue
        
        # 🛡️ PROTECTION LAYER 2: Dynamic batch sizing
        would_exceed = current_tokens + chunk_tokens > MAX_BATCH_TOKENS
        
        if would_exceed and current_batch:
            # Finalize current batch
            log.info(f"🎯 Created batch with {len(current_batch)} chunks (~{current_tokens} tokens)")
            batches.append(current_batch)
            current_batch = []
            current_tokens = 0
        
        # Add chunk to current batch
        current_batch.append({"id": row["id"], "text": text})
        current_tokens += chunk_tokens
        
        # 🛡️ PROTECTION LAYER 3: Safety validation
        if current_tokens > MAX_BATCH_TOKENS:
            log.error(f"🚨 CRITICAL: Batch exceeded limit! {current_tokens} > {MAX_BATCH_TOKENS}")
            # Emergency fallback - remove last chunk and finalize batch
            if len(current_batch) > 1:
                current_batch.pop()
                current_tokens -= chunk_tokens
                log.info(f"🎯 Emergency: Created batch with {len(current_batch)} chunks (~{current_tokens} tokens)")
                batches.append(current_batch)
                current_batch = [{"id": row["id"], "text": text}]
                current_tokens = chunk_tokens
            else:
                log.error(f"🚨 Single chunk too large: {chunk_tokens} tokens")
                current_batch = []
                current_tokens = 0
                skipped_chunks += 1
    
    # Add final batch if not empty
    if current_batch and current_tokens >= MIN_BATCH_TOKENS:
        log.info(f"🎯 Created final batch with {len(current_batch)} chunks (~{current_tokens} tokens)")
        batches.append(current_batch)
    elif current_batch:
        log.warning(f"🎯 Skipping tiny final batch: {current_tokens} tokens < {MIN_BATCH_TOKENS} minimum")
    
    # 🛡️ PROTECTION LAYER 4: Final validation
    total_chunks = sum(len(batch) for batch in batches)
    max_batch_tokens = max((sum(count_tokens_accurate(chunk["text"]) for chunk in batch) for batch in batches), default=0)
    
    log.info(f"🎯 DYNAMIC BATCHING COMPLETE:")
    log.info(f"   📊 {len(batches)} batches created")
    log.info(f"   📊 {total_chunks} chunks processed")
    log.info(f"   📊 {skipped_chunks} chunks skipped")
    log.info(f"   📊 Largest batch: {max_batch_tokens} tokens (limit: {MAX_BATCH_TOKENS})")
    log.info(f"   ✅ GUARANTEED: No batch exceeds {MAX_BATCH_TOKENS} tokens")
    
    if max_batch_tokens > MAX_BATCH_TOKENS:
        log.error(f"🚨 CRITICAL ERROR: Batch validation failed!")
        raise RuntimeError(f"Batch token validation failed: {max_batch_tokens} > {MAX_BATCH_TOKENS}")
    
    return batches

def embed_slice(sb,slice_rows:List[Dict[str,Any]])->int:
    embeds=generate_embedding_batch([r["text"] for r in slice_rows])
    ok=0
    for row,emb in zip(slice_rows,embeds):
        attempt=0
        while attempt<MAX_RETRIES:
            res=sb.table("documents_chunks").update({"embedding":emb}).eq("id",row["id"]).execute()
            if getattr(res,"error",None):
                attempt+=1; log.warning("Update %s failed (%s/%s): %s",row["id"],attempt,MAX_RETRIES,res.error)
                time.sleep(RETRY_DELAY)
            else: ok+=1; break
    return ok

if __name__=="__main__": 
    asyncio.run(main_async_cli())


================================================================================


################################################################################
# File: scripts/graph_stages/agenda_ontology_extractor.py
################################################################################

# File: scripts/graph_stages/agenda_ontology_extractor.py

"""
City Clerk Ontology Extractor - FIXED VERSION
Uses OpenAI LLM to extract structured data from city agenda documents.
"""

import logging
import json
import re
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
import os
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

log = logging.getLogger('ontology_extractor')


class CityClerkOntologyExtractor:
    """Extract structured ontology from city clerk documents using LLM."""
    
    def __init__(self, 
                 openai_api_key: Optional[str] = None,
                 model: str = "gpt-4.1-mini-2025-04-14",
                 output_dir: Optional[Path] = None,
                 max_tokens: int = 32768):
        """Initialize the extractor with OpenAI client."""
        self.api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY not found in environment")
        
        self.client = OpenAI(api_key=self.api_key)
        self.model = model
        self.max_tokens = max_tokens
        self.output_dir = output_dir or Path("city_clerk_documents/graph_json")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Create debug directory
        self.debug_dir = self.output_dir / "debug"
        self.debug_dir.mkdir(exist_ok=True)
    
    def extract(self, pdf_path: Path) -> Dict[str, Any]:
        """Extract complete ontology from agenda PDF."""
        log.info(f"🧠 Extracting ontology from {pdf_path.name}")
        
        # First, load the extracted text
        extracted_path = self.output_dir / f"{pdf_path.stem}_extracted.json"
        if extracted_path.exists():
            with open(extracted_path, 'r') as f:
                extracted_data = json.load(f)
        else:
            raise FileNotFoundError(f"No extracted data found for {pdf_path.name}. Run PDF extraction first.")
        
        # Get full text from sections
        full_text = "\n".join(section.get("text", "") for section in extracted_data.get("sections", []))
        
        # Save full text for debugging
        with open(self.debug_dir / f"{pdf_path.stem}_full_text.txt", 'w', encoding='utf-8') as f:
            f.write(full_text)
        
        # Extract meeting date from filename first (more reliable)
        meeting_date = self._extract_meeting_date_from_filename(pdf_path.stem)
        if not meeting_date:
            meeting_date = self._extract_meeting_date(pdf_path.stem, full_text[:1000])
        
        log.info(f"📅 Extracted meeting date: {meeting_date}")
        
        # Step 1: Extract meeting information
        meeting_info = self._extract_meeting_info(full_text[:4000])
        
        # Step 2: Extract complete agenda structure
        agenda_structure = self._extract_agenda_structure(full_text)
        
        # Step 3: Extract entities
        entities = self._extract_entities(full_text[:15000])
        
        # Step 4: Extract relationships between items
        relationships = self._extract_relationships(agenda_structure)
        
        # Build complete ontology
        ontology = {
            "meeting_date": meeting_date,
            "meeting_info": meeting_info,
            "agenda_structure": agenda_structure,
            "entities": entities,
            "relationships": relationships,
            "hyperlinks": extracted_data.get("hyperlinks", {}),
            "metadata": {
                "source_pdf": str(pdf_path.absolute()),
                "extraction_date": datetime.utcnow().isoformat() + "Z",
                "model": self.model
            }
        }
        
        # Save ontology
        output_path = self.output_dir / f"{pdf_path.stem}_ontology.json"
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(ontology, f, indent=2, ensure_ascii=False)
        
        log.info(f"✅ Ontology extraction complete: {len(agenda_structure)} sections, {sum(len(s.get('items', [])) for s in agenda_structure)} items")
        return ontology
    
    def _extract_meeting_date_from_filename(self, filename: str) -> Optional[str]:
        """Extract meeting date from filename like 'Agenda 01.9.2024'."""
        # Try to extract date from filename
        date_patterns = [
            r'(\d{1,2})\.(\d{1,2})\.(\d{4})',  # 01.9.2024
            r'(\d{1,2})-(\d{1,2})-(\d{4})',    # 01-9-2024
            r'(\d{1,2})_(\d{1,2})_(\d{4})',    # 01_9_2024
        ]
        
        for pattern in date_patterns:
            match = re.search(pattern, filename)
            if match:
                month, day, year = match.groups()
                return f"{month.zfill(2)}.{day.zfill(2)}.{year}"
        
        return None
    
    def _extract_meeting_date(self, filename: str, text: str) -> str:
        """Extract meeting date in MM.DD.YYYY format."""
        # Try filename first
        date = self._extract_meeting_date_from_filename(filename)
        if date:
            return date
        
        # Try MM/DD/YYYY format in text
        date_match = re.search(r'(\d{1,2})/(\d{1,2})/(\d{4})', text)
        if date_match:
            month, day, year = date_match.groups()
            return f"{month.zfill(2)}.{day.zfill(2)}.{year}"
        
        # Default fallback
        return "01.09.2024"  # Based on the actual filename
    
    def _clean_json_response(self, json_text: str) -> str:
        """Clean and fix common JSON formatting issues from LLM responses."""
        # Remove markdown code blocks
        json_text = re.sub(r'```json\s*', '', json_text)
        json_text = re.sub(r'\s*```', '', json_text)
        
        # Remove any text before the first { or [
        json_start = json_text.find('{')
        array_start = json_text.find('[')
        
        if json_start == -1 and array_start == -1:
            return json_text
        
        if json_start == -1:
            start_pos = array_start
        elif array_start == -1:
            start_pos = json_start
        else:
            start_pos = min(json_start, array_start)
        
        json_text = json_text[start_pos:]
        
        # Fix common escape issues
        json_text = json_text.replace('\\n', ' ')
        json_text = json_text.replace('\\"', '"')
        json_text = json_text.replace('\\/', '/')
        
        # Fix truncated strings by closing them
        # Count quotes to detect unclosed strings
        in_string = False
        escape_next = False
        cleaned_chars = []
        
        for i, char in enumerate(json_text):
            if escape_next:
                escape_next = False
                cleaned_chars.append(char)
                continue
                
            if char == '\\':
                escape_next = True
                cleaned_chars.append(char)
                continue
                
            if char == '"':
                in_string = not in_string
                
            cleaned_chars.append(char)
        
        # If we end while still in a string, close it
        if in_string:
            cleaned_chars.append('"')
            # Also close any open braces/brackets
            open_braces = cleaned_chars.count('{') - cleaned_chars.count('}')
            open_brackets = cleaned_chars.count('[') - cleaned_chars.count(']')
            
            if open_braces > 0:
                cleaned_chars.append('}' * open_braces)
            if open_brackets > 0:
                cleaned_chars.append(']' * open_brackets)
        
        return ''.join(cleaned_chars)
    
    def _extract_meeting_info(self, text: str) -> Dict[str, Any]:
        """Extract meeting metadata using LLM."""
        prompt = """Analyze this city commission meeting agenda and extract meeting details.

Text:
{text}

IMPORTANT: Return ONLY the JSON object below. Do not include any other text, markdown formatting, or code blocks.

{{
    "meeting_type": "Regular Meeting or Special Meeting or Workshop",
    "meeting_time": "time if mentioned",
    "location": {{
        "name": "venue name",
        "address": "full address"
    }},
    "officials_present": {{
        "mayor": "name or null",
        "vice_mayor": "name or null",
        "commissioners": ["names"],
        "city_attorney": "name or null",
        "city_manager": "name or null",
        "city_clerk": "name or null"
    }}
}}""".format(text=text)
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a JSON extractor. Return only valid JSON, no markdown or other formatting."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_tokens=self.max_tokens  # Use the configurable value
            )
            
            # Save LLM response for debugging
            raw_response = response.choices[0].message.content.strip()
            with open(self.debug_dir / "meeting_info_llm_response.txt", 'w', encoding='utf-8') as f:
                f.write(raw_response)
            
            # Clean and parse JSON
            json_text = self._clean_json_response(raw_response)
            result = json.loads(json_text)
            
            # Save parsed result
            with open(self.debug_dir / "meeting_info_parsed.json", 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2)
            
            return result
            
        except json.JSONDecodeError as e:
            log.error(f"JSON decode error in meeting info: {e}")
            log.error(f"Raw response saved to debug/meeting_info_llm_response.txt")
            return self._default_meeting_info()
        except Exception as e:
            log.error(f"Failed to extract meeting info: {e}")
            return self._default_meeting_info()
    
    def _extract_agenda_structure(self, text: str) -> List[Dict[str, Any]]:
        """Extract complete agenda structure with all items."""
        # Split text into smaller chunks to avoid token limits
        max_chunk_size = 30000  # characters
        
        if len(text) > max_chunk_size:
            # Process in chunks
            chunks = [text[i:i+max_chunk_size] for i in range(0, len(text), max_chunk_size-1000)]
            all_sections = []
            
            for i, chunk in enumerate(chunks):
                log.info(f"Processing chunk {i+1}/{len(chunks)} for agenda structure")
                sections = self._extract_agenda_structure_chunk(chunk, i)
                all_sections.extend(sections)
            
            return all_sections
        else:
            return self._extract_agenda_structure_chunk(text, 0)
    
    def _extract_agenda_structure_chunk(self, text: str, chunk_num: int) -> List[Dict[str, Any]]:
        """Extract agenda structure from a text chunk."""
        prompt = """Extract the complete agenda structure from this city commission agenda.

Extract ALL sections and their items, including:
- PRESENTATIONS AND PROTOCOL DOCUMENTS
- APPROVAL OF MINUTES
- PUBLIC COMMENTS
- CONSENT AGENDA
- PUBLIC HEARINGS
- RESOLUTIONS
- CITY MANAGER ITEMS
- CITY ATTORNEY ITEMS
- BOARDS AND COMMITTEES
- DISCUSSION ITEMS

Text:
{text}

Return ONLY a JSON array. Each section should have this structure:
[
    {{
        "section_name": "PRESENTATIONS AND PROTOCOL DOCUMENTS",
        "section_type": "PRESENTATIONS",
        "order": 1,
        "items": [
            {{
                "item_code": "A.-1.",
                "document_reference": "23-6764",
                "title": "Presentation of a Proclamation declaring...",
                "item_type": "Presentation"
            }}
        ]
    }},
    {{
        "section_name": "CONSENT AGENDA",
        "section_type": "CONSENT",
        "order": 4,
        "items": [
            {{
                "item_code": "D.-1.",
                "document_reference": "23-6830",
                "title": "A Resolution of the City Commission appointing...",
                "item_type": "Resolution"
            }}
        ]
    }}
]""".format(text=text)
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "Extract ALL agenda items. Do not skip any items in the sequence. If you see E-1 and E-3, look carefully for E-2."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_tokens=self.max_tokens
            )
            
            raw_response = response.choices[0].message.content.strip()
            
            # Save LLM response for debugging
            with open(self.debug_dir / f"agenda_structure_chunk{chunk_num}_llm_response.txt", 'w', encoding='utf-8') as f:
                f.write(raw_response)
            
            # For OpenAI, just clean the JSON response
            json_text = self._clean_json_response(raw_response)
            
            # Try to parse
            try:
                agenda_structure = json.loads(json_text)
                
                # Validate for missing items in sequence
                all_items = []
                for section in agenda_structure:
                    all_items.extend(section.get('items', []))
                
                # Check for missing E items
                e_items = sorted([item['item_code'] for item in all_items if item['item_code'].startswith('E')])
                if e_items:
                    log.info(f"Found E-section items: {e_items}")
                    # Check for gaps
                    for i in range(len(e_items) - 1):
                        current = e_items[i]
                        next_item = e_items[i + 1]
                        # Extract numbers
                        current_num = int(re.search(r'\d+', current).group())
                        next_num = int(re.search(r'\d+', next_item).group())
                        if next_num - current_num > 1:
                            log.warning(f"⚠️  Gap detected: {current} -> {next_item}. Missing items in between!")
                
            except json.JSONDecodeError:
                # If parsing fails, try to extract items manually from the text
                log.warning(f"Failed to parse LLM response, extracting items manually")
                agenda_structure = self._extract_items_manually(text)
            
            # Save parsed result
            with open(self.debug_dir / f"agenda_structure_chunk{chunk_num}_parsed.json", 'w', encoding='utf-8') as f:
                json.dump(agenda_structure, f, indent=2)
            
            return agenda_structure
            
        except Exception as e:
            log.error(f"Failed to extract agenda structure chunk {chunk_num}: {e}")
            # Try manual extraction as fallback
            return self._extract_items_manually(text)
    
    def _extract_items_manually(self, text: str) -> List[Dict[str, Any]]:
        """Manually extract agenda items using regex patterns."""
        log.info("Attempting manual extraction of agenda items")
        
        sections = []
        current_section = None
        
        # Define section headers (no letter mapping needed)
        section_patterns = [
            (r'^(PRESENTATIONS AND PROTOCOL DOCUMENTS)', 'PRESENTATIONS'),
            (r'^(APPROVAL OF MINUTES)', 'MINUTES'),
            (r'^(PUBLIC COMMENTS)', 'PUBLIC_COMMENTS'),
            (r'^(CONSENT AGENDA)', 'CONSENT'),
            (r'^(PUBLIC HEARINGS)', 'PUBLIC_HEARINGS'),
            (r'^(RESOLUTIONS)', 'RESOLUTIONS'),
            (r'^(ORDINANCES.*)', 'ORDINANCES'),
            (r'^(CITY MANAGER.*)', 'CITY_MANAGER'),
            (r'^(CITY ATTORNEY.*)', 'CITY_ATTORNEY'),
            (r'^(DISCUSSION ITEMS)', 'DISCUSSION'),
            (r'^(BOARDS AND COMMITTEES)', 'BOARDS'),
            # Also match letter-prefixed sections like "A. PRESENTATIONS..."
            (r'^[A-Z]\.\s+(.+)$', 'SECTION'),
        ]
        
        lines = text.split('\n')
        
        for i, line in enumerate(lines):
            line_stripped = line.strip()
            
            # Skip empty lines
            if not line_stripped:
                continue
            
            # Check for section headers
            section_found = False
            for pattern, section_type in section_patterns:
                match = re.match(pattern, line_stripped, re.IGNORECASE)
                if match:
                    # Save previous section if exists
                    if current_section and current_section["items"]:
                        sections.append(current_section)
                    
                    # Extract section name
                    if section_type == 'SECTION':
                        section_name = match.group(1)
                    else:
                        section_name = match.group(1)
                    
                    current_section = {
                        "section_name": section_name.strip(),
                        "section_type": section_type,
                        "order": len(sections) + 1,
                        "items": []
                    }
                    section_found = True
                    break
            
            if section_found:
                continue
            
            # Check for agenda items
            # Patterns: A.-1. 23-6764, D.-1. 23-6830, 1.-1. 23-6797
            item_patterns = [
                r'^([A-Z]\.-\d+\.?)\s+(\d{2}-\d{4,5})\s+(.+)$',  # A.-1. 23-6764
                r'^(\d+\.-\d+\.?)\s+(\d{2}-\d{4,5})\s+(.+)$',    # 1.-1. 23-6797
                r'^([A-Z]-\d+)\s+(\d{2}-\d{4,5})\s+(.+)$',       # E-1 23-6784
            ]
            
            for pattern in item_patterns:
                match = re.match(pattern, line_stripped)
                if match and current_section:
                    item_code = match.group(1)
                    doc_ref = match.group(2)
                    title = match.group(3).strip()
                    
                    # Determine item type based on title or section
                    item_type = self._determine_item_type(title, current_section.get("section_type", ""))
                    
                    item = {
                        "item_code": item_code,
                        "document_reference": doc_ref,
                        "title": title[:300],
                        "item_type": item_type
                    }
                    
                    current_section["items"].append(item)
                    log.info(f"Extracted {item_type}: {item_code} - {doc_ref}")
                    break
        
        # Don't forget the last section
        if current_section and current_section["items"]:
            sections.append(current_section)
        
        total_items = sum(len(s['items']) for s in sections)
        log.info(f"Manual extraction complete: {total_items} items in {len(sections)} sections")
        
        return sections

    def _normalize_agenda_item_code(self, code: str) -> str:
        """Normalize agenda item code to consistent format for agenda display."""
        # Remove all spaces
        code = code.strip()
        
        # Ensure we have the letter part
        match = re.match(r'([A-Z])\.?-?(\d+)\.?', code)
        if match:
            letter = match.group(1)
            number = match.group(2)
            # Return in consistent format: "E.-9."
            return f"{letter}.-{number}."
        
        return code
    
    def _determine_item_type(self, title: str, section_type: str) -> str:
        """Determine item type from title and section."""
        title_lower = title.lower()
        
        # Check title first for explicit type
        if 'an ordinance' in title_lower:
            return 'Ordinance'
        elif 'a resolution' in title_lower:  # Changed: more specific
            return 'Resolution'
        elif 'proclamation' in title_lower:
            return 'Proclamation'
        elif 'recognition' in title_lower:
            return 'Recognition'
        elif 'congratulations' in title_lower:
            return 'Recognition'
        elif 'presentation' in title_lower:
            return 'Presentation'
        elif section_type == 'PRESENTATIONS':
            return 'Presentation'
        elif section_type == 'MINUTES':
            return 'Minutes Approval'
        else:
            return 'Agenda Item'  # Generic fallback

    def _determine_section_type(self, section_name: str) -> str:
        """Determine section type from section name."""
        section_name_upper = section_name.upper()
        if "RESOLUTION" in section_name_upper:
            return "RESOLUTION"
        elif "ORDINANCE" in section_name_upper:
            return "ORDINANCE"
        elif "COMMISSION" in section_name_upper:
            return "COMMISSION"
        elif "CONSENT" in section_name_upper:
            return "CONSENT"
        else:
            return "GENERAL"
    
    def _extract_entities(self, text: str) -> Dict[str, List[Dict[str, Any]]]:
        """Extract all entities mentioned in the document."""
        prompt = """Extract entities from this city agenda document.

Text:
{text}

IMPORTANT: Return ONLY the JSON object below. No markdown, no code blocks, no other text.

{{
    "people": [
        {{"name": "John Smith", "role": "Mayor", "context": "presiding"}}
    ],
    "organizations": [
        {{"name": "City Commission", "type": "government", "context": "governing body"}}
    ],
    "locations": [
        {{"name": "City Hall", "address": "405 Biltmore Way", "type": "government building"}}
    ],
    "monetary_amounts": [
        {{"amount": "$100,000", "purpose": "budget allocation", "context": "parks improvement"}}
    ],
    "dates": [
        {{"date": "01/23/2024", "event": "meeting date", "type": "meeting"}}
    ],
    "legal_references": [
        {{"type": "Resolution", "number": "2024-01", "title": "Budget Amendment"}}
    ]
}}""".format(text=text[:10000])  # Limit text to avoid token issues
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "Extract entities. Return only JSON, no formatting."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_tokens=self.max_tokens  # Use the configurable value
            )
            
            raw_response = response.choices[0].message.content.strip()
            
            # Save LLM response for debugging
            with open(self.debug_dir / "entities_llm_response.txt", 'w', encoding='utf-8') as f:
                f.write(raw_response)
            
            # Clean and parse JSON
            json_text = self._clean_json_response(raw_response)
            entities = json.loads(json_text)
            
            # Save parsed result
            with open(self.debug_dir / "entities_parsed.json", 'w', encoding='utf-8') as f:
                json.dump(entities, f, indent=2)
            
            return entities
            
        except json.JSONDecodeError as e:
            log.error(f"JSON decode error in entities: {e}")
            log.error(f"Raw response saved to debug/entities_llm_response.txt")
            return self._default_entities()
        except Exception as e:
            log.error(f"Failed to extract entities: {e}")
            return self._default_entities()
    
    def _extract_relationships(self, agenda_structure: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Extract relationships between agenda items."""
        relationships = []
        
        # Find related items based on common references
        all_items = []
        for section in agenda_structure:
            for item in section.get("items", []):
                item["section"] = section.get("section_name")
                all_items.append(item)
        
        # Save all items for debugging
        with open(self.debug_dir / "all_agenda_items.json", 'w', encoding='utf-8') as f:
            json.dump(all_items, f, indent=2)
        
        # Look for items that reference each other
        for i, item1 in enumerate(all_items):
            for j, item2 in enumerate(all_items[i+1:], i+1):
                # Check if items share document references
                if (item1.get("document_reference") and 
                    item1.get("document_reference") == item2.get("document_reference")):
                    relationships.append({
                        "from_code": item1.get("item_code"),
                        "to_code": item2.get("item_code"),
                        "relationship_type": "REFERENCES_SAME_DOCUMENT",
                        "description": f"Both reference document {item1.get('document_reference')}"
                    })
        
        return relationships
    
    def _default_meeting_info(self) -> Dict[str, Any]:
        """Return default meeting info structure."""
        return {
            "meeting_type": "Regular Meeting",
            "meeting_time": "9:00 a.m.",
            "location": {
                "name": "City Hall, Commission Chambers",
                "address": "405 Biltmore Way, Coral Gables, FL 33134"
            },
            "officials_present": {
                "mayor": None,
                "vice_mayor": None,
                "commissioners": [],
                "city_attorney": None,
                "city_manager": None,
                "city_clerk": None
            }
        }
    
    def _default_entities(self) -> Dict[str, List]:
        """Return default empty entities structure."""
        return {
            "people": [],
            "organizations": [],
            "locations": [],
            "monetary_amounts": [],
            "dates": [],
            "legal_references": []
        }


================================================================================


################################################################################
# File: scripts/stages/chunk_text.py
################################################################################

# File: scripts/stages/chunk_text.py

#!/usr/bin/env python3
"""
Stage 5 — Token-based chunking with tiktoken validation.
"""
from __future__ import annotations
import json, logging, math, pathlib, re
from typing import Dict, List, Sequence, Tuple, Any, Optional
import asyncio
from concurrent.futures import ProcessPoolExecutor
import multiprocessing as mp

# 🎯 TIKTOKEN VALIDATION - Add token validation layer
try:
    import tiktoken
    TIKTOKEN_AVAILABLE = True
except ImportError:
    TIKTOKEN_AVAILABLE = False
    logging.warning("tiktoken not available - token-based chunking requires tiktoken")

# Token limits for validation (consistent with embed_vectors.py)
MAX_CHUNK_TOKENS = 6000  # Maximum tokens per chunk for embedding
MIN_CHUNK_TOKENS = 100   # Minimum viable chunk size
EMBEDDING_MODEL = "text-embedding-ada-002"

# ─── TOKEN-BASED CHUNKING PARAMETERS ───────────────────────────
WINDOW_TOKENS = 3000   # Token-based window (was 768 words)
OVERLAP_TOKENS = 600   # Token-based overlap (20% of window)
log = logging.getLogger(__name__)

# ─── helpers to split into token windows ───────────────────────────
def concat_tokens_by_encoding(sections: Sequence[Dict[str, Any]]) -> Tuple[List[int], List[int]]:
    """Convert sections to encoded tokens with page mapping."""
    if not TIKTOKEN_AVAILABLE:
        raise RuntimeError("tiktoken is required for token-based chunking")
    
    encoder = tiktoken.encoding_for_model(EMBEDDING_MODEL)
    all_tokens = []
    token_to_page = []
    
    for s in sections:
        # Be defensive - build text if missing
        if 'text' not in s:
            if 'elements' in s:
                text = '\n'.join(
                    el.get('text', '') for el in s['elements'] 
                    if el.get('text', '') and not el.get('text', '').startswith('self_ref=')
                )
            else:
                text = ""
            log.warning(f"Section missing 'text' field, built from elements: {s.get('section', 'untitled')}")
        else:
            text = s.get("text", "")
        
        # Encode text to tokens
        tokens = encoder.encode(text)
        all_tokens.extend(tokens)
        
        # Handle page mapping
        if 'page_number' in s:
            page_num = s['page_number']
        elif 'page_start' in s and 'page_end' in s:
            page_start = s['page_start']
            page_end = s['page_end']
            if page_start == page_end:
                page_num = page_start
            else:
                # For multi-page sections, distribute tokens across pages
                pages_span = page_end - page_start + 1
                tokens_per_page = max(1, len(tokens) // pages_span)
                for i, token in enumerate(tokens):
                    page = min(page_end, page_start + (i // tokens_per_page))
                    token_to_page.append(page)
                continue  # Skip the extend below since we handled it above
        else:
            page_num = 1
            log.warning(f"Section has no page info: {s.get('section', 'untitled')}")
        
        # Map all tokens in this section to the page (for single page sections)
        if 'page_start' not in s or s['page_start'] == s.get('page_end', s['page_start']):
            token_to_page.extend([page_num] * len(tokens))
    
    return all_tokens, token_to_page

def sliding_chunks_by_tokens(
    encoded_tokens: List[int], 
    token_to_page: List[int], 
    *, 
    window_tokens: int, 
    overlap_tokens: int
) -> List[Dict[str, Any]]:
    """Create sliding chunks based on token count."""
    if not TIKTOKEN_AVAILABLE:
        raise RuntimeError("tiktoken is required for token-based chunking")
    
    encoder = tiktoken.encoding_for_model(EMBEDDING_MODEL)
    step = max(1, window_tokens - overlap_tokens)
    chunks = []
    i = 0
    
    while i < len(encoded_tokens):
        start = i
        end = min(len(encoded_tokens), i + window_tokens)
        
        # Extract token chunk
        chunk_tokens = encoded_tokens[start:end]
        
        # Decode back to text
        chunk_text = encoder.decode(chunk_tokens)
        
        # Determine page range for this chunk
        page_start = token_to_page[start] if start < len(token_to_page) else 1
        page_end = token_to_page[end - 1] if end - 1 < len(token_to_page) else page_start
        
        chunk = {
            "chunk_index": len(chunks),
            "token_start": start,
            "token_end": end - 1,
            "page_start": page_start,
            "page_end": page_end,
            "text": chunk_text,
            "token_count": len(chunk_tokens)  # Actual token count
        }
        
        chunks.append(chunk)
        
        # Break if we've reached the end
        if end == len(encoded_tokens):
            break
            
        i += step
    
    return chunks

# Legacy word-based functions (kept for fallback if tiktoken unavailable)
def concat_tokens(sections:Sequence[Dict[str,Any]])->Tuple[List[str],List[int]]:
    tokens,page_map=[],[]
    for s in sections:
        # Be defensive - build text if missing
        if 'text' not in s:
            if 'elements' in s:
                text = '\n'.join(
                    el.get('text', '') for el in s['elements'] 
                    if el.get('text', '') and not el.get('text', '').startswith('self_ref=')
                )
            else:
                text = ""
            log.warning(f"Section missing 'text' field, built from elements: {s.get('section', 'untitled')}")
        else:
            text = s.get("text", "")
            
        words=text.split()
        tokens.extend(words)
        
        # Handle both page_number (from page_secs) and page_start/page_end (from logical_secs)
        if 'page_number' in s:
            # Single page section
            page_map.extend([s['page_number']]*len(words))
        elif 'page_start' in s and 'page_end' in s:
            # Multi-page section - distribute words across pages
            page_start = s['page_start']
            page_end = s['page_end']
            if page_start == page_end:
                # All on same page
                page_map.extend([page_start]*len(words))
            else:
                # Distribute words evenly across pages
                pages_span = page_end - page_start + 1
                words_per_page = max(1, len(words) // pages_span)
                for i, word in enumerate(words):
                    page = min(page_end, page_start + (i // words_per_page))
                    page_map.append(page)
        else:
            # Fallback to page 1
            page_map.extend([1]*len(words))
            log.warning(f"Section has no page info: {s.get('section', 'untitled')}")
            
    return tokens,page_map

def sliding_chunks(tokens:List[str],page_map:List[int],*,window:int,overlap:int)->List[Dict[str,Any]]:
    step=max(1,window-overlap)
    out,i=[],0
    SENT_END_RE=re.compile(r"[.!?]$")
    while i<len(tokens):
        start,end=i,min(len(tokens),i+window)
        while end<len(tokens) and not SENT_END_RE.search(tokens[end-1]) and end-start<window+256:
            end+=1
        out.append({"chunk_index":len(out),"token_start":start,"token_end":end-1,
                    "page_start":page_map[start],"page_end":page_map[end-1],
                    "text":" ".join(tokens[start:end])})
        if end==len(tokens): break
        i+=step
    return out
# ───────────────────────────────────────────────────────────────────

# 🎯 TIKTOKEN VALIDATION FUNCTIONS
def count_tiktoken_accurate(text: str) -> int:
    """Count tokens accurately using tiktoken."""
    if TIKTOKEN_AVAILABLE:
        try:
            encoder = tiktoken.encoding_for_model(EMBEDDING_MODEL)
            return len(encoder.encode(text))
        except:
            pass
    
    # Conservative fallback estimation
    return int(len(text.split()) * 1.5) + 50

def smart_split_chunk(chunk: Dict[str, Any], max_tokens: int = MAX_CHUNK_TOKENS) -> List[Dict[str, Any]]:
    """Split an oversized chunk into smaller token-compliant chunks."""
    text = chunk["text"]
    tokens = count_tiktoken_accurate(text)
    
    if tokens <= max_tokens:
        return [chunk]
    
    # Calculate how many sub-chunks we need
    num_splits = math.ceil(tokens / max_tokens)
    target_words_per_split = len(text.split()) // num_splits
    
    log.info(f"🎯 Splitting oversized chunk: {tokens} tokens → {num_splits} chunks of ~{max_tokens} tokens each")
    
    words = text.split()
    sentences = re.split(r'[.!?]+', text)
    
    sub_chunks = []
    current_words = []
    current_tokens = 0
    
    # Split by sentences when possible, otherwise by words
    for sentence in sentences:
        sentence_words = sentence.strip().split()
        if not sentence_words:
            continue
            
        sentence_tokens = count_tiktoken_accurate(sentence)
        
        # If adding this sentence would exceed limit, finalize current chunk
        if current_tokens + sentence_tokens > max_tokens and current_words:
            # Create sub-chunk
            sub_text = " ".join(current_words)
            sub_chunk = {
                "chunk_index": chunk["chunk_index"],
                "sub_chunk_index": len(sub_chunks),
                "token_start": chunk["token_start"] + len(" ".join(words[:words.index(current_words[0])])),
                "token_end": chunk["token_start"] + len(" ".join(words[:words.index(current_words[-1])])) + len(current_words[-1]),
                "page_start": chunk["page_start"],
                "page_end": chunk["page_end"],
                "text": sub_text
            }
            sub_chunks.append(sub_chunk)
            current_words = []
            current_tokens = 0
        
        # Add sentence to current chunk
        current_words.extend(sentence_words)
        current_tokens += sentence_tokens
    
    # Add final sub-chunk if there's remaining content
    if current_words:
        sub_text = " ".join(current_words)
        if count_tiktoken_accurate(sub_text) >= MIN_CHUNK_TOKENS:
            sub_chunk = {
                "chunk_index": chunk["chunk_index"],
                "sub_chunk_index": len(sub_chunks),
                "token_start": chunk["token_start"],
                "token_end": chunk["token_end"],
                "page_start": chunk["page_start"],
                "page_end": chunk["page_end"],
                "text": sub_text
            }
            sub_chunks.append(sub_chunk)
    
    log.info(f"🎯 Split complete: {len(sub_chunks)} sub-chunks created")
    return sub_chunks

def validate_and_fix_chunks(chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Validate chunks against tiktoken limits and fix oversized ones."""
    if not TIKTOKEN_AVAILABLE:
        log.warning("🎯 tiktoken not available - skipping token validation")
        return chunks
    
    valid_chunks = []
    oversized_count = 0
    total_tokens_before = 0
    total_tokens_after = 0
    
    log.info(f"🎯 TIKTOKEN VALIDATION: Processing {len(chunks)} chunks")
    
    for chunk in chunks:
        tokens = count_tiktoken_accurate(chunk["text"])
        total_tokens_before += tokens
        
        if tokens <= MAX_CHUNK_TOKENS:
            # Chunk is fine, keep as-is
            valid_chunks.append(chunk)
            total_tokens_after += tokens
        else:
            # Split oversized chunk
            log.warning(f"🎯 Oversized chunk detected: {tokens} tokens (max: {MAX_CHUNK_TOKENS})")
            oversized_count += 1
            
            split_chunks = smart_split_chunk(chunk, MAX_CHUNK_TOKENS)
            valid_chunks.extend(split_chunks)
            
            # Count tokens in split chunks
            for split_chunk in split_chunks:
                total_tokens_after += count_tiktoken_accurate(split_chunk["text"])
    
    # Final validation check
    max_tokens_found = max((count_tiktoken_accurate(chunk["text"]) for chunk in valid_chunks), default=0)
    
    log.info(f"🎯 TIKTOKEN VALIDATION COMPLETE:")
    log.info(f"   📊 Original chunks: {len(chunks)}")
    log.info(f"   📊 Final chunks: {len(valid_chunks)}")
    log.info(f"   📊 Oversized chunks split: {oversized_count}")
    log.info(f"   📊 Largest chunk: {max_tokens_found} tokens (limit: {MAX_CHUNK_TOKENS})")
    log.info(f"   ✅ GUARANTEED: All chunks ≤ {MAX_CHUNK_TOKENS} tokens")
    
    if max_tokens_found > MAX_CHUNK_TOKENS:
        log.error(f"🚨 VALIDATION FAILED: Chunk still exceeds limit!")
        raise RuntimeError(f"Chunk validation failed: {max_tokens_found} > {MAX_CHUNK_TOKENS}")
    
    return valid_chunks

# Legacy constants for fallback
WINDOW  = 768
OVERLAP = int(WINDOW*0.20)           # 154-token (20 %) overlap

# Parallel chunking for multiple documents
async def chunk_batch_async(
    json_paths: List[pathlib.Path],
    max_workers: Optional[int] = None
) -> Dict[pathlib.Path, List[Dict[str, Any]]]:
    """Chunk multiple documents in parallel with token-based chunking."""
    from .acceleration_utils import hardware
    
    results = {}
    
    # 🎯 Show token-based chunking status
    if TIKTOKEN_AVAILABLE:
        log.info(f"🎯 TOKEN-BASED CHUNKING ENABLED: Using {WINDOW_TOKENS} token windows with {OVERLAP_TOKENS} token overlap")
    else:
        log.warning(f"⚠️  TIKTOKEN NOT AVAILABLE: Falling back to word-based chunking (install: pip install tiktoken)")
    
    # CPU-bound task - use process pool
    with hardware.get_process_pool(max_workers) as executor:
        future_to_path = {
            executor.submit(chunk, path): path 
            for path in json_paths
        }
        
        from tqdm import tqdm
        from concurrent.futures import as_completed
        
        for future in tqdm(as_completed(future_to_path), 
                          total=len(json_paths), 
                          desc="Chunking documents"):
            path = future_to_path[future]
            try:
                chunks = future.result()
                results[path] = chunks
            except Exception as exc:
                log.error(f"Failed to chunk {path.name}: {exc}")
                results[path] = []
    
    # 🎯 Summary of token-based chunking results
    total_chunks = sum(len(chunks) for chunks in results.values())
    total_tokens = sum(chunk.get('token_count', count_tiktoken_accurate(chunk['text'])) 
                      for chunks in results.values() 
                      for chunk in chunks)
    log.info(f"🎯 BATCH CHUNKING COMPLETE: {total_chunks} chunks created ({total_tokens} total tokens)")
    
    return results

# Optimized chunking with better memory management
def chunk_optimized(json_path: pathlib.Path) -> List[Dict[str, Any]]:
    """Token-based optimized chunking with validation."""
    root = json_path.with_name(json_path.stem + "_chunks.json")
    if root.exists():
        existing_chunks = json.loads(root.read_text())
        # Validate existing chunks if they don't have tiktoken validation yet
        if existing_chunks and "sub_chunk_index" not in str(existing_chunks):
            log.info(f"🎯 Re-validating existing chunks in {root.name}")
            validated_chunks = validate_and_fix_chunks(existing_chunks)
            if len(validated_chunks) != len(existing_chunks):
                # Chunks were modified, save the validated version
                with open(root, 'w') as f:
                    json.dump(validated_chunks, f, indent=2, ensure_ascii=False)
                log.info(f"🎯 Updated {root.name} with validated chunks")
            return validated_chunks
        return existing_chunks
    
    # Stream large JSON files
    with open(json_path, 'r') as f:
        data = json.load(f)
    
    if "sections" not in data:
        raise RuntimeError(f"{json_path} has no 'sections' key")
    
    # Process in smaller batches to reduce memory usage
    sections = data["sections"]
    batch_size = 100  # Process 100 sections at a time
    
    if TIKTOKEN_AVAILABLE:
        # 🎯 TOKEN-BASED CHUNKING - Primary approach
        log.info(f"🎯 Using token-based chunking with {WINDOW_TOKENS} token windows")
        
        all_tokens = []
        all_token_to_page = []
        
        for i in range(0, len(sections), batch_size):
            batch = sections[i:i + batch_size]
            tokens, page_map = concat_tokens_by_encoding(batch)
            all_tokens.extend(tokens)
            all_token_to_page.extend(page_map)
        
        chunks = sliding_chunks_by_tokens(
            all_tokens, 
            all_token_to_page, 
            window_tokens=WINDOW_TOKENS, 
            overlap_tokens=OVERLAP_TOKENS
        )
        
        # Token-based chunks are already guaranteed to be within limits
        log.info(f"🎯 Token-based chunking produced {len(chunks)} chunks")
        
        # Additional validation for safety
        max_tokens = max((chunk.get('token_count', count_tiktoken_accurate(chunk['text'])) for chunk in chunks), default=0)
        if max_tokens > MAX_CHUNK_TOKENS:
            log.warning(f"🎯 Unexpected: Token-based chunk exceeds limit ({max_tokens} > {MAX_CHUNK_TOKENS}), applying fallback validation")
            chunks = validate_and_fix_chunks(chunks)
    else:
        # 🎯 FALLBACK: Word-based chunking when tiktoken unavailable
        log.warning(f"🎯 Falling back to word-based chunking (tiktoken unavailable)")
        
        all_tokens = []
        all_page_map = []
        
        for i in range(0, len(sections), batch_size):
            batch = sections[i:i + batch_size]
            toks, pmap = concat_tokens(batch)
            all_tokens.extend(toks)
            all_page_map.extend(pmap)
        
        chunks = sliding_chunks(all_tokens, all_page_map, window=WINDOW, overlap=OVERLAP)
        
        # Apply validation for word-based chunks
        if chunks:
            log.info(f"🎯 Applying validation to {len(chunks)} word-based chunks")
            chunks = validate_and_fix_chunks(chunks)
    
    # Write chunks
    if chunks:
        with open(root, 'w') as f:
            json.dump(chunks, f, indent=2, ensure_ascii=False)
        log.info("✓ %s chunks → %s", len(chunks), root.name)
        
        return chunks
    else:
        log.warning("%s – no chunks produced; file skipped", json_path.name)
        return []

# Keep original interface
def chunk(json_path: pathlib.Path) -> List[Dict[str, Any]]:
    """Original interface maintained for compatibility."""
    return chunk_optimized(json_path)

if __name__ == "__main__":
    import argparse, logging
    logging.basicConfig(level=logging.INFO, format="%(message)s")
    p=argparse.ArgumentParser(); p.add_argument("json",type=pathlib.Path)
    chunk(p.parse_args().json)


================================================================================


################################################################################
# File: scripts/rag_local_web_app.py
################################################################################

# File: scripts/rag_local_web_app.py

#!/usr/bin/env python3
################################################################################
################################################################################
"""
Mini Flask app that answers misophonia questions with Retrieval‑Augmented
Generation (gpt-4.1-mini-2025-04-14 + Supabase pgvector).

### Patch 2  (2025‑05‑06)
• **Embeddings** now created with **text‑embedding‑ada‑002** (1536‑D).  
• Similarity is re‑computed client‑side with a **plain cosine function** so the
  ranking no longer depends on pgvector's built‑in distance or any RPC
  threshold quirks.

The rest of the grounded‑answer logic (added in Patch 1) is unchanged.
"""
from __future__ import annotations

import logging
import math
import os
import re
from pathlib import Path        # (unused but left in to mirror original)
from typing import Dict, List
import json

from dotenv import load_dotenv
from flask import Flask, jsonify, request, make_response
from openai import OpenAI
from supabase import create_client
from flask_compress import Compress
from flask_cors import CORS

# ────────────────────────────── configuration ───────────────────────────── #

load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
SUPABASE_URL   = os.getenv("SUPABASE_URL")
SUPABASE_KEY   = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
PORT           = int(os.getenv("PORT", 8080))

if not (OPENAI_API_KEY and SUPABASE_URL and SUPABASE_KEY):
    raise SystemExit(
        "❌  Required env vars: OPENAI_API_KEY, SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY"
    )

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s — %(levelname)s — %(message)s",
)
log = logging.getLogger("rag_app")

sb = create_client(SUPABASE_URL, SUPABASE_KEY)
oa = OpenAI(api_key=OPENAI_API_KEY)

app = Flask(__name__)
CORS(app)
app.config['COMPRESS_ALGORITHM'] = 'gzip'
Compress(app)

# ────────────────────────────── helper functions ────────────────────────── #


def embed(text: str) -> List[float]:
    """
    Return OpenAI embedding vector for *text* using text‑embedding‑ada‑002.

    ada‑002 has 1536 dimensions and is inexpensive yet solid for similarity.
    """
    resp = oa.embeddings.create(
        model="text-embedding-ada-002",
        input=text[:8192],  # safety slice
    )
    return resp.data[0].embedding


def cosine_similarity(a: List[float], b: List[float]) -> float:
    """Plain cosine similarity between two equal‑length vectors."""
    dot = sum(x * y for x, y in zip(a, b))
    na = math.sqrt(sum(x * x for x in a))
    nb = math.sqrt(sum(y * y for y in b))
    return dot / (na * nb + 1e-9)


# Add in-memory embedding cache
_qcache = {}
def embed_cached(text):
    if text in _qcache: return _qcache[text]
    vec = embed(text)
    _qcache[text] = vec
    return vec


# Add regex patterns for bibliography detection
_DOI_RE   = re.compile(r'\b10\.\d{4,9}/[-._;()/:A-Z0-9]+\b', re.I)
_YEAR_RE  = re.compile(r'\b(19|20)\d{2}\b')

def looks_like_refs(text: str) -> bool:
    """
    Return True if this chunk is likely just a bibliography list:
      • more than 12 DOIs, or
      • more than 15 year mentions.
    """
    doi_count  = len(_DOI_RE.findall(text))
    year_count = len(_YEAR_RE.findall(text))
    return doi_count > 12 or year_count > 15


def semantic_search(
    query: str,
    *,
    limit: int = 8,
    threshold: float = 0.0,
) -> List[Dict]:
    """
    Retrieve candidate chunks via the pgvector RPC, then re-rank with an
    **explicit cosine similarity** so the final score is always in
    **[-100 … +100] percent**.

    Why the extra work?
    -------------------
    •  The SQL function returns a raw inner-product that can be > 1.  
       (embeddings are *not* unit-length.)  
    •  By pulling the real 1 536-D vectors and re-computing a cosine we get a
       true, bounded similarity that front-end code can safely show.

    The -100 … +100 range is produced by:  
        pct = clamp(cosine × 100, -100, 100)
    """
    # 1. Embed the query once and keep it cached
    q_vec = embed_cached(query)

    # 2. Fast ANN search in Postgres (over-fetch 4× so we can re-rank)
    rows = (
        sb.rpc(
            "match_documents_chunks",
            {
                "query_embedding": q_vec,
                "match_threshold": threshold,
                "match_count": limit * 4,
            },
        )
        .execute()
        .data
    ) or []

    # 3. Filter out bibliography-only chunks
    rows = [r for r in rows if not looks_like_refs(r["text"])]

    if not rows:
        return []

    # 4. Fetch document metadata (title, authors …) in one round-trip
    doc_ids = {r["document_id"] for r in rows}
    meta = {
        d["id"]: d
        for d in (
            sb.table("city_clerk_documents")
              .select("id,document_type,title,date,year,month,day,mayor,vice_mayor,commissioners,city_attorney,city_manager,city_clerk,public_works_director,agenda,keywords,source_pdf")
              .in_("id", list(doc_ids))
              .execute()
              .data
            or []
        )
    }

    # 5. Pull embeddings and page info once and compute **plain cosine** (no scaling)
    chunk_ids = [r["id"] for r in rows]

    emb_rows = (
        sb.table("documents_chunks")
          .select("id, embedding, page_start, page_end")
          .in_("id", chunk_ids)
          .execute()
          .data
    ) or []

    emb_map: Dict[str, List[float]] = {}
    page_map: Dict[str, Dict] = {}
    for e in emb_rows:
        raw = e["embedding"]
        if isinstance(raw, list):                    # list[Decimal]
            emb_map[e["id"]] = [float(x) for x in raw]
        elif isinstance(raw, str) and raw.startswith('['):   # TEXT  "[…]"
            emb_map[e["id"]] = [float(x) for x in raw.strip('[]').split(',')]
        
        # Store page info
        page_map[e["id"]] = {
            "page_start": e.get("page_start", 1),
            "page_end": e.get("page_end", 1)
        }

    for r in rows:
        vec = emb_map.get(r["id"])
        if vec:                                     # we now have the real vector
            cos = cosine_similarity(q_vec, vec)
            r["similarity"] = round(cos * 100, 1)   # –100…+100 % (or 0…100 %)
        else:                                       # fallback if something failed
            dist = float(r.get("similarity", 1.0))  # 0…2 cosine-distance
            r["similarity"] = round((1.0 - dist) * 100, 1)

        r["doc"] = meta.get(r["document_id"], {})
        
        # Add page info to the row
        page_info = page_map.get(r["id"], {"page_start": 1, "page_end": 1})
        r["page_start"] = page_info["page_start"]
        r["page_end"] = page_info["page_end"]

    # 6. Keep the top *limit* rows after proper re-ranking
    ranked = sorted(rows, key=lambda x: x["similarity"], reverse=True)[:limit]
    return ranked


# ──────────────────────── NEW RAG‑PROMPT HELPERS ───────────────────────── #

MAX_PROMPT_CHARS: int = 24_000  # ~6 k tokens @ 4 chars/token heuristic


def trim_chunks(chunks: List[Dict]) -> List[Dict]:
    """
    Fail‑safe guard: ensure concatenated chunk texts remain under the
    MAX_PROMPT_CHARS budget.  Keeps highest‑similarity chunks first.
    """
    sorted_chunks = sorted(chunks, key=lambda c: c.get("similarity", 0), reverse=True)
    output: List[Dict] = []
    total_chars = 0
    for c in sorted_chunks:
        chunk_len = len(c["text"])
        if total_chars + chunk_len > MAX_PROMPT_CHARS:
            break
        output.append(c)
        total_chars += chunk_len
    return output


def build_prompt(question: str, chunks: List[Dict]) -> str:
    """
    Build a structured prompt that asks GPT to:
      • answer in Markdown with short intro + numbered list of key points
      • cite inline like [1], [2] …
      • finish with a Bibliography that includes the document title and type
    """
    snippet_lines, biblio_lines = [], []
    for i, c in enumerate(chunks, 1):
        page_start = c.get('page_start', 1)
        page_end = c.get('page_end', 1)
        snippet_lines.append(
            f"[{i}] \"{c['text'].strip()}\" "
            f"(pp. {page_start}-{page_end})"
        )

        d = c["doc"]
        title = d.get("title", "Untitled Document")
        doc_type = d.get("document_type", "Document")
        date = d.get("date", "Unknown date")
        year = d.get("year", "n.d.")
        pages = f"pp. {page_start}-{page_end}"
        source_pdf = d.get("source_pdf", "")

        # City clerk document bibliography format
        biblio_lines.append(
            f"[{i}] *{title}* · {doc_type} · {date} · {pages}"
        )

    prompt_parts = [
        "You are City Clerk Assistant, a knowledgeable AI that helps with questions about city government documents, including resolutions, ordinances, proclamations, contracts, meeting minutes, and agendas.",
        "You draw on evidence from official city documents and municipal records.",
        "Your responses are clear, professional, and grounded in the provided context.",
        "====",
        "QUESTION:",
        question,
        "====",
        "CONTEXT:",
        *snippet_lines,
        "====",
        "INSTRUCTIONS:",
        "• Write your answer in **Markdown**.",
        "• Begin with a concise summary (2–3 sentences).",
        "• Then elaborate on key points using well-structured paragraphs.",
        "• Provide relevant insights about city governance, policies, or procedures.",
        "• If helpful, use lists, subheadings, or clear explanations to enhance understanding.",
        "• Use a professional and informative tone.",
        "• Cite sources inline like [1], [2] etc.",
        "• After the answer, include a 'BIBLIOGRAPHY:' section that lists each source exactly as provided below.",
        "• If none of the context answers the question, reply: \"I'm sorry, I don't have sufficient information to answer that.\"",
        "====",
        "BEGIN OUTPUT",
        "ANSWER:",
        "",  # where the model writes the main response
        "BIBLIOGRAPHY:",
        *biblio_lines,
    ]

    return '\n'.join(prompt_parts)


def extract_citations(answer: str) -> List[str]:
    """
    Parse numeric citations (e.g., "[1]", "[2]") from the answer text.
    Returns unique citation numbers in ascending order.
    """
    citations = re.findall(r"\[(\d+)\]", answer)
    return sorted(set(citations), key=int)


# ──────────────────────────────── routes ────────────────────────────────── #

@app.route("/")
def home():
    """Simple homepage for the City Clerk RAG application."""
    html = """
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>City Clerk RAG Assistant</title>
        <style>
            body { 
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
                max-width: 800px; 
                margin: 0 auto; 
                padding: 2rem;
                line-height: 1.6;
                color: #333;
            }
            .header { 
                text-align: center; 
                margin-bottom: 2rem;
                padding-bottom: 1rem;
                border-bottom: 2px solid #e0e0e0;
            }
            .search-container {
                background: #f8f9fa;
                padding: 2rem;
                border-radius: 8px;
                margin: 2rem 0;
            }
            .search-box {
                width: 100%;
                padding: 1rem;
                border: 2px solid #ddd;
                border-radius: 4px;
                font-size: 16px;
                margin-bottom: 1rem;
            }
            .search-btn {
                background: #007bff;
                color: white;
                padding: 1rem 2rem;
                border: none;
                border-radius: 4px;
                cursor: pointer;
                font-size: 16px;
            }
            .search-btn:hover { background: #0056b3; }
            .results { margin-top: 2rem; }
            .answer { 
                background: white; 
                padding: 1.5rem; 
                border-radius: 8px; 
                border-left: 4px solid #007bff;
                margin: 1rem 0;
            }
            .sources { 
                background: #f8f9fa; 
                padding: 1rem; 
                border-radius: 4px; 
                margin-top: 1rem;
                font-size: 0.9em;
            }
            .loading { color: #666; font-style: italic; }
            .error { color: #dc3545; background: #f8d7da; padding: 1rem; border-radius: 4px; }
        </style>
    </head>
    <body>
        <div class="header">
            <h1>🏛️ City Clerk RAG Assistant</h1>
            <p>Ask questions about city government documents, resolutions, ordinances, and meeting minutes</p>
        </div>
        
        <div class="search-container">
            <input type="text" id="queryInput" class="search-box" 
                   placeholder="Ask a question about city documents..." 
                   onkeypress="if(event.key==='Enter') search()">
            <button onclick="search()" class="search-btn">Search</button>
        </div>
        
        <div id="results" class="results"></div>
        
        <script>
            async function search() {
                const query = document.getElementById('queryInput').value.trim();
                if (!query) return;
                
                const resultsDiv = document.getElementById('results');
                resultsDiv.innerHTML = '<div class="loading">Searching...</div>';
                
                try {
                    const response = await fetch('/search', {
                        method: 'POST',
                        headers: { 'Content-Type': 'application/json' },
                        body: JSON.stringify({ query: query })
                    });
                    
                    const data = await response.json();
                    
                    if (data.error) {
                        resultsDiv.innerHTML = `<div class="error">Error: ${data.error}</div>`;
                        return;
                    }
                    
                    let html = `<div class="answer">${data.answer.replace(/\\n/g, '<br>')}</div>`;
                    
                    if (data.results && data.results.length > 0) {
                        html += '<div class="sources"><strong>Sources:</strong><ul>';
                        data.results.forEach((result, i) => {
                            const doc = result.doc || {};
                            const title = doc.title || 'Untitled Document';
                            const similarity = Math.round(result.similarity || 0);
                            html += `<li>${title} (${similarity}% match)</li>`;
                        });
                        html += '</ul></div>';
                    }
                    
                    resultsDiv.innerHTML = html;
                } catch (error) {
                    resultsDiv.innerHTML = `<div class="error">Error: ${error.message}</div>`;
                }
            }
        </script>
    </body>
    </html>
    """
    return html

@app.post("/search")
def search():
    payload = request.get_json(force=True, silent=True) or {}
    question = (payload.get("query") or "").strip()
    if not question:
        return jsonify({"error": "Missing 'query'"}), 400

    try:
        # Retrieve semantic matches (client‑side cosine re‑ranked)
        raw_matches = semantic_search(question, limit=int(payload.get("limit", 8)))

        if not raw_matches:
            return jsonify(
                {
                    "answer": "I'm sorry, I don't have sufficient information to answer that.",
                    "citations": [],
                    "results": [],
                }
            )

        # ──────────────────── TRIM CHUNKS TO BUDGET ──────────────────── #
        chunks = trim_chunks(raw_matches)

        # ──────────────────── BUILD PROMPT & CALL LLM ─────────────────── #
        prompt = build_prompt(question, chunks)

        completion = oa.chat.completions.create(
            model="gpt-4.1-mini-2025-04-14",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0,
        )
        answer_text: str = completion.choices[0].message.content.strip()

        # ──────────────────── EXTRACT CITATIONS ──────────────────────── #
        citations = extract_citations(answer_text)

        # Remove embedding vectors before sending back to the browser
        for m in raw_matches:
            m.pop("embedding", None)

        # ──────────────────── RETURN JSON ─────────────────────────────── #
        response = jsonify(
            {
                "answer": answer_text,
                "citations": citations,
                "results": raw_matches,
            }
        )
        response.headers['Connection'] = 'keep-alive'
        return response
    except Exception as exc:  # noqa: BLE001
        log.exception("search failed")
        return jsonify({"error": str(exc)}), 500


@app.get("/stats")
def stats():
    """Tiny ops endpoint—count total chunks."""
    resp = sb.table("documents_chunks").select("id", count="exact").execute()
    return jsonify({"total_chunks": resp.count})


# ──────────────────────────────── main ─────────────────────────────────── #

if __name__ == "__main__":
    log.info("Starting Flask on 0.0.0.0:%s …", PORT)
    app.run(host="0.0.0.0", port=PORT, debug=True)


================================================================================


################################################################################
# File: scripts/pipeline_modular_optimized.py
################################################################################

# File: scripts/pipeline_modular_optimized.py

#!/usr/bin/env python3
"""
Optimized pipeline orchestrator with full parallelization.
Maintains compatibility with original pipeline_modular.py interface.
"""
from __future__ import annotations
import argparse, logging, pathlib, random
from collections import Counter
from rich.console import Console
import asyncio
from typing import List, Optional
import multiprocessing as mp

# Import stages
from stages import extract_clean, llm_enrich, chunk_text, db_upsert, embed_vectors
from stages.acceleration_utils import hardware

# Keep existing toggles
RUN_EXTRACT    = True
RUN_LLM_ENRICH = True
RUN_CHUNK      = True
RUN_DB         = True
RUN_EMBED      = True

log = logging.getLogger("pipeline-modular-optimized")

class OptimizedPipeline:
    """Optimized pipeline with parallel processing and rate limiting."""
    
    def __init__(self, batch_size: int = 15, max_api_concurrent: int = 3):  # 🛡️ Rate limited: was 50, 20
        self.batch_size = batch_size
        self.max_api_concurrent = max_api_concurrent
        self.stats = Counter()
        
        # 🛡️ Log rate limiting settings
        log.info("🛡️  Rate-limited pipeline initialized:")
        log.info(f"   Batch size: {batch_size} (was 50)")
        log.info(f"   Max concurrent: {max_api_concurrent} (was 20)")
        log.info("   Target: <800K tokens/minute (safe margin)")
    
    async def process_batch(self, pdfs: List[pathlib.Path], start_doc_num: int = 1, total_docs: int = None) -> None:
        """Process a batch of PDFs through all stages with individual document progress tracking."""
        
        if total_docs is None:
            total_docs = len(pdfs)
        
        batch_size = len(pdfs)
        log.info(f"📊 BATCH PROCESSING START:")
        log.info(f"   📄 Documents in this batch: {batch_size}")
        log.info(f"   🎯 Document range: {start_doc_num} to {start_doc_num + batch_size - 1}")
        log.info(f"   📈 Overall progress: {start_doc_num-1}/{total_docs} completed ({((start_doc_num-1)/total_docs*100):.1f}%)")
        
        # Stage 1-2: Extract & Clean (CPU-bound, use process pool)
        json_docs = []
        if RUN_EXTRACT:
            log.info(f"🔄 EXTRACTION STAGE - Processing {len(pdfs)} PDFs...")
            for i, pdf in enumerate(pdfs):
                doc_num = start_doc_num + i
                doc_progress = ((doc_num-1) / total_docs * 100)
                log.info(f"📄 [{doc_num}/{total_docs}] ({doc_progress:.1f}%) Extracting: {pdf.name}")
            
            json_docs = await extract_clean.extract_batch_async(
                pdfs, 
                enrich_llm=False  # We'll do LLM enrichment separately
            )
            
            log.info(f"📊 EXTRACTION STAGE COMPLETE:")
            for i, pdf in enumerate(pdfs):
                doc_num = start_doc_num + i
                log.info(f"   ✅ [{doc_num}/{total_docs}] Extracted: {pdf.name}")
        else:
            # Use existing JSON files
            json_docs = [extract_clean.json_path_for(pdf) for pdf in pdfs]
            log.info(f"📊 EXTRACTION STAGE SKIPPED - Using existing JSON files")

        # Stage 4: LLM Enrich (I/O-bound, use async)
        if RUN_LLM_ENRICH and json_docs:
            log.info(f"🔄 ENRICHMENT STAGE - Processing {len(json_docs)} documents...")
            for i, json_doc in enumerate(json_docs):
                doc_num = start_doc_num + i
                doc_progress = ((doc_num-1) / total_docs * 100)
                doc_name = pathlib.Path(json_doc).stem.replace('_clean', '')
                log.info(f"📄 [{doc_num}/{total_docs}] ({doc_progress:.1f}%) Enriching: {doc_name}")
            
            await llm_enrich.enrich_batch_async(
                json_docs,
                max_concurrent=self.max_api_concurrent
            )
            
            log.info(f"📊 ENRICHMENT STAGE COMPLETE:")
            for i, json_doc in enumerate(json_docs):
                doc_num = start_doc_num + i
                doc_name = pathlib.Path(json_doc).stem.replace('_clean', '')
                log.info(f"   ✅ [{doc_num}/{total_docs}] Enriched: {doc_name}")

        # Stage 5: Chunk (CPU-bound, use process pool)
        chunks_map = {}
        if RUN_CHUNK and json_docs:
            log.info(f"🔄 CHUNKING STAGE - Processing {len(json_docs)} documents...")
            for i, json_doc in enumerate(json_docs):
                doc_num = start_doc_num + i
                doc_progress = ((doc_num-1) / total_docs * 100)
                doc_name = pathlib.Path(json_doc).stem.replace('_clean', '')
                log.info(f"📄 [{doc_num}/{total_docs}] ({doc_progress:.1f}%) Chunking: {doc_name}")
            
            chunks_map = await chunk_text.chunk_batch_async(json_docs)
            
            log.info(f"📊 CHUNKING STAGE COMPLETE:")
            total_chunks_created = 0
            for i, json_doc in enumerate(json_docs):
                doc_num = start_doc_num + i
                doc_name = pathlib.Path(json_doc).stem.replace('_clean', '')
                chunk_count = len(chunks_map.get(json_doc, []))
                total_chunks_created += chunk_count
                log.info(f"   ✅ [{doc_num}/{total_docs}] Chunked: {doc_name} ({chunk_count} chunks)")
            log.info(f"   📊 Total chunks created in this batch: {total_chunks_created}")

        # Stage 6: DB Upsert (I/O-bound, use async)
        if RUN_DB and chunks_map:
            log.info(f"🔄 DATABASE UPSERT STAGE - Processing {len(chunks_map)} documents...")
            
            # Show progress before upserting
            total_chunks_to_upsert = sum(len(chunks) for chunks in chunks_map.values())
            log.info(f"📊 DATABASE UPSERT PROGRESS:")
            log.info(f"   💾 Total chunks to upsert: {total_chunks_to_upsert}")
            
            for i, (json_path, chunks) in enumerate(chunks_map.items()):
                doc_num = start_doc_num + i
                doc_progress = ((doc_num-1) / total_docs * 100)
                doc_name = pathlib.Path(json_path).stem.replace('_clean', '')
                log.info(f"📄 [{doc_num}/{total_docs}] ({doc_progress:.1f}%) Upserting: {doc_name} ({len(chunks)} chunks)")
            
            documents = [
                {
                    "json_path": json_path,
                    "chunks": chunks,
                    "do_embed": False  # We'll embed in batch later
                }
                for json_path, chunks in chunks_map.items()
            ]
            await db_upsert.upsert_batch_async(documents)
            
            log.info(f"📊 DATABASE UPSERT STAGE COMPLETE:")
            total_chunks_upserted = 0
            for i, (json_path, chunks) in enumerate(chunks_map.items()):
                doc_num = start_doc_num + i
                doc_name = pathlib.Path(json_path).stem.replace('_clean', '')
                total_chunks_upserted += len(chunks)
                log.info(f"   ✅ [{doc_num}/{total_docs}] Upserted: {doc_name}")
            log.info(f"   📊 Total chunks upserted: {total_chunks_upserted}")

        # Update stats
        self.stats["ok"] += len([c for c in chunks_map.values() if c])
        self.stats["fail"] += len([c for c in chunks_map.values() if not c])
        
        # Final batch summary
        end_doc_num = start_doc_num + batch_size - 1
        overall_progress = (end_doc_num / total_docs * 100)
        log.info(f"📊 BATCH COMPLETE:")
        log.info(f"   ✅ Documents processed: {start_doc_num}-{end_doc_num}")
        log.info(f"   📈 Overall progress: {end_doc_num}/{total_docs} ({overall_progress:.1f}%)")
        log.info(f"   📊 Successful documents: {self.stats['ok']}")
        log.info(f"   ❌ Failed documents: {self.stats['fail']}")
    
    async def run(self, src: pathlib.Path, selection: str = "sequential", cap: int = 0):
        """Run the optimized pipeline."""
        Console().rule("[bold cyan]Misophonia PDF → Vector pipeline (optimized)")
        
        # Get PDF list
        pdfs = [src] if src.is_file() else sorted(src.rglob("*.pdf"))
        if cap:
            pdfs = random.sample(pdfs, cap) if selection == "random" else pdfs[:cap]
        
        total_pdfs = len(pdfs)
        log.info(f"Processing {total_pdfs} PDFs in batches of {self.batch_size}")
        
        # Track PDF-level progress
        pdfs_processed = 0
        
        # Process in batches
        for i in range(0, len(pdfs), self.batch_size):
            batch = pdfs[i:i + self.batch_size]
            batch_num = i//self.batch_size + 1
            total_batches = (len(pdfs) + self.batch_size - 1)//self.batch_size
            
            # Calculate the starting document number for this batch
            start_doc_num = pdfs_processed + 1
            
            # Enhanced progress logging with both batch and PDF-level progress
            log.info(f"📄 Processing batch {batch_num}/{total_batches} ({len(batch)} PDFs)")
            log.info(f"📊 Overall progress: {pdfs_processed}/{total_pdfs} PDFs completed ({pdfs_processed/total_pdfs*100:.1f}%)")
            log.info(f"🔢 Document range: {start_doc_num}-{start_doc_num + len(batch) - 1} of {total_pdfs}")
            
            await self.process_batch(batch, start_doc_num, total_pdfs)
            
            # Update PDF progress counter
            pdfs_processed += len(batch)
            
            # Log completion of this batch
            log.info(f"✅ Batch {batch_num} complete - Total PDFs processed: {pdfs_processed}/{total_pdfs} ({pdfs_processed/total_pdfs*100:.1f}%)")
        
        # Final progress summary
        log.info(f"🎉 All PDFs processed: {pdfs_processed}/{total_pdfs} ({pdfs_processed/total_pdfs*100:.1f}%)")
        
        # Stage 7: Batch embed all at once with conservative settings
        if RUN_EMBED:
            log.info("🔄 STARTING EMBEDDING STAGE...")
            log.info("📊 EMBEDDING STAGE PROGRESS:")
            log.info("   🎯 Running batch embedding with rate limiting...")
            log.info("   🎯 This stage will process all upserted chunks for embedding")
            
            # 🛡️ Pass conservative embedding parameters (consistent with embed_vectors.py defaults)
            await embed_vectors.main_async(
                batch_size=200,     # 🛡️ Rate limited: fetch 200 chunks at once (conservative default)
                commit_size=10,     # 🛡️ Rate limited: 10 chunks per API call (conservative default) 
                max_concurrent=3    # 🛡️ Rate limited: max 3 concurrent embedding calls (conservative default)
            )
            
            log.info("📊 EMBEDDING STAGE COMPLETE ✅")
        
        Console().rule("[green]Finished")
        log.info("📊 PIPELINE COMPLETE - FINAL SUMMARY:")
        log.info("🛡️  Rate limiting successful - no API limits hit")
        log.info(f"📊 Total documents processed: {self.stats['ok']}")
        log.info(f"📊 Total documents failed: {self.stats['fail']}")
        log.info(f"📊 Success rate: {(self.stats['ok']/(self.stats['ok']+self.stats['fail'])*100):.1f}%" if (self.stats['ok']+self.stats['fail']) > 0 else "100%")

def main(src: pathlib.Path, selection: str = "sequential", cap: int = 0) -> None:
    """Main entry point compatible with original pipeline_modular.py"""
    # Set multiprocessing start method for macOS
    mp.set_start_method('spawn', force=True)
    
    # Create and run pipeline with conservative rate limiting
    pipeline = OptimizedPipeline(
        batch_size=15,  # 🛡️ Rate limited: process 15 PDFs at a time (was 50)
        max_api_concurrent=3  # 🛡️ Rate limited: max 3 concurrent API requests (was 20)
    )
    
    # Run async pipeline
    asyncio.run(pipeline.run(src, selection, cap))

if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s — %(levelname)s — %(message)s"
    )
    
    p = argparse.ArgumentParser()
    p.add_argument("src", type=pathlib.Path,
                   default=pathlib.Path("city_clerk_documents/global"), nargs="?")
    p.add_argument("--selection", choices=["sequential", "random"],
                   default="sequential")
    p.add_argument("--cap", type=int, default=0)
    p.add_argument("--batch-size", type=int, default=15,  # 🛡️ Rate limited: default to 15 (was 50)
                   help="Number of PDFs to process in parallel")
    p.add_argument("--api-concurrent", type=int, default=3,  # 🛡️ Rate limited: default to 3 (was 20)
                   help="Max concurrent API calls")
    args = p.parse_args()
    
    # Override batch size if specified
    if args.batch_size:
        pipeline = OptimizedPipeline(
            batch_size=args.batch_size,
            max_api_concurrent=args.api_concurrent
        )
        asyncio.run(pipeline.run(args.src, args.selection, args.cap))
    else:
        main(args.src, args.selection, args.cap)


================================================================================


################################################################################
# File: city_clerk_documents/graph_json/debug/verbatim/verbatim_linking_report_01_09_2024.json
################################################################################

{
  "meeting_date": "01.09.2024",
  "timestamp": "2025-06-03T15:25:01.352489",
  "summary": {
    "total_transcripts": 9,
    "item_transcripts": 8,
    "public_comments": 1,
    "section_transcripts": 0
  },
  "transcripts": {
    "item_transcripts": [
      {
        "path": "city_clerk_documents/global/City Comissions 2024/Verbating Items/2024/01_09_2024 - Verbatim Transcripts - E-8.pdf",
        "filename": "01_09_2024 - Verbatim Transcripts - E-8.pdf",
        "meeting_date": "01.09.2024",
        "item_info_raw": "E-8",
        "item_codes": [
          "E-8"
        ],
        "section_codes": [],
        "transcript_type": "single_item",
        "page_count": 11,
        "text_excerpt": "CityCommission Meeting\nJanuary9, 2024\nAgendaItemE-8 - Ordinance oftheCityCommission providing foratextamendment totheCityof\nCoralGablesofficialZoningCode, amending Article11, \u201cSigns,\u201d Section11-107 \u201cRealEstate, ForSale, \nLeaseorRentalofPropertyorBuildings,\u201d toapplysameregulations tosignspertaining tothesale, lease, \norrentalofpropertyorbuildings inanyusedistrict.  Page1CityofCoralGablesCityCommission Meeting\nAgendaItemE-8\nJanuary9, 2024\nCityCommission Chambers\n405BiltmoreWay, CoralGables, FL\nCit"
      },
      {
        "path": "city_clerk_documents/global/City Comissions 2024/Verbating Items/2024/01_09_2024 - Verbatim Transcripts - E-5.pdf",
        "filename": "01_09_2024 - Verbatim Transcripts - E-5.pdf",
        "meeting_date": "01.09.2024",
        "item_info_raw": "E-5",
        "item_codes": [
          "E-5"
        ],
        "section_codes": [],
        "transcript_type": "single_item",
        "page_count": 8,
        "text_excerpt": "CityCommission Meeting\nJanuary9, 2024\nAgendaItemE-5 \u2013 Ordinance providingfortextamendments toofficialzoningcode\nAmending Article10 \u2013 \u201cParkingandAccess\u201d 1 [ Date] CityofCoralGablesCityCommission Meeting\nAgendaItemE-5\nJanuary9, 2024\nCityCommission Chambers\n405BiltmoreWay, CoralGables, FL\nCityCommission\nMayorVinceLago\nViceMayorRhondaAnderson\nCommissioner MelissaCastro\nCommissioner ArielFernandez\nCommissioner KirkMenendez\nCityStaff\nCityAttorney, CristinaSu\u00e1rez\nCityManager, PeterIglesias\nCityClerk, B"
      },
      {
        "path": "city_clerk_documents/global/City Comissions 2024/Verbating Items/2024/01_09_2024 - Verbatim Transcripts - E-7.pdf",
        "filename": "01_09_2024 - Verbatim Transcripts - E-7.pdf",
        "meeting_date": "01.09.2024",
        "item_info_raw": "E-7",
        "item_codes": [
          "E-7"
        ],
        "section_codes": [],
        "transcript_type": "single_item",
        "page_count": 5,
        "text_excerpt": "CityCommission Meeting\nJanuary9, 2024\nAgendaItemE-7 \u2013 Ordinance providingforatextamendment totheofficialzoningcode\nCreatingSection5-314 \u2013 \u201cWindowandHurricane Shutters\u201d 1 [ Date] CityofCoralGablesCityCommission Meeting\nAgendaItemE-7\nJanuary9, 2024\nCityCommission Chambers\n405BiltmoreWay, CoralGables, FL\nCityCommission\nMayorVinceLago\nViceMayorRhondaAnderson\nCommissioner MelissaCastro\nCommissioner ArielFernandez\nCommissioner KirkMenendez\nCityStaff\nCityAttorney, CristinaSu\u00e1rez\nCityManager, PeterIgles"
      },
      {
        "path": "city_clerk_documents/global/City Comissions 2024/Verbating Items/2024/01_09_2024 - Verbatim Transcripts - F-10.pdf",
        "filename": "01_09_2024 - Verbatim Transcripts - F-10.pdf",
        "meeting_date": "01.09.2024",
        "item_info_raw": "F-10",
        "item_codes": [
          "F-10"
        ],
        "section_codes": [],
        "transcript_type": "single_item",
        "page_count": 11,
        "text_excerpt": "CityCommission Meeting\nJanuary9, 2024\nAgendaItemF-10 \u2013 Resolution directingtheCityClerkandCityManagertoschedulebimonthly\nPresentation andprotocolceremonies1 [ Date] CityofCoralGablesCityCommission Meeting\nAgendaItemF-10\nJanuary9, 2024\nCityCommission Chambers\n405BiltmoreWay, CoralGables, FL\nCityCommission\nMayorVinceLago\nViceMayorRhondaAnderson\nCommissioner MelissaCastro\nCommissioner ArielFernandez\nCommissioner KirkMenendez\nCityStaff\nCityAttorney, CristinaSu\u00e1rez\nCityManager, PeterIglesias\nCityCler"
      },
      {
        "path": "city_clerk_documents/global/City Comissions 2024/Verbating Items/2024/01_09_2024 - Verbatim Transcripts - F-6.pdf",
        "filename": "01_09_2024 - Verbatim Transcripts - F-6.pdf",
        "meeting_date": "01.09.2024",
        "item_info_raw": "F-6",
        "item_codes": [
          "F-6"
        ],
        "section_codes": [],
        "transcript_type": "single_item",
        "page_count": 3,
        "text_excerpt": "CityCommission Meeting\nJanuary9, 2024\nAgendaItemF-6 \u2013 Discussion regarding thetreecanopyreplacement1 [ Date] CityofCoralGablesCityCommission Meeting\nAgendaItemF-6\nJanuary9, 2024\nCityCommission Chambers\n405BiltmoreWay, CoralGables, FL\nCityCommission\nMayorVinceLago\nViceMayorRhondaAnderson\nCommissioner MelissaCastro\nCommissioner ArielFernandez\nCommissioner KirkMenendez\nCityStaff\nCityAttorney, CristinaSu\u00e1rez\nCityManager, PeterIglesias\nCityClerk, BillyUrquia\nGreenspace Management Director, DeenaBell-"
      },
      {
        "path": "city_clerk_documents/global/City Comissions 2024/Verbating Items/2024/01_09_2024 - Verbatim Transcripts - F-2.pdf",
        "filename": "01_09_2024 - Verbatim Transcripts - F-2.pdf",
        "meeting_date": "01.09.2024",
        "item_info_raw": "F-2",
        "item_codes": [
          "F-2"
        ],
        "section_codes": [],
        "transcript_type": "single_item",
        "page_count": 4,
        "text_excerpt": "CityCommission Meeting\nJanuary9, 2024\nAgendaItemF-2 \u2013 Updateregarding PittmanPark1 [ Date] CityofCoralGablesCityCommission Meeting\nAgendaItemF-2\nJanuary9, 2024\nCityCommission Chambers\n405BiltmoreWay, CoralGables, FL\nCityCommission\nMayorVinceLago\nViceMayorRhondaAnderson\nCommissioner MelissaCastro\nCommissioner ArielFernandez\nCommissioner KirkMenendez\nCityStaff\nCityAttorney, CristinaSu\u00e1rez\nCityManager, PeterIglesias\nCityClerk, BillyUrquia\nPublicWorksDirector, HermesDiaz\nPublicSpeaker(s) \nAgendaItem"
      },
      {
        "path": "city_clerk_documents/global/City Comissions 2024/Verbating Items/2024/01_09_2024 - Verbatim Transcripts - F-5.pdf",
        "filename": "01_09_2024 - Verbatim Transcripts - F-5.pdf",
        "meeting_date": "01.09.2024",
        "item_info_raw": "F-5",
        "item_codes": [
          "F-5"
        ],
        "section_codes": [],
        "transcript_type": "single_item",
        "page_count": 5,
        "text_excerpt": "CityCommission Meeting\nJanuary9, 2024\nAgendaItemF-5 \u2013 Discussion regarding theChristmas decorations onMiracleMile1 [ Date] CityofCoralGablesCityCommission Meeting\nAgendaItemF-5\nJanuary9, 2024\nCityCommission Chambers\n405BiltmoreWay, CoralGables, FL\nCityCommission\nMayorVinceLago\nViceMayorRhondaAnderson\nCommissioner MelissaCastro\nCommissioner ArielFernandez\nCommissioner KirkMenendez\nCityStaff\nCityAttorney, CristinaSu\u00e1rez\nCityManager, PeterIglesias\nCityClerk, BillyUrquia\nEconomic Development Directo"
      },
      {
        "path": "city_clerk_documents/global/City Comissions 2024/Verbating Items/2024/01_09_2024 - Verbatim Transcripts - E-4.pdf",
        "filename": "01_09_2024 - Verbatim Transcripts - E-4.pdf",
        "meeting_date": "01.09.2024",
        "item_info_raw": "E-4",
        "item_codes": [
          "E-4"
        ],
        "section_codes": [],
        "transcript_type": "single_item",
        "page_count": 25,
        "text_excerpt": "CityCommission Meeting\nJanuary9, 2024\nAgendaItemE-4 - Ordinance oftheCityCommission amending St. Philip\u2019sSchoolsiteplanapproved\nunderOrdinance No. 3576toreplaceanexistingbuildingwithanewpre-Kbuildinglocatedat1109\nAndalusia Avenue, CoralGables, Florida; allotherconditions ofapprovalcontained inOrdinance No.  \n3576shallremainineffect.  Page1CityofCoralGablesCityCommission Meeting\nAgendaItemE-4\nJanuary9, 2024\nCityCommission Chambers\n405BiltmoreWay, CoralGables, FL\nCityCommission\nMayorVinceLago\nVice"
      }
    ],
    "public_comments": [
      {
        "path": "city_clerk_documents/global/City Comissions 2024/Verbating Items/2024/01_09_2024 - Verbatim Transcripts - Public Comment.pdf",
        "filename": "01_09_2024 - Verbatim Transcripts - Public Comment.pdf",
        "meeting_date": "01.09.2024",
        "item_info_raw": "Public Comment",
        "item_codes": [
          "PUBLIC_COMMENT"
        ],
        "section_codes": [],
        "transcript_type": "public_comment",
        "page_count": 9,
        "text_excerpt": "CityCommission Meeting\nJanuary9, 2024\nAgendaItemC \u2013 PublicComment1 [ Date] CityofCoralGablesCityCommission Meeting\nAgendaItemC\nJanuary9, 2024\nCityCommission Chambers\n405BiltmoreWay, CoralGables, FL\nCityCommission\nMayorVinceLago\nViceMayorRhondaAnderson\nCommissioner MelissaCastro\nCommissioner ArielFernandez\nCommissioner KirkMenendez\nCityStaff\nCityAttorney, CristinaSu\u00e1rez\nCityManager, PeterIglesias\nCityClerk, BillyUrquia\nCommunications Director, MarthaPantin\nPublicSpeaker(s) \nMichaelMaxwell\nTomWell"
      }
    ],
    "section_transcripts": []
  }
}


================================================================================


################################################################################
# File: scripts/supabase_clear_database.py
################################################################################

# File: scripts/supabase_clear_database.py

#!/usr/bin/env python3
"""
Database Clear Utility
======================

Safely clears Supabase database tables for the Misophonia Research system.
This script will delete all data from:
- research_documents table
- documents_chunks table

⚠️  WARNING: This operation is irreversible!
"""
from __future__ import annotations
import os
import sys
import logging
from typing import Optional

from dotenv import load_dotenv
from supabase import create_client

# Load environment variables
load_dotenv()

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")

if not SUPABASE_URL or not SUPABASE_KEY:
    sys.exit("❌ Missing SUPABASE_URL or SUPABASE_SERVICE_ROLE_KEY environment variables")

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s — %(levelname)s — %(message)s"
)
log = logging.getLogger(__name__)

def init_supabase():
    """Initialize Supabase client."""
    return create_client(SUPABASE_URL, SUPABASE_KEY)

def get_table_counts(sb) -> dict:
    """Get current row counts for all tables."""
    counts = {}
    
    try:
        # Count documents
        doc_res = sb.table("research_documents").select("id", count="exact").execute()
        counts["research_documents"] = doc_res.count or 0
        
        # Count chunks
        chunk_res = sb.table("documents_chunks").select("id", count="exact").execute()
        counts["documents_chunks"] = chunk_res.count or 0
        
        # Count chunks with embeddings
        embedded_res = sb.table("documents_chunks").select("id", count="exact").not_.is_("embedding", "null").execute()
        counts["chunks_with_embeddings"] = embedded_res.count or 0
        
    except Exception as e:
        log.error(f"Error getting table counts: {e}")
        return {}
    
    return counts

def confirm_deletion() -> bool:
    """Ask user for confirmation before deletion."""
    print("\n" + "="*60)
    print("⚠️  DATABASE CLEAR WARNING")
    print("="*60)
    print("This will permanently delete ALL data from:")
    print("  • research_documents table")
    print("  • documents_chunks table")
    print("  • All embeddings and metadata")
    print("\n❌ This operation CANNOT be undone!")
    print("="*60)
    
    response = input("\nType 'DELETE ALL DATA' to confirm (or anything else to cancel): ")
    return response.strip() == "DELETE ALL DATA"

def clear_table_batch(sb, table_name: str, batch_size: int = 1000) -> int:
    """Clear all rows from a specific table in batches to avoid timeouts."""
    log.info(f"Clearing table: {table_name} (batch size: {batch_size})")
    
    total_deleted = 0
    
    while True:
        try:
            # Get a batch of IDs to delete
            result = sb.table(table_name).select("id").limit(batch_size).execute()
            
            if not result.data or len(result.data) == 0:
                break
            
            ids_to_delete = [row["id"] for row in result.data]
            log.info(f"Deleting batch of {len(ids_to_delete)} rows from {table_name}")
            
            # Delete this batch
            delete_result = sb.table(table_name).delete().in_("id", ids_to_delete).execute()
            
            if hasattr(delete_result, 'error') and delete_result.error:
                log.error(f"Error deleting batch from {table_name}: {delete_result.error}")
                break
            
            batch_deleted = len(delete_result.data) if delete_result.data else 0
            total_deleted += batch_deleted
            log.info(f"✅ Deleted {batch_deleted} rows from {table_name} (total: {total_deleted})")
            
            # If we deleted fewer than the batch size, we're done
            if batch_deleted < batch_size:
                break
                
        except Exception as e:
            log.error(f"Exception deleting batch from {table_name}: {e}")
            break
    
    log.info(f"✅ Total deleted from {table_name}: {total_deleted}")
    return total_deleted

def clear_table(sb, table_name: str) -> int:
    """Clear all rows from a specific table."""
    return clear_table_batch(sb, table_name, batch_size=500)

def main():
    """Main function to clear the database."""
    print("🗑️  Misophonia Database Clear Utility")
    print("=" * 50)
    
    # Initialize Supabase
    sb = init_supabase()
    
    # Get current counts
    print("\n📊 Current database status:")
    counts = get_table_counts(sb)
    
    if not counts:
        print("❌ Could not retrieve database counts. Exiting.")
        return
    
    print(f"  • Documents: {counts['research_documents']:,}")
    print(f"  • Chunks: {counts['documents_chunks']:,}")
    print(f"  • Chunks with embeddings: {counts['chunks_with_embeddings']:,}")
    
    if counts['research_documents'] == 0 and counts['documents_chunks'] == 0:
        print("\n✅ Database is already empty!")
        return
    
    # Get confirmation
    if not confirm_deletion():
        print("\n✅ Operation cancelled. Database unchanged.")
        return
    
    print("\n🗑️  Starting database clear operation...")
    
    # Clear chunks first (has foreign key to documents)
    chunks_deleted = clear_table(sb, "documents_chunks")
    
    # Clear documents
    docs_deleted = clear_table(sb, "research_documents")
    
    # Verify deletion
    print("\n📊 Verifying deletion...")
    final_counts = get_table_counts(sb)
    
    if final_counts:
        print(f"  • Documents remaining: {final_counts['research_documents']:,}")
        print(f"  • Chunks remaining: {final_counts['documents_chunks']:,}")
        
        if final_counts['research_documents'] == 0 and final_counts['documents_chunks'] == 0:
            print("\n✅ Database successfully cleared!")
            print(f"  • Deleted {docs_deleted:,} documents")
            print(f"  • Deleted {chunks_deleted:,} chunks")
        else:
            print("\n⚠️  Some data may remain in the database.")
    else:
        print("❌ Could not verify deletion status.")

if __name__ == "__main__":
    main()


================================================================================


################################################################################
# File: city_clerk_documents/graph_json/debug/enhanced_linked_documents.json
################################################################################

{
  "ordinances": [
    {
      "path": "city_clerk_documents/global/City Comissions 2024/Ordinances/2024/2024-01 - 01_09_2024.pdf",
      "filename": "2024-01 - 01_09_2024.pdf",
      "document_number": "2024-01",
      "item_code": "E-1",
      "document_type": "Ordinance",
      "title": "A\nORDINANCE NO.",
      "parsed_data": {
        "document_type": "ordinance"
      }
    },
    {
      "path": "city_clerk_documents/global/City Comissions 2024/Ordinances/2024/2024-02 - 01_09_2024.pdf",
      "filename": "2024-02 - 01_09_2024.pdf",
      "document_number": "2024-02",
      "item_code": "E-2",
      "document_type": "Ordinance",
      "title": "A\nORDINANCE NO.",
      "parsed_data": {
        "document_type": "ordinance"
      }
    },
    {
      "path": "city_clerk_documents/global/City Comissions 2024/Ordinances/2024/2024-03 - 01_09_2024.pdf",
      "filename": "2024-03 - 01_09_2024.pdf",
      "document_number": "2024-03",
      "item_code": "E-3",
      "document_type": "Ordinance",
      "title": "A\nORDINANCE NO.",
      "parsed_data": {
        "document_type": "ordinance"
      }
    }
  ],
  "resolutions": [
    {
      "path": "city_clerk_documents/global/City Comissions 2024/Resolutions/2024/2024-01 - 01_09_2024.pdf",
      "filename": "2024-01 - 01_09_2024.pdf",
      "document_number": "2024-01",
      "item_code": "D-1",
      "document_type": "Resolution",
      "title": "A\nRESOLUTION NO.",
      "parsed_data": {
        "document_type": "resolution"
      }
    },
    {
      "path": "city_clerk_documents/global/City Comissions 2024/Resolutions/2024/2024-02 - 01_09_2024.pdf",
      "filename": "2024-02 - 01_09_2024.pdf",
      "document_number": "2024-02",
      "item_code": "D-2",
      "document_type": "Resolution",
      "title": "A\nRESOLUTION NO.",
      "parsed_data": {
        "document_type": "resolution"
      }
    },
    {
      "path": "city_clerk_documents/global/City Comissions 2024/Resolutions/2024/2024-03 - 01_09_2024.pdf",
      "filename": "2024-03 - 01_09_2024.pdf",
      "document_number": "2024-03",
      "item_code": "F-11",
      "document_type": "Resolution",
      "title": "A\nRESOLUTION NO.",
      "parsed_data": {
        "document_type": "resolution"
      }
    },
    {
      "path": "city_clerk_documents/global/City Comissions 2024/Resolutions/2024/2024-04 - 01_09_2024.pdf",
      "filename": "2024-04 - 01_09_2024.pdf",
      "document_number": "2024-04",
      "item_code": "E-9",
      "document_type": "Resolution",
      "title": "A\nRESOLUTION NO.",
      "parsed_data": {
        "document_type": "resolution"
      }
    },
    {
      "path": "city_clerk_documents/global/City Comissions 2024/Resolutions/2024/2024-05 - 01_09_2024.pdf",
      "filename": "2024-05 - 01_09_2024.pdf",
      "document_number": "2024-05",
      "item_code": "F-10",
      "document_type": "Resolution",
      "title": "A\nRESOLUTION NO.",
      "parsed_data": {
        "document_type": "resolution"
      }
    },
    {
      "path": "city_clerk_documents/global/City Comissions 2024/Resolutions/2024/2024-06 - 01_09_2024.pdf",
      "filename": "2024-06 - 01_09_2024.pdf",
      "document_number": "2024-06",
      "item_code": "H-1",
      "document_type": "Resolution",
      "title": "A\nRESOLUTION NO.",
      "parsed_data": {
        "document_type": "resolution"
      }
    },
    {
      "path": "city_clerk_documents/global/City Comissions 2024/Resolutions/2024/2024-07 - 01_09_2024.pdf",
      "filename": "2024-07 - 01_09_2024.pdf",
      "document_number": "2024-07",
      "item_code": "H-2",
      "document_type": "Resolution",
      "title": "A\nRESOLUTION NO.",
      "parsed_data": {
        "document_type": "resolution"
      }
    }
  ]
}


================================================================================


################################################################################
# File: requirements.txt
################################################################################

################################################################################
################################################################################
# Python dependencies for Misophonia Research RAG System

# Core dependencies
openai>=1.82.0
groq>=0.4.1
gremlinpython>=3.7.3
aiofiles>=24.1.0
python-dotenv>=1.0.0

# Graph Visualization dependencies
dash>=2.14.0
plotly>=5.17.0
networkx>=3.2.0
dash-table>=5.0.0
dash-cytoscape>=0.3.0

# Database and API dependencies
supabase>=2.0.0

# Optional for async progress bars
tqdm

# Data processing
pandas>=2.0.0
numpy

# PDF processing
PyPDF2>=3.0.0
unstructured[pdf]==0.10.30
pytesseract>=0.3.10
pdf2image>=1.16.3
docling
pdfminer.six>=20221105

# PDF processing with hyperlink support
PyMuPDF>=1.23.0

# Utilities
colorama>=0.4.6

# For concurrent processing
concurrent-log-handler>=0.9.20

# Async dependencies for optimized pipeline
aiohttp>=3.8.0

# Token counting for OpenAI rate limiting
tiktoken>=0.5.0


================================================================================

