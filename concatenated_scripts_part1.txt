# Concatenated Project Code - Part 1 of 3
# Generated: 2025-06-03 15:41:20
# Root Directory: /Users/gianmariatroiani/Documents/knologi̊/graph_database
================================================================================

# Directory Structure
################################################################################
├── .gitignore (939.0B, no ext)
├── city_clerk_documents/
│   ├── global copy/
│   │   ├── City Comissions 2024/
│   │   │   ├── Agendas/
│   │   │   │   ├── Agenda 01.23.2024.pdf (155.1KB, .pdf)
│   │   │   │   ├── Agenda 01.9.2024.pdf (151.1KB, .pdf)
│   │   │   │   ├── Agenda 02.13.2024.pdf (155.0KB, .pdf)
│   │   │   │   ├── Agenda 02.27.2024.pdf (152.9KB, .pdf)
│   │   │   │   ├── Agenda 03.12.2024.pdf (166.6KB, .pdf)
│   │   │   │   ├── Agenda 05.07.2024.pdf (167.6KB, .pdf)
│   │   │   │   ├── Agenda 05.21.2024.pdf (172.2KB, .pdf)
│   │   │   │   └── Agenda 06.11.2024.pdf (160.2KB, .pdf)
│   │   │   ├── ExportedFolderContents.zip (62.4MB, .zip)
│   │   │   ├── Ordinances/
│   │   │   │   ├── 2024/
│   │   │   │   │   ├── 2024-01 - 01_09_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-02 - 01_09_2024.pdf (15.1MB, .pdf)
│   │   │   │   │   ├── 2024-03 - 01_09_2024.pdf (11.1MB, .pdf)
│   │   │   │   │   ├── 2024-04 - 01_23_2024.pdf (2.3MB, .pdf)
│   │   │   │   │   ├── 2024-05 - 02_13_2024.pdf (2.0MB, .pdf)
│   │   │   │   │   ├── 2024-06 - 02_13_2024.pdf (1.9MB, .pdf)
│   │   │   │   │   ├── 2024-07 - 02_13_2024.pdf (6.5MB, .pdf)
│   │   │   │   │   ├── 2024-08 - 02_27_2024.pdf (2.5MB, .pdf)
│   │   │   │   │   ├── 2024-09 - 02_27_2024.pdf (1.6MB, .pdf)
│   │   │   │   │   ├── 2024-10 - 03_12_2024.pdf (3.7MB, .pdf)
│   │   │   │   │   ├── 2024-11 - 03_12_2024 - (As Amended).pdf (5.6MB, .pdf)
│   │   │   │   │   ├── 2024-11 - 03_12_2024.pdf (5.5MB, .pdf)
│   │   │   │   │   ├── 2024-12 - 03_12_2024.pdf (1.8MB, .pdf)
│   │   │   │   │   ├── 2024-13 - 03_12_2024.pdf (3.2MB, .pdf)
│   │   │   │   │   ├── 2024-14 - 05_07_2024.pdf (2.8MB, .pdf)
│   │   │   │   │   ├── 2024-15 - 05_07_2024.pdf (3.9MB, .pdf)
│   │   │   │   │   ├── 2024-16 - 05_07_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-17 - 05_07_2024.pdf (4.3MB, .pdf)
│   │   │   │   │   ├── 2024-18 - 05_21_2024.pdf (2.4MB, .pdf)
│   │   │   │   │   ├── 2024-19 - 05_21_2024.pdf (1.7MB, .pdf)
│   │   │   │   │   ├── 2024-20 - 05_21_2024.pdf (1.6MB, .pdf)
│   │   │   │   │   ├── 2024-21 - 06_11_2024.pdf (1.7MB, .pdf)
│   │   │   │   │   ├── 2024-22 - 06_11_2024.pdf (2.0MB, .pdf)
│   │   │   │   │   ├── 2024-23 - 06_11_2024.pdf (1.9MB, .pdf)
│   │   │   │   │   ├── 2024-24 - 06_11_2024.pdf (2.1MB, .pdf)
│   │   │   │   │   └── 2024-25 - 06_11_2024.pdf (1.3MB, .pdf)
│   │   │   │   ├── [EXCLUDED] 1 items: .DS_Store (excluded file)
│   │   │   ├── Resolutions/
│   │   │   │   ├── 2024/
│   │   │   │   │   ├── 2024-01 - 01_09_2024.pdf (448.9KB, .pdf)
│   │   │   │   │   ├── 2024-02 - 01_09_2024.pdf (451.9KB, .pdf)
│   │   │   │   │   ├── 2024-03 - 01_09_2024.pdf (867.3KB, .pdf)
│   │   │   │   │   ├── 2024-04 - 01_09_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-05 - 01_09_2024.pdf (936.9KB, .pdf)
│   │   │   │   │   ├── 2024-06 - 01_09_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-07 - 01_09_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-08 - 01_23_2024.pdf (457.9KB, .pdf)
│   │   │   │   │   ├── 2024-09 - 01_23_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-10 - 01_23_2024.pdf (454.8KB, .pdf)
│   │   │   │   │   ├── 2024-10 - 03_12_2024.pdf (3.7MB, .pdf)
│   │   │   │   │   ├── 2024-100 - 05_21_2024.pdf (3.0MB, .pdf)
│   │   │   │   │   ├── 2024-101 - 05_21_2024.pdf (947.4KB, .pdf)
│   │   │   │   │   ├── 2024-102 - 05_21_2024.pdf (466.0KB, .pdf)
│   │   │   │   │   ├── 2024-103 - 05_21_2024.pdf (991.8KB, .pdf)
│   │   │   │   │   ├── 2024-104 - 05_21_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-105 - 05_21_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-106 - 05_21_2024.pdf (2.4MB, .pdf)
│   │   │   │   │   ├── 2024-107 - 05_21_2024.pdf (6.0MB, .pdf)
│   │   │   │   │   ├── 2024-108 - 05_21_2024.pdf (2.1MB, .pdf)
│   │   │   │   │   ├── 2024-109 - 05_21_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-11 - 01_23_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-110 - 05_21_2024.pdf (921.5KB, .pdf)
│   │   │   │   │   ├── 2024-111 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-112 - 05_21_2024.pdf (6.2MB, .pdf)
│   │   │   │   │   ├── 2024-113 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-114 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-115 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-116 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-117 - 05_21_2024.pdf (6.4MB, .pdf)
│   │   │   │   │   ├── 2024-118 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-119 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-12 - 01_23_2024.pdf (588.3KB, .pdf)
│   │   │   │   │   ├── 2024-120 - 05_21_2024.pdf (6.3MB, .pdf)
│   │   │   │   │   ├── 2024-121 - 05_21_2024.pdf (6.4MB, .pdf)
│   │   │   │   │   ├── 2024-122 - 05_21_2024.pdf (6.8MB, .pdf)
│   │   │   │   │   ├── 2024-123 - 05_21_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-124 - 05_21_2024.pdf (11.2MB, .pdf)
│   │   │   │   │   ├── 2024-125 - 05_21_2024.pdf (776.0KB, .pdf)
│   │   │   │   │   ├── 2024-126 - 05_21_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-127 - 05_21_2024.pdf (8.1MB, .pdf)
│   │   │   │   │   ├── 2024-129 - 06_11_2024.pdf (1.3MB, .pdf)
│   │   │   │   │   ├── 2024-13 - 01_23_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-130 - 06_11_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-131 - 06_11_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-132 - 06_11_2024.pdf (458.0KB, .pdf)
│   │   │   │   │   ├── 2024-133 - 06_11_2024 -As Amended.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-134 - 06_11_2024.pdf (884.8KB, .pdf)
│   │   │   │   │   ├── 2024-135 - 06_11_2024.pdf (1022.1KB, .pdf)
│   │   │   │   │   ├── 2024-136 - 06_11_2024.pdf (537.4KB, .pdf)
│   │   │   │   │   ├── 2024-137 - 06_11_2024.pdf (1.7MB, .pdf)
│   │   │   │   │   ├── 2024-138 - 06_11_2024.pdf (1.7MB, .pdf)
│   │   │   │   │   ├── 2024-139 - 06_11_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-14 - 01_23_2024.pdf (996.6KB, .pdf)
│   │   │   │   │   ├── 2024-140 - 06_11_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-141 - 06_11_2024.pdf (13.2MB, .pdf)
│   │   │   │   │   ├── 2024-142 - 06_11_2024.pdf (790.5KB, .pdf)
│   │   │   │   │   ├── 2024-143 -06_11_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-15 - 01_23_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-16 - 01_23_2024.pdf (1.8MB, .pdf)
│   │   │   │   │   ├── 2024-17 - 01_23_2024.pdf (488.0KB, .pdf)
│   │   │   │   │   ├── 2024-18 - 01_23_2024.pdf (849.1KB, .pdf)
│   │   │   │   │   ├── 2024-19 - 02_19_2024.pdf (1019.5KB, .pdf)
│   │   │   │   │   ├── 2024-20 - 02_13_2024.pdf (824.5KB, .pdf)
│   │   │   │   │   ├── 2024-21 - 02_13_2024.pdf (502.6KB, .pdf)
│   │   │   │   │   ├── 2024-22 - 02_13_2024.pdf (450.6KB, .pdf)
│   │   │   │   │   ├── 2024-23 - 02_13_2024.pdf (447.7KB, .pdf)
│   │   │   │   │   ├── 2024-24 - 02_13_2024.pdf (464.1KB, .pdf)
│   │   │   │   │   ├── 2024-25 - 02_13_2024.pdf (458.3KB, .pdf)
│   │   │   │   │   ├── 2024-26 - 02_13_2024.pdf (465.7KB, .pdf)
│   │   │   │   │   ├── 2024-27 - 02_13_2024.pdf (754.2KB, .pdf)
│   │   │   │   │   ├── 2024-28 - 02_13_2024.pdf (761.7KB, .pdf)
│   │   │   │   │   ├── 2024-29 - 02_13_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-30 - 02_13_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-31 - 02_13_2024.pdf (936.6KB, .pdf)
│   │   │   │   │   ├── 2024-32 - 02_13_2024.pdf (1.9MB, .pdf)
│   │   │   │   │   ├── 2024-33 - 02_13_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-34 - 02_13_2024.pdf (1.9MB, .pdf)
│   │   │   │   │   ├── 2024-35 - 02_27_2024.pdf (480.7KB, .pdf)
│   │   │   │   │   ├── 2024-36 - 02_27_2024.pdf (1.3MB, .pdf)
│   │   │   │   │   ├── 2024-37 - 02_27_2024.pdf (940.3KB, .pdf)
│   │   │   │   │   ├── 2024-38 - 02_27_2024.pdf (829.6KB, .pdf)
│   │   │   │   │   ├── 2024-39 - 02_27_2024.pdf (816.5KB, .pdf)
│   │   │   │   │   ├── 2024-41 - 02_27_2024.pdf (833.1KB, .pdf)
│   │   │   │   │   ├── 2024-42 - 02_27_2024.pdf (473.4KB, .pdf)
│   │   │   │   │   ├── 2024-43 - 02_27_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-44 - 02_27_2024.pdf (1.8MB, .pdf)
│   │   │   │   │   ├── 2024-45 - 03_12_2024.pdf (1002.1KB, .pdf)
│   │   │   │   │   ├── 2024-46 - 03_12_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-47 - 03_12_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-48 - 03_12_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-49 - 03_12_2024.pdf (899.6KB, .pdf)
│   │   │   │   │   ├── 2024-50 - 03_12_2024.pdf (462.1KB, .pdf)
│   │   │   │   │   ├── 2024-51 - 03_12_2024.pdf (538.1KB, .pdf)
│   │   │   │   │   ├── 2024-52 - 03_12_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-53 - 03_12_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-54 - 03_12_2024.pdf (2.0MB, .pdf)
│   │   │   │   │   ├── 2024-55 - 03_12_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-56 - 03_12_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-57 - 03_12_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-58 - 03_12_2024.pdf (775.8KB, .pdf)
│   │   │   │   │   ├── 2024-60 - 03_12_2024.pdf (2.3MB, .pdf)
│   │   │   │   │   ├── 2024-61 - 04_16_2024.pdf (6.4MB, .pdf)
│   │   │   │   │   ├── 2024-62 - 04_16_2024.pdf (2.7MB, .pdf)
│   │   │   │   │   ├── 2024-63 -  04_16_2024.pdf (832.3KB, .pdf)
│   │   │   │   │   ├── 2024-64 - 04_16_2024.pdf (452.6KB, .pdf)
│   │   │   │   │   ├── 2024-65 - 04_16_2024.pdf (894.5KB, .pdf)
│   │   │   │   │   ├── 2024-66 - 04_16_2024.pdf (446.6KB, .pdf)
│   │   │   │   │   ├── 2024-67 - 04_16_2024.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 2024-68 - 04_16_2024.pdf (5.6MB, .pdf)
│   │   │   │   │   ├── 2024-69- -04_16_2024.pdf (443.1KB, .pdf)
│   │   │   │   │   ├── 2024-70 - 04_16_2024.pdf (878.9KB, .pdf)
│   │   │   │   │   ├── 2024-71 - 04_16_2024.pdf (951.8KB, .pdf)
│   │   │   │   │   ├── 2024-72 - 04_16_2024.pdf (821.7KB, .pdf)
│   │   │   │   │   ├── 2024-73 - 04_16_2024.pdf (810.8KB, .pdf)
│   │   │   │   │   ├── 2024-74 - 04_16_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-75 - 04_16_2024.pdf (1.6MB, .pdf)
│   │   │   │   │   ├── 2024-76 - 04_16_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-77 - 04_16_2024.pdf (1013.9KB, .pdf)
│   │   │   │   │   ├── 2024-78 - 04_16_2024.pdf (1007.5KB, .pdf)
│   │   │   │   │   ├── 2024-79 - 04_16_2024.pdf (997.7KB, .pdf)
│   │   │   │   │   ├── 2024-80 - 04_16_2024.pdf (525.7KB, .pdf)
│   │   │   │   │   ├── 2024-81 - 04_16_2024.pdf (923.2KB, .pdf)
│   │   │   │   │   ├── 2024-82 - 04_16_2024.pdf (473.6KB, .pdf)
│   │   │   │   │   ├── 2024-83 - 04_16_2024.pdf (915.3KB, .pdf)
│   │   │   │   │   ├── 2024-84 - 05_07_2024.pdf (992.7KB, .pdf)
│   │   │   │   │   ├── 2024-85 - 05_07_2024.pdf (2.0MB, .pdf)
│   │   │   │   │   ├── 2024-86 - 05_07_2024.pdf (503.1KB, .pdf)
│   │   │   │   │   ├── 2024-87 - 05_07_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-88 - 05_07_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-89 - 05_07_2024.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 2024-90 - 05_07_2024.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 2024-91 - 05_07_2024.pdf (1.8MB, .pdf)
│   │   │   │   │   ├── 2024-92 - 05_07_2024.pdf (452.4KB, .pdf)
│   │   │   │   │   ├── 2024-93 - 05_07_2024.pdf (1014.3KB, .pdf)
│   │   │   │   │   ├── 2024-94 - 05_05_2024.pdf (1.3MB, .pdf)
│   │   │   │   │   ├── 2024-95 - 05_07_2024.pdf (974.5KB, .pdf)
│   │   │   │   │   ├── 2024-96 - 05_07_2024.pdf (1.5MB, .pdf)
│   │   │   │   │   ├── 2024-97 - 05_07_2024.pdf (1023.3KB, .pdf)
│   │   │   │   │   ├── 2024-98 - 05_07_2024.pdf (2.2MB, .pdf)
│   │   │   │   │   └── 2024-99 - 05_07_2024.pdf (792.7KB, .pdf)
│   │   │   │   ├── [EXCLUDED] 1 items: .DS_Store (excluded file)
│   │   │   ├── Verbating Items/
│   │   │   │   ├── 2024/
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - E-4.pdf (2.4MB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - E-5.pdf (735.6KB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - E-7.pdf (306.9KB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - E-8.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - F-10.pdf (925.3KB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - F-2.pdf (288.2KB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - F-5.pdf (360.6KB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - F-6.pdf (212.8KB, .pdf)
│   │   │   │   │   ├── 01_09_2024 - Verbatim Transcripts - Public Comment.pdf (762.7KB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - C- Public Comment.pdf (1.3MB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - E-3 and E-4.pdf (2.4MB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - E-5.pdf (290.7KB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - F-1.pdf (855.5KB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - F-5.pdf (3.7MB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - F-7.pdf (768.6KB, .pdf)
│   │   │   │   │   ├── 01_23_2024 - Verbatim Transcripts - H-1.pdf (1.5MB, .pdf)
│   │   │   │   │   ├── 02_13_2024 - Meeting Minutes - Public.pdf (363.5KB, .pdf)
│   │   │   │   │   ├── 02_13_2024 - Verbatim Transcripts - F-12.pdf (7.2MB, .pdf)
│   │   │   │   │   ├── 02_13_2024 - Verbatim Transcripts - H-1.pdf (570.8KB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - E-1.pdf (4.2MB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - E-2.pdf (281.3KB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - E-3.pdf (1.2MB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - F-10.pdf (2.5MB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - F-12.pdf (448.2KB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - H-1.pdf (794.2KB, .pdf)
│   │   │   │   │   ├── 02_27_2024 - Verbatim Transcripts - Public Comment.pdf (439.4KB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - E-1.pdf (124.3KB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - E-2.pdf (487.9KB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - E-4 and E-11.pdf (1.1MB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - F-1.pdf (416.0KB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - F-2.pdf (1.5MB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - F-4 and F-5 and F-11 and F-12.pdf (1.4MB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - H-1.pdf (121.5KB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - K - Discussion Items.pdf (285.0KB, .pdf)
│   │   │   │   │   ├── 03_12_2024 - Verbatim Transcripts - Public Comment.pdf (412.4KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - 2-1 AND 2-2.pdf (652.3KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - E-11.pdf (365.8KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - E-2.pdf (369.8KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - E-4.pdf (410.2KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - E-5 E-6 E-7 E-8 E-9 E-10.pdf (3.6MB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-1.pdf (751.9KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-10.pdf (1.3MB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-11.pdf (2.4MB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-12.pdf (2.8MB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-15.pdf (255.0KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-5.pdf (205.0KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-8.pdf (961.1KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - F-9.pdf (330.1KB, .pdf)
│   │   │   │   │   ├── 04_16_2024 - Verbatim Transcripts - Public Comment.pdf (1.5MB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - 2-1.pdf (1.7MB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - E-1.pdf (4.6MB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - E-10.pdf (128.5KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - E-11.pdf (559.6KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - E-12.pdf (156.1KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - E-8.pdf (288.5KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - E-9.pdf (263.4KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - F-1.pdf (561.1KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - F-13.pdf (151.9KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - F-16.pdf (782.8KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - F-3.pdf (352.6KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - F-9.pdf (736.0KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - H-1.pdf (455.3KB, .pdf)
│   │   │   │   │   ├── 05_07_2024 - Verbatim Transcripts - Public Comment.pdf (1.6MB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - D-3.pdf (539.3KB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - E-10.pdf (629.7KB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - E-3.pdf (350.4KB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - F-2.pdf (1.7MB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - F-3.pdf (2.4MB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - F-4.pdf (3.4MB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - F-5.pdf (1.0MB, .pdf)
│   │   │   │   │   ├── 06_11_2024 - Verbatim Transcripts - F-7 and F-10.pdf (1.6MB, .pdf)
│   │   │   │   │   └── 06_11_2024 - Verbatim Transcripts - Public Comment.pdf (513.4KB, .pdf)
│   │   │   │   ├── [EXCLUDED] 1 items: .DS_Store (excluded file)
│   │   │   ├── [EXCLUDED] 1 items: .DS_Store (excluded file)
│   │   ├── [EXCLUDED] 1 items: .DS_Store (excluded file)
│   ├── graph_json/
│   │   ├── Agenda 01.9.2024_docling_extracted.json (34.4KB, .json)
│   │   ├── Agenda 01.9.2024_extracted.json (34.4KB, .json)
│   │   ├── Agenda 01.9.2024_full_text.txt (16.1KB, .txt)
│   │   ├── Agenda 01.9.2024_ontology.json (34.8KB, .json)
│   │   ├── debug/
│   │   │   ├── enhanced_linked_documents.json (3.7KB, .json)
│   │   │   ├── linking_report_01_09_2024.json (1.5KB, .json)
│   │   │   ├── llm_response_ordinance_2024-01_cleaned.txt (16.0B, .txt)
│   │   │   ├── llm_response_ordinance_2024-01_raw.txt (16.0B, .txt)
│   │   │   ├── llm_response_ordinance_2024-02_cleaned.txt (16.0B, .txt)
│   │   │   ├── llm_response_ordinance_2024-02_raw.txt (16.0B, .txt)
│   │   │   ├── llm_response_ordinance_2024-03_cleaned.txt (16.0B, .txt)
│   │   │   ├── llm_response_ordinance_2024-03_raw.txt (16.0B, .txt)
│   │   │   ├── llm_response_resolution_2024-01_cleaned.txt (16.0B, .txt)
│   │   │   ├── llm_response_resolution_2024-01_raw.txt (16.0B, .txt)
│   │   │   ├── llm_response_resolution_2024-02_cleaned.txt (16.0B, .txt)
│   │   │   ├── llm_response_resolution_2024-02_raw.txt (16.0B, .txt)
│   │   │   ├── llm_response_resolution_2024-03_cleaned.txt (17.0B, .txt)
│   │   │   ├── llm_response_resolution_2024-03_raw.txt (17.0B, .txt)
│   │   │   ├── llm_response_resolution_2024-04_cleaned.txt (16.0B, .txt)
│   │   │   ├── llm_response_resolution_2024-04_raw.txt (16.0B, .txt)
│   │   │   ├── llm_response_resolution_2024-05_cleaned.txt (17.0B, .txt)
│   │   │   ├── llm_response_resolution_2024-05_raw.txt (17.0B, .txt)
│   │   │   ├── llm_response_resolution_2024-06_cleaned.txt (16.0B, .txt)
│   │   │   ├── llm_response_resolution_2024-06_raw.txt (16.0B, .txt)
│   │   │   ├── llm_response_resolution_2024-07_cleaned.txt (16.0B, .txt)
│   │   │   ├── llm_response_resolution_2024-07_raw.txt (16.0B, .txt)
│   │   │   └── verbatim/
│   │   │       └── verbatim_linking_report_01_09_2024.json (9.1KB, .json)
│   │   ├── pipeline_report_20250603_170358.json (1.2KB, .json)
│   │   ├── pipeline_report_20250603_171806.json (5.3KB, .json)
│   │   ├── pipeline_report_20250603_174911.json (5.3KB, .json)
│   │   ├── pipeline_report_20250603_181819.json (1.8KB, .json)
│   │   ├── pipeline_report_20250603_183222.json (1.2KB, .json)
│   │   ├── pipeline_report_20250603_185650.json (1.2KB, .json)
│   │   └── pipeline_report_20250603_192623.json (6.8KB, .json)
│   ├── [EXCLUDED] 2 items: .DS_Store (excluded file), global (excluded dir)
├── config.py (1.7KB, .py)
├── debug/
│   ├── entities_response_chunk_0.txt (5.4KB, .txt)
│   ├── entities_response_chunk_1.txt (4.1KB, .txt)
│   ├── extracted_items.json (9.9KB, .json)
│   ├── meeting_info_parsed.json (471.0B, .json)
│   └── meeting_info_response.txt (467.0B, .txt)
├── graph_clear_database.py (963.0B, .py)
├── graph_visualizer.py (23.6KB, .py)
├── ontology_model.txt (6.6KB, .txt)
├── ontology_modelv2.txt (10.2KB, .txt)
├── repository_directory_structure.txt (5.7KB, .txt)
├── requirements.txt (1018.0B, .txt)
├── run_city_clerk_pipeline.sh (440.0B, .sh)
├── run_graph_pipeline.sh (1.3KB, .sh)
├── run_graph_visualizer.sh (3.6KB, .sh)
├── scripts/
│   ├── graph_pipeline.py (20.9KB, .py)
│   ├── graph_stages/
│   │   ├── __init__.py (647.0B, .py)
│   │   ├── agenda_graph_builder.py (52.2KB, .py)
│   │   ├── agenda_ontology_extractor.py (25.5KB, .py)
│   │   ├── agenda_pdf_extractor.py (13.4KB, .py)
│   │   ├── cosmos_db_client.py (10.2KB, .py)
│   │   ├── document_linker.py (12.9KB, .py)
│   │   ├── enhanced_document_linker.py (18.3KB, .py)
│   │   ├── ontology_extractor.py (38.6KB, .py)
│   │   ├── pdf_extractor.py (6.5KB, .py)
│   │   └── verbatim_transcript_linker.py (13.9KB, .py)
│   ├── pipeline_modular_optimized.py (12.7KB, .py)
│   ├── rag_local_web_app.py (18.4KB, .py)
│   ├── stages/
│   │   ├── __init__.py (378.0B, .py)
│   │   ├── acceleration_utils.py (3.8KB, .py)
│   │   ├── chunk_text.py (18.6KB, .py)
│   │   ├── db_upsert.py (8.9KB, .py)
│   │   ├── embed_vectors.py (27.0KB, .py)
│   │   ├── extract_clean.py (21.7KB, .py)
│   │   └── llm_enrich.py (5.9KB, .py)
│   └── supabase_clear_database.py (6.0KB, .py)
├── [EXCLUDED] 6 items: .DS_Store (excluded file), .env (excluded file), .git (excluded dir)
    ... and 3 more excluded items


================================================================================


# File Index - Which Files Are in Which Parts
################################################################################

## Part 1 (9 files):
  - scripts/graph_stages/agenda_graph_builder.py
  - scripts/stages/extract_clean.py
  - scripts/graph_stages/verbatim_transcript_linker.py
  - scripts/graph_stages/document_linker.py
  - debug/extracted_items.json
  - scripts/graph_stages/pdf_extractor.py
  - scripts/stages/acceleration_utils.py
  - graph_clear_database.py
  - debug/meeting_info_parsed.json

## Part 2 (11 files):
  - scripts/graph_stages/ontology_extractor.py
  - scripts/graph_pipeline.py
  - scripts/graph_stages/enhanced_document_linker.py
  - scripts/graph_stages/agenda_pdf_extractor.py
  - scripts/graph_stages/cosmos_db_client.py
  - scripts/stages/db_upsert.py
  - scripts/stages/llm_enrich.py
  - config.py
  - city_clerk_documents/graph_json/debug/linking_report_01_09_2024.json
  - scripts/graph_stages/__init__.py
  - scripts/stages/__init__.py

## Part 3 (9 files):
  - scripts/stages/embed_vectors.py
  - scripts/graph_stages/agenda_ontology_extractor.py
  - scripts/stages/chunk_text.py
  - scripts/rag_local_web_app.py
  - scripts/pipeline_modular_optimized.py
  - city_clerk_documents/graph_json/debug/verbatim/verbatim_linking_report_01_09_2024.json
  - scripts/supabase_clear_database.py
  - city_clerk_documents/graph_json/debug/enhanced_linked_documents.json
  - requirements.txt


================================================================================


################################################################################
# File: scripts/graph_stages/agenda_graph_builder.py
################################################################################

# File: scripts/graph_stages/agenda_graph_builder.py

"""
Enhanced Agenda Graph Builder - RICH VERSION
Builds comprehensive graph representation from LLM-extracted agenda ontology.
"""
import logging
from typing import Dict, List, Optional
from pathlib import Path
import hashlib
import json
import calendar
import re

from .cosmos_db_client import CosmosGraphClient

log = logging.getLogger('pipeline_debug.graph_builder')


class AgendaGraphBuilder:
    """Build comprehensive graph representation from rich agenda ontology."""
    
    def __init__(self, cosmos_client: CosmosGraphClient, upsert_mode: bool = True):
        self.cosmos = cosmos_client
        self.upsert_mode = upsert_mode
        self.entity_id_cache = {}  # Cache for entity IDs
        self.partition_value = 'demo'  # Partition value property
        
        # Track statistics
        self.stats = {
            'nodes_created': 0,
            'nodes_updated': 0,
            'edges_created': 0,
            'edges_skipped': 0
        }
    
    @staticmethod
    def normalize_item_code(code: str) -> str:
        """Normalize item codes to consistent format for matching."""
        if not code:
            return code
        
        # Log original code for debugging
        original = code
        
        # Apply normalization patterns
        patterns = [
            (r'^([A-Z])\.?-?(\d+)\.?$', r'\1-\2'),     # A.-1. -> A-1
            (r'^(\d+)\.?-?(\d+)\.?$', r'\1-\2'),       # 1.-1. -> 1-1
            (r'^([A-Z]\d+)$', r'\1'),                   # E1 -> E1 (no change)
        ]
        
        for pattern, replacement in patterns:
            match = re.match(pattern, code)
            if match:
                code = re.sub(pattern, replacement, code)
                break
        else:
            # First, extract valid code pattern if input is messy
            code_match = re.match(r'^([A-Z][-.]?\d+)', code)
            if code_match:
                code = code_match.group(1)
            
            # Remove all dots and ensure consistent format
            code = code.rstrip('.')
            code = re.sub(r'([A-Z])\.(-)', r'\1\2', code)
            code = re.sub(r'([A-Z])\.(\d)', r'\1-\2', code)
            code = re.sub(r'([A-Z])(\d)', r'\1-\2', code)
            code = code.replace('.', '')
            
            # Ensure format is always "E-9" not "E9"
            if re.match(r'^[A-Z]\d+$', code):
                code = f"{code[0]}-{code[1:]}"
        
        if original != code:
            log.debug(f"Normalized '{original}' -> '{code}'")
        
        return code
    
    @staticmethod
    def ensure_us_date_format(date_str: str) -> str:
        """Ensure date is in US format MM-DD-YYYY with dashes."""
        # Handle different input formats
        if '.' in date_str:
            # Format: 01.23.2024 -> 01-23-2024
            return date_str.replace('.', '-')
        elif '/' in date_str:
            # Format: 01/23/2024 -> 01-23-2024
            return date_str.replace('/', '-')
        elif re.match(r'^\d{4}-\d{2}-\d{2}$', date_str):
            # ISO format: 2024-01-23 -> 01-23-2024
            parts = date_str.split('-')
            return f"{parts[1]}-{parts[2]}-{parts[0]}"
        else:
            # Already in correct format or unknown
            return date_str

    async def build_graph(self, ontology_file: Path, linked_docs: Optional[Dict] = None, upsert: bool = True) -> Dict:
        """Build graph from ontology file - main entry point."""
        # Load ontology
        with open(ontology_file, 'r', encoding='utf-8') as f:
            ontology = json.load(f)
        
        return await self.build_graph_from_ontology(ontology, ontology_file, linked_docs)

    async def build_graph_from_ontology(self, ontology: Dict, source_path: Path, linked_docs: Optional[Dict] = None) -> Dict:
        """Build comprehensive graph representation from rich ontology."""
        log.info(f"🔨 Starting enhanced graph build for {source_path.name}")
        log.info(f"🔧 Upsert mode: {'ENABLED' if self.upsert_mode else 'DISABLED'}")
        
        # Reset statistics
        self.stats = {
            'nodes_created': 0,
            'nodes_updated': 0,
            'edges_created': 0,
            'edges_skipped': 0
        }
        
        try:
            graph_data = {
                'nodes': {},
                'edges': [],
                'statistics': {}
            }
            
            # Store agenda structure for reference
            self.current_agenda_structure = ontology.get('agenda_structure', [])
            
            # Store ontology for reference
            self.current_ontology = ontology
            
            # CRITICAL: Ensure meeting date is in US format
            meeting_date_original = ontology['meeting_date']
            meeting_date_us = self.ensure_us_date_format(meeting_date_original)
            meeting_info = ontology['meeting_info']
            
            log.info(f"📅 Meeting date: {meeting_date_original} -> {meeting_date_us}")
            
            # 1. Create Meeting node as the root
            meeting_id = f"meeting-{meeting_date_us}"
            await self._create_meeting_node(meeting_date_us, meeting_info, source_path.name)
            log.info(f"✅ Created meeting node: {meeting_id}")
            
            # 1.5 Create Date node and link to meeting
            try:
                date_id = await self._create_date_node(meeting_date_original, meeting_id)
                graph_data['nodes'][date_id] = {
                    'type': 'Date',
                    'date': meeting_date_original
                }
            except Exception as e:
                log.error(f"Failed to create date node: {e}")
            
            graph_data['nodes'][meeting_id] = {
                'type': 'Meeting',
                'date': meeting_date_us,
                'info': meeting_info
            }
            
            # 2. Create nodes for officials present  
            await self._create_official_nodes(meeting_info, meeting_id)
            
            # 3. Process sections and agenda items
            section_count = 0
            item_count = 0
            
            sections = ontology.get('sections', [])
            log.info(f"📑 Processing {len(sections)} sections")
            
            for section_idx, section in enumerate(sections):
                try:
                    section_count += 1
                    section_id = f"section-{meeting_date_us}-{section_idx}"
                    
                    # Create Section node
                    await self._create_section_node(section_id, section, section_idx)
                    log.info(f"✅ Created section {section_idx}: {section.get('section_name', 'Unknown')}")
                    
                    graph_data['nodes'][section_id] = {
                        'type': 'Section',
                        'name': section['section_name'],
                        'order': section_idx
                    }
                    
                    # Link section to meeting
                    if await self.cosmos.create_edge_if_not_exists(
                        from_id=meeting_id,
                        to_id=section_id,
                        edge_type='HAS_SECTION',
                        properties={'order': section_idx}
                    ):
                        self.stats['edges_created'] += 1
                    else:
                        self.stats['edges_skipped'] += 1
                    
                    # Process items in section
                    previous_item_id = None
                    items = section.get('items', [])
                    
                    for item_idx, item in enumerate(items):
                        try:
                            if not item.get('item_code'):
                                log.warning(f"Skipping item without code in section {section['section_name']}")
                                continue
                                
                            item_count += 1
                            # Normalize the item code
                            normalized_code = self.normalize_item_code(item['item_code'])
                            # Use US date format for item ID
                            item_id = f"item-{meeting_date_us}-{normalized_code}"
                            
                            log.info(f"Creating item: {item_id} (from code: {item['item_code']})")
                            
                            # Create enhanced AgendaItem node
                            await self._create_enhanced_agenda_item_node(item_id, item, section)
                            
                            graph_data['nodes'][item_id] = {
                                'type': 'AgendaItem',
                                'code': normalized_code,
                                'original_code': item['item_code'],
                                'title': item.get('title', 'Unknown')
                            }
                            
                            # Link item to section
                            if await self.cosmos.create_edge_if_not_exists(
                                from_id=section_id,
                                to_id=item_id,
                                edge_type='CONTAINS_ITEM',
                                properties={'order': item_idx}
                            ):
                                self.stats['edges_created'] += 1
                            else:
                                self.stats['edges_skipped'] += 1
                            
                            # Create sequential relationships
                            if previous_item_id:
                                if await self.cosmos.create_edge_if_not_exists(
                                    from_id=previous_item_id,
                                    to_id=item_id,
                                    edge_type='FOLLOWS',
                                    properties={'sequence': item_idx}
                                ):
                                    self.stats['edges_created'] += 1
                                else:
                                    self.stats['edges_skipped'] += 1
                            
                            previous_item_id = item_id
                            
                            # Create rich relationships for this item
                            await self._create_item_relationships(item, item_id, meeting_date_us)
                            
                            # Create URL nodes and relationships
                            await self._create_url_relationships(item, item_id)
                                
                        except Exception as e:
                            log.error(f"Failed to process item {item.get('item_code', 'unknown')}: {e}")
                            
                except Exception as e:
                    log.error(f"Failed to process section {section.get('section_name', 'unknown')}: {e}")
            
            # 4. Create entity nodes from extracted entities
            entity_count = await self._create_entity_nodes(ontology.get('entities', []), meeting_id)
            
            # 5. Create relationships from ontology
            relationship_count = 0
            for rel in ontology.get('relationships', []):
                try:
                    await self._create_ontology_relationship(rel, meeting_date_us)
                    relationship_count += 1
                except Exception as e:
                    log.error(f"Failed to create relationship: {e}")
            
            # Initialize verbatim count
            verbatim_count = 0
            
            # 6. Process linked documents if available
            if linked_docs:
                await self.process_linked_documents(linked_docs, meeting_id, meeting_date_us)
                
                # Process verbatim transcripts if available
                if "verbatim_transcripts" in linked_docs:
                    verbatim_count = await self.process_verbatim_transcripts(
                        linked_docs["verbatim_transcripts"], 
                        meeting_id, 
                        meeting_date_us
                    )
            
            # Update statistics
            graph_data['statistics'] = {
                'sections': section_count,
                'items': item_count, 
                'entities': entity_count,
                'relationships': relationship_count,
                'meeting_date': meeting_date_us,
                'verbatim_transcripts': verbatim_count if verbatim_count else 0
            }
            
            log.info(f"🎉 Enhanced graph build complete for {source_path.name}")
            log.info(f"   📊 Statistics:")
            log.info(f"      - Nodes created: {self.stats['nodes_created']}")
            log.info(f"      - Nodes updated: {self.stats['nodes_updated']}")
            log.info(f"      - Edges created: {self.stats['edges_created']}")
            log.info(f"      - Edges skipped: {self.stats['edges_skipped']}")
            log.info(f"   - Sections: {section_count}")
            log.info(f"   - Items: {item_count}")
            log.info(f"   - Entities: {entity_count}")
            log.info(f"   - Relationships: {relationship_count}")
            
            return graph_data
            
        except Exception as e:
            log.error(f"CRITICAL ERROR in build_graph_from_ontology: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    async def _create_meeting_node(self, meeting_date: str, meeting_info: Dict, source_file: str = None) -> str:
        """Create or update Meeting node with comprehensive metadata."""
        meeting_id = f"meeting-{meeting_date}"
        
        # Handle case where meeting_info might be a list (API response error)
        if isinstance(meeting_info, list):
            log.error(f"meeting_info is a list instead of dict: {meeting_info}")
            # Use default values
            meeting_info = {
                'type': 'Regular Meeting',
                'time': '5:30 PM',
                'location': 'City Commission Chambers',
                'commissioners': [],
                'officials': {}
            }
        
        # Handle location - could be string or dict
        location = meeting_info.get('location', 'City Commission Chambers')
        if isinstance(location, dict):
            location_str = f"{location.get('name', 'City Commission Chambers')}"
            if location.get('address'):
                location_str += f" - {location['address']}"
        else:
            location_str = str(location) if location else "City Commission Chambers"
        
        properties = {
            'nodeType': 'Meeting',
            'date': meeting_date,
            'type': meeting_info.get('type', 'Regular Meeting'),
            'time': meeting_info.get('time', '5:30 PM'),
            'location': location_str
        }
        
        if source_file:
            properties['source_file'] = source_file
        
        # Use upsert instead of create
        if self.upsert_mode:
            created = await self.cosmos.upsert_vertex('Meeting', meeting_id, properties)
            if created:
                self.stats['nodes_created'] += 1
                log.info(f"✅ Created Meeting node: {meeting_id}")
            else:
                self.stats['nodes_updated'] += 1
                log.info(f"📝 Updated Meeting node: {meeting_id}")
        else:
            await self.cosmos.create_vertex('Meeting', meeting_id, properties)
            self.stats['nodes_created'] += 1
            log.info(f"✅ Created Meeting node: {meeting_id}")
        
        return meeting_id
    
    async def _create_date_node(self, date_str: str, meeting_id: str) -> str:
        """Create a Date node and link it to the meeting."""
        from datetime import datetime
        
        # Parse date from MM.DD.YYYY format
        parts = date_str.split('.')
        if len(parts) != 3:
            log.error(f"Invalid date format: {date_str}")
            return None
            
        month, day, year = int(parts[0]), int(parts[1]), int(parts[2])
        
        # Create consistent date ID in ISO format
        date_id = f"date-{year:04d}-{month:02d}-{day:02d}"
        
        # Check if date already exists
        if await self.cosmos.vertex_exists(date_id):
            log.info(f"Date {date_id} already exists")
            # Still create the relationship
            await self.cosmos.create_edge(
                from_id=meeting_id,
                to_id=date_id,
                edge_type='OCCURRED_ON',
                properties={'primary_date': True}
            )
            return date_id
        
        # Get day of week
        date_obj = datetime(year, month, day)
        day_of_week = date_obj.strftime('%A')
        
        # Create date node
        properties = {
            'nodeType': 'Date',
            'full_date': date_str,
            'year': year,
            'month': month,
            'day': day,
            'quarter': (month - 1) // 3 + 1,
            'month_name': calendar.month_name[month],
            'day_of_week': day_of_week,
            'iso_date': f'{year:04d}-{month:02d}-{day:02d}'
        }
        
        await self.cosmos.create_vertex('Date', date_id, properties)
        log.info(f"✅ Created Date node: {date_id}")
        
        # Create relationship: Meeting -> OCCURRED_ON -> Date
        await self.cosmos.create_edge(
            from_id=meeting_id,
            to_id=date_id,
            edge_type='OCCURRED_ON',
            properties={'primary_date': True}
        )
        
        return date_id
    
    async def _create_section_node(self, section_id: str, section: Dict, order: int) -> str:
        """Create or update Section node."""
        properties = {
            'nodeType': 'Section',
            'title': section.get('section_name', 'Unknown'),
            'type': section.get('section_type', 'OTHER'),
            'description': section.get('description', ''),
            'order': order
        }
        
        if self.upsert_mode:
            created = await self.cosmos.upsert_vertex('Section', section_id, properties)
            if created:
                self.stats['nodes_created'] += 1
            else:
                self.stats['nodes_updated'] += 1
        else:
            if await self.cosmos.vertex_exists(section_id):
                log.info(f"Section {section_id} already exists, skipping creation")
                return section_id
            await self.cosmos.create_vertex('Section', section_id, properties)
            self.stats['nodes_created'] += 1
        
        return section_id
    
    async def _create_enhanced_agenda_item_node(self, item_id: str, item: Dict, section: Dict) -> str:
        """Create or update AgendaItem node with rich metadata from LLM extraction."""
        # Store both original and normalized codes
        original_code = item.get('item_code', '')
        normalized_code = self.normalize_item_code(original_code)
        
        properties = {
            'nodeType': 'AgendaItem',
            'code': normalized_code,
            'original_code': original_code,
            'title': item.get('title', 'Unknown'),
            'type': item.get('item_type', 'Item'),
            'section': section.get('section_name', 'Unknown'),
            'section_type': section.get('section_type', 'other')
        }
        
        # Add enhanced details from LLM extraction
        if item.get('description'):
            properties['description'] = item['description'][:500]  # Limit length
        
        if item.get('document_reference'):
            properties['document_reference'] = item['document_reference']
        
        # Add sponsors as JSON array
        if item.get('sponsors'):
            properties['sponsors_json'] = json.dumps(item['sponsors'])
        
        # Add departments as JSON array  
        if item.get('departments'):
            properties['departments_json'] = json.dumps(item['departments'])
        
        # Add actions as JSON array
        if item.get('actions'):
            properties['actions_json'] = json.dumps(item['actions'])
        
        # Add stakeholders as JSON array
        if item.get('stakeholders'):
            properties['stakeholders_json'] = json.dumps(item['stakeholders'])
        
        # Add URLs as JSON array
        if item.get('urls'):
            properties['urls_json'] = json.dumps(item['urls'])
            properties['has_urls'] = True
        
        if self.upsert_mode:
            created = await self.cosmos.upsert_vertex('AgendaItem', item_id, properties)
            if created:
                self.stats['nodes_created'] += 1
            else:
                self.stats['nodes_updated'] += 1
        else:
            await self.cosmos.create_vertex('AgendaItem', item_id, properties)
            self.stats['nodes_created'] += 1
        
        return item_id
    
    async def _create_official_nodes(self, meeting_info: Dict, meeting_id: str):
        """Create nodes for city officials and link to meeting."""
        officials = meeting_info.get('officials', {})
        commissioners = meeting_info.get('commissioners', [])
        
        # Create official nodes
        for role, name in officials.items():
            if name and name != 'null':
                person_id = await self._ensure_person_node(name, role.replace('_', ' ').title())
                await self.cosmos.create_edge_if_not_exists(
                    from_id=person_id,
                    to_id=meeting_id,
                    edge_type='ATTENDED',
                    properties={'role': role.replace('_', ' ').title()}
                )
        
        # Create commissioner nodes
        for idx, commissioner in enumerate(commissioners):
            if commissioner and commissioner != 'null':
                person_id = await self._ensure_person_node(commissioner, 'Commissioner')
                await self.cosmos.create_edge_if_not_exists(
                    from_id=person_id,
                    to_id=meeting_id,
                    edge_type='ATTENDED',
                    properties={'role': 'Commissioner', 'seat': idx + 1}
                )
    
    async def _create_entity_nodes(self, entities: List[Dict], meeting_id: str) -> int:
        """Create nodes for all extracted entities."""
        entity_count = 0
        
        for entity in entities:
            try:
                entity_type = entity.get('type', 'unknown')
                entity_name = entity.get('name', '')
                entity_role = entity.get('role', '')
                entity_context = entity.get('context', '')
                
                if not entity_name:
                    continue
                
                if entity_type == 'person':
                    person_id = await self._ensure_person_node(entity_name, entity_role)
                    await self.cosmos.create_edge_if_not_exists(
                        from_id=person_id,
                        to_id=meeting_id,
                        edge_type='MENTIONED_IN',
                        properties={
                            'context': entity_context[:100],
                            'role': entity_role
                        }
                    )
                    entity_count += 1
                    
                elif entity_type == 'organization':
                    org_id = await self._ensure_organization_node(entity_name, entity_role)
                    await self.cosmos.create_edge_if_not_exists(
                        from_id=org_id,
                        to_id=meeting_id,
                        edge_type='MENTIONED_IN',
                        properties={
                            'context': entity_context[:100],
                            'org_type': entity_role
                        }
                    )
                    entity_count += 1
                    
                elif entity_type == 'department':
                    dept_id = await self._ensure_department_node(entity_name)
                    await self.cosmos.create_edge_if_not_exists(
                        from_id=dept_id,
                        to_id=meeting_id,
                        edge_type='INVOLVED_IN',
                        properties={'context': entity_context[:100]}
                    )
                    entity_count += 1
                    
                elif entity_type == 'location':
                    loc_id = await self._ensure_location_node(entity_name, entity_context)
                    await self.cosmos.create_edge_if_not_exists(
                        from_id=loc_id,
                        to_id=meeting_id,
                        edge_type='REFERENCED_IN',
                        properties={'context': entity_context[:100]}
                    )
                    entity_count += 1
                    
            except Exception as e:
                log.error(f"Failed to create entity node for {entity}: {e}")
        
        return entity_count
    
    async def _create_item_relationships(self, item: Dict, item_id: str, meeting_date: str):
        """Create rich relationships for agenda items."""
        
        # Sponsor relationships
        for sponsor in item.get('sponsors', []):
            try:
                person_id = await self._ensure_person_node(sponsor, 'Commissioner')
                await self.cosmos.create_edge_if_not_exists(
                    from_id=person_id,
                    to_id=item_id,
                    edge_type='SPONSORS',
                    properties={'role': 'sponsor'}
                )
                self.stats['edges_created'] += 1
            except Exception as e:
                log.error(f"Failed to create sponsor relationship: {e}")
        
        # Department relationships
        for dept in item.get('departments', []):
            try:
                dept_id = await self._ensure_department_node(dept)
                await self.cosmos.create_edge_if_not_exists(
                    from_id=dept_id,
                    to_id=item_id,
                    edge_type='RESPONSIBLE_FOR',
                    properties={'role': 'responsible_department'}
                )
                self.stats['edges_created'] += 1
            except Exception as e:
                log.error(f"Failed to create department relationship: {e}")
        
        # Stakeholder relationships  
        for stakeholder in item.get('stakeholders', []):
            try:
                org_id = await self._ensure_organization_node(stakeholder, 'Stakeholder')
                await self.cosmos.create_edge_if_not_exists(
                    from_id=org_id,
                    to_id=item_id,
                    edge_type='INVOLVED_IN',
                    properties={'role': 'stakeholder'}
                )
                self.stats['edges_created'] += 1
            except Exception as e:
                log.error(f"Failed to create stakeholder relationship: {e}")
        
        # Action relationships
        for action in item.get('actions', []):
            try:
                action_id = await self._ensure_action_node(action)
                await self.cosmos.create_edge_if_not_exists(
                    from_id=item_id,
                    to_id=action_id,
                    edge_type='REQUIRES_ACTION',
                    properties={'action_type': action}
                )
                self.stats['edges_created'] += 1
            except Exception as e:
                log.error(f"Failed to create action relationship: {e}")
    
    async def _create_url_relationships(self, item: Dict, item_id: str):
        """Create URL nodes and link to agenda items."""
        for url in item.get('urls', []):
            try:
                url_id = await self._ensure_url_node(url)
                await self.cosmos.create_edge_if_not_exists(
                    from_id=item_id,
                    to_id=url_id,
                    edge_type='HAS_URL',
                    properties={'url_type': 'document_link'}
                )
                self.stats['edges_created'] += 1
            except Exception as e:
                log.error(f"Failed to create URL relationship: {e}")
    
    async def _create_ontology_relationship(self, rel: Dict, meeting_date: str):
        """Create relationship from ontology data."""
        try:
            source = rel.get('source', '')
            target = rel.get('target', '')
            relationship = rel.get('relationship', '')
            source_type = rel.get('source_type', '')
            target_type = rel.get('target_type', '')
            
            # Determine source and target IDs based on type
            if source_type == 'person':
                source_id = await self._ensure_person_node(source, 'Participant')
            elif source_type == 'department':
                source_id = await self._ensure_department_node(source)
            elif source_type == 'organization':
                source_id = await self._ensure_organization_node(source, 'Organization')
            else:
                log.warning(f"Unknown source type: {source_type}")
                return
            
            if target_type == 'agenda_item':
                # Normalize target agenda item code
                normalized_target = self.normalize_item_code(target)
                target_id = f"item-{meeting_date}-{normalized_target}"
            else:
                log.warning(f"Unknown target type: {target_type}")
                return
            
            # Create the relationship
            await self.cosmos.create_edge_if_not_exists(
                from_id=source_id,
                to_id=target_id,
                edge_type=relationship.upper(),
                properties={
                    'source_type': source_type,
                    'target_type': target_type
                }
            )
            
        except Exception as e:
            log.error(f"Failed to create ontology relationship: {e}")
    
    async def _ensure_person_node(self, name: str, role: str) -> str:
        """Create or retrieve person node with upsert support."""
        clean_name = name.strip()
        # Clean the ID by removing invalid characters
        cleaned_id_part = clean_name.lower().replace(' ', '-').replace('.', '').replace("'", '').replace('"', '').replace('/', '-')
        person_id = f"person-{cleaned_id_part}"
        
        # Check cache first
        if person_id in self.entity_id_cache:
            return person_id
        
        # Check if exists in database
        if await self.cosmos.vertex_exists(person_id):
            self.entity_id_cache[person_id] = True
            return person_id
        
        # Create new person
        properties = {
            'nodeType': 'Person',
            'name': clean_name,
            'roles': role
        }
        
        if self.upsert_mode:
            created = await self.cosmos.upsert_vertex('Person', person_id, properties)
            if created:
                self.stats['nodes_created'] += 1
            else:
                self.stats['nodes_updated'] += 1
        else:
            if not await self.cosmos.vertex_exists(person_id):
                await self.cosmos.create_vertex('Person', person_id, properties)
                self.stats['nodes_created'] += 1
        
        self.entity_id_cache[person_id] = True
        return person_id
    
    async def _ensure_organization_node(self, name: str, org_type: str) -> str:
        """Create or retrieve organization node."""
        # Clean the ID
        cleaned_org_name = name.lower().replace(' ', '-').replace('.', '').replace("'", '').replace('"', '').replace('/', '-').replace(',', '')
        org_id = f"org-{cleaned_org_name}"
        
        if org_id in self.entity_id_cache:
            return org_id
        
        if await self.cosmos.vertex_exists(org_id):
            self.entity_id_cache[org_id] = True
            return org_id
        
        properties = {
            'nodeType': 'Organization',
            'name': name,
            'type': org_type
        }
        
        await self.cosmos.create_vertex('Organization', org_id, properties)
        self.entity_id_cache[org_id] = True
        self.stats['nodes_created'] += 1
        return org_id
    
    async def _ensure_department_node(self, name: str) -> str:
        """Create or retrieve department node."""
        cleaned_dept_name = name.lower().replace(' ', '-').replace('.', '').replace("'", '').replace('"', '').replace('/', '-')
        dept_id = f"dept-{cleaned_dept_name}"
        
        if dept_id in self.entity_id_cache:
            return dept_id
        
        if await self.cosmos.vertex_exists(dept_id):
            self.entity_id_cache[dept_id] = True
            return dept_id
        
        properties = {
            'nodeType': 'Department', 
            'name': name,
            'type': 'CityDepartment'
        }
        
        await self.cosmos.create_vertex('Department', dept_id, properties)
        self.entity_id_cache[dept_id] = True
        self.stats['nodes_created'] += 1
        return dept_id
    
    async def _ensure_location_node(self, name: str, context: str = '') -> str:
        """Create or retrieve location node."""
        cleaned_loc_name = name.lower().replace(' ', '-').replace('.', '').replace("'", '').replace('"', '').replace('/', '-').replace(',', '')
        loc_id = f"location-{cleaned_loc_name}"
        
        if loc_id in self.entity_id_cache:
            return loc_id
        
        if await self.cosmos.vertex_exists(loc_id):
            self.entity_id_cache[loc_id] = True
            return loc_id
        
        properties = {
            'nodeType': 'Location',
            'name': name,
            'context': context[:200] if context else '',
            'type': 'Location'
        }
        
        await self.cosmos.create_vertex('Location', loc_id, properties)
        self.entity_id_cache[loc_id] = True
        self.stats['nodes_created'] += 1
        return loc_id
    
    async def _ensure_action_node(self, action: str) -> str:
        """Create or retrieve action node."""
        cleaned_action = action.lower().replace(' ', '-').replace('.', '')
        action_id = f"action-{cleaned_action}"
        
        if action_id in self.entity_id_cache:
            return action_id
        
        if await self.cosmos.vertex_exists(action_id):
            self.entity_id_cache[action_id] = True
            return action_id
        
        properties = {
            'nodeType': 'Action',
            'name': action,
            'type': 'RequiredAction'
        }
        
        await self.cosmos.create_vertex('Action', action_id, properties)
        self.entity_id_cache[action_id] = True
        self.stats['nodes_created'] += 1
        return action_id
    
    async def _ensure_url_node(self, url: str) -> str:
        """Create or retrieve URL node."""
        url_hash = hashlib.md5(url.encode()).hexdigest()[:12]
        url_id = f"url-{url_hash}"
        
        if url_id in self.entity_id_cache:
            return url_id
        
        if await self.cosmos.vertex_exists(url_id):
            self.entity_id_cache[url_id] = True
            return url_id
        
        properties = {
            'nodeType': 'URL',
            'url': url,
            'domain': url.split('/')[2] if '://' in url else 'unknown',
            'type': 'Hyperlink'
        }
        
        await self.cosmos.create_vertex('URL', url_id, properties)
        self.entity_id_cache[url_id] = True
        self.stats['nodes_created'] += 1
        return url_id

    async def process_linked_documents(self, linked_docs: Dict, meeting_id: str, meeting_date: str):
        """Process and create nodes for linked documents."""
        log.info("📄 Processing linked documents...")
        
        created_count = 0
        missing_items = []
        
        for doc_type, docs in linked_docs.items():
            # Skip verbatim_transcripts as they're processed separately
            if doc_type == "verbatim_transcripts":
                log.info(f"⏭️  Skipping {doc_type} - processed separately")
                continue
                
            if not docs:
                continue
                
            log.info(f"\n   📂 Processing {len(docs)} {doc_type}")
            
            for doc in docs:
                # Validate item_code before processing
                item_code = doc.get('item_code')
                
                # Enhanced matching for documents without item codes
                if not doc.get("item_code"):
                    doc_number = doc.get("document_number", "")
                    log.info(f"🔍 Searching for document {doc_number} in agenda structure")
                    
                    # Search through all sections and items for matching document reference
                    found_item = None
                    for section in self.current_ontology.get("sections", []):
                        for item in section.get("items", []):
                            if item.get("document_reference") == doc_number:
                                doc["item_code"] = item.get("item_code")
                                log.info(f"✅ Found matching agenda item by document reference: {doc['item_code']}")
                                found_item = item
                                break
                        if found_item:
                            break
                
                item_code = doc.get('item_code')
                if item_code and len(item_code) > 10:  # Suspiciously long
                    log.error(f"Invalid item code detected: {item_code[:50]}...")
                    # Try to extract a valid code
                    code_match = re.match(r'^([A-Z]-?\d+)', item_code)
                    if code_match:
                        item_code = code_match.group(1)
                        doc['item_code'] = item_code
                        log.info(f"Extracted valid code: {item_code}")
                    else:
                        log.error(f"Could not extract valid code, skipping document {doc.get('document_number')}")
                        continue
                
                # Use the singular form for logging
                doc_type_singular = doc_type[:-1] if doc_type.endswith('s') else doc_type
                
                if doc_type in ['ordinances', 'resolutions']:
                    log.info(f"\n   Processing {doc_type_singular} {doc.get('document_number', 'unknown')}")
                    log.info(f"      Item code: {doc.get('item_code', 'MISSING')}")
                    
                    # Create document node
                    doc_id = await self._create_document_node(doc, doc_type, meeting_date)
                    
                    if doc_id:
                        created_count += 1
                        log.info(f"      ✅ Created document node: {doc_id}")
                        
                        # Link to meeting
                        await self.cosmos.create_edge(
                            from_id=doc_id,
                            to_id=meeting_id,
                            edge_type='PRESENTED_AT',
                            properties={'date': meeting_date}
                        )
                        
                        # Try to link to agenda item if item_code exists
                        item_code = doc.get('item_code')
                        if item_code:
                            # Log the normalization process
                            log.debug(f"Original item code: '{item_code}'")
                            normalized_code = self.normalize_item_code(item_code)
                            log.debug(f"Normalized item code: '{normalized_code}'")
                            
                            item_id = f"item-{meeting_date}-{normalized_code}"
                            log.info(f"Looking for agenda item: {item_id}")
                            
                            # Check if agenda item exists
                            if await self.cosmos.vertex_exists(item_id):
                                log.info(f"✅ Found agenda item: {item_id}")
                                await self.cosmos.create_edge(
                                    from_id=item_id,
                                    to_id=doc_id,
                                    edge_type='REFERENCES_DOCUMENT',
                                    properties={'document_type': doc_type_singular}
                                )
                                log.info(f"      🔗 Linked to agenda item: {item_id}")
                            else:
                                # Try alternative formats
                                alt_ids = [
                                    f"item-{meeting_date}-E-9",
                                    f"item-{meeting_date}-E9",
                                    f"item-{meeting_date}-E.-9.",
                                    f"item-{meeting_date}-E.-9"
                                ]
                                
                                found = False
                                for alt_id in alt_ids:
                                    if await self.cosmos.vertex_exists(alt_id):
                                        log.info(f"✅ Found agenda item with alternative ID: {alt_id}")
                                        item_id = alt_id
                                        found = True
                                        await self.cosmos.create_edge(
                                            from_id=item_id,
                                            to_id=doc_id,
                                            edge_type='REFERENCES_DOCUMENT',
                                            properties={'document_type': doc_type_singular}
                                        )
                                        log.info(f"      🔗 Linked to agenda item: {alt_id}")
                                        break
                                
                                if not found:
                                    # Try to find by document number
                                    doc_num = doc.get('document_number', '')
                                    
                                    # Search all agenda items for matching document reference
                                    if hasattr(self, 'current_agenda_structure'):
                                        for section in self.current_agenda_structure:  # Need to store this
                                            for item in section.get('items', []):
                                                if item.get('document_reference') == doc_num:
                                                    # Found matching item by document number
                                                    item_code = self.normalize_item_code(item['item_code'])
                                                    item_id = f"item-{meeting_date}-{item_code}"
                                                    log.info(f"✅ Found item by document reference: {item_id}")
                                                    await self.cosmos.create_edge(
                                                        from_id=item_id,
                                                        to_id=doc_id,
                                                        edge_type='REFERENCES_DOCUMENT',
                                                        properties={'document_type': doc_type_singular}
                                                    )
                                                    log.info(f"      🔗 Linked to agenda item via document reference: {item_id}")
                                                    found = True
                                                    break
                                            if found:
                                                break
                                    
                                    if not found:
                                        log.warning(f"❌ Agenda item not found: {item_id} or alternatives")
                                        missing_items.append({
                                            'document_number': doc.get('document_number'),
                                            'item_code': item_code,
                                            'normalized_code': normalized_code,
                                            'expected_item_id': item_id,
                                            'document_type': doc_type_singular
                                        })
                        else:
                            log.warning(f"      ⚠️  No item_code found for {doc.get('document_number')}")
        
        log.info(f"📄 Document processing complete: {created_count} documents created")
        if missing_items:
            log.warning(f"⚠️  {len(missing_items)} documents could not be linked to agenda items")
        
        return missing_items

    async def _create_document_node(self, doc_info: Dict, doc_type: str, meeting_date: str) -> str:
        """Create or update an Ordinance or Resolution node."""
        doc_number = doc_info.get('document_number', 'unknown')
        
        # Use the document type from doc_info if available, otherwise use the passed type
        actual_doc_type = doc_info.get('document_type', doc_type)
        
        # Ensure consistency in ID generation
        if actual_doc_type.lower() == 'resolution':
            doc_id = f"resolution-{doc_number}"
            node_type = 'Resolution'
        else:
            doc_id = f"ordinance-{doc_number}"
            node_type = 'Ordinance'
        
        # Get full title without truncation
        title = doc_info.get('title', '')
        if not title and doc_info.get('parsed_data', {}).get('title'):
            title = doc_info['parsed_data']['title']
        
        if title is None:
            title = f"Untitled {actual_doc_type.capitalize()} {doc_number}"
            log.warning(f"No title found for {actual_doc_type} {doc_number}, using default")
        
        properties = {
            'nodeType': node_type,
            'document_number': doc_number,
            'full_title': title,
            'title': title[:200] if len(title) > 200 else title,
            'document_type': actual_doc_type.capitalize(),
            'meeting_date': meeting_date
        }
        
        # Add parsed metadata
        parsed_data = doc_info.get('parsed_data', {})
        
        if parsed_data.get('date_passed'):
            properties['date_passed'] = parsed_data['date_passed']
        
        if parsed_data.get('agenda_item'):
            properties['agenda_item'] = parsed_data['agenda_item']
        
        # Add vote details as JSON
        if parsed_data.get('vote_details'):
            properties['vote_details'] = json.dumps(parsed_data['vote_details'])
        
        # Add signatories
        if parsed_data.get('signatories', {}).get('mayor'):
            properties['mayor_signature'] = parsed_data['signatories']['mayor']
        
        if self.upsert_mode:
            created = await self.cosmos.upsert_vertex(node_type, doc_id, properties)
            if created:
                self.stats['nodes_created'] += 1
                log.info(f"✅ Created document node: {doc_id}")
            else:
                self.stats['nodes_updated'] += 1
                log.info(f"📝 Updated document node: {doc_id}")
        else:
            await self.cosmos.create_vertex(node_type, doc_id, properties)
            self.stats['nodes_created'] += 1
        
        # Create edges for sponsors
        if parsed_data.get('motion', {}).get('moved_by'):
            person_id = await self._ensure_person_node(
                parsed_data['motion']['moved_by'], 
                'Commissioner'
            )
            if await self.cosmos.create_edge_if_not_exists(person_id, doc_id, 'MOVED'):
                self.stats['edges_created'] += 1
            else:
                self.stats['edges_skipped'] += 1
        
        return doc_id

    async def process_verbatim_transcripts(self, transcripts: Dict, meeting_id: str, meeting_date: str) -> int:
        """Process and create nodes for verbatim transcript documents."""
        log.info("🎤 Processing verbatim transcripts...")
        
        # Handle empty results gracefully
        if not any(transcripts.values()):
            log.info("🎤 No verbatim transcripts found for this meeting")
            return 0
        
        created_count = 0
        
        # Process item-specific transcripts
        for transcript in transcripts.get("item_transcripts", []):
            transcript_id = await self._create_transcript_node(transcript, meeting_date, "item")
            if transcript_id:
                created_count += 1
                
                # Link to meeting
                await self.cosmos.create_edge(
                    from_id=transcript_id,
                    to_id=meeting_id,
                    edge_type='TRANSCRIBED_AT',
                    properties={'date': meeting_date}
                )
                
                # Link to specific agenda items
                for item_code in transcript.get("item_codes", []):
                    normalized_code = self.normalize_item_code(item_code)
                    item_id = f"item-{meeting_date}-{normalized_code}"
                    
                    if await self.cosmos.vertex_exists(item_id):
                        await self.cosmos.create_edge(
                            from_id=item_id,
                            to_id=transcript_id,
                            edge_type='HAS_TRANSCRIPT',
                            properties={'transcript_type': 'verbatim'}
                        )
                        log.info(f"   🔗 Linked transcript to item: {item_id}")
                    else:
                        log.warning(f"   ⚠️  Agenda item not found: {item_id}")
        
        # Process public comment transcripts
        for transcript in transcripts.get("public_comments", []):
            transcript_id = await self._create_transcript_node(transcript, meeting_date, "public_comment")
            if transcript_id:
                created_count += 1
                
                # Link to meeting
                await self.cosmos.create_edge(
                    from_id=transcript_id,
                    to_id=meeting_id,
                    edge_type='PUBLIC_COMMENT_AT',
                    properties={'date': meeting_date}
                )
        
        # Process section transcripts
        for transcript in transcripts.get("section_transcripts", []):
            transcript_id = await self._create_transcript_node(transcript, meeting_date, "section")
            if transcript_id:
                created_count += 1
                
                # Link to meeting
                await self.cosmos.create_edge(
                    from_id=transcript_id,
                    to_id=meeting_id,
                    edge_type='TRANSCRIBED_AT',
                    properties={'date': meeting_date}
                )
        
        log.info(f"🎤 Transcript processing complete: {created_count} transcripts created")
        return created_count

    async def _create_transcript_node(self, transcript_info: Dict, meeting_date: str, transcript_type: str) -> str:
        """Create or update a Transcript node."""
        filename = transcript_info.get("filename", "unknown")
        
        # Create unique ID based on filename
        transcript_id = f"transcript-{meeting_date}-{filename.replace('.pdf', '').replace(' ', '-').lower()}"
        
        properties = {
            'nodeType': 'Transcript',
            'filename': filename,
            'transcript_type': transcript_type,
            'meeting_date': meeting_date,
            'page_count': transcript_info.get('page_count', 0),
            'item_info': transcript_info.get('item_info_raw', ''),
            'items_covered': json.dumps(transcript_info.get('item_codes', [])),
            'sections_covered': json.dumps(transcript_info.get('section_codes', [])),
            'text_excerpt': transcript_info.get('text_excerpt', '')[:500]
        }
        
        if self.upsert_mode:
            created = await self.cosmos.upsert_vertex('Transcript', transcript_id, properties)
            if created:
                self.stats['nodes_created'] += 1
                log.info(f"✅ Created transcript node: {transcript_id}")
            else:
                self.stats['nodes_updated'] += 1
                log.info(f"📝 Updated transcript node: {transcript_id}")
        else:
            await self.cosmos.create_vertex('Transcript', transcript_id, properties)
            self.stats['nodes_created'] += 1
        
        return transcript_id


================================================================================


################################################################################
# File: scripts/stages/extract_clean.py
################################################################################

# File: scripts/stages/extract_clean.py

#!/usr/bin/env python3
"""
Stage 1-2 — *Extract PDF → clean text → logical sections*  
Optimized version with concurrent processing support.
Outputs `<n>.json` (+ a pretty `.txt`) in `city_clerk_documents/{json,txt}/`.
This file is **fully self-contained** – no import from `stages.common`.
"""
from __future__ import annotations

import json, logging, os, pathlib, re, sys
from collections import Counter
from datetime import datetime
from textwrap import dedent
from typing import Any, Dict, List, Sequence, Optional
import asyncio
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing as mp

# ─── helpers formerly in common.py ──────────────────────────────────
_LATIN1_REPLACEMENTS = {  # windows-1252 fallback
    0x82: "‚",
    0x83: "ƒ",
    0x84: "„",
    0x85: "…",
    0x86: "†",
    0x87: "‡",
    0x88: "ˆ",
    0x89: "‰",
    0x8A: "Š",
    0x8B: "‹",
    0x8C: "Œ",
    0x8E: "Ž",
    0x91: "'",
    0x92: "'",
    0x93: '"',
    0x94: '"',
    0x95: "•",
    0x96: "–",
    0x97: "—",
    0x98: "˜",
    0x99: "™",
    0x9A: "š",
    0x9B: "›",
    0x9C: "œ",
    0x9E: "ž",
    0x9F: "Ÿ",
}
_TRANSLATE_LAT1 = str.maketrans(_LATIN1_REPLACEMENTS)
def latin1_scrub(txt:str)->str: return txt.translate(_TRANSLATE_LAT1)

_WS_RE = re.compile(r"[ \t]+\n")
def normalize_ws(txt:str)->str:
    return re.sub(r"[ \t]{2,}", " ", _WS_RE.sub("\n", txt)).strip()

def pct_ascii_letters(txt:str)->float:
    letters=sum(ch.isascii() and ch.isalpha() for ch in txt)
    return letters/max(1,len(txt))

def needs_ocr(txt:str)->bool:
    return (not txt.strip()) or ("\x00" in txt) or (pct_ascii_letters(txt)<0.15)

def scrub_nuls(obj:Any)->Any:
    if isinstance(obj,str):  return obj.replace("\x00","")
    if isinstance(obj,list): return [scrub_nuls(x) for x in obj]
    if isinstance(obj,dict): return {k:scrub_nuls(v) for k,v in obj.items()}
    return obj

def _make_converter()->DocumentConverter:
    opts = PdfPipelineOptions()
    opts.do_ocr = True
    return DocumentConverter({InputFormat.PDF: PdfFormatOption(pipeline_options=opts)})
# ───────────────────────────────────────────────────────────────────

from dotenv import load_dotenv
from openai import OpenAI
from supabase import create_client            # only needed if you later push rows
from tqdm import tqdm

# ─── optional heavy deps ----------------------------------------------------
try:
    import PyPDF2                             # raw fallback
    from unstructured.partition.pdf import partition_pdf
    from docling.datamodel.base_models import InputFormat
    from docling.datamodel.pipeline_options import PdfPipelineOptions
    from docling.document_converter import DocumentConverter, PdfFormatOption
except ImportError as exc:                    # pragma: no cover
    sys.exit(f"Missing dependency → {exc}.  Run `pip install -r requirements.txt`.")

# ─── env / paths ------------------------------------------------------------
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

REPO_ROOT  = pathlib.Path(__file__).resolve().parents[2]
TXT_DIR    = REPO_ROOT / "city_clerk_documents" / "txt"
JSON_DIR   = REPO_ROOT / "city_clerk_documents" / "json"
TXT_DIR.mkdir(parents=True, exist_ok=True)
JSON_DIR.mkdir(parents=True, exist_ok=True)

GPT_META_MODEL = "gpt-4.1-mini-2025-04-14"

log = logging.getLogger(__name__)

# ╔══════════════════════════════════════════════════════════════════════════╗
# ║                           helper utilities                              ║
# ╚══════════════════════════════════════════════════════════════════════════╝

_LATIN = {0x91:"'",0x92:"'",0x93:'"',0x94:'"',0x95:"•",0x96:"–",0x97:"—"}
_DOI_RE = re.compile(r"\b10\.\d{4,9}/[-._;()/:A-Z0-9]+", re.I)
_JOURNAL_RE = re.compile(r"(Journal|Proceedings|Annals|Psychiatry|Psychology|Nature|Science)[^\n]{0,120}", re.I)
_ABSTRACT_RE = re.compile(r"(?<=\bAbstract\b[:\s])(.{50,2000}?)(?:\n[A-Z][^\n]{0,60}\n|\Z)", re.S)
_KEYWORDS_RE = re.compile(r"\bKey\s*words?\b[:\s]*(.+)", re.I)
_HEADING_TYPES = {"title","heading","header","subtitle","subheading"}

# ─── minimal bib helpers ───────────────────────────────────────────────────
def _authors(val)->List[str]:
    if val is None: return []
    if isinstance(val,list): return [str(x).strip() for x in val if x]
    return re.split(r"\s*,\s*|\s+and\s+", str(val).strip())

_DEF_META = {
    "document_type": None,
    "title": None,
    "date": None,
    "year": None,
    "month": None,
    "day": None,
    "mayor": None,
    "vice_mayor": None,
    "commissioners": [],
    "city_attorney": None,
    "city_manager": None,
    "city_clerk": None,
    "public_works_director": None,
    "agenda": None,
    "keywords": [],
}

def merge_meta(*sources:Dict[str,Any])->Dict[str,Any]:
    m=_DEF_META.copy()
    for src in sources:
        for k,v in src.items():
            if v not in (None,"",[],{}): m[k]=v
    m["commissioners"]=m["commissioners"] or []
    m["keywords"]=m["keywords"] or []
    return m

def bib_from_filename(pdf:pathlib.Path)->Dict[str,Any]:
    s=pdf.stem
    m=re.search(r"\b(19|20)\d{2}\b",s)
    yr=int(m.group(0)) if m else None
    if m:
        title=s[m.end():].strip(" -_")
    else:
        parts=s.split(" ",1); title=parts[1] if len(parts)==2 else s
    return {"year":yr,"title":title}

def bib_from_header(txt:str)->Dict[str,Any]:
    md={}
    if m:=_DOI_RE.search(txt): md["doi"]=m.group(0)
    if m:=_JOURNAL_RE.search(txt): md["journal"]=" ".join(m.group(0).split())
    if m:=_ABSTRACT_RE.search(txt): md["abstract"]=" ".join(m.group(1).split())
    if m:=_KEYWORDS_RE.search(txt):
        kws=[k.strip(" ;.,") for k in re.split(r"[;,]",m.group(1)) if k.strip()]
        md["keywords"]=kws
    return md

# ─── GPT enrichment ────────────────────────────────────────────────────────
def _first_words(txt:str,n:int=3000)->str: return " ".join(txt.split()[:n])

def gpt_metadata(text:str)->Dict[str,Any]:
    if not OPENAI_API_KEY: return {}
    cli=OpenAI(api_key=OPENAI_API_KEY)
    prompt=dedent(f"""
        Extract metadata from this city clerk document and return a JSON object with these fields:
        - document_type: one of [Resolution, Ordinance, Proclamation, Contract, Meeting Minutes, Agenda]
        - title: document title
        - date: full date string
        - year: numeric year
        - month: numeric month (1-12)
        - day: numeric day
        - mayor: name only (e.g., "John Smith")
        - vice_mayor: name only (e.g., "Jane Doe")
        - commissioners: array of commissioner names only (e.g., ["Robert Brown", "Sarah Johnson", "Michael Davis"])
        - city_attorney: name only
        - city_manager: name only
        - city_clerk: name only
        - public_works_director: name only
        - agenda: agenda items or summary if present
        - keywords: array of relevant keywords (e.g., ["budget", "zoning", "public safety"])
        
        Text:
        {text}
    """)
    rsp=cli.chat.completions.create(model=GPT_META_MODEL,temperature=0,
        messages=[{"role":"system","content":"Structured metadata extractor"},
                  {"role":"user","content":prompt}])
    txt=rsp.choices[0].message.content
    json_txt=re.search(r"{[\s\S]*}",txt)
    return json.loads(json_txt.group(0) if json_txt else "{}")        # type: ignore[arg-type]

# ╔══════════════════════════════════════════════════════════════════════════╗
# ║                         core extraction logic                            ║
# ╚══════════════════════════════════════════════════════════════════════════╝

def _docling_elements(doc)->List[Dict[str,Any]]:
    p:Dict[int,List[Dict[str,str]]]={}
    for it,_ in doc.iterate_items():
        pg=getattr(it.prov[0],"page_no",1)
        lbl=(getattr(it,"label","") or "").upper()
        typ="heading" if lbl in ("TITLE","SECTION_HEADER","HEADER") else \
            "list_item" if lbl=="LIST_ITEM" else \
            "table" if lbl=="TABLE" else "paragraph"
        p.setdefault(pg,[]).append({"type":typ,"text":str(getattr(it,"text",it)).strip()})
    out=[]
    for pn in sorted(p):
        out.append({"section":f"Page {pn}","page_number":pn,
                    "text":"\n".join(el["text"] for el in p[pn]),
                    "elements":p[pn]})
    return out

def _unstructured_elements(pdf:pathlib.Path)->List[Dict[str,Any]]:
    els=partition_pdf(str(pdf),strategy="hi_res")
    pages:Dict[int,List[Dict[str,str]]]={}
    for el in els:
        pn=getattr(el.metadata,"page_number",1)
        pages.setdefault(pn,[]).append({"type":el.category or "paragraph",
                                        "text":normalize_ws(str(el))})
    return [{"section":f"Page {pn}","page_number":pn,
             "text":"\n".join(e["text"] for e in it),"elements":it}
            for pn,it in sorted(pages.items())]

def _pypdf_elements(pdf:pathlib.Path)->List[Dict[str,Any]]:
    out=[]
    with open(pdf,"rb") as fh:
        for pn,pg in enumerate(PyPDF2.PdfReader(fh).pages,1):
            raw=normalize_ws(pg.extract_text() or "")
            paras=[p for p in re.split(r"\n{2,}",raw) if p.strip()]
            out.append({"section":f"Page {pn}","page_number":pn,
                        "text":"\n".join(paras),
                        "elements":[{"type":"paragraph","text":p} for p in paras]})
    return out

def _group_by_headings(page_secs:Sequence[Dict[str,Any]])->List[Dict[str,Any]]:
    out=[]; cur=None; last=1
    for s in page_secs:
        pn=s.get("page_number",last); last=pn
        for el in s.get("elements",[]):
            kind=(el.get("type")or"").lower(); txt=el.get("text","").strip()
            if kind in _HEADING_TYPES and txt:
                if cur: out.append(cur)
                cur={"section":txt,"page_start":pn,"page_end":pn,"elements":[el.copy()]}
            else:
                if not cur:
                    cur={"section":"(untitled)","page_start":pn,"page_end":pn,"elements":[]}
                cur["elements"].append(el.copy()); cur["page_end"]=pn
    if cur and not out: out.append(cur)
    return out

def _sections_md(sections:List[Dict[str,Any]])->str:
    md=[]
    for s in sections:
        md.append(f"# {s.get('section','(Untitled)')}")
        for el in s.get("elements",[]): md.append(el.get("text",""))
        md.append("")
    return "\n".join(md).strip()

# ╔══════════════════════════════════════════════════════════════════════════╗
# ║                         Inline extract_pdf from common                   ║
# ╚══════════════════════════════════════════════════════════════════════════╝

def extract_pdf(
    pdf: pathlib.Path,
    txt_dir: pathlib.Path,
    json_dir: pathlib.Path,
    conv: DocumentConverter,
    *,
    overwrite: bool = False,
    ocr_lang: str = "eng",
    keep_markup: bool = True,
    docling_only: bool = False,
    stats: Counter | None = None,
    enrich_llm: bool = True,
) -> pathlib.Path:
    """Return path to JSON payload ready for downstream stages."""
    txt_path = txt_dir / f"{pdf.stem}.txt"
    json_path = json_dir / f"{pdf.stem}.json"
    if not overwrite and txt_path.exists() and json_path.exists():
        return json_path

    # –– 1. Docling
    page_secs: List[Dict[str, Any]] = []
    bundle = None
    try:
        bundle = conv.convert(str(pdf))
        if keep_markup:
            page_secs = _docling_elements(bundle.document)
        else:
            full = bundle.document.export_to_text(page_break_marker="\f")
            page_secs = [{
                "section": "Full document",
                "page_number": 1,
                "text": full,
                "elements": [{"type": "paragraph", "text": full}],
            }]
    except Exception as exc:
        log.warning("Docling failed on %s → %s", pdf.name, exc)

    # –– 2. Unstructured fallback
    if not page_secs and not docling_only:
        try:
            page_secs = _unstructured_elements(pdf)
        except Exception as exc:
            log.warning("unstructured failed on %s → %s", pdf.name, exc)

    # –– 3. PyPDF last resort
    if not page_secs and not docling_only:
        log.info("PyPDF2 fallback on %s", pdf.name)
        page_secs = _pypdf_elements(pdf)

    if not page_secs:
        raise RuntimeError("No text extracted from PDF")

    # Latin-1 scrub + OCR repair
    for sec in page_secs:
        sec["text"] = latin1_scrub(sec.get("text", ""))
        for el in sec.get("elements", []):
            el["text"] = latin1_scrub(el.get("text", ""))
        pn = sec.get("page_number")
        need_ocr_before = bool(pn and needs_ocr(sec["text"]))
        if need_ocr_before:
            try:
                from pdfplumber import open as pdfopen
                import pytesseract

                with pdfopen(str(pdf)) as doc:
                    pil = doc.pages[pn - 1].to_image(resolution=300).original
                ocr_txt = normalize_ws(
                    pytesseract.image_to_string(pil, lang=ocr_lang)
                )
                if ocr_txt:
                    sec["text"] = ocr_txt
                    sec["elements"] = [
                        {"type": "paragraph", "text": p}
                        for p in re.split(r"\n{2,}", ocr_txt)
                        if p.strip()
                    ]
                    if stats is not None:
                        stats["ocr_pages"] += 1
            except Exception:
                pass

    # when markup is disabled we already have final sections
    logical_secs = (
        _group_by_headings(page_secs) if keep_markup else page_secs
    )

    # CRITICAL: Ensure EVERY section has a 'text' field
    # This is required by the chunking stage
    for sec in logical_secs:
        if 'elements' in sec:
            # Build text from elements if not already present
            element_texts = []
            for el in sec.get("elements", []):
                el_text = el.get("text", "")
                # Skip internal docling metadata that starts with self_ref=
                if el_text and not el_text.startswith('self_ref='):
                    element_texts.append(el_text)
            
            # Always set text field (overwrite if exists to ensure consistency)
            sec["text"] = "\n".join(element_texts)
        elif 'text' not in sec:
            # Fallback for sections without elements
            sec["text"] = ""
    
    # Also ensure page_secs have text (for debugging/consistency)
    for sec in page_secs:
        if 'text' not in sec and 'elements' in sec:
            element_texts = []
            for el in sec.get("elements", []):
                el_text = el.get("text", "")
                if el_text and not el_text.startswith('self_ref='):
                    element_texts.append(el_text)
            sec["text"] = "\n".join(element_texts)

    header_txt = " ".join(s["text"] for s in page_secs[:2])[:8000]
    heuristic_meta = merge_meta(
        bundle.document.metadata.model_dump()
        if "bundle" in locals() and hasattr(bundle.document, "metadata")
        else {},
        bib_from_filename(pdf),
        bib_from_header(header_txt),
    )

    # single GPT call for **all** metadata fields -----------------------
    llm_meta: Dict[str, Any] = {}
    if enrich_llm and OPENAI_API_KEY:
        try:
            oa = OpenAI(api_key=OPENAI_API_KEY)
            llm_meta = gpt_metadata(
                _first_words(" ".join(s["text"] for s in logical_secs))
            )
        except Exception as exc:
            log.warning("LLM metadata extraction failed on %s → %s", pdf.name, exc)

    meta = merge_meta(heuristic_meta, llm_meta)

    payload = {
        **meta,
        "created_at": datetime.utcnow().isoformat() + "Z",
        "source_pdf": str(pdf.resolve()),
        "sections": logical_secs,
    }

    json_path.parent.mkdir(parents=True, exist_ok=True)
    txt_path.parent.mkdir(parents=True, exist_ok=True)
    json_path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), "utf-8")

    # --- Markdown serialisation (rich) ---
    md_text = _sections_md(logical_secs) if keep_markup else "\n".join(
        "# " + s.get("section", "(No title)") + "\n\n" + s.get("text", "") for s in logical_secs
    )
    txt_path.write_text(md_text, "utf-8")

    if stats is not None:
        stats["processed"] += 1

    return json_path

# New: Thread-safe converter pool
class ConverterPool:
    """Thread-safe pool of DocumentConverter instances."""
    def __init__(self, size: int = None):
        self.size = size or mp.cpu_count()
        self._converters = []
        self._lock = mp.Lock()
        self._initialized = False
    
    def get(self):
        """Get a converter from the pool."""
        with self._lock:
            if not self._initialized:
                self._initialize()
            return self._converters[mp.current_process()._identity[0] % self.size]
    
    def _initialize(self):
        """Lazy initialize converters."""
        for _ in range(self.size):
            self._converters.append(_make_converter())
        self._initialized = True

# Global converter pool
_converter_pool = ConverterPool()

def extract_pdf_concurrent(
    pdf: pathlib.Path,
    txt_dir: pathlib.Path,
    json_dir: pathlib.Path,
    *,
    overwrite: bool = False,
    ocr_lang: str = "eng",
    keep_markup: bool = True,
    docling_only: bool = False,
    stats: Counter | None = None,
    enrich_llm: bool = True,
) -> pathlib.Path:
    """Thread-safe version of extract_pdf that uses converter pool."""
    conv = _converter_pool.get()
    return extract_pdf(
        pdf, txt_dir, json_dir, conv,
        overwrite=overwrite,
        ocr_lang=ocr_lang,
        keep_markup=keep_markup,
        docling_only=docling_only,
        stats=stats,
        enrich_llm=enrich_llm
    )

async def extract_batch_async(
    pdfs: List[pathlib.Path],
    *,
    overwrite: bool = False,
    keep_markup: bool = True,
    ocr_lang: str = "eng",
    enrich_llm: bool = True,
    max_workers: Optional[int] = None,
) -> List[pathlib.Path]:
    """Extract multiple PDFs concurrently."""
    from .acceleration_utils import hardware
    
    stats = Counter()
    results = []
    
    # Use process pool for CPU-bound extraction
    with hardware.get_process_pool(max_workers) as executor:
        # Submit all tasks
        future_to_pdf = {
            executor.submit(
                extract_pdf_concurrent,
                pdf, TXT_DIR, JSON_DIR,
                overwrite=overwrite,
                ocr_lang=ocr_lang,
                keep_markup=keep_markup,
                enrich_llm=enrich_llm,
                stats=stats
            ): pdf
            for pdf in pdfs
        }
        
        # Process completed tasks
        from tqdm import tqdm
        for future in tqdm(as_completed(future_to_pdf), total=len(pdfs), desc="Extracting PDFs"):
            pdf = future_to_pdf[future]
            try:
                json_path = future.result()
                results.append(json_path)
            except Exception as exc:
                log.error(f"Failed to extract {pdf.name}: {exc}")
                
    log.info(f"Extraction complete: {stats['processed']} processed, {stats['ocr_pages']} OCR pages")
    return results

# ╔══════════════════════════════════════════════════════════════════════════╗
# ║                          public entry-point                             ║
# ╚══════════════════════════════════════════════════════════════════════════╝
# ---------------------------------------------------------------------------
#  Stage-1/2 wrapper (now paper-thin)
# ---------------------------------------------------------------------------
def run_one(
    pdf: pathlib.Path,
    *,
    overwrite: bool = False,
    keep_markup: bool = True,
    ocr_lang: str = "eng",
    enrich_llm: bool = True,
    conv=None,
    stats: Counter | None = None,
) -> pathlib.Path:
    conv = conv or _make_converter()
    out = extract_pdf(
        pdf,
        TXT_DIR,
        JSON_DIR,
        conv,
        overwrite=overwrite,
        ocr_lang=ocr_lang,
        keep_markup=keep_markup,
        enrich_llm=enrich_llm,
        stats=stats,
    )
    return out

def json_path_for(pdf:pathlib.Path)->pathlib.Path:
    """Helper if Stage 1 is disabled but its artefact exists."""
    return JSON_DIR / f"{pdf.stem}.json"

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format="%(message)s")
    import argparse
    p=argparse.ArgumentParser(); p.add_argument("pdf",type=pathlib.Path)
    args=p.parse_args()
    print(run_one(args.pdf))


================================================================================


################################################################################
# File: scripts/graph_stages/verbatim_transcript_linker.py
################################################################################

# File: scripts/graph_stages/verbatim_transcript_linker.py

"""
Verbatim Transcript Linker
Links verbatim transcript documents to their corresponding agenda items.
Handles various transcript types including individual items, item ranges, and public comments.
"""

import logging
import re
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Set
import json
from datetime import datetime
import PyPDF2
import os

log = logging.getLogger('verbatim_transcript_linker')


class VerbatimTranscriptLinker:
    """Links verbatim transcript documents to agenda items in the graph."""
    
    def __init__(self):
        """Initialize the verbatim transcript linker."""
        # Pattern to extract date and item info from filename
        self.filename_pattern = re.compile(
            r'(\d{2})_(\d{2})_(\d{4})\s*-\s*Verbatim Transcripts\s*-\s*(.+)\.pdf',
            re.IGNORECASE
        )
        
        # Debug directory for logging - ensure parent exists
        self.debug_dir = Path("city_clerk_documents/graph_json/debug/verbatim")
        self.debug_dir.mkdir(parents=True, exist_ok=True)
    
    async def link_transcripts_for_meeting(self, 
                                         meeting_date: str,
                                         verbatim_dir: Path) -> Dict[str, List[Dict]]:
        """Find and link all verbatim transcripts for a specific meeting date."""
        log.info(f"🎤 Linking verbatim transcripts for meeting date: {meeting_date}")
        log.info(f"📁 Verbatim directory: {verbatim_dir}")
        
        # Debug logging for troubleshooting
        log.info(f"🔍 Looking for verbatim transcripts in: {verbatim_dir}")
        log.info(f"🔍 Directory exists: {verbatim_dir.exists()}")
        if verbatim_dir.exists():
            all_files = list(verbatim_dir.glob("*.pdf"))
            log.info(f"🔍 Total PDF files in directory: {len(all_files)}")
            if all_files:
                log.info(f"🔍 Sample files: {[f.name for f in all_files[:3]]}")
        
        # Convert meeting date format: "01.09.2024" -> "01_09_2024"
        date_underscore = meeting_date.replace(".", "_")
        
        # Initialize results
        linked_transcripts = {
            "item_transcripts": [],      # Transcripts for specific agenda items
            "public_comments": [],       # Public comment transcripts
            "section_transcripts": []    # Transcripts for entire sections
        }
        
        if not verbatim_dir.exists():
            log.warning(f"⚠️  Verbatim directory not found: {verbatim_dir}")
            return linked_transcripts
        
        # Find all transcript files for this date
        # Try multiple patterns to ensure we catch all files
        patterns = [
            f"{date_underscore}*Verbatim*.pdf",
            f"{date_underscore} - Verbatim*.pdf",
            f"*{date_underscore}*Verbatim*.pdf"
        ]

        transcript_files = []
        for pattern in patterns:
            files = list(verbatim_dir.glob(pattern))
            log.info(f"🔍 Pattern '{pattern}' found {len(files)} files")
            transcript_files.extend(files)

        # Remove duplicates
        transcript_files = list(set(transcript_files))
        
        log.info(f"📄 Found {len(transcript_files)} transcript files")
        
        # Process each transcript file
        for transcript_path in transcript_files:
            try:
                transcript_info = await self._process_transcript(transcript_path, meeting_date)
                if transcript_info:
                    # Categorize based on transcript type
                    if transcript_info['transcript_type'] == 'public_comment':
                        linked_transcripts['public_comments'].append(transcript_info)
                    elif transcript_info['transcript_type'] == 'section':
                        linked_transcripts['section_transcripts'].append(transcript_info)
                    else:
                        linked_transcripts['item_transcripts'].append(transcript_info)
                        
            except Exception as e:
                log.error(f"Error processing transcript {transcript_path.name}: {e}")
        
        # Save linked transcripts info for debugging
        self._save_linking_report(meeting_date, linked_transcripts)
        
        # Log summary
        total_linked = (len(linked_transcripts['item_transcripts']) + 
                       len(linked_transcripts['public_comments']) + 
                       len(linked_transcripts['section_transcripts']))
        
        log.info(f"✅ Verbatim transcript linking complete:")
        log.info(f"   🎤 Item transcripts: {len(linked_transcripts['item_transcripts'])}")
        log.info(f"   🎤 Public comments: {len(linked_transcripts['public_comments'])}")
        log.info(f"   🎤 Section transcripts: {len(linked_transcripts['section_transcripts'])}")
        log.info(f"   📄 Total linked: {total_linked}")
        
        return linked_transcripts
    
    async def _process_transcript(self, transcript_path: Path, meeting_date: str) -> Optional[Dict[str, Any]]:
        """Process a single transcript file and extract item references."""
        try:
            # Parse filename
            match = self.filename_pattern.match(transcript_path.name)
            if not match:
                log.warning(f"Could not parse transcript filename: {transcript_path.name}")
                return None
            
            month, day, year = match.groups()[:3]
            item_info = match.group(4).strip()
            
            # Parse item codes from the item info
            parsed_items = self._parse_item_codes(item_info)
            
            # Extract text from PDF (first few pages for context)
            text_excerpt = self._extract_pdf_text(transcript_path, max_pages=3)
            
            # Determine transcript type and normalize item codes
            transcript_type = self._determine_transcript_type(item_info, parsed_items)
            
            transcript_info = {
                "path": str(transcript_path),
                "filename": transcript_path.name,
                "meeting_date": meeting_date,
                "item_info_raw": item_info,
                "item_codes": parsed_items['item_codes'],
                "section_codes": parsed_items['section_codes'],
                "transcript_type": transcript_type,
                "page_count": self._get_pdf_page_count(transcript_path),
                "text_excerpt": text_excerpt[:500] if text_excerpt else ""
            }
            
            log.info(f"📄 Processed transcript: {transcript_path.name}")
            log.info(f"   Items: {parsed_items['item_codes']}")
            log.info(f"   Type: {transcript_type}")
            
            return transcript_info
            
        except Exception as e:
            log.error(f"Error processing transcript {transcript_path.name}: {e}")
            return None
    
    def _parse_item_codes(self, item_info: str) -> Dict[str, List[str]]:
        """Parse item codes from the filename item info section."""
        result = {
            'item_codes': [],
            'section_codes': []
        }
        
        # Special case: Meeting Minutes or other general labels
        if re.search(r'meeting\s+minutes', item_info, re.IGNORECASE):
            result['item_codes'].append('MEETING_MINUTES')
            return result
        
        # Special case: Full meeting transcript
        if re.search(r'public|full\s+meeting', item_info, re.IGNORECASE) and not re.search(r'comment', item_info, re.IGNORECASE):
            result['item_codes'].append('FULL_MEETING')
            return result
        
        # Special case: Public Comment
        if re.search(r'public\s+comment', item_info, re.IGNORECASE):
            result['item_codes'].append('PUBLIC_COMMENT')
            return result
        
        # Special case: Discussion Items (K section)
        if re.match(r'^K\s*$', item_info.strip()):
            result['section_codes'].append('K')
            return result
        
        # Clean the item info
        item_info = item_info.strip()
        
        # Handle multiple items with "and" or "AND"
        # Examples: "F-7 and F-10", "2-1 AND 2-2"
        if ' and ' in item_info.lower():
            parts = re.split(r'\s+and\s+', item_info, flags=re.IGNORECASE)
            for part in parts:
                codes = self._extract_single_item_codes(part.strip())
                result['item_codes'].extend(codes)
        
        # Handle space-separated items
        # Examples: "E-5 E-6 E-7 E-8 E-9 E-10"
        elif re.match(r'^([A-Z]-?\d+\s*)+$', item_info):
            # Split by spaces and extract each item
            items = item_info.split()
            for item in items:
                if re.match(r'^[A-Z]-?\d+$', item):
                    normalized = self._normalize_item_code(item)
                    if normalized:
                        result['item_codes'].append(normalized)
        
        # Handle comma-separated items
        elif ',' in item_info:
            parts = item_info.split(',')
            for part in parts:
                codes = self._extract_single_item_codes(part.strip())
                result['item_codes'].extend(codes)
        
        # Single item or other format
        else:
            codes = self._extract_single_item_codes(item_info)
            result['item_codes'].extend(codes)
        
        # Remove duplicates while preserving order
        result['item_codes'] = list(dict.fromkeys(result['item_codes']))
        result['section_codes'] = list(dict.fromkeys(result['section_codes']))
        
        return result
    
    def _extract_single_item_codes(self, text: str) -> List[str]:
        """Extract item codes from a single text segment."""
        codes = []
        
        # Pattern for item codes: letter-number, letter.number, or just number-number
        # Handles: E-1, E1, E.-1., E.1, 2-1, etc.
        patterns = [
            r'([A-Z])\.?\-?(\d+)\.?',  # Letter-based items
            r'(\d+)\-(\d+)'             # Number-only items like 2-1
        ]
        
        for pattern in patterns:
            for match in re.finditer(pattern, text):
                if pattern.startswith('(\\d'):  # Number-only pattern
                    # For number-only, just use as is
                    codes.append(f"{match.group(1)}-{match.group(2)}")
                else:
                    # For letter-number format
                    letter = match.group(1)
                    number = match.group(2)
                    codes.append(f"{letter}-{number}")
        
        return codes
    
    def _normalize_item_code(self, code: str) -> str:
        """Normalize item code to consistent format (e.g., E-1, 2-1)."""
        # Remove dots and ensure dash format
        code = code.strip('. ')
        
        # Pattern: letter followed by optional punctuation and number
        letter_match = re.match(r'^([A-Z])\.?\-?(\d+)\.?$', code)
        if letter_match:
            letter = letter_match.group(1)
            number = letter_match.group(2)
            return f"{letter}-{number}"
        
        # Pattern: number-number format
        number_match = re.match(r'^(\d+)\-(\d+)$', code)
        if number_match:
            return code  # Already in correct format
        
        return code
    
    def _determine_transcript_type(self, item_info: str, parsed_items: Dict) -> str:
        """Determine the type of transcript based on parsed information."""
        if 'PUBLIC_COMMENT' in parsed_items['item_codes']:
            return 'public_comment'
        elif parsed_items['section_codes']:
            return 'section'
        elif len(parsed_items['item_codes']) > 3:
            return 'multi_item'
        elif len(parsed_items['item_codes']) == 1:
            return 'single_item'
        else:
            return 'item_group'
    
    def _extract_pdf_text(self, pdf_path: Path, max_pages: int = 3) -> str:
        """Extract text from first few pages of PDF."""
        try:
            with open(pdf_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                text_parts = []
                
                # Extract text from first few pages
                pages_to_read = min(len(reader.pages), max_pages)
                for i in range(pages_to_read):
                    text = reader.pages[i].extract_text()
                    if text:
                        text_parts.append(text)
                
                return "\n".join(text_parts)
        except Exception as e:
            log.error(f"Failed to extract text from {pdf_path.name}: {e}")
            return ""
    
    def _get_pdf_page_count(self, pdf_path: Path) -> int:
        """Get total page count of PDF."""
        try:
            with open(pdf_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                return len(reader.pages)
        except:
            return 0
    
    def _save_linking_report(self, meeting_date: str, linked_transcripts: Dict):
        """Save detailed report of transcript linking."""
        report = {
            "meeting_date": meeting_date,
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total_transcripts": sum(len(v) for v in linked_transcripts.values()),
                "item_transcripts": len(linked_transcripts["item_transcripts"]),
                "public_comments": len(linked_transcripts["public_comments"]),
                "section_transcripts": len(linked_transcripts["section_transcripts"])
            },
            "transcripts": linked_transcripts
        }
        
        report_filename = f"verbatim_linking_report_{meeting_date.replace('.', '_')}.json"
        report_path = self.debug_dir / report_filename
        
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        log.info(f"📊 Verbatim linking report saved to: {report_path}")
    
    def _validate_meeting_date(self, meeting_date: str) -> bool:
        """Validate meeting date format MM.DD.YYYY"""
        return bool(re.match(r'^\d{2}\.\d{2}\.\d{4}$', meeting_date))


================================================================================


################################################################################
# File: scripts/graph_stages/document_linker.py
################################################################################

# File: scripts/graph_stages/document_linker.py

"""
Document Linker
Links ordinance documents to their corresponding agenda items.
"""

import logging
import re
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import json
from datetime import datetime
import PyPDF2
from openai import OpenAI
import os
from dotenv import load_dotenv

load_dotenv()

log = logging.getLogger('document_linker')


class DocumentLinker:
    """Links ordinance and resolution documents to agenda items."""
    
    def __init__(self,
                 openai_api_key: Optional[str] = None,
                 model: str = "gpt-4.1-mini-2025-04-14",
                 agenda_extraction_max_tokens: int = 32768):
        """Initialize the document linker."""
        self.api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY not found in environment")
        
        self.client = OpenAI(api_key=self.api_key)
        self.model = model
        self.agenda_extraction_max_tokens = agenda_extraction_max_tokens
    
    async def link_documents_for_meeting(self, 
                                       meeting_date: str,
                                       documents_dir: Path) -> Dict[str, List[Dict]]:
        """Find and link all documents for a specific meeting date."""
        log.info(f"🔗 Linking documents for meeting date: {meeting_date}")
        log.info(f"📁 Looking for ordinances in: {documents_dir}")
        
        # Create debug directory
        debug_dir = Path("city_clerk_documents/graph_json/debug")
        debug_dir.mkdir(exist_ok=True)
        
        # Convert date format: "01.09.2024" -> "01_09_2024"
        date_underscore = meeting_date.replace(".", "_")
        
        # Log what we're searching for
        with open(debug_dir / "document_search_info.txt", 'w') as f:
            f.write(f"Meeting Date: {meeting_date}\n")
            f.write(f"Date with underscores: {date_underscore}\n")
            f.write(f"Search directory: {documents_dir}\n")
            f.write(f"Search pattern: *{date_underscore}.pdf\n")
        
        # Find all matching ordinance/resolution files
        # Pattern: YYYY-## - MM_DD_YYYY.pdf
        pattern = f"*{date_underscore}.pdf"
        matching_files = list(documents_dir.glob(pattern))
        
        # Also try without spaces in case filenames vary
        pattern2 = f"*{date_underscore}*.pdf"
        additional_files = [f for f in documents_dir.glob(pattern2) if f not in matching_files]
        matching_files.extend(additional_files)
        
        # List all files in directory for debugging
        all_files = list(documents_dir.glob("*.pdf"))
        with open(debug_dir / "all_ordinance_files.txt", 'w') as f:
            f.write(f"Total files in {documents_dir}: {len(all_files)}\n")
            for file in sorted(all_files):
                f.write(f"  - {file.name}\n")
        
        log.info(f"📄 Found {len(matching_files)} documents for date {meeting_date}")
        
        if matching_files:
            log.info("📄 Documents found:")
            for f in matching_files[:5]:
                log.info(f"   - {f.name}")
            if len(matching_files) > 5:
                log.info(f"   ... and {len(matching_files) - 5} more")
        
        # Save matched files list
        with open(debug_dir / "matched_documents.txt", 'w') as f:
            f.write(f"Documents matching date {meeting_date}:\n")
            for file in matching_files:
                f.write(f"  - {file.name}\n")
        
        # Process each document
        linked_documents = {
            "ordinances": [],
            "resolutions": []
        }
        
        for doc_path in matching_files:
            # Extract document info
            doc_info = await self._process_document(doc_path, meeting_date)
            
            if doc_info:
                # Categorize by type
                if "ordinance" in doc_info.get("title", "").lower():
                    linked_documents["ordinances"].append(doc_info)
                else:
                    linked_documents["resolutions"].append(doc_info)
        
        # Save linked documents info
        with open(debug_dir / "linked_documents.json", 'w') as f:
            json.dump(linked_documents, f, indent=2)
        
        log.info(f"✅ Linked {len(linked_documents['ordinances'])} ordinances, {len(linked_documents['resolutions'])} resolutions")
        return linked_documents
    
    async def _process_document(self, doc_path: Path, meeting_date: str) -> Optional[Dict[str, Any]]:
        """Process a single document to extract agenda item reference."""
        try:
            # Extract document number from filename (e.g., "2024-04" from "2024-04 - 01_23_2024.pdf")
            doc_match = re.match(r'^(\d{4}-\d{2})', doc_path.name)
            if not doc_match:
                log.warning(f"Could not parse document number from {doc_path.name}")
                return None
            
            document_number = doc_match.group(1)
            
            # Extract text from PDF
            text = self._extract_pdf_text(doc_path)
            if not text:
                log.warning(f"No text extracted from {doc_path.name}")
                return None
            
            # Extract agenda item code using LLM
            item_code = await self._extract_agenda_item_code(text, document_number)
            
            # Extract additional metadata
            parsed_data = self._parse_document_metadata(text)
            
            doc_info = {
                "path": str(doc_path),
                "filename": doc_path.name,
                "document_number": document_number,
                "item_code": item_code,
                "document_type": "Ordinance" if "ordinance" in text[:500].lower() else "Resolution",
                "title": self._extract_title(text),
                "parsed_data": parsed_data
            }
            
            log.info(f"📄 Processed {doc_path.name}: Item {item_code or 'NOT_FOUND'}")
            return doc_info
            
        except Exception as e:
            log.error(f"Error processing {doc_path.name}: {e}")
            return None
    
    def _extract_pdf_text(self, pdf_path: Path) -> str:
        """Extract text from PDF file."""
        try:
            with open(pdf_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                text_parts = []
                
                # Extract text from all pages
                for page in reader.pages:
                    text = page.extract_text()
                    if text:
                        text_parts.append(text)
                
                return "\n".join(text_parts)
        except Exception as e:
            log.error(f"Failed to extract text from {pdf_path.name}: {e}")
            return ""
    
    async def _extract_agenda_item_code(self, text: str, document_number: str) -> Optional[str]:
        """Extract agenda item code from document text using LLM."""
        # Create debug directory if it doesn't exist
        debug_dir = Path("city_clerk_documents/graph_json/debug")
        debug_dir.mkdir(exist_ok=True)
        
        # Send the entire document to qwen-32b
        text_excerpt = text
        
        # Save the full text being sent to LLM for debugging
        with open(debug_dir / f"llm_input_{document_number}.txt", 'w', encoding='utf-8') as f:
            f.write(f"Document: {document_number}\n")
            f.write(f"Text length: {len(text)} characters\n")
            f.write(f"Using model: {self.model}\n")
            f.write(f"Max tokens: {self.agenda_extraction_max_tokens}\n")
            f.write("\n--- FULL DOCUMENT SENT TO LLM ---\n")
            f.write(text_excerpt)
        
        log.info(f"📄 Sending full document to LLM for {document_number}: {len(text)} characters")
        
        prompt = f"""You are analyzing a City of Coral Gables ordinance document (Document #{document_number}).

Your task is to find the AGENDA ITEM CODE referenced in this document.

IMPORTANT: The agenda item can appear ANYWHERE in the document - on page 3, at the end, or anywhere else. Search the ENTIRE document carefully.

The agenda item typically appears in formats like:
- (Agenda Item: E-1)
- Agenda Item: E-3)
- (Agenda Item E-1)
- Item H-3
- H.-3. (with periods and dots)
- E.-2. (with dots)
- E-2 (without dots)
- Item E-2

Full document text:
{text_excerpt}

Respond in this EXACT format:
AGENDA_ITEM: [code] or AGENDA_ITEM: NOT_FOUND

Important: Return the code as it appears (e.g., E-2, not E.-2.)

Examples:
- If you find "(Agenda Item: E-2)" → respond: AGENDA_ITEM: E-2
- If you find "Item E-2" → respond: AGENDA_ITEM: E-2
- If you find "H.-3." → respond: AGENDA_ITEM: H-3
- If no agenda item found → respond: AGENDA_ITEM: NOT_FOUND"""
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a precise data extractor. Find and extract only the agenda item code. Search the ENTIRE document thoroughly."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_tokens=self.agenda_extraction_max_tokens  # Use 100,000 for qwen
            )
            
            raw_response = response.choices[0].message.content.strip()
            
            # Save raw LLM response for debugging
            with open(debug_dir / f"llm_response_{document_number}_raw.txt", 'w', encoding='utf-8') as f:
                f.write(raw_response)
            
            # Parse response directly (no qwen parsing needed)
            result = raw_response
            
            # Log the response for debugging
            log.info(f"LLM response for {document_number} (first 200 chars): {result[:200]}")
            
            # Parse the response
            if "AGENDA_ITEM:" in result:
                code = result.split("AGENDA_ITEM:")[1].strip()
                if code != "NOT_FOUND":
                    # Normalize the code (remove dots, ensure format)
                    code = self._normalize_item_code(code)
                    log.info(f"✅ Found agenda item code for {document_number}: {code}")
                    return code
                else:
                    log.warning(f"❌ LLM could not find agenda item in {document_number}")
            else:
                log.error(f"❌ Invalid LLM response format for {document_number}: {result[:100]}")
            
            return None
            
        except Exception as e:
            log.error(f"Failed to extract agenda item for {document_number}: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _normalize_item_code(self, code: str) -> str:
        """Normalize item code to consistent format."""
        if not code:
            return code
        
        # Remove trailing dots and spaces
        code = code.rstrip('. ')
        
        # Remove dots between letter and dash: "E.-1" -> "E-1"
        code = re.sub(r'([A-Z])\.(-)', r'\1\2', code)
        
        # Handle cases without dash: "E.1" -> "E-1"
        code = re.sub(r'([A-Z])\.(\d)', r'\1-\2', code)
        
        # Remove any remaining dots
        code = code.replace('.', '')
        
        return code
    
    def _extract_title(self, text: str) -> str:
        """Extract document title from text."""
        # Look for "AN ORDINANCE" or "A RESOLUTION" pattern
        title_match = re.search(r'(AN?\s+(ORDINANCE|RESOLUTION)[^.]+\.)', text[:2000], re.IGNORECASE)
        if title_match:
            return title_match.group(1).strip()
        
        # Fallback to first substantive line
        lines = text.split('\n')
        for line in lines[:20]:
            if len(line) > 20 and not line.isdigit():
                return line.strip()[:200]
        
        return "Untitled Document"
    
    def _parse_document_metadata(self, text: str) -> Dict[str, Any]:
        """Parse additional metadata from document."""
        metadata = {}
        
        # Extract date passed
        date_match = re.search(r'day\s+of\s+(\w+),?\s+(\d{4})', text)
        if date_match:
            metadata["date_passed"] = date_match.group(0)
        
        # Extract vote information
        vote_match = re.search(r'PASSED\s+AND\s+ADOPTED.*?(\d+).*?(\d+)', text, re.IGNORECASE | re.DOTALL)
        if vote_match:
            metadata["vote_details"] = {
                "ayes": vote_match.group(1),
                "nays": vote_match.group(2) if len(vote_match.groups()) > 1 else "0"
            }
        
        # Extract motion information
        motion_match = re.search(r'motion\s+(?:was\s+)?made\s+by\s+([^,]+)', text, re.IGNORECASE)
        if motion_match:
            metadata["motion"] = {"moved_by": motion_match.group(1).strip()}
        
        # Extract mayor signature
        mayor_match = re.search(r'Mayor[:\s]+([^\n]+)', text[-1000:])
        if mayor_match:
            metadata["signatories"] = {"mayor": mayor_match.group(1).strip()}
        
        return metadata


================================================================================


################################################################################
# File: debug/extracted_items.json
################################################################################

[
  {
    "item_code": "D-1",
    "document_reference": "23-6830",
    "title": "A Resolution of the City Commission appointing Judith Alexander (Nominated by Commissioner  Fernandez)  to  serve  as  a  member  of  the  Senior  Citizens  Advisory  Board, for the remainder  of  an  unexpired  term,  which  began  on  June 1, 2023 and  continues through May 31, 2025.",
    "item_type": "Other"
  },
  {
    "item_code": "D-2",
    "document_reference": "23-6829",
    "title": "A  Resolution  of  the  City  Commission  confirming  the appointment  of  John  'Miles'  Maronto (Nominated  by  Board-As-A-Whole)  to  serve  as  a  member  of  the  Transportation  Advisory Board,  for  a  two  (2)  year  term  which  began  on  June  1,  2023 and  continues  through  May 31, 2025.",
    "item_type": "Other"
  },
  {
    "item_code": "E-1",
    "document_reference": "23-6723",
    "title": "An  Ordinance  of  the  City  Commission  amending  the  Cocoplum  Phase 1 Security  Guard District, as created by Miami-Dade County pursuant to County Ordinance 95-214, to expand  the  scope  of  services  to  include  additional  security  measures  including  but  not limited to security cameras.",
    "item_type": "Ordinance"
  },
  {
    "item_code": "E-2",
    "document_reference": "23-6785",
    "title": "An  Ordinance  of  the City Commission  amending  Section 1-2 'Definitions and  Rules  of Construction'; Section 2-464 'Same-Authority to Fix Schedule of Charges; Method of Determination'; Section 2-1091 'City-Owned Property Sale or Lease Generally; Advertised Public Bidding Process; Section 18-2 'Canvas of Returns; Duty of Commission; Declaration of Results'; Section 34-60 'Statement of Costs; Filing; Publication of Work; Cost and Lien'; Section 34-239 'Forfeiture Proceedings'; Section 58-53 'Statement of Costs; Publication'; Section 58-119 'Notice by Publication'; Section 62-329 'Procedures Relating to Applications'; Section 62-331 'Adoption of Ordinance'; Section 78-281 'Requirement for Underground Utilities'; Section 101-109 'Notices'; Section 109-41 'Permit Application ProceduresClass 1 Permits'; and Section 109-42 'Same-  Class  2 Permits'  of  the  City  Code  in  order  to  provide  a  definition  of  publication and  remove  the  requirement  for  newspaper  publication,  providing  for  a  repealer  provision, severability clause, codification, and providing for an effective date.",
    "item_type": "Ordinance"
  },
  {
    "item_code": "E-3",
    "document_reference": "23-6786",
    "title": "An Ordinance of the City Commission amending Section 1-104 'Jurisdiction and Applicability'; Section 8-109 'Moving of Existing Improvements'; Section 8-106 'Certificates of Appropriateness';  Section  14-209.4;  Section  14-215.3 'Notice  and  Hearing Procedures'; Section 15-102 'Notice' of the City's Zoning Code  in order to provide a definition  of  publication  and  remove  the  requirement  for  newspaper  publication,  providing for a repealer provision, severability clause, codification, and  providing for an effective date.",
    "item_type": "Ordinance"
  },
  {
    "item_code": "E-4",
    "document_reference": "23-6567",
    "title": "An  Ordinance  of  the  City  Commission  amending  St. Philip's School  site  plan  approved under Ordinance No. 3576 to replace an existing building with a new pre-K building located  at  1109 Andalusia  Avenue,  Coral  Gables,  Florida;  all  other  conditions  of  approval contained  in  Ordinance  No.  3576 shall  remain  in  effect;  and  providing  an  effective  date. (11 03 23 PZB recommended approval with conditions, Vote 7-0)",
    "item_type": "Ordinance"
  },
  {
    "item_code": "E-5",
    "document_reference": "23-6826",
    "title": "An  Ordinance  of  the  City  Commission  providing  for  text  amendments  to  the  City  of  Coral Gables  Official  Zoning  Code  (Zoning  Code),  amending  Article  10,  'Parking  and  Access,' considering reduction  of    parking  requirements  for  affordable  housing  located  near  a  major  transit  stop 2023-17,  Laws  of  Florida;  providing  for  repealer Section 10-112 'Miscellaneous Parking Standards,' creating provisions for as  required  by  the  Live  Local  Act,  Ch. provision, severability clause, codification, and providing for an effective date.",
    "item_type": "Ordinance"
  },
  {
    "item_code": "E-6",
    "document_reference": "23-6827",
    "title": "An  Ordinance  of  the  City  Commission  providing  for  text  amendments  to  the  City  of  Coral Gables Official Zoning Code, amending  Article 1, 'General Provisions,' Section 1-104 'Jurisdiction and Applicability,' amending provisions for the citing of city facilities to include  facilities  for  workforce  housing  that  are  owned,  financed,  or  operated  by  the  City, the  County,  or  other  public  (governmental)  entity  as  required  by  the  Code  of  Miami-Dade County Section 33-193.7 'Applicability in the Incorporated and Unincorporated Areas; Minimum Standards; Exemptions.,' providing for repealer provision, severability clause, codification, and providing for an effective date.",
    "item_type": "Ordinance"
  },
  {
    "item_code": "E-7",
    "document_reference": "23-6828",
    "title": "An  Ordinance  of  the  City  Commission  providing  for  a  text  amendment  to  the  City  of  Coral Gables  Official  Zoning  Code,  Creating  Section  5-314 'Window  and  Hurricane  Shutters'  to Code, Chapter  1-General  Provisions,  Section  1-7 entitled  'Penalties'  to  provide  a  penalty  for  the closure  of  window  and  hurricane  shutters  outside  of  the  hurricane  season;  providing  for severability clause, repealer provision, codification, and providing for an effective date. (Sponsored by Commissioner Fernandez)",
    "item_type": "Ordinance"
  },
  {
    "item_code": "E-8",
    "document_reference": "23-6881",
    "title": "An  Ordinance  of  the  City  Commission  providing  for  a  text  amendment  to  the  City  of  Coral Gables  official Zoning  Code,  amending  Article 11. 'Signs,' Section  11-107 'Real  Estate, For  Sale,  Lease  or  Rental  of  Property  or  Buildings,'  to  apply  same regulations  to  signs pertaining  to  the  sale,  lease,  or  rental  of  property  or  buildings  in  any  use  district;  providing for severability clause, repealer provision, codification, and providing for an effective date. (Sponsored by Commissioner Fernandez)",
    "item_type": "Ordinance"
  },
  {
    "item_code": "E-9",
    "document_reference": "23-6825",
    "title": "A  Resolution  of  the  City  Commission  pursuant  to  Florida  Statute 166.0451,  approving  a blank  inventory  list  of  city-owned  real  property  within  Coral  Gables  which  is  appropriate for use as affordable housing.",
    "item_type": "Other"
  },
  {
    "item_code": "F-1",
    "document_reference": "23-6762",
    "title": "Update on Uber Pilot Program.",
    "item_type": "Resolution"
  },
  {
    "item_code": "F-2",
    "document_reference": "23-6779",
    "title": "Update regarding Pittman Park",
    "item_type": "Resolution"
  },
  {
    "item_code": "F-3",
    "document_reference": "23-6783",
    "title": "Update on Development Services Department.",
    "item_type": "Resolution"
  },
  {
    "item_code": "F-4",
    "document_reference": "23-6862",
    "title": "Discussion regarding Form 6 financial disclosure by municipal elected officials.",
    "item_type": "Resolution"
  },
  {
    "item_code": "F-5",
    "document_reference": "23-6868",
    "title": "Discussion regarding the Christmas decorations on Miracle Mile.",
    "item_type": "Resolution"
  },
  {
    "item_code": "F-6",
    "document_reference": "23-6869",
    "title": "Discussion regarding the tree canopy replacement.",
    "item_type": "Resolution"
  },
  {
    "item_code": "F-7",
    "document_reference": "23-6875",
    "title": "Update of arrival of the White Way Lights.",
    "item_type": "Resolution"
  },
  {
    "item_code": "F-8",
    "document_reference": "23-6874",
    "title": "Update on the maintenance plan for the White Way Lights.",
    "item_type": "Resolution"
  },
  {
    "item_code": "F-9",
    "document_reference": "23-6878",
    "title": "Update on the naming of the park in honor of SSGT Carl Enis.",
    "item_type": "Resolution"
  },
  {
    "item_code": "F-10",
    "document_reference": "23-5686",
    "title": "A Resolution of the City Commission directing the City Clerk and City Manager to schedule bimonthly presentation and protocol ceremonies.",
    "item_type": "Resolution"
  },
  {
    "item_code": "F-11",
    "document_reference": "23-6832",
    "title": "A  Resolution  of  the  City  Commission  directing  the  City  Clerk  to  schedule  a  joint  meeting between  the  City's  Historic  Preservation  Board  and  Landmarks  Advisory  Board  to  discuss the replacement of certain Florida power and Light streetlights throughout the City.",
    "item_type": "Resolution"
  },
  {
    "item_code": "H-1",
    "document_reference": "23-6819",
    "title": "A Resolution of the City Commission accepting the recommendation of the Chief Procurement Officer to authorize access to a 'piggyback' contract through the Sourcewell  Cooperative  with Oshkosh  Corporation (Pierce Manufacturing) in an amount not  to  exceed  the  available  budget  for  firefighting  equipment,  pursuant  to  Section 2-946, Use  of  Other  Governmental  Unit  Contracts (Piggyback)  and  Section  2-947,  Cooperative Purchasing under the Procurement Code.",
    "item_type": "Other"
  },
  {
    "item_code": "H-2",
    "document_reference": "23-6824",
    "title": "A  Resolution  of  the  City  Commission  accepting  the  recommendation  of  the  Innovations AT&T Procurement/Bid and Technology Department to waive the competitive process to purchase Communication Service Maintenance and Support as a 'Special Waiver,' pursuant to Section 2-691of the Procurement Code.",
    "item_type": "Other"
  },
  {
    "item_code": "H-3",
    "document_reference": "23-6765",
    "title": "Illegal dumping mitigation technology update.",
    "item_type": "Other"
  }
]


================================================================================


################################################################################
# File: scripts/graph_stages/pdf_extractor.py
################################################################################

# scripts/graph_stages/pdf_extractor.py

import os
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import logging
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.datamodel.base_models import InputFormat, DocumentStream
from docling.datamodel.pipeline_options import PdfPipelineOptions
import json
from io import BytesIO

# Set up logging
logging.basicConfig(level=logging.INFO)
log = logging.getLogger(__name__)

class PDFExtractor:
    """Extract text from PDFs using Docling for accurate OCR and text extraction."""
    
    def __init__(self, pdf_dir: Path, output_dir: Optional[Path] = None):
        """Initialize the PDF extractor with Docling."""
        self.pdf_dir = Path(pdf_dir)
        self.output_dir = output_dir or Path("city_clerk_documents/extracted_text")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Create debug directory
        self.debug_dir = self.output_dir / "debug"
        self.debug_dir.mkdir(exist_ok=True)
        
        # Initialize Docling converter with OCR enabled
        # Configure for better OCR and structure detection
        pipeline_options = PdfPipelineOptions()
        pipeline_options.do_ocr = True  # Enable OCR for better text extraction
        pipeline_options.do_table_structure = True  # Better table extraction
        
        self.converter = DocumentConverter(
            format_options={
                InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
            }
        )
    
    def extract_text_from_pdf(self, pdf_path: Path) -> Tuple[str, List[Dict[str, str]]]:
        """
        Extract text from PDF using Docling with OCR support.
        
        Returns:
            Tuple of (full_text, pages) where pages is a list of dicts with 'text' and 'page_num'
        """
        log.info(f"📄 Extracting text from: {pdf_path.name}")
        
        # Convert PDF with Docling - just pass the path directly
        result = self.converter.convert(str(pdf_path))
        
        # Get the document
        doc = result.document
        
        # Get the markdown representation which preserves structure better
        full_markdown = doc.export_to_markdown()
        
        # Extract pages if available
        pages = []
        
        # Try to extract page-level content from the document structure
        if hasattr(doc, 'pages') and doc.pages:
            for page_num, page in enumerate(doc.pages, 1):
                # Get page text
                page_text = ""
                if hasattr(page, 'text'):
                    page_text = page.text
                elif hasattr(page, 'get_text'):
                    page_text = page.get_text()
                else:
                    # Try to extract from elements
                    page_elements = []
                    if hasattr(page, 'elements'):
                        for element in page.elements:
                            if hasattr(element, 'text'):
                                page_elements.append(element.text)
                    page_text = "\n".join(page_elements)
                
                if page_text:
                    pages.append({
                        'text': page_text,
                        'page_num': page_num
                    })
        
        # If we couldn't extract pages, create one page with all content
        if not pages and full_markdown:
            pages = [{
                'text': full_markdown,
                'page_num': 1
            }]
        
        # Use markdown as the full text (better structure preservation)
        full_text = full_markdown if full_markdown else ""
        
        # Save debug information
        debug_info = {
            'file': pdf_path.name,
            'total_pages': len(pages),
            'total_characters': len(full_text),
            'extraction_method': 'docling',
            'has_markdown': bool(full_markdown),
            'doc_attributes': list(dir(doc)) if doc else []
        }
        
        debug_file = self.debug_dir / f"{pdf_path.stem}_extraction_debug.json"
        with open(debug_file, 'w', encoding='utf-8') as f:
            json.dump(debug_info, f, indent=2)
        
        # Save the full extracted text for inspection
        text_file = self.debug_dir / f"{pdf_path.stem}_full_text.txt"
        with open(text_file, 'w', encoding='utf-8') as f:
            f.write(full_text)
        
        # Save markdown version separately for debugging
        if full_markdown:
            markdown_file = self.debug_dir / f"{pdf_path.stem}_markdown.md"
            with open(markdown_file, 'w', encoding='utf-8') as f:
                f.write(full_markdown)
        
        log.info(f"✅ Extracted {len(pages)} pages, {len(full_text)} characters")
        
        return full_text, pages
    
    def extract_all_pdfs(self) -> Dict[str, Dict[str, any]]:
        """Extract text from all PDFs in the directory."""
        pdf_files = list(self.pdf_dir.glob("*.pdf"))
        log.info(f"📚 Found {len(pdf_files)} PDF files to process")
        
        extracted_data = {}
        
        for pdf_path in pdf_files:
            try:
                full_text, pages = self.extract_text_from_pdf(pdf_path)
                
                extracted_data[pdf_path.stem] = {
                    'full_text': full_text,
                    'pages': pages,
                    'metadata': {
                        'filename': pdf_path.name,
                        'num_pages': len(pages),
                        'total_chars': len(full_text),
                        'extraction_method': 'docling'
                    }
                }
                
                # Save extracted text
                output_file = self.output_dir / f"{pdf_path.stem}_extracted.json"
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(extracted_data[pdf_path.stem], f, indent=2, ensure_ascii=False)
                
                log.info(f"✅ Saved extracted text to: {output_file}")
                
            except Exception as e:
                log.error(f"❌ Failed to process {pdf_path.name}: {e}")
                import traceback
                traceback.print_exc()
                # No fallback - if Docling fails, we fail
                raise
        
        return extracted_data

# Add this to requirements.txt:
# unstructured[pdf]==0.10.30
# pytesseract>=0.3.10
# pdf2image>=1.16.3
# poppler-utils (system dependency - install with: brew install poppler)


================================================================================


################################################################################
# File: scripts/stages/acceleration_utils.py
################################################################################

# File: scripts/stages/acceleration_utils.py

"""
Hardware acceleration utilities for the pipeline.
Provides Apple Silicon optimization when available, with CPU fallback.
"""
import os
import platform
import multiprocessing as mp
from typing import Optional, Callable, Any, List
import logging
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import asyncio
from functools import partial

log = logging.getLogger(__name__)

class HardwareAccelerator:
    """Detects and manages hardware acceleration capabilities."""
    
    def __init__(self):
        self.is_apple_silicon = self._detect_apple_silicon()
        self.cpu_count = mp.cpu_count()
        self.optimal_workers = self._calculate_optimal_workers()
        
        # Set environment for better performance on macOS
        if self.is_apple_silicon:
            os.environ['OPENBLAS_NUM_THREADS'] = '1'
            os.environ['MKL_NUM_THREADS'] = '1'
            os.environ['OMP_NUM_THREADS'] = '1'
        
        log.info(f"Hardware: {'Apple Silicon' if self.is_apple_silicon else platform.processor()}")
        log.info(f"CPU cores: {self.cpu_count}, Optimal workers: {self.optimal_workers}")
    
    def _detect_apple_silicon(self) -> bool:
        """Detect if running on Apple Silicon."""
        if platform.system() != 'Darwin':
            return False
        try:
            import subprocess
            result = subprocess.run(['sysctl', '-n', 'hw.optional.arm64'], 
                                  capture_output=True, text=True)
            return result.stdout.strip() == '1'
        except:
            return False
    
    def _calculate_optimal_workers(self) -> int:
        """Calculate optimal number of workers based on hardware."""
        if self.is_apple_silicon:
            # Apple Silicon has efficiency and performance cores
            # Use 75% of cores to leave room for system
            return max(1, int(self.cpu_count * 0.75))
        else:
            # Traditional CPU - use all but one core
            return max(1, self.cpu_count - 1)
    
    def get_process_pool(self, max_workers: Optional[int] = None) -> ProcessPoolExecutor:
        """Get optimized process pool executor."""
        workers = max_workers or self.optimal_workers
        return ProcessPoolExecutor(
            max_workers=workers,
            mp_context=mp.get_context('spawn')  # Required for macOS
        )
    
    def get_thread_pool(self, max_workers: Optional[int] = None) -> ThreadPoolExecutor:
        """Get optimized thread pool executor for I/O operations."""
        workers = max_workers or min(32, self.cpu_count * 4)
        return ThreadPoolExecutor(max_workers=workers)

# Global instance
hardware = HardwareAccelerator()

async def run_cpu_bound_concurrent(func: Callable, items: List[Any], 
                                 max_workers: Optional[int] = None,
                                 desc: str = "Processing") -> List[Any]:
    """Run CPU-bound function concurrently on multiple items."""
    loop = asyncio.get_event_loop()
    with hardware.get_process_pool(max_workers) as executor:
        futures = [loop.run_in_executor(executor, func, item) for item in items]
        
        from tqdm.asyncio import tqdm_asyncio
        results = await tqdm_asyncio.gather(*futures, desc=desc)
        return results

async def run_io_bound_concurrent(func: Callable, items: List[Any],
                                max_workers: Optional[int] = None,
                                desc: str = "Processing") -> List[Any]:
    """Run I/O-bound function concurrently on multiple items."""
    loop = asyncio.get_event_loop()
    with hardware.get_thread_pool(max_workers) as executor:
        futures = [loop.run_in_executor(executor, func, item) for item in items]
        
        from tqdm.asyncio import tqdm_asyncio
        results = await tqdm_asyncio.gather(*futures, desc=desc)
        return results


================================================================================


################################################################################
# File: graph_clear_database.py
################################################################################

# File: graph_clear_database.py

#!/usr/bin/env python3
"""
Clear Cosmos DB Graph Database
This script will clear all vertices and edges from the graph database.
"""

import asyncio
import sys
import os
from pathlib import Path

# Add scripts directory to path
script_dir = Path(__file__).parent / 'scripts'
sys.path.append(str(script_dir))

from graph_stages.cosmos_db_client import CosmosGraphClient

async def clear_database():
    """Clear the entire graph database."""
    print('🗑️  Clearing Cosmos DB graph database...')
    print('⚠️  This will delete ALL vertices and edges!')
    
    try:
        async with CosmosGraphClient() as client:
            await client.clear_graph()
        print('✅ Graph database cleared successfully!')
        return True
        
    except Exception as e:
        print(f'❌ Error clearing database: {e}')
        return False

if __name__ == "__main__":
    success = asyncio.run(clear_database())
    if not success:
        sys.exit(1)


================================================================================


################################################################################
# File: debug/meeting_info_parsed.json
################################################################################

{
  "type": "Regular Meeting",
  "time": "9:00 AM",
  "location": "City Hall, Commission Chambers",
  "commissioners": [
    "Mayor Vince Lago",
    "Vice Mayor Rhonda Anderson",
    "Commissioner Melissa Castro",
    "Commissioner Ariel Fernandez",
    "Commissioner Kirk R. Menendez"
  ],
  "officials": {
    "mayor": "Vince Lago",
    "city_manager": "Peter J. Iglesias, P.E.",
    "city_attorney": "Cristina M. Su\u00e1rez",
    "city_clerk": "Billy Y. Urquia"
  }
}


================================================================================

